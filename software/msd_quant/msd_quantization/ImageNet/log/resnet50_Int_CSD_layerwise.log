/home/jjc/miniconda3/envs/ant_quant/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
Namespace(a_low=75, a_up=150, abit=8, batch_size=256, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='sel-2-3-4-5', epoch=5, layer_8bit_l=None, layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='resnet50', percent=100, ptq=False, resume=False, search=False, tag='', train=True, w_low=75, w_up=150, wbit=8)
01/04/2023 18:11:11 - INFO - __main__ -   output/resnet50_imagenet/int_W8A8_61512/gpu_0
01/04/2023 18:11:11 - INFO - __main__ -   Namespace(a_low=75, a_up=150, abit=8, batch_size=256, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='sel-2-3-4-5', epoch=5, layer_8bit_l=None, layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='resnet50', percent=100, ptq=False, resume=False, search=False, tag='', train=True, w_low=75, w_up=150, wbit=8)
01/04/2023 18:11:11 - INFO - __main__ -   ==> Preparing data..
01/04/2023 18:11:13 - INFO - __main__ -   ==> Setting quantizer..
Namespace(a_low=75, a_up=150, abit=8, batch_size=256, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='sel-2-3-4-5', epoch=5, layer_8bit_l=None, layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='resnet50', percent=100, ptq=False, resume=False, search=False, tag='', train=True, w_low=75, w_up=150, wbit=8)
01/04/2023 18:11:13 - INFO - __main__ -   Namespace(a_low=75, a_up=150, abit=8, batch_size=256, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='sel-2-3-4-5', epoch=5, layer_8bit_l=None, layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='resnet50', percent=100, ptq=False, resume=False, search=False, tag='', train=True, w_low=75, w_up=150, wbit=8)
01/04/2023 18:11:13 - INFO - __main__ -   ==> Building model..
ResNet(
  (conv1): Conv2dQuantizer(
    (quant_weight): TensorQuantizer()
    (quant_input): TensorQuantizer()
  )
  (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (1): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (1): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): LinearQuantizer(
    (quant_weight): TensorQuantizer()
    (quant_input): TensorQuantizer()
  )
)
01/04/2023 18:11:14 - INFO - __main__ -   {'milestones': Counter({2: 1, 3: 1, 4: 1}), 'gamma': 0.1, 'base_lrs': [0.0005], 'last_epoch': 0, '_step_count': 1, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.0005]}
01/04/2023 18:11:14 - INFO - __main__ -   
Epoch: 0
csd_eb2 search, INT   core: 0.000888
csd_eb3 search, INT   core: 0.000084
lsb eb2 search, INT   core: 0.003420
lsb eb3 search, INT   core: 0.002807
Layer quant EB csd_eb3
int	8-bit 	 conv1.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.001650
csd_eb3 search, INT   core: 0.000090
lsb eb2 search, INT   core: 0.005046
lsb eb3 search, INT   core: 0.001438
Layer quant EB abit-1
int	8-bit 	 conv1.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000181
csd_eb3 search, INT   core: 0.000033
lsb eb2 search, INT   core: 0.000835
lsb eb3 search, INT   core: 0.001374
Layer quant EB csd_eb3
int	8-bit 	 layer1.0.conv1.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000459
csd_eb3 search, INT   core: 0.000196
lsb eb2 search, INT   core: 0.001451
lsb eb3 search, INT   core: 0.001372
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv1.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000061
csd_eb3 search, INT   core: 0.000012
lsb eb2 search, INT   core: 0.000269
lsb eb3 search, INT   core: 0.000418
Layer quant EB csd_eb3
int	8-bit 	 layer1.0.conv2.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000047
csd_eb3 search, INT   core: 0.000012
lsb eb2 search, INT   core: 0.000142
lsb eb3 search, INT   core: 0.000092
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv2.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000209
csd_eb3 search, INT   core: 0.000024
lsb eb2 search, INT   core: 0.000859
lsb eb3 search, INT   core: 0.000806
Layer quant EB csd_eb3
int	8-bit 	 layer1.0.conv3.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000083
csd_eb3 search, INT   core: 0.000015
lsb eb2 search, INT   core: 0.000266
lsb eb3 search, INT   core: 0.000117
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv3.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000511
csd_eb3 search, INT   core: 0.000085
lsb eb2 search, INT   core: 0.002342
lsb eb3 search, INT   core: 0.003483
Layer quant EB csd_eb3
int	8-bit 	 layer1.0.downsample.0.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000459
csd_eb3 search, INT   core: 0.000196
lsb eb2 search, INT   core: 0.001451
lsb eb3 search, INT   core: 0.001372
Layer quant EB abit-1
int	8-bit 	 layer1.0.downsample.0.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000061
csd_eb3 search, INT   core: 0.000007
lsb eb2 search, INT   core: 0.000238
lsb eb3 search, INT   core: 0.000188
Layer quant EB csd_eb3
int	8-bit 	 layer1.1.conv1.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000074
csd_eb3 search, INT   core: 0.000036
lsb eb2 search, INT   core: 0.000246
lsb eb3 search, INT   core: 0.000286
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv1.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000062
csd_eb3 search, INT   core: 0.000010
lsb eb2 search, INT   core: 0.000262
lsb eb3 search, INT   core: 0.000324
Layer quant EB csd_eb3
int	8-bit 	 layer1.1.conv2.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000032
csd_eb3 search, INT   core: 0.000010
lsb eb2 search, INT   core: 0.000114
lsb eb3 search, INT   core: 0.000080
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv2.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000193
csd_eb3 search, INT   core: 0.000020
lsb eb2 search, INT   core: 0.000759
lsb eb3 search, INT   core: 0.000666
Layer quant EB csd_eb3
int	8-bit 	 layer1.1.conv3.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000042
csd_eb3 search, INT   core: 0.000016
lsb eb2 search, INT   core: 0.000143
lsb eb3 search, INT   core: 0.000135
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv3.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000058
csd_eb3 search, INT   core: 0.000005
lsb eb2 search, INT   core: 0.000224
lsb eb3 search, INT   core: 0.000147
Layer quant EB csd_eb3
int	8-bit 	 layer1.2.conv1.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000104
csd_eb3 search, INT   core: 0.000061
lsb eb2 search, INT   core: 0.000346
lsb eb3 search, INT   core: 0.000509
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv1.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000075
csd_eb3 search, INT   core: 0.000008
lsb eb2 search, INT   core: 0.000287
lsb eb3 search, INT   core: 0.000228
Layer quant EB csd_eb3
int	8-bit 	 layer1.2.conv2.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000027
csd_eb3 search, INT   core: 0.000007
lsb eb2 search, INT   core: 0.000088
lsb eb3 search, INT   core: 0.000059
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv2.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000188
csd_eb3 search, INT   core: 0.000016
lsb eb2 search, INT   core: 0.000681
lsb eb3 search, INT   core: 0.000501
Layer quant EB csd_eb3
int	8-bit 	 layer1.2.conv3.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000039
csd_eb3 search, INT   core: 0.000022
lsb eb2 search, INT   core: 0.000128
lsb eb3 search, INT   core: 0.000218
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv3.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000162
csd_eb3 search, INT   core: 0.000021
lsb eb2 search, INT   core: 0.000676
lsb eb3 search, INT   core: 0.000674
Layer quant EB csd_eb3
int	8-bit 	 layer2.0.conv1.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000113
csd_eb3 search, INT   core: 0.000059
lsb eb2 search, INT   core: 0.000369
lsb eb3 search, INT   core: 0.000441
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv1.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000079
csd_eb3 search, INT   core: 0.000009
lsb eb2 search, INT   core: 0.000308
lsb eb3 search, INT   core: 0.000257
Layer quant EB csd_eb3
int	8-bit 	 layer2.0.conv2.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000037
csd_eb3 search, INT   core: 0.000026
lsb eb2 search, INT   core: 0.000122
lsb eb3 search, INT   core: 0.000260
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv2.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000358
csd_eb3 search, INT   core: 0.000041
lsb eb2 search, INT   core: 0.001410
lsb eb3 search, INT   core: 0.001278
Layer quant EB csd_eb3
int	8-bit 	 layer2.0.conv3.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000044
csd_eb3 search, INT   core: 0.000014
lsb eb2 search, INT   core: 0.000146
lsb eb3 search, INT   core: 0.000119
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv3.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000267
csd_eb3 search, INT   core: 0.000052
lsb eb2 search, INT   core: 0.001282
lsb eb3 search, INT   core: 0.002028
Layer quant EB csd_eb3
int	8-bit 	 layer2.0.downsample.0.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000113
csd_eb3 search, INT   core: 0.000059
lsb eb2 search, INT   core: 0.000369
lsb eb3 search, INT   core: 0.000441
Layer quant EB abit-1
int	8-bit 	 layer2.0.downsample.0.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000040
csd_eb3 search, INT   core: 0.000007
lsb eb2 search, INT   core: 0.000174
lsb eb3 search, INT   core: 0.000241
Layer quant EB csd_eb3
int	8-bit 	 layer2.1.conv1.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000067
csd_eb3 search, INT   core: 0.000029
lsb eb2 search, INT   core: 0.000223
lsb eb3 search, INT   core: 0.000258
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv1.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000061
csd_eb3 search, INT   core: 0.000013
lsb eb2 search, INT   core: 0.000294
lsb eb3 search, INT   core: 0.000504
Layer quant EB csd_eb3
int	8-bit 	 layer2.1.conv2.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000027
csd_eb3 search, INT   core: 0.000004
lsb eb2 search, INT   core: 0.000086
lsb eb3 search, INT   core: 0.000031
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv2.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000221
csd_eb3 search, INT   core: 0.000025
lsb eb2 search, INT   core: 0.000862
lsb eb3 search, INT   core: 0.000840
Layer quant EB csd_eb3
int	8-bit 	 layer2.1.conv3.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000026
csd_eb3 search, INT   core: 0.000010
lsb eb2 search, INT   core: 0.000086
lsb eb3 search, INT   core: 0.000079
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv3.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000079
csd_eb3 search, INT   core: 0.000013
lsb eb2 search, INT   core: 0.000338
lsb eb3 search, INT   core: 0.000418
Layer quant EB csd_eb3
int	8-bit 	 layer2.2.conv1.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000076
csd_eb3 search, INT   core: 0.000042
lsb eb2 search, INT   core: 0.000251
lsb eb3 search, INT   core: 0.000365
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv1.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000075
csd_eb3 search, INT   core: 0.000013
lsb eb2 search, INT   core: 0.000329
lsb eb3 search, INT   core: 0.000441
Layer quant EB csd_eb3
int	8-bit 	 layer2.2.conv2.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000026
csd_eb3 search, INT   core: 0.000006
lsb eb2 search, INT   core: 0.000086
lsb eb3 search, INT   core: 0.000053
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv2.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000311
csd_eb3 search, INT   core: 0.000033
lsb eb2 search, INT   core: 0.001220
lsb eb3 search, INT   core: 0.001013
Layer quant EB csd_eb3
int	8-bit 	 layer2.2.conv3.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000027
csd_eb3 search, INT   core: 0.000008
lsb eb2 search, INT   core: 0.000090
lsb eb3 search, INT   core: 0.000062
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv3.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000087
csd_eb3 search, INT   core: 0.000011
lsb eb2 search, INT   core: 0.000353
lsb eb3 search, INT   core: 0.000326
Layer quant EB csd_eb3
int	8-bit 	 layer2.3.conv1.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000083
csd_eb3 search, INT   core: 0.000051
lsb eb2 search, INT   core: 0.000277
lsb eb3 search, INT   core: 0.000497
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv1.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000081
csd_eb3 search, INT   core: 0.000009
lsb eb2 search, INT   core: 0.000311
lsb eb3 search, INT   core: 0.000247
Layer quant EB csd_eb3
int	8-bit 	 layer2.3.conv2.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000022
csd_eb3 search, INT   core: 0.000009
lsb eb2 search, INT   core: 0.000073
lsb eb3 search, INT   core: 0.000073
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv2.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000273
csd_eb3 search, INT   core: 0.000025
lsb eb2 search, INT   core: 0.001028
lsb eb3 search, INT   core: 0.000715
Layer quant EB csd_eb3
int	8-bit 	 layer2.3.conv3.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000027
csd_eb3 search, INT   core: 0.000013
lsb eb2 search, INT   core: 0.000090
lsb eb3 search, INT   core: 0.000117
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv3.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000278
csd_eb3 search, INT   core: 0.000042
lsb eb2 search, INT   core: 0.001171
lsb eb3 search, INT   core: 0.001379
Layer quant EB csd_eb3
int	8-bit 	 layer3.0.conv1.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000086
csd_eb3 search, INT   core: 0.000054
lsb eb2 search, INT   core: 0.000285
lsb eb3 search, INT   core: 0.000528
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv1.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000102
csd_eb3 search, INT   core: 0.000015
lsb eb2 search, INT   core: 0.000415
lsb eb3 search, INT   core: 0.000466
Layer quant EB csd_eb3
int	8-bit 	 layer3.0.conv2.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000032
csd_eb3 search, INT   core: 0.000021
lsb eb2 search, INT   core: 0.000107
lsb eb3 search, INT   core: 0.000216
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv2.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000578
csd_eb3 search, INT   core: 0.000063
lsb eb2 search, INT   core: 0.002282
lsb eb3 search, INT   core: 0.001873
Layer quant EB csd_eb3
int	8-bit 	 layer3.0.conv3.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000039
csd_eb3 search, INT   core: 0.000012
lsb eb2 search, INT   core: 0.000129
lsb eb3 search, INT   core: 0.000097
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv3.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000307
csd_eb3 search, INT   core: 0.000048
lsb eb2 search, INT   core: 0.001315
lsb eb3 search, INT   core: 0.001602
Layer quant EB csd_eb3
int	8-bit 	 layer3.0.downsample.0.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000086
csd_eb3 search, INT   core: 0.000054
lsb eb2 search, INT   core: 0.000285
lsb eb3 search, INT   core: 0.000528
Layer quant EB abit-1
int	8-bit 	 layer3.0.downsample.0.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000076
csd_eb3 search, INT   core: 0.000015
lsb eb2 search, INT   core: 0.000351
lsb eb3 search, INT   core: 0.000569
Layer quant EB csd_eb3
int	8-bit 	 layer3.1.conv1.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000045
csd_eb3 search, INT   core: 0.000023
lsb eb2 search, INT   core: 0.000151
lsb eb3 search, INT   core: 0.000216
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv1.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000078
csd_eb3 search, INT   core: 0.000014
lsb eb2 search, INT   core: 0.000341
lsb eb3 search, INT   core: 0.000476
Layer quant EB csd_eb3
int	8-bit 	 layer3.1.conv2.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000029
csd_eb3 search, INT   core: 0.000019
lsb eb2 search, INT   core: 0.000096
lsb eb3 search, INT   core: 0.000187
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv2.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000438
csd_eb3 search, INT   core: 0.000054
lsb eb2 search, INT   core: 0.001773
lsb eb3 search, INT   core: 0.001729
Layer quant EB csd_eb3
int	8-bit 	 layer3.1.conv3.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000020
csd_eb3 search, INT   core: 0.000008
lsb eb2 search, INT   core: 0.000067
lsb eb3 search, INT   core: 0.000069
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv3.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000081
csd_eb3 search, INT   core: 0.000017
lsb eb2 search, INT   core: 0.000376
lsb eb3 search, INT   core: 0.000601
Layer quant EB csd_eb3
int	8-bit 	 layer3.2.conv1.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000068
csd_eb3 search, INT   core: 0.000051
lsb eb2 search, INT   core: 0.000224
lsb eb3 search, INT   core: 0.000566
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv1.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000076
csd_eb3 search, INT   core: 0.000011
lsb eb2 search, INT   core: 0.000313
lsb eb3 search, INT   core: 0.000338
Layer quant EB csd_eb3
int	8-bit 	 layer3.2.conv2.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000016
csd_eb3 search, INT   core: 0.000006
lsb eb2 search, INT   core: 0.000052
lsb eb3 search, INT   core: 0.000054
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv2.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000388
csd_eb3 search, INT   core: 0.000040
lsb eb2 search, INT   core: 0.001515
lsb eb3 search, INT   core: 0.001190
Layer quant EB csd_eb3
int	8-bit 	 layer3.2.conv3.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000015
csd_eb3 search, INT   core: 0.000003
lsb eb2 search, INT   core: 0.000047
lsb eb3 search, INT   core: 0.000026
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv3.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000097
csd_eb3 search, INT   core: 0.000018
lsb eb2 search, INT   core: 0.000434
lsb eb3 search, INT   core: 0.000585
Layer quant EB csd_eb3
int	8-bit 	 layer3.3.conv1.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000058
csd_eb3 search, INT   core: 0.000040
lsb eb2 search, INT   core: 0.000191
lsb eb3 search, INT   core: 0.000436
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv1.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000076
csd_eb3 search, INT   core: 0.000010
lsb eb2 search, INT   core: 0.000305
lsb eb3 search, INT   core: 0.000310
Layer quant EB csd_eb3
int	8-bit 	 layer3.3.conv2.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000017
csd_eb3 search, INT   core: 0.000011
lsb eb2 search, INT   core: 0.000057
lsb eb3 search, INT   core: 0.000106
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv2.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000360
csd_eb3 search, INT   core: 0.000035
lsb eb2 search, INT   core: 0.001355
lsb eb3 search, INT   core: 0.000977
Layer quant EB csd_eb3
int	8-bit 	 layer3.3.conv3.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000016
csd_eb3 search, INT   core: 0.000008
lsb eb2 search, INT   core: 0.000052
lsb eb3 search, INT   core: 0.000070
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv3.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000106
csd_eb3 search, INT   core: 0.000018
lsb eb2 search, INT   core: 0.000460
lsb eb3 search, INT   core: 0.000576
Layer quant EB csd_eb3
int	8-bit 	 layer3.4.conv1.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000064
csd_eb3 search, INT   core: 0.000049
lsb eb2 search, INT   core: 0.000211
lsb eb3 search, INT   core: 0.000540
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv1.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000075
csd_eb3 search, INT   core: 0.000010
lsb eb2 search, INT   core: 0.000301
lsb eb3 search, INT   core: 0.000299
Layer quant EB csd_eb3
int	8-bit 	 layer3.4.conv2.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000015
csd_eb3 search, INT   core: 0.000008
lsb eb2 search, INT   core: 0.000049
lsb eb3 search, INT   core: 0.000080
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv2.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000356
csd_eb3 search, INT   core: 0.000036
lsb eb2 search, INT   core: 0.001382
lsb eb3 search, INT   core: 0.001035
Layer quant EB csd_eb3
int	8-bit 	 layer3.4.conv3.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000013
csd_eb3 search, INT   core: 0.000005
lsb eb2 search, INT   core: 0.000042
lsb eb3 search, INT   core: 0.000042
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv3.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000125
csd_eb3 search, INT   core: 0.000019
lsb eb2 search, INT   core: 0.000523
lsb eb3 search, INT   core: 0.000592
Layer quant EB csd_eb3
int	8-bit 	 layer3.5.conv1.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000054
csd_eb3 search, INT   core: 0.000041
lsb eb2 search, INT   core: 0.000178
lsb eb3 search, INT   core: 0.000445
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv1.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000079
csd_eb3 search, INT   core: 0.000011
lsb eb2 search, INT   core: 0.000322
lsb eb3 search, INT   core: 0.000330
Layer quant EB csd_eb3
int	8-bit 	 layer3.5.conv2.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000020
csd_eb3 search, INT   core: 0.000013
lsb eb2 search, INT   core: 0.000066
lsb eb3 search, INT   core: 0.000141
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv2.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000406
csd_eb3 search, INT   core: 0.000038
lsb eb2 search, INT   core: 0.001540
lsb eb3 search, INT   core: 0.001061
Layer quant EB csd_eb3
int	8-bit 	 layer3.5.conv3.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000023
csd_eb3 search, INT   core: 0.000016
lsb eb2 search, INT   core: 0.000076
lsb eb3 search, INT   core: 0.000164
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv3.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000362
csd_eb3 search, INT   core: 0.000043
lsb eb2 search, INT   core: 0.001434
lsb eb3 search, INT   core: 0.001235
Layer quant EB csd_eb3
int	8-bit 	 layer4.0.conv1.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000040
csd_eb3 search, INT   core: 0.000029
lsb eb2 search, INT   core: 0.000130
lsb eb3 search, INT   core: 0.000333
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv1.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000110
csd_eb3 search, INT   core: 0.000014
lsb eb2 search, INT   core: 0.000437
lsb eb3 search, INT   core: 0.000436
Layer quant EB csd_eb3
int	8-bit 	 layer4.0.conv2.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000014
csd_eb3 search, INT   core: 0.000008
lsb eb2 search, INT   core: 0.000047
lsb eb3 search, INT   core: 0.000079
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv2.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000561
csd_eb3 search, INT   core: 0.000069
lsb eb2 search, INT   core: 0.002246
lsb eb3 search, INT   core: 0.002052
Layer quant EB csd_eb3
int	8-bit 	 layer4.0.conv3.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000024
csd_eb3 search, INT   core: 0.000015
lsb eb2 search, INT   core: 0.000080
lsb eb3 search, INT   core: 0.000159
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv3.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000260
csd_eb3 search, INT   core: 0.000046
lsb eb2 search, INT   core: 0.001158
lsb eb3 search, INT   core: 0.001643
Layer quant EB csd_eb3
int	8-bit 	 layer4.0.downsample.0.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000040
csd_eb3 search, INT   core: 0.000029
lsb eb2 search, INT   core: 0.000130
lsb eb3 search, INT   core: 0.000333
Layer quant EB abit-1
int	8-bit 	 layer4.0.downsample.0.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000148
csd_eb3 search, INT   core: 0.000022
lsb eb2 search, INT   core: 0.000628
lsb eb3 search, INT   core: 0.000711
Layer quant EB csd_eb3
int	8-bit 	 layer4.1.conv1.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000346
csd_eb3 search, INT   core: 0.000268
lsb eb2 search, INT   core: 0.001125
lsb eb3 search, INT   core: 0.003150
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv1.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000104
csd_eb3 search, INT   core: 0.000011
lsb eb2 search, INT   core: 0.000392
lsb eb3 search, INT   core: 0.000296
Layer quant EB csd_eb3
int	8-bit 	 layer4.1.conv2.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000017
csd_eb3 search, INT   core: 0.000010
lsb eb2 search, INT   core: 0.000055
lsb eb3 search, INT   core: 0.000107
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv2.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000533
csd_eb3 search, INT   core: 0.000055
lsb eb2 search, INT   core: 0.002046
lsb eb3 search, INT   core: 0.001511
Layer quant EB csd_eb3
int	8-bit 	 layer4.1.conv3.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000018
csd_eb3 search, INT   core: 0.000012
lsb eb2 search, INT   core: 0.000059
lsb eb3 search, INT   core: 0.000128
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv3.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000220
csd_eb3 search, INT   core: 0.000027
lsb eb2 search, INT   core: 0.000867
lsb eb3 search, INT   core: 0.000784
Layer quant EB csd_eb3
int	8-bit 	 layer4.2.conv1.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000685
csd_eb3 search, INT   core: 0.000523
lsb eb2 search, INT   core: 0.002244
lsb eb3 search, INT   core: 0.006273
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv1.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000083
csd_eb3 search, INT   core: 0.000008
lsb eb2 search, INT   core: 0.000306
lsb eb3 search, INT   core: 0.000195
Layer quant EB csd_eb3
int	8-bit 	 layer4.2.conv2.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000016
csd_eb3 search, INT   core: 0.000009
lsb eb2 search, INT   core: 0.000054
lsb eb3 search, INT   core: 0.000097
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv2.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000479
csd_eb3 search, INT   core: 0.000068
lsb eb2 search, INT   core: 0.002000
lsb eb3 search, INT   core: 0.002083
Layer quant EB csd_eb3
int	8-bit 	 layer4.2.conv3.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000022
csd_eb3 search, INT   core: 0.000014
lsb eb2 search, INT   core: 0.000072
lsb eb3 search, INT   core: 0.000146
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv3.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.001546
csd_eb3 search, INT   core: 0.000412
lsb eb2 search, INT   core: 0.007701
lsb eb3 search, INT   core: 0.015604
Layer quant EB csd_eb3
int	8-bit 	 fc.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000746
csd_eb3 search, INT   core: 0.000325
lsb eb2 search, INT   core: 0.002442
lsb eb3 search, INT   core: 0.003249
Layer quant EB abit-1
int	8-bit 	 fc.quant_input,
set init to 1
01/04/2023 18:15:38 - INFO - __main__ -   test: [epoch: 0 | batch: 0/5005 ] | Loss: 0.793 | Acc: 82.422% (211/256)
01/04/2023 18:16:41 - INFO - __main__ -   test: [epoch: 0 | batch: 100/5005 ] | Loss: 0.833 | Acc: 79.366% (20521/25856)
01/04/2023 18:17:44 - INFO - __main__ -   test: [epoch: 0 | batch: 200/5005 ] | Loss: 0.842 | Acc: 79.114% (40709/51456)
01/04/2023 18:18:47 - INFO - __main__ -   test: [epoch: 0 | batch: 300/5005 ] | Loss: 0.844 | Acc: 78.949% (60835/77056)
01/04/2023 18:19:49 - INFO - __main__ -   test: [epoch: 0 | batch: 400/5005 ] | Loss: 0.846 | Acc: 78.929% (81025/102656)
01/04/2023 18:20:52 - INFO - __main__ -   test: [epoch: 0 | batch: 500/5005 ] | Loss: 0.847 | Acc: 78.951% (101259/128256)
01/04/2023 18:21:55 - INFO - __main__ -   test: [epoch: 0 | batch: 600/5005 ] | Loss: 0.849 | Acc: 78.891% (121379/153856)
01/04/2023 18:22:57 - INFO - __main__ -   test: [epoch: 0 | batch: 700/5005 ] | Loss: 0.849 | Acc: 78.886% (141565/179456)
01/04/2023 18:24:00 - INFO - __main__ -   test: [epoch: 0 | batch: 800/5005 ] | Loss: 0.850 | Acc: 78.866% (161719/205056)
01/04/2023 18:25:02 - INFO - __main__ -   test: [epoch: 0 | batch: 900/5005 ] | Loss: 0.849 | Acc: 78.920% (182033/230656)
01/04/2023 18:26:05 - INFO - __main__ -   test: [epoch: 0 | batch: 1000/5005 ] | Loss: 0.848 | Acc: 78.918% (202233/256256)
01/04/2023 18:27:07 - INFO - __main__ -   test: [epoch: 0 | batch: 1100/5005 ] | Loss: 0.848 | Acc: 78.912% (222419/281856)
01/04/2023 18:28:10 - INFO - __main__ -   test: [epoch: 0 | batch: 1200/5005 ] | Loss: 0.847 | Acc: 78.963% (242778/307456)
01/04/2023 18:29:12 - INFO - __main__ -   test: [epoch: 0 | batch: 1300/5005 ] | Loss: 0.847 | Acc: 78.967% (263004/333056)
01/04/2023 18:30:13 - INFO - __main__ -   test: [epoch: 0 | batch: 1400/5005 ] | Loss: 0.847 | Acc: 78.955% (283178/358656)
01/04/2023 18:31:14 - INFO - __main__ -   test: [epoch: 0 | batch: 1500/5005 ] | Loss: 0.846 | Acc: 78.962% (303415/384256)
01/04/2023 18:32:17 - INFO - __main__ -   test: [epoch: 0 | batch: 1600/5005 ] | Loss: 0.847 | Acc: 78.929% (323497/409856)
01/04/2023 18:33:19 - INFO - __main__ -   test: [epoch: 0 | batch: 1700/5005 ] | Loss: 0.847 | Acc: 78.911% (343623/435456)
01/04/2023 18:34:21 - INFO - __main__ -   test: [epoch: 0 | batch: 1800/5005 ] | Loss: 0.847 | Acc: 78.912% (363830/461056)
01/04/2023 18:35:24 - INFO - __main__ -   test: [epoch: 0 | batch: 1900/5005 ] | Loss: 0.847 | Acc: 78.911% (384023/486656)
01/04/2023 18:36:27 - INFO - __main__ -   test: [epoch: 0 | batch: 2000/5005 ] | Loss: 0.847 | Acc: 78.924% (404295/512256)
01/04/2023 18:37:30 - INFO - __main__ -   test: [epoch: 0 | batch: 2100/5005 ] | Loss: 0.847 | Acc: 78.927% (424514/537856)
01/04/2023 18:38:32 - INFO - __main__ -   test: [epoch: 0 | batch: 2200/5005 ] | Loss: 0.846 | Acc: 78.940% (444790/563456)
01/04/2023 18:39:35 - INFO - __main__ -   test: [epoch: 0 | batch: 2300/5005 ] | Loss: 0.846 | Acc: 78.928% (464931/589056)
01/04/2023 18:40:37 - INFO - __main__ -   test: [epoch: 0 | batch: 2400/5005 ] | Loss: 0.846 | Acc: 78.930% (485148/614656)
01/04/2023 18:41:40 - INFO - __main__ -   test: [epoch: 0 | batch: 2500/5005 ] | Loss: 0.846 | Acc: 78.950% (505480/640256)
01/04/2023 18:42:42 - INFO - __main__ -   test: [epoch: 0 | batch: 2600/5005 ] | Loss: 0.846 | Acc: 78.949% (525686/665856)
01/04/2023 18:43:45 - INFO - __main__ -   test: [epoch: 0 | batch: 2700/5005 ] | Loss: 0.846 | Acc: 78.926% (545742/691456)
01/04/2023 18:44:47 - INFO - __main__ -   test: [epoch: 0 | batch: 2800/5005 ] | Loss: 0.846 | Acc: 78.922% (565914/717056)
01/04/2023 18:45:49 - INFO - __main__ -   test: [epoch: 0 | batch: 2900/5005 ] | Loss: 0.846 | Acc: 78.936% (586222/742656)
01/04/2023 18:46:52 - INFO - __main__ -   test: [epoch: 0 | batch: 3000/5005 ] | Loss: 0.846 | Acc: 78.937% (606438/768256)
01/04/2023 18:47:54 - INFO - __main__ -   test: [epoch: 0 | batch: 3100/5005 ] | Loss: 0.847 | Acc: 78.929% (626583/793856)
01/04/2023 18:48:57 - INFO - __main__ -   test: [epoch: 0 | batch: 3200/5005 ] | Loss: 0.847 | Acc: 78.924% (646744/819456)
01/04/2023 18:49:59 - INFO - __main__ -   test: [epoch: 0 | batch: 3300/5005 ] | Loss: 0.847 | Acc: 78.932% (667019/845056)
01/04/2023 18:51:01 - INFO - __main__ -   test: [epoch: 0 | batch: 3400/5005 ] | Loss: 0.847 | Acc: 78.927% (687185/870656)
01/04/2023 18:52:04 - INFO - __main__ -   test: [epoch: 0 | batch: 3500/5005 ] | Loss: 0.847 | Acc: 78.923% (707350/896256)
01/04/2023 18:53:06 - INFO - __main__ -   test: [epoch: 0 | batch: 3600/5005 ] | Loss: 0.847 | Acc: 78.923% (727559/921856)
01/04/2023 18:54:09 - INFO - __main__ -   test: [epoch: 0 | batch: 3700/5005 ] | Loss: 0.847 | Acc: 78.915% (747688/947456)
01/04/2023 18:55:11 - INFO - __main__ -   test: [epoch: 0 | batch: 3800/5005 ] | Loss: 0.847 | Acc: 78.921% (767950/973056)
01/04/2023 18:56:14 - INFO - __main__ -   test: [epoch: 0 | batch: 3900/5005 ] | Loss: 0.847 | Acc: 78.912% (788062/998656)
01/04/2023 18:57:16 - INFO - __main__ -   test: [epoch: 0 | batch: 4000/5005 ] | Loss: 0.847 | Acc: 78.919% (808330/1024256)
01/04/2023 18:58:19 - INFO - __main__ -   test: [epoch: 0 | batch: 4100/5005 ] | Loss: 0.846 | Acc: 78.920% (828543/1049856)
01/04/2023 18:59:21 - INFO - __main__ -   test: [epoch: 0 | batch: 4200/5005 ] | Loss: 0.847 | Acc: 78.917% (848718/1075456)
01/04/2023 19:00:24 - INFO - __main__ -   test: [epoch: 0 | batch: 4300/5005 ] | Loss: 0.846 | Acc: 78.922% (868972/1101056)
01/04/2023 19:01:26 - INFO - __main__ -   test: [epoch: 0 | batch: 4400/5005 ] | Loss: 0.846 | Acc: 78.923% (889188/1126656)
01/04/2023 19:02:28 - INFO - __main__ -   test: [epoch: 0 | batch: 4500/5005 ] | Loss: 0.846 | Acc: 78.925% (909418/1152256)
01/04/2023 19:03:31 - INFO - __main__ -   test: [epoch: 0 | batch: 4600/5005 ] | Loss: 0.846 | Acc: 78.924% (929606/1177856)
01/04/2023 19:04:33 - INFO - __main__ -   test: [epoch: 0 | batch: 4700/5005 ] | Loss: 0.846 | Acc: 78.924% (949812/1203456)
01/04/2023 19:05:35 - INFO - __main__ -   test: [epoch: 0 | batch: 4800/5005 ] | Loss: 0.846 | Acc: 78.927% (970056/1229056)
01/04/2023 19:06:38 - INFO - __main__ -   test: [epoch: 0 | batch: 4900/5005 ] | Loss: 0.847 | Acc: 78.926% (990247/1254656)
01/04/2023 19:07:40 - INFO - __main__ -   test: [epoch: 0 | batch: 5000/5005 ] | Loss: 0.847 | Acc: 78.922% (1010409/1280256)
01/04/2023 19:07:43 - INFO - __main__ -   Saving Checkpoint
01/04/2023 19:07:44 - INFO - __main__ -   test: [batch: 0/196 ] | Loss: 0.448 | Acc: 87.109% (223/256)/ 98.438% (252/256)
01/04/2023 19:07:45 - INFO - __main__ -   test: [batch: 1/196 ] | Loss: 0.564 | Acc: 84.375% (432/512)/ 96.680% (495/512)
01/04/2023 19:07:45 - INFO - __main__ -   test: [batch: 2/196 ] | Loss: 0.426 | Acc: 88.151% (677/768)/ 97.786% (751/768)
01/04/2023 19:07:46 - INFO - __main__ -   test: [batch: 3/196 ] | Loss: 0.403 | Acc: 89.160% (913/1024)/ 97.754% (1001/1024)
01/04/2023 19:07:46 - INFO - __main__ -   test: [batch: 4/196 ] | Loss: 0.400 | Acc: 89.688% (1148/1280)/ 97.734% (1251/1280)
01/04/2023 19:07:47 - INFO - __main__ -   test: [batch: 5/196 ] | Loss: 0.454 | Acc: 88.542% (1360/1536)/ 97.461% (1497/1536)
01/04/2023 19:07:48 - INFO - __main__ -   test: [batch: 6/196 ] | Loss: 0.550 | Acc: 85.882% (1539/1792)/ 96.708% (1733/1792)
01/04/2023 19:07:48 - INFO - __main__ -   test: [batch: 7/196 ] | Loss: 0.578 | Acc: 84.668% (1734/2048)/ 96.826% (1983/2048)
01/04/2023 19:07:49 - INFO - __main__ -   test: [batch: 8/196 ] | Loss: 0.615 | Acc: 84.028% (1936/2304)/ 96.267% (2218/2304)
01/04/2023 19:07:49 - INFO - __main__ -   test: [batch: 9/196 ] | Loss: 0.645 | Acc: 83.242% (2131/2560)/ 96.094% (2460/2560)
01/04/2023 19:07:50 - INFO - __main__ -   test: [batch: 10/196 ] | Loss: 0.674 | Acc: 82.635% (2327/2816)/ 95.774% (2697/2816)
01/04/2023 19:07:51 - INFO - __main__ -   test: [batch: 11/196 ] | Loss: 0.717 | Acc: 81.608% (2507/3072)/ 95.508% (2934/3072)
01/04/2023 19:07:51 - INFO - __main__ -   test: [batch: 12/196 ] | Loss: 0.760 | Acc: 80.529% (2680/3328)/ 95.012% (3162/3328)
01/04/2023 19:07:52 - INFO - __main__ -   test: [batch: 13/196 ] | Loss: 0.756 | Acc: 80.301% (2878/3584)/ 95.061% (3407/3584)
01/04/2023 19:07:53 - INFO - __main__ -   test: [batch: 14/196 ] | Loss: 0.758 | Acc: 79.870% (3067/3840)/ 95.234% (3657/3840)
01/04/2023 19:07:53 - INFO - __main__ -   test: [batch: 15/196 ] | Loss: 0.768 | Acc: 80.029% (3278/4096)/ 95.190% (3899/4096)
01/04/2023 19:07:54 - INFO - __main__ -   test: [batch: 16/196 ] | Loss: 0.747 | Acc: 80.515% (3504/4352)/ 95.312% (4148/4352)
01/04/2023 19:07:54 - INFO - __main__ -   test: [batch: 17/196 ] | Loss: 0.721 | Acc: 81.272% (3745/4608)/ 95.399% (4396/4608)
01/04/2023 19:07:55 - INFO - __main__ -   test: [batch: 18/196 ] | Loss: 0.696 | Acc: 82.031% (3990/4864)/ 95.559% (4648/4864)
01/04/2023 19:07:56 - INFO - __main__ -   test: [batch: 19/196 ] | Loss: 0.685 | Acc: 82.227% (4210/5120)/ 95.664% (4898/5120)
01/04/2023 19:07:56 - INFO - __main__ -   test: [batch: 20/196 ] | Loss: 0.686 | Acc: 82.422% (4431/5376)/ 95.499% (5134/5376)
01/04/2023 19:07:57 - INFO - __main__ -   test: [batch: 21/196 ] | Loss: 0.683 | Acc: 82.457% (4644/5632)/ 95.526% (5380/5632)
01/04/2023 19:07:57 - INFO - __main__ -   test: [batch: 22/196 ] | Loss: 0.683 | Acc: 82.677% (4868/5888)/ 95.431% (5619/5888)
01/04/2023 19:07:58 - INFO - __main__ -   test: [batch: 23/196 ] | Loss: 0.687 | Acc: 82.568% (5073/6144)/ 95.508% (5868/6144)
01/04/2023 19:07:59 - INFO - __main__ -   test: [batch: 24/196 ] | Loss: 0.695 | Acc: 82.484% (5279/6400)/ 95.391% (6105/6400)
01/04/2023 19:07:59 - INFO - __main__ -   test: [batch: 25/196 ] | Loss: 0.679 | Acc: 82.873% (5516/6656)/ 95.508% (6357/6656)
01/04/2023 19:08:00 - INFO - __main__ -   test: [batch: 26/196 ] | Loss: 0.666 | Acc: 83.160% (5748/6912)/ 95.602% (6608/6912)
01/04/2023 19:08:01 - INFO - __main__ -   test: [batch: 27/196 ] | Loss: 0.653 | Acc: 83.440% (5981/7168)/ 95.675% (6858/7168)
01/04/2023 19:08:01 - INFO - __main__ -   test: [batch: 28/196 ] | Loss: 0.639 | Acc: 83.836% (6224/7424)/ 95.770% (7110/7424)
01/04/2023 19:08:02 - INFO - __main__ -   test: [batch: 29/196 ] | Loss: 0.639 | Acc: 83.815% (6437/7680)/ 95.781% (7356/7680)
01/04/2023 19:08:02 - INFO - __main__ -   test: [batch: 30/196 ] | Loss: 0.644 | Acc: 83.745% (6646/7936)/ 95.766% (7600/7936)
01/04/2023 19:08:03 - INFO - __main__ -   test: [batch: 31/196 ] | Loss: 0.652 | Acc: 83.582% (6847/8192)/ 95.703% (7840/8192)
01/04/2023 19:08:04 - INFO - __main__ -   test: [batch: 32/196 ] | Loss: 0.673 | Acc: 82.872% (7001/8448)/ 95.573% (8074/8448)
01/04/2023 19:08:04 - INFO - __main__ -   test: [batch: 33/196 ] | Loss: 0.681 | Acc: 82.755% (7203/8704)/ 95.554% (8317/8704)
01/04/2023 19:08:05 - INFO - __main__ -   test: [batch: 34/196 ] | Loss: 0.677 | Acc: 82.790% (7418/8960)/ 95.614% (8567/8960)
01/04/2023 19:08:05 - INFO - __main__ -   test: [batch: 35/196 ] | Loss: 0.676 | Acc: 82.737% (7625/9216)/ 95.627% (8813/9216)
01/04/2023 19:08:06 - INFO - __main__ -   test: [batch: 36/196 ] | Loss: 0.682 | Acc: 82.517% (7816/9472)/ 95.650% (9060/9472)
01/04/2023 19:08:07 - INFO - __main__ -   test: [batch: 37/196 ] | Loss: 0.689 | Acc: 82.309% (8007/9728)/ 95.600% (9300/9728)
01/04/2023 19:08:07 - INFO - __main__ -   test: [batch: 38/196 ] | Loss: 0.689 | Acc: 82.242% (8211/9984)/ 95.633% (9548/9984)
01/04/2023 19:08:08 - INFO - __main__ -   test: [batch: 39/196 ] | Loss: 0.688 | Acc: 82.129% (8410/10240)/ 95.693% (9799/10240)
01/04/2023 19:08:09 - INFO - __main__ -   test: [batch: 40/196 ] | Loss: 0.687 | Acc: 82.117% (8619/10496)/ 95.722% (10047/10496)
01/04/2023 19:08:09 - INFO - __main__ -   test: [batch: 41/196 ] | Loss: 0.686 | Acc: 82.189% (8837/10752)/ 95.694% (10289/10752)
01/04/2023 19:08:10 - INFO - __main__ -   test: [batch: 42/196 ] | Loss: 0.680 | Acc: 82.313% (9061/11008)/ 95.767% (10542/11008)
01/04/2023 19:08:10 - INFO - __main__ -   test: [batch: 43/196 ] | Loss: 0.682 | Acc: 82.280% (9268/11264)/ 95.756% (10786/11264)
01/04/2023 19:08:11 - INFO - __main__ -   test: [batch: 44/196 ] | Loss: 0.688 | Acc: 82.248% (9475/11520)/ 95.703% (11025/11520)
01/04/2023 19:08:12 - INFO - __main__ -   test: [batch: 45/196 ] | Loss: 0.687 | Acc: 82.227% (9683/11776)/ 95.712% (11271/11776)
01/04/2023 19:08:12 - INFO - __main__ -   test: [batch: 46/196 ] | Loss: 0.692 | Acc: 82.031% (9870/12032)/ 95.711% (11516/12032)
01/04/2023 19:08:13 - INFO - __main__ -   test: [batch: 47/196 ] | Loss: 0.692 | Acc: 81.966% (10072/12288)/ 95.728% (11763/12288)
01/04/2023 19:08:14 - INFO - __main__ -   test: [batch: 48/196 ] | Loss: 0.697 | Acc: 81.680% (10246/12544)/ 95.767% (12013/12544)
01/04/2023 19:08:14 - INFO - __main__ -   test: [batch: 49/196 ] | Loss: 0.688 | Acc: 81.922% (10486/12800)/ 95.836% (12267/12800)
01/04/2023 19:08:15 - INFO - __main__ -   test: [batch: 50/196 ] | Loss: 0.683 | Acc: 82.077% (10716/13056)/ 95.879% (12518/13056)
01/04/2023 19:08:15 - INFO - __main__ -   test: [batch: 51/196 ] | Loss: 0.681 | Acc: 82.024% (10919/13312)/ 95.928% (12770/13312)
01/04/2023 19:08:16 - INFO - __main__ -   test: [batch: 52/196 ] | Loss: 0.682 | Acc: 81.958% (11120/13568)/ 95.932% (13016/13568)
01/04/2023 19:08:17 - INFO - __main__ -   test: [batch: 53/196 ] | Loss: 0.686 | Acc: 81.959% (11330/13824)/ 95.906% (13258/13824)
01/04/2023 19:08:17 - INFO - __main__ -   test: [batch: 54/196 ] | Loss: 0.690 | Acc: 81.804% (11518/14080)/ 95.923% (13506/14080)
01/04/2023 19:08:18 - INFO - __main__ -   test: [batch: 55/196 ] | Loss: 0.696 | Acc: 81.578% (11695/14336)/ 95.898% (13748/14336)
01/04/2023 19:08:18 - INFO - __main__ -   test: [batch: 56/196 ] | Loss: 0.696 | Acc: 81.641% (11913/14592)/ 95.916% (13996/14592)
01/04/2023 19:08:19 - INFO - __main__ -   test: [batch: 57/196 ] | Loss: 0.689 | Acc: 81.809% (12147/14848)/ 95.959% (14248/14848)
01/04/2023 19:08:20 - INFO - __main__ -   test: [batch: 58/196 ] | Loss: 0.691 | Acc: 81.800% (12355/15104)/ 95.941% (14491/15104)
01/04/2023 19:08:20 - INFO - __main__ -   test: [batch: 59/196 ] | Loss: 0.692 | Acc: 81.738% (12555/15360)/ 95.970% (14741/15360)
01/04/2023 19:08:21 - INFO - __main__ -   test: [batch: 60/196 ] | Loss: 0.696 | Acc: 81.647% (12750/15616)/ 95.946% (14983/15616)
01/04/2023 19:08:22 - INFO - __main__ -   test: [batch: 61/196 ] | Loss: 0.699 | Acc: 81.603% (12952/15872)/ 95.911% (15223/15872)
01/04/2023 19:08:22 - INFO - __main__ -   test: [batch: 62/196 ] | Loss: 0.694 | Acc: 81.715% (13179/16128)/ 95.951% (15475/16128)
01/04/2023 19:08:23 - INFO - __main__ -   test: [batch: 63/196 ] | Loss: 0.689 | Acc: 81.885% (13416/16384)/ 95.972% (15724/16384)
01/04/2023 19:08:23 - INFO - __main__ -   test: [batch: 64/196 ] | Loss: 0.688 | Acc: 81.887% (13626/16640)/ 95.968% (15969/16640)
01/04/2023 19:08:24 - INFO - __main__ -   test: [batch: 65/196 ] | Loss: 0.685 | Acc: 82.031% (13860/16896)/ 95.993% (16219/16896)
01/04/2023 19:08:25 - INFO - __main__ -   test: [batch: 66/196 ] | Loss: 0.684 | Acc: 82.049% (14073/17152)/ 95.989% (16464/17152)
01/04/2023 19:08:25 - INFO - __main__ -   test: [batch: 67/196 ] | Loss: 0.682 | Acc: 82.071% (14287/17408)/ 96.008% (16713/17408)
01/04/2023 19:08:26 - INFO - __main__ -   test: [batch: 68/196 ] | Loss: 0.678 | Acc: 82.094% (14501/17664)/ 96.060% (16968/17664)
01/04/2023 19:08:26 - INFO - __main__ -   test: [batch: 69/196 ] | Loss: 0.683 | Acc: 81.959% (14687/17920)/ 96.049% (17212/17920)
01/04/2023 19:08:27 - INFO - __main__ -   test: [batch: 70/196 ] | Loss: 0.684 | Acc: 81.905% (14887/18176)/ 96.055% (17459/18176)
01/04/2023 19:08:28 - INFO - __main__ -   test: [batch: 71/196 ] | Loss: 0.682 | Acc: 82.004% (15115/18432)/ 96.056% (17705/18432)
01/04/2023 19:08:28 - INFO - __main__ -   test: [batch: 72/196 ] | Loss: 0.687 | Acc: 81.972% (15319/18688)/ 96.003% (17941/18688)
01/04/2023 19:08:29 - INFO - __main__ -   test: [batch: 73/196 ] | Loss: 0.688 | Acc: 81.957% (15526/18944)/ 95.962% (18179/18944)
01/04/2023 19:08:29 - INFO - __main__ -   test: [batch: 74/196 ] | Loss: 0.694 | Acc: 81.802% (15706/19200)/ 95.922% (18417/19200)
01/04/2023 19:08:30 - INFO - __main__ -   test: [batch: 75/196 ] | Loss: 0.694 | Acc: 81.754% (15906/19456)/ 95.929% (18664/19456)
01/04/2023 19:08:31 - INFO - __main__ -   test: [batch: 76/196 ] | Loss: 0.694 | Acc: 81.808% (16126/19712)/ 95.926% (18909/19712)
01/04/2023 19:08:31 - INFO - __main__ -   test: [batch: 77/196 ] | Loss: 0.694 | Acc: 81.821% (16338/19968)/ 95.903% (19150/19968)
01/04/2023 19:08:32 - INFO - __main__ -   test: [batch: 78/196 ] | Loss: 0.698 | Acc: 81.710% (16525/20224)/ 95.851% (19385/20224)
01/04/2023 19:08:33 - INFO - __main__ -   test: [batch: 79/196 ] | Loss: 0.700 | Acc: 81.650% (16722/20480)/ 95.825% (19625/20480)
01/04/2023 19:08:33 - INFO - __main__ -   test: [batch: 80/196 ] | Loss: 0.710 | Acc: 81.399% (16879/20736)/ 95.727% (19850/20736)
01/04/2023 19:08:34 - INFO - __main__ -   test: [batch: 81/196 ] | Loss: 0.718 | Acc: 81.250% (17056/20992)/ 95.641% (20077/20992)
01/04/2023 19:08:34 - INFO - __main__ -   test: [batch: 82/196 ] | Loss: 0.720 | Acc: 81.156% (17244/21248)/ 95.651% (20324/21248)
01/04/2023 19:08:35 - INFO - __main__ -   test: [batch: 83/196 ] | Loss: 0.722 | Acc: 81.106% (17441/21504)/ 95.619% (20562/21504)
01/04/2023 19:08:36 - INFO - __main__ -   test: [batch: 84/196 ] | Loss: 0.723 | Acc: 81.071% (17641/21760)/ 95.588% (20800/21760)
01/04/2023 19:08:36 - INFO - __main__ -   test: [batch: 85/196 ] | Loss: 0.731 | Acc: 80.918% (17815/22016)/ 95.521% (21030/22016)
01/04/2023 19:08:37 - INFO - __main__ -   test: [batch: 86/196 ] | Loss: 0.734 | Acc: 80.819% (18000/22272)/ 95.497% (21269/22272)
01/04/2023 19:08:37 - INFO - __main__ -   test: [batch: 87/196 ] | Loss: 0.737 | Acc: 80.784% (18199/22528)/ 95.455% (21504/22528)
01/04/2023 19:08:38 - INFO - __main__ -   test: [batch: 88/196 ] | Loss: 0.741 | Acc: 80.706% (18388/22784)/ 95.392% (21734/22784)
01/04/2023 19:08:39 - INFO - __main__ -   test: [batch: 89/196 ] | Loss: 0.746 | Acc: 80.638% (18579/23040)/ 95.330% (21964/23040)
01/04/2023 19:08:39 - INFO - __main__ -   test: [batch: 90/196 ] | Loss: 0.758 | Acc: 80.383% (18726/23296)/ 95.235% (22186/23296)
01/04/2023 19:08:40 - INFO - __main__ -   test: [batch: 91/196 ] | Loss: 0.764 | Acc: 80.286% (18909/23552)/ 95.151% (22410/23552)
01/04/2023 19:08:41 - INFO - __main__ -   test: [batch: 92/196 ] | Loss: 0.763 | Acc: 80.326% (19124/23808)/ 95.132% (22649/23808)
01/04/2023 19:08:41 - INFO - __main__ -   test: [batch: 93/196 ] | Loss: 0.770 | Acc: 80.178% (19294/24064)/ 95.071% (22878/24064)
01/04/2023 19:08:42 - INFO - __main__ -   test: [batch: 94/196 ] | Loss: 0.775 | Acc: 79.984% (19452/24320)/ 95.053% (23117/24320)
01/04/2023 19:08:42 - INFO - __main__ -   test: [batch: 95/196 ] | Loss: 0.782 | Acc: 79.862% (19627/24576)/ 95.007% (23349/24576)
01/04/2023 19:08:43 - INFO - __main__ -   test: [batch: 96/196 ] | Loss: 0.788 | Acc: 79.764% (19807/24832)/ 94.918% (23570/24832)
01/04/2023 19:08:44 - INFO - __main__ -   test: [batch: 97/196 ] | Loss: 0.796 | Acc: 79.632% (19978/25088)/ 94.850% (23796/25088)
01/04/2023 19:08:44 - INFO - __main__ -   test: [batch: 98/196 ] | Loss: 0.804 | Acc: 79.455% (20137/25344)/ 94.796% (24025/25344)
01/04/2023 19:08:45 - INFO - __main__ -   test: [batch: 99/196 ] | Loss: 0.806 | Acc: 79.406% (20328/25600)/ 94.762% (24259/25600)
01/04/2023 19:08:45 - INFO - __main__ -   test: [batch: 100/196 ] | Loss: 0.809 | Acc: 79.278% (20498/25856)/ 94.725% (24492/25856)
01/04/2023 19:08:46 - INFO - __main__ -   test: [batch: 101/196 ] | Loss: 0.811 | Acc: 79.251% (20694/26112)/ 94.700% (24728/26112)
01/04/2023 19:08:47 - INFO - __main__ -   test: [batch: 102/196 ] | Loss: 0.816 | Acc: 79.073% (20850/26368)/ 94.653% (24958/26368)
01/04/2023 19:08:47 - INFO - __main__ -   test: [batch: 103/196 ] | Loss: 0.822 | Acc: 78.970% (21025/26624)/ 94.610% (25189/26624)
01/04/2023 19:08:48 - INFO - __main__ -   test: [batch: 104/196 ] | Loss: 0.823 | Acc: 78.940% (21219/26880)/ 94.598% (25428/26880)
01/04/2023 19:08:48 - INFO - __main__ -   test: [batch: 105/196 ] | Loss: 0.827 | Acc: 78.877% (21404/27136)/ 94.557% (25659/27136)
01/04/2023 19:08:49 - INFO - __main__ -   test: [batch: 106/196 ] | Loss: 0.829 | Acc: 78.830% (21593/27392)/ 94.542% (25897/27392)
01/04/2023 19:08:50 - INFO - __main__ -   test: [batch: 107/196 ] | Loss: 0.829 | Acc: 78.834% (21796/27648)/ 94.535% (26137/27648)
01/04/2023 19:08:50 - INFO - __main__ -   test: [batch: 108/196 ] | Loss: 0.832 | Acc: 78.809% (21991/27904)/ 94.495% (26368/27904)
01/04/2023 19:08:51 - INFO - __main__ -   test: [batch: 109/196 ] | Loss: 0.833 | Acc: 78.775% (22183/28160)/ 94.460% (26600/28160)
01/04/2023 19:08:52 - INFO - __main__ -   test: [batch: 110/196 ] | Loss: 0.834 | Acc: 78.762% (22381/28416)/ 94.468% (26844/28416)
01/04/2023 19:08:52 - INFO - __main__ -   test: [batch: 111/196 ] | Loss: 0.832 | Acc: 78.816% (22598/28672)/ 94.468% (27086/28672)
01/04/2023 19:08:53 - INFO - __main__ -   test: [batch: 112/196 ] | Loss: 0.831 | Acc: 78.837% (22806/28928)/ 94.472% (27329/28928)
01/04/2023 19:08:53 - INFO - __main__ -   test: [batch: 113/196 ] | Loss: 0.833 | Acc: 78.797% (22996/29184)/ 94.466% (27569/29184)
01/04/2023 19:08:54 - INFO - __main__ -   test: [batch: 114/196 ] | Loss: 0.841 | Acc: 78.665% (23159/29440)/ 94.361% (27780/29440)
01/04/2023 19:08:55 - INFO - __main__ -   test: [batch: 115/196 ] | Loss: 0.844 | Acc: 78.603% (23342/29696)/ 94.319% (28009/29696)
01/04/2023 19:08:55 - INFO - __main__ -   test: [batch: 116/196 ] | Loss: 0.844 | Acc: 78.576% (23535/29952)/ 94.314% (28249/29952)
01/04/2023 19:08:56 - INFO - __main__ -   test: [batch: 117/196 ] | Loss: 0.851 | Acc: 78.443% (23696/30208)/ 94.240% (28468/30208)
01/04/2023 19:08:56 - INFO - __main__ -   test: [batch: 118/196 ] | Loss: 0.850 | Acc: 78.493% (23912/30464)/ 94.216% (28702/30464)
01/04/2023 19:08:57 - INFO - __main__ -   test: [batch: 119/196 ] | Loss: 0.847 | Acc: 78.574% (24138/30720)/ 94.225% (28946/30720)
01/04/2023 19:08:58 - INFO - __main__ -   test: [batch: 120/196 ] | Loss: 0.850 | Acc: 78.503% (24317/30976)/ 94.179% (29173/30976)
01/04/2023 19:08:58 - INFO - __main__ -   test: [batch: 121/196 ] | Loss: 0.859 | Acc: 78.266% (24444/31232)/ 94.089% (29386/31232)
01/04/2023 19:08:59 - INFO - __main__ -   test: [batch: 122/196 ] | Loss: 0.858 | Acc: 78.277% (24648/31488)/ 94.074% (29622/31488)
01/04/2023 19:09:00 - INFO - __main__ -   test: [batch: 123/196 ] | Loss: 0.865 | Acc: 78.160% (24811/31744)/ 93.996% (29838/31744)
01/04/2023 19:09:00 - INFO - __main__ -   test: [batch: 124/196 ] | Loss: 0.867 | Acc: 77.997% (24959/32000)/ 93.975% (30072/32000)
01/04/2023 19:09:01 - INFO - __main__ -   test: [batch: 125/196 ] | Loss: 0.868 | Acc: 77.989% (25156/32256)/ 93.964% (30309/32256)
01/04/2023 19:09:01 - INFO - __main__ -   test: [batch: 126/196 ] | Loss: 0.869 | Acc: 77.990% (25356/32512)/ 93.944% (30543/32512)
01/04/2023 19:09:02 - INFO - __main__ -   test: [batch: 127/196 ] | Loss: 0.875 | Acc: 77.890% (25523/32768)/ 93.893% (30767/32768)
01/04/2023 19:09:03 - INFO - __main__ -   test: [batch: 128/196 ] | Loss: 0.881 | Acc: 77.756% (25678/33024)/ 93.847% (30992/33024)
01/04/2023 19:09:03 - INFO - __main__ -   test: [batch: 129/196 ] | Loss: 0.886 | Acc: 77.626% (25834/33280)/ 93.822% (31224/33280)
01/04/2023 19:09:04 - INFO - __main__ -   test: [batch: 130/196 ] | Loss: 0.884 | Acc: 77.645% (26039/33536)/ 93.836% (31469/33536)
01/04/2023 19:09:04 - INFO - __main__ -   test: [batch: 131/196 ] | Loss: 0.886 | Acc: 77.604% (26224/33792)/ 93.818% (31703/33792)
01/04/2023 19:09:05 - INFO - __main__ -   test: [batch: 132/196 ] | Loss: 0.890 | Acc: 77.570% (26411/34048)/ 93.759% (31923/34048)
01/04/2023 19:09:06 - INFO - __main__ -   test: [batch: 133/196 ] | Loss: 0.892 | Acc: 77.530% (26596/34304)/ 93.741% (32157/34304)
01/04/2023 19:09:06 - INFO - __main__ -   test: [batch: 134/196 ] | Loss: 0.893 | Acc: 77.491% (26781/34560)/ 93.718% (32389/34560)
01/04/2023 19:09:07 - INFO - __main__ -   test: [batch: 135/196 ] | Loss: 0.897 | Acc: 77.436% (26960/34816)/ 93.684% (32617/34816)
01/04/2023 19:09:08 - INFO - __main__ -   test: [batch: 136/196 ] | Loss: 0.898 | Acc: 77.406% (27148/35072)/ 93.667% (32851/35072)
01/04/2023 19:09:08 - INFO - __main__ -   test: [batch: 137/196 ] | Loss: 0.900 | Acc: 77.369% (27333/35328)/ 93.665% (33090/35328)
01/04/2023 19:09:09 - INFO - __main__ -   test: [batch: 138/196 ] | Loss: 0.901 | Acc: 77.310% (27510/35584)/ 93.654% (33326/35584)
01/04/2023 19:09:09 - INFO - __main__ -   test: [batch: 139/196 ] | Loss: 0.901 | Acc: 77.327% (27714/35840)/ 93.644% (33562/35840)
01/04/2023 19:09:10 - INFO - __main__ -   test: [batch: 140/196 ] | Loss: 0.902 | Acc: 77.313% (27907/36096)/ 93.634% (33798/36096)
01/04/2023 19:09:11 - INFO - __main__ -   test: [batch: 141/196 ] | Loss: 0.902 | Acc: 77.316% (28106/36352)/ 93.629% (34036/36352)
01/04/2023 19:09:11 - INFO - __main__ -   test: [batch: 142/196 ] | Loss: 0.908 | Acc: 77.226% (28271/36608)/ 93.559% (34250/36608)
01/04/2023 19:09:12 - INFO - __main__ -   test: [batch: 143/196 ] | Loss: 0.909 | Acc: 77.197% (28458/36864)/ 93.541% (34483/36864)
01/04/2023 19:09:12 - INFO - __main__ -   test: [batch: 144/196 ] | Loss: 0.910 | Acc: 77.188% (28652/37120)/ 93.513% (34712/37120)
01/04/2023 19:09:13 - INFO - __main__ -   test: [batch: 145/196 ] | Loss: 0.912 | Acc: 77.122% (28825/37376)/ 93.509% (34950/37376)
01/04/2023 19:09:14 - INFO - __main__ -   test: [batch: 146/196 ] | Loss: 0.913 | Acc: 77.067% (29002/37632)/ 93.490% (35182/37632)
01/04/2023 19:09:14 - INFO - __main__ -   test: [batch: 147/196 ] | Loss: 0.913 | Acc: 77.085% (29206/37888)/ 93.489% (35421/37888)
01/04/2023 19:09:15 - INFO - __main__ -   test: [batch: 148/196 ] | Loss: 0.916 | Acc: 77.063% (29395/38144)/ 93.456% (35648/38144)
01/04/2023 19:09:15 - INFO - __main__ -   test: [batch: 149/196 ] | Loss: 0.919 | Acc: 77.023% (29577/38400)/ 93.406% (35868/38400)
01/04/2023 19:09:16 - INFO - __main__ -   test: [batch: 150/196 ] | Loss: 0.920 | Acc: 77.015% (29771/38656)/ 93.383% (36098/38656)
01/04/2023 19:09:17 - INFO - __main__ -   test: [batch: 151/196 ] | Loss: 0.922 | Acc: 76.961% (29947/38912)/ 93.357% (36327/38912)
01/04/2023 19:09:17 - INFO - __main__ -   test: [batch: 152/196 ] | Loss: 0.922 | Acc: 76.928% (30131/39168)/ 93.357% (36566/39168)
01/04/2023 19:09:18 - INFO - __main__ -   test: [batch: 153/196 ] | Loss: 0.924 | Acc: 76.915% (30323/39424)/ 93.319% (36790/39424)
01/04/2023 19:09:19 - INFO - __main__ -   test: [batch: 154/196 ] | Loss: 0.925 | Acc: 76.898% (30513/39680)/ 93.299% (37021/39680)
01/04/2023 19:09:19 - INFO - __main__ -   test: [batch: 155/196 ] | Loss: 0.928 | Acc: 76.855% (30693/39936)/ 93.274% (37250/39936)
01/04/2023 19:09:20 - INFO - __main__ -   test: [batch: 156/196 ] | Loss: 0.927 | Acc: 76.878% (30899/40192)/ 93.270% (37487/40192)
01/04/2023 19:09:20 - INFO - __main__ -   test: [batch: 157/196 ] | Loss: 0.929 | Acc: 76.854% (31086/40448)/ 93.251% (37718/40448)
01/04/2023 19:09:21 - INFO - __main__ -   test: [batch: 158/196 ] | Loss: 0.933 | Acc: 76.712% (31225/40704)/ 93.205% (37938/40704)
01/04/2023 19:09:22 - INFO - __main__ -   test: [batch: 159/196 ] | Loss: 0.935 | Acc: 76.687% (31411/40960)/ 93.188% (38170/40960)
01/04/2023 19:09:22 - INFO - __main__ -   test: [batch: 160/196 ] | Loss: 0.933 | Acc: 76.740% (31629/41216)/ 93.192% (38410/41216)
01/04/2023 19:09:23 - INFO - __main__ -   test: [batch: 161/196 ] | Loss: 0.937 | Acc: 76.688% (31804/41472)/ 93.162% (38636/41472)
01/04/2023 19:09:23 - INFO - __main__ -   test: [batch: 162/196 ] | Loss: 0.937 | Acc: 76.690% (32001/41728)/ 93.153% (38871/41728)
01/04/2023 19:09:24 - INFO - __main__ -   test: [batch: 163/196 ] | Loss: 0.943 | Acc: 76.527% (32129/41984)/ 93.097% (39086/41984)
01/04/2023 19:09:25 - INFO - __main__ -   test: [batch: 164/196 ] | Loss: 0.945 | Acc: 76.470% (32301/42240)/ 93.080% (39317/42240)
01/04/2023 19:09:25 - INFO - __main__ -   test: [batch: 165/196 ] | Loss: 0.947 | Acc: 76.395% (32465/42496)/ 93.068% (39550/42496)
01/04/2023 19:09:26 - INFO - __main__ -   test: [batch: 166/196 ] | Loss: 0.946 | Acc: 76.408% (32666/42752)/ 93.072% (39790/42752)
01/04/2023 19:09:27 - INFO - __main__ -   test: [batch: 167/196 ] | Loss: 0.949 | Acc: 76.351% (32837/43008)/ 93.039% (40014/43008)
01/04/2023 19:09:27 - INFO - __main__ -   test: [batch: 168/196 ] | Loss: 0.950 | Acc: 76.294% (33008/43264)/ 93.017% (40243/43264)
01/04/2023 19:09:28 - INFO - __main__ -   test: [batch: 169/196 ] | Loss: 0.953 | Acc: 76.243% (33181/43520)/ 93.001% (40474/43520)
01/04/2023 19:09:28 - INFO - __main__ -   test: [batch: 170/196 ] | Loss: 0.951 | Acc: 76.277% (33391/43776)/ 93.028% (40724/43776)
01/04/2023 19:09:29 - INFO - __main__ -   test: [batch: 171/196 ] | Loss: 0.952 | Acc: 76.222% (33562/44032)/ 93.023% (40960/44032)
01/04/2023 19:09:30 - INFO - __main__ -   test: [batch: 172/196 ] | Loss: 0.955 | Acc: 76.156% (33728/44288)/ 92.980% (41179/44288)
01/04/2023 19:09:30 - INFO - __main__ -   test: [batch: 173/196 ] | Loss: 0.956 | Acc: 76.156% (33923/44544)/ 92.973% (41414/44544)
01/04/2023 19:09:31 - INFO - __main__ -   test: [batch: 174/196 ] | Loss: 0.958 | Acc: 76.132% (34107/44800)/ 92.953% (41643/44800)
01/04/2023 19:09:31 - INFO - __main__ -   test: [batch: 175/196 ] | Loss: 0.960 | Acc: 76.085% (34281/45056)/ 92.935% (41873/45056)
01/04/2023 19:09:32 - INFO - __main__ -   test: [batch: 176/196 ] | Loss: 0.962 | Acc: 76.044% (34457/45312)/ 92.898% (42094/45312)
01/04/2023 19:09:33 - INFO - __main__ -   test: [batch: 177/196 ] | Loss: 0.967 | Acc: 75.920% (34595/45568)/ 92.874% (42321/45568)
01/04/2023 19:09:33 - INFO - __main__ -   test: [batch: 178/196 ] | Loss: 0.966 | Acc: 75.947% (34802/45824)/ 92.886% (42564/45824)
01/04/2023 19:09:34 - INFO - __main__ -   test: [batch: 179/196 ] | Loss: 0.965 | Acc: 75.983% (35013/46080)/ 92.886% (42802/46080)
01/04/2023 19:09:34 - INFO - __main__ -   test: [batch: 180/196 ] | Loss: 0.967 | Acc: 75.952% (35193/46336)/ 92.887% (43040/46336)
01/04/2023 19:09:35 - INFO - __main__ -   test: [batch: 181/196 ] | Loss: 0.967 | Acc: 75.944% (35384/46592)/ 92.894% (43281/46592)
01/04/2023 19:09:36 - INFO - __main__ -   test: [batch: 182/196 ] | Loss: 0.965 | Acc: 75.988% (35599/46848)/ 92.909% (43526/46848)
01/04/2023 19:09:36 - INFO - __main__ -   test: [batch: 183/196 ] | Loss: 0.963 | Acc: 76.051% (35823/47104)/ 92.939% (43778/47104)
01/04/2023 19:09:37 - INFO - __main__ -   test: [batch: 184/196 ] | Loss: 0.961 | Acc: 76.075% (36029/47360)/ 92.956% (44024/47360)
01/04/2023 19:09:38 - INFO - __main__ -   test: [batch: 185/196 ] | Loss: 0.961 | Acc: 76.063% (36218/47616)/ 92.958% (44263/47616)
01/04/2023 19:09:38 - INFO - __main__ -   test: [batch: 186/196 ] | Loss: 0.958 | Acc: 76.143% (36451/47872)/ 92.983% (44513/47872)
01/04/2023 19:09:39 - INFO - __main__ -   test: [batch: 187/196 ] | Loss: 0.960 | Acc: 76.105% (36628/48128)/ 92.956% (44738/48128)
01/04/2023 19:09:39 - INFO - __main__ -   test: [batch: 188/196 ] | Loss: 0.961 | Acc: 76.089% (36815/48384)/ 92.938% (44967/48384)
01/04/2023 19:09:40 - INFO - __main__ -   test: [batch: 189/196 ] | Loss: 0.965 | Acc: 75.999% (36966/48640)/ 92.901% (45187/48640)
01/04/2023 19:09:41 - INFO - __main__ -   test: [batch: 190/196 ] | Loss: 0.966 | Acc: 75.953% (37138/48896)/ 92.909% (45429/48896)
01/04/2023 19:09:41 - INFO - __main__ -   test: [batch: 191/196 ] | Loss: 0.968 | Acc: 75.926% (37319/49152)/ 92.891% (45658/49152)
01/04/2023 19:09:42 - INFO - __main__ -   test: [batch: 192/196 ] | Loss: 0.965 | Acc: 75.978% (37539/49408)/ 92.912% (45906/49408)
01/04/2023 19:09:42 - INFO - __main__ -   test: [batch: 193/196 ] | Loss: 0.961 | Acc: 76.063% (37776/49664)/ 92.939% (46157/49664)
01/04/2023 19:09:43 - INFO - __main__ -   test: [batch: 194/196 ] | Loss: 0.959 | Acc: 76.114% (37996/49920)/ 92.953% (46402/49920)
01/04/2023 19:09:44 - INFO - __main__ -   test: [batch: 195/196 ] | Loss: 0.963 | Acc: 76.068% (38034/50000)/ 92.942% (46471/50000)
01/04/2023 19:09:44 - INFO - __main__ -   Final accuracy: 76.068
01/04/2023 19:09:44 - INFO - __main__ -   {'milestones': Counter({2: 1, 3: 1, 4: 1}), 'gamma': 0.1, 'base_lrs': [0.0005], 'last_epoch': 1, '_step_count': 2, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.0005]}
01/04/2023 19:09:44 - INFO - __main__ -   
Epoch: 1
01/04/2023 19:09:44 - INFO - __main__ -   test: [epoch: 1 | batch: 0/5005 ] | Loss: 0.811 | Acc: 80.859% (207/256)
01/04/2023 19:10:47 - INFO - __main__ -   test: [epoch: 1 | batch: 100/5005 ] | Loss: 0.844 | Acc: 79.011% (20429/25856)
01/04/2023 19:11:50 - INFO - __main__ -   test: [epoch: 1 | batch: 200/5005 ] | Loss: 0.852 | Acc: 78.677% (40484/51456)
01/04/2023 19:12:52 - INFO - __main__ -   test: [epoch: 1 | batch: 300/5005 ] | Loss: 0.850 | Acc: 78.727% (60664/77056)
01/04/2023 19:13:55 - INFO - __main__ -   test: [epoch: 1 | batch: 400/5005 ] | Loss: 0.852 | Acc: 78.724% (80815/102656)
01/04/2023 19:14:57 - INFO - __main__ -   test: [epoch: 1 | batch: 500/5005 ] | Loss: 0.851 | Acc: 78.746% (100996/128256)
01/04/2023 19:16:00 - INFO - __main__ -   test: [epoch: 1 | batch: 600/5005 ] | Loss: 0.853 | Acc: 78.709% (121098/153856)
01/04/2023 19:17:02 - INFO - __main__ -   test: [epoch: 1 | batch: 700/5005 ] | Loss: 0.850 | Acc: 78.794% (141401/179456)
01/04/2023 19:18:04 - INFO - __main__ -   test: [epoch: 1 | batch: 800/5005 ] | Loss: 0.851 | Acc: 78.776% (161534/205056)
01/04/2023 19:19:07 - INFO - __main__ -   test: [epoch: 1 | batch: 900/5005 ] | Loss: 0.851 | Acc: 78.820% (181802/230656)
01/04/2023 19:20:09 - INFO - __main__ -   test: [epoch: 1 | batch: 1000/5005 ] | Loss: 0.850 | Acc: 78.849% (202056/256256)
01/04/2023 19:21:12 - INFO - __main__ -   test: [epoch: 1 | batch: 1100/5005 ] | Loss: 0.851 | Acc: 78.841% (222217/281856)
01/04/2023 19:22:14 - INFO - __main__ -   test: [epoch: 1 | batch: 1200/5005 ] | Loss: 0.851 | Acc: 78.872% (242496/307456)
01/04/2023 19:23:17 - INFO - __main__ -   test: [epoch: 1 | batch: 1300/5005 ] | Loss: 0.851 | Acc: 78.860% (262648/333056)
01/04/2023 19:24:19 - INFO - __main__ -   test: [epoch: 1 | batch: 1400/5005 ] | Loss: 0.850 | Acc: 78.837% (282755/358656)
01/04/2023 19:25:22 - INFO - __main__ -   test: [epoch: 1 | batch: 1500/5005 ] | Loss: 0.850 | Acc: 78.844% (302963/384256)
01/04/2023 19:26:25 - INFO - __main__ -   test: [epoch: 1 | batch: 1600/5005 ] | Loss: 0.850 | Acc: 78.844% (323148/409856)
01/04/2023 19:27:28 - INFO - __main__ -   test: [epoch: 1 | batch: 1700/5005 ] | Loss: 0.850 | Acc: 78.862% (343410/435456)
01/04/2023 19:28:30 - INFO - __main__ -   test: [epoch: 1 | batch: 1800/5005 ] | Loss: 0.849 | Acc: 78.883% (363697/461056)
01/04/2023 19:29:33 - INFO - __main__ -   test: [epoch: 1 | batch: 1900/5005 ] | Loss: 0.849 | Acc: 78.889% (383916/486656)
01/04/2023 19:30:36 - INFO - __main__ -   test: [epoch: 1 | batch: 2000/5005 ] | Loss: 0.849 | Acc: 78.883% (404085/512256)
01/04/2023 19:31:38 - INFO - __main__ -   test: [epoch: 1 | batch: 2100/5005 ] | Loss: 0.849 | Acc: 78.887% (424297/537856)
01/04/2023 19:32:41 - INFO - __main__ -   test: [epoch: 1 | batch: 2200/5005 ] | Loss: 0.848 | Acc: 78.894% (444532/563456)
01/04/2023 19:33:43 - INFO - __main__ -   test: [epoch: 1 | batch: 2300/5005 ] | Loss: 0.849 | Acc: 78.878% (464636/589056)
01/04/2023 19:34:46 - INFO - __main__ -   test: [epoch: 1 | batch: 2400/5005 ] | Loss: 0.848 | Acc: 78.883% (484862/614656)
01/04/2023 19:35:49 - INFO - __main__ -   test: [epoch: 1 | batch: 2500/5005 ] | Loss: 0.848 | Acc: 78.901% (505171/640256)
01/04/2023 19:36:49 - INFO - __main__ -   test: [epoch: 1 | batch: 2600/5005 ] | Loss: 0.848 | Acc: 78.884% (525256/665856)
01/04/2023 19:37:50 - INFO - __main__ -   test: [epoch: 1 | batch: 2700/5005 ] | Loss: 0.848 | Acc: 78.889% (545484/691456)
01/04/2023 19:38:51 - INFO - __main__ -   test: [epoch: 1 | batch: 2800/5005 ] | Loss: 0.848 | Acc: 78.883% (565634/717056)
01/04/2023 19:39:52 - INFO - __main__ -   test: [epoch: 1 | batch: 2900/5005 ] | Loss: 0.848 | Acc: 78.886% (585854/742656)
01/04/2023 19:40:53 - INFO - __main__ -   test: [epoch: 1 | batch: 3000/5005 ] | Loss: 0.848 | Acc: 78.900% (606154/768256)
01/04/2023 19:41:53 - INFO - __main__ -   test: [epoch: 1 | batch: 3100/5005 ] | Loss: 0.848 | Acc: 78.891% (626282/793856)
01/04/2023 19:42:54 - INFO - __main__ -   test: [epoch: 1 | batch: 3200/5005 ] | Loss: 0.848 | Acc: 78.883% (646415/819456)
01/04/2023 19:43:55 - INFO - __main__ -   test: [epoch: 1 | batch: 3300/5005 ] | Loss: 0.849 | Acc: 78.886% (666631/845056)
01/04/2023 19:44:56 - INFO - __main__ -   test: [epoch: 1 | batch: 3400/5005 ] | Loss: 0.849 | Acc: 78.886% (686825/870656)
01/04/2023 19:45:57 - INFO - __main__ -   test: [epoch: 1 | batch: 3500/5005 ] | Loss: 0.848 | Acc: 78.889% (707051/896256)
01/04/2023 19:46:58 - INFO - __main__ -   test: [epoch: 1 | batch: 3600/5005 ] | Loss: 0.848 | Acc: 78.897% (727317/921856)
01/04/2023 19:47:59 - INFO - __main__ -   test: [epoch: 1 | batch: 3700/5005 ] | Loss: 0.848 | Acc: 78.887% (747415/947456)
01/04/2023 19:48:59 - INFO - __main__ -   test: [epoch: 1 | batch: 3800/5005 ] | Loss: 0.848 | Acc: 78.887% (767610/973056)
01/04/2023 19:50:00 - INFO - __main__ -   test: [epoch: 1 | batch: 3900/5005 ] | Loss: 0.848 | Acc: 78.878% (787716/998656)
01/04/2023 19:51:01 - INFO - __main__ -   test: [epoch: 1 | batch: 4000/5005 ] | Loss: 0.848 | Acc: 78.877% (807903/1024256)
01/04/2023 19:52:02 - INFO - __main__ -   test: [epoch: 1 | batch: 4100/5005 ] | Loss: 0.848 | Acc: 78.890% (828236/1049856)
01/04/2023 19:53:03 - INFO - __main__ -   test: [epoch: 1 | batch: 4200/5005 ] | Loss: 0.848 | Acc: 78.886% (848389/1075456)
01/04/2023 19:54:04 - INFO - __main__ -   test: [epoch: 1 | batch: 4300/5005 ] | Loss: 0.848 | Acc: 78.891% (868639/1101056)
01/04/2023 19:55:05 - INFO - __main__ -   test: [epoch: 1 | batch: 4400/5005 ] | Loss: 0.848 | Acc: 78.904% (888974/1126656)
01/04/2023 19:56:06 - INFO - __main__ -   test: [epoch: 1 | batch: 4500/5005 ] | Loss: 0.848 | Acc: 78.899% (909122/1152256)
01/04/2023 19:57:06 - INFO - __main__ -   test: [epoch: 1 | batch: 4600/5005 ] | Loss: 0.848 | Acc: 78.894% (929259/1177856)
01/04/2023 19:58:07 - INFO - __main__ -   test: [epoch: 1 | batch: 4700/5005 ] | Loss: 0.848 | Acc: 78.895% (949464/1203456)
01/04/2023 19:59:08 - INFO - __main__ -   test: [epoch: 1 | batch: 4800/5005 ] | Loss: 0.848 | Acc: 78.892% (969627/1229056)
01/04/2023 20:00:09 - INFO - __main__ -   test: [epoch: 1 | batch: 4900/5005 ] | Loss: 0.848 | Acc: 78.892% (989820/1254656)
01/04/2023 20:01:10 - INFO - __main__ -   test: [epoch: 1 | batch: 5000/5005 ] | Loss: 0.848 | Acc: 78.895% (1010053/1280256)
01/04/2023 20:01:12 - INFO - __main__ -   Saving Checkpoint
01/04/2023 20:01:13 - INFO - __main__ -   test: [batch: 0/196 ] | Loss: 0.456 | Acc: 87.109% (223/256)/ 98.047% (251/256)
01/04/2023 20:01:14 - INFO - __main__ -   test: [batch: 1/196 ] | Loss: 0.564 | Acc: 84.766% (434/512)/ 96.484% (494/512)
01/04/2023 20:01:14 - INFO - __main__ -   test: [batch: 2/196 ] | Loss: 0.426 | Acc: 88.411% (679/768)/ 97.656% (750/768)
01/04/2023 20:01:15 - INFO - __main__ -   test: [batch: 3/196 ] | Loss: 0.402 | Acc: 89.453% (916/1024)/ 97.656% (1000/1024)
01/04/2023 20:01:15 - INFO - __main__ -   test: [batch: 4/196 ] | Loss: 0.399 | Acc: 90.000% (1152/1280)/ 97.656% (1250/1280)
01/04/2023 20:01:16 - INFO - __main__ -   test: [batch: 5/196 ] | Loss: 0.453 | Acc: 88.867% (1365/1536)/ 97.461% (1497/1536)
01/04/2023 20:01:16 - INFO - __main__ -   test: [batch: 6/196 ] | Loss: 0.551 | Acc: 86.217% (1545/1792)/ 96.596% (1731/1792)
01/04/2023 20:01:17 - INFO - __main__ -   test: [batch: 7/196 ] | Loss: 0.579 | Acc: 84.961% (1740/2048)/ 96.777% (1982/2048)
01/04/2023 20:01:18 - INFO - __main__ -   test: [batch: 8/196 ] | Loss: 0.614 | Acc: 84.332% (1943/2304)/ 96.137% (2215/2304)
01/04/2023 20:01:18 - INFO - __main__ -   test: [batch: 9/196 ] | Loss: 0.643 | Acc: 83.516% (2138/2560)/ 95.938% (2456/2560)
01/04/2023 20:01:19 - INFO - __main__ -   test: [batch: 10/196 ] | Loss: 0.674 | Acc: 82.955% (2336/2816)/ 95.703% (2695/2816)
01/04/2023 20:01:19 - INFO - __main__ -   test: [batch: 11/196 ] | Loss: 0.717 | Acc: 81.999% (2519/3072)/ 95.378% (2930/3072)
01/04/2023 20:01:20 - INFO - __main__ -   test: [batch: 12/196 ] | Loss: 0.760 | Acc: 80.980% (2695/3328)/ 94.952% (3160/3328)
01/04/2023 20:01:21 - INFO - __main__ -   test: [batch: 13/196 ] | Loss: 0.756 | Acc: 80.720% (2893/3584)/ 95.006% (3405/3584)
01/04/2023 20:01:21 - INFO - __main__ -   test: [batch: 14/196 ] | Loss: 0.757 | Acc: 80.260% (3082/3840)/ 95.182% (3655/3840)
01/04/2023 20:01:22 - INFO - __main__ -   test: [batch: 15/196 ] | Loss: 0.767 | Acc: 80.396% (3293/4096)/ 95.142% (3897/4096)
01/04/2023 20:01:22 - INFO - __main__ -   test: [batch: 16/196 ] | Loss: 0.746 | Acc: 80.813% (3517/4352)/ 95.267% (4146/4352)
01/04/2023 20:01:23 - INFO - __main__ -   test: [batch: 17/196 ] | Loss: 0.720 | Acc: 81.532% (3757/4608)/ 95.378% (4395/4608)
01/04/2023 20:01:24 - INFO - __main__ -   test: [batch: 18/196 ] | Loss: 0.695 | Acc: 82.237% (4000/4864)/ 95.539% (4647/4864)
01/04/2023 20:01:24 - INFO - __main__ -   test: [batch: 19/196 ] | Loss: 0.683 | Acc: 82.422% (4220/5120)/ 95.625% (4896/5120)
01/04/2023 20:01:25 - INFO - __main__ -   test: [batch: 20/196 ] | Loss: 0.686 | Acc: 82.608% (4441/5376)/ 95.461% (5132/5376)
01/04/2023 20:01:25 - INFO - __main__ -   test: [batch: 21/196 ] | Loss: 0.682 | Acc: 82.653% (4655/5632)/ 95.490% (5378/5632)
01/04/2023 20:01:26 - INFO - __main__ -   test: [batch: 22/196 ] | Loss: 0.683 | Acc: 82.863% (4879/5888)/ 95.397% (5617/5888)
01/04/2023 20:01:26 - INFO - __main__ -   test: [batch: 23/196 ] | Loss: 0.687 | Acc: 82.764% (5085/6144)/ 95.492% (5867/6144)
01/04/2023 20:01:27 - INFO - __main__ -   test: [batch: 24/196 ] | Loss: 0.695 | Acc: 82.672% (5291/6400)/ 95.375% (6104/6400)
01/04/2023 20:01:28 - INFO - __main__ -   test: [batch: 25/196 ] | Loss: 0.679 | Acc: 83.038% (5527/6656)/ 95.493% (6356/6656)
01/04/2023 20:01:28 - INFO - __main__ -   test: [batch: 26/196 ] | Loss: 0.667 | Acc: 83.333% (5760/6912)/ 95.587% (6607/6912)
01/04/2023 20:01:29 - INFO - __main__ -   test: [batch: 27/196 ] | Loss: 0.653 | Acc: 83.636% (5995/7168)/ 95.661% (6857/7168)
01/04/2023 20:01:29 - INFO - __main__ -   test: [batch: 28/196 ] | Loss: 0.639 | Acc: 84.025% (6238/7424)/ 95.744% (7108/7424)
01/04/2023 20:01:30 - INFO - __main__ -   test: [batch: 29/196 ] | Loss: 0.639 | Acc: 84.023% (6453/7680)/ 95.755% (7354/7680)
01/04/2023 20:01:31 - INFO - __main__ -   test: [batch: 30/196 ] | Loss: 0.644 | Acc: 83.934% (6661/7936)/ 95.741% (7598/7936)
01/04/2023 20:01:31 - INFO - __main__ -   test: [batch: 31/196 ] | Loss: 0.652 | Acc: 83.740% (6860/8192)/ 95.703% (7840/8192)
01/04/2023 20:01:32 - INFO - __main__ -   test: [batch: 32/196 ] | Loss: 0.673 | Acc: 83.049% (7016/8448)/ 95.561% (8073/8448)
01/04/2023 20:01:32 - INFO - __main__ -   test: [batch: 33/196 ] | Loss: 0.680 | Acc: 82.962% (7221/8704)/ 95.542% (8316/8704)
01/04/2023 20:01:33 - INFO - __main__ -   test: [batch: 34/196 ] | Loss: 0.676 | Acc: 82.958% (7433/8960)/ 95.603% (8566/8960)
01/04/2023 20:01:34 - INFO - __main__ -   test: [batch: 35/196 ] | Loss: 0.676 | Acc: 82.921% (7642/9216)/ 95.616% (8812/9216)
01/04/2023 20:01:34 - INFO - __main__ -   test: [batch: 36/196 ] | Loss: 0.682 | Acc: 82.675% (7831/9472)/ 95.640% (9059/9472)
01/04/2023 20:01:35 - INFO - __main__ -   test: [batch: 37/196 ] | Loss: 0.689 | Acc: 82.453% (8021/9728)/ 95.631% (9303/9728)
01/04/2023 20:01:35 - INFO - __main__ -   test: [batch: 38/196 ] | Loss: 0.689 | Acc: 82.392% (8226/9984)/ 95.663% (9551/9984)
01/04/2023 20:01:36 - INFO - __main__ -   test: [batch: 39/196 ] | Loss: 0.688 | Acc: 82.266% (8424/10240)/ 95.732% (9803/10240)
01/04/2023 20:01:36 - INFO - __main__ -   test: [batch: 40/196 ] | Loss: 0.686 | Acc: 82.279% (8636/10496)/ 95.760% (10051/10496)
01/04/2023 20:01:37 - INFO - __main__ -   test: [batch: 41/196 ] | Loss: 0.685 | Acc: 82.329% (8852/10752)/ 95.740% (10294/10752)
01/04/2023 20:01:38 - INFO - __main__ -   test: [batch: 42/196 ] | Loss: 0.680 | Acc: 82.449% (9076/11008)/ 95.812% (10547/11008)
01/04/2023 20:01:38 - INFO - __main__ -   test: [batch: 43/196 ] | Loss: 0.682 | Acc: 82.422% (9284/11264)/ 95.792% (10790/11264)
01/04/2023 20:01:39 - INFO - __main__ -   test: [batch: 44/196 ] | Loss: 0.687 | Acc: 82.387% (9491/11520)/ 95.738% (11029/11520)
01/04/2023 20:01:39 - INFO - __main__ -   test: [batch: 45/196 ] | Loss: 0.687 | Acc: 82.354% (9698/11776)/ 95.737% (11274/11776)
01/04/2023 20:01:40 - INFO - __main__ -   test: [batch: 46/196 ] | Loss: 0.692 | Acc: 82.148% (9884/12032)/ 95.728% (11518/12032)
01/04/2023 20:01:41 - INFO - __main__ -   test: [batch: 47/196 ] | Loss: 0.692 | Acc: 82.064% (10084/12288)/ 95.744% (11765/12288)
01/04/2023 20:01:41 - INFO - __main__ -   test: [batch: 48/196 ] | Loss: 0.697 | Acc: 81.784% (10259/12544)/ 95.783% (12015/12544)
01/04/2023 20:01:42 - INFO - __main__ -   test: [batch: 49/196 ] | Loss: 0.688 | Acc: 82.039% (10501/12800)/ 95.852% (12269/12800)
01/04/2023 20:01:42 - INFO - __main__ -   test: [batch: 50/196 ] | Loss: 0.683 | Acc: 82.192% (10731/13056)/ 95.902% (12521/13056)
01/04/2023 20:01:43 - INFO - __main__ -   test: [batch: 51/196 ] | Loss: 0.682 | Acc: 82.136% (10934/13312)/ 95.944% (12772/13312)
01/04/2023 20:01:44 - INFO - __main__ -   test: [batch: 52/196 ] | Loss: 0.682 | Acc: 82.075% (11136/13568)/ 95.946% (13018/13568)
01/04/2023 20:01:44 - INFO - __main__ -   test: [batch: 53/196 ] | Loss: 0.687 | Acc: 82.082% (11347/13824)/ 95.920% (13260/13824)
01/04/2023 20:01:45 - INFO - __main__ -   test: [batch: 54/196 ] | Loss: 0.691 | Acc: 81.903% (11532/14080)/ 95.938% (13508/14080)
01/04/2023 20:01:45 - INFO - __main__ -   test: [batch: 55/196 ] | Loss: 0.697 | Acc: 81.676% (11709/14336)/ 95.905% (13749/14336)
01/04/2023 20:01:46 - INFO - __main__ -   test: [batch: 56/196 ] | Loss: 0.696 | Acc: 81.743% (11928/14592)/ 95.922% (13997/14592)
01/04/2023 20:01:46 - INFO - __main__ -   test: [batch: 57/196 ] | Loss: 0.690 | Acc: 81.910% (12162/14848)/ 95.966% (14249/14848)
01/04/2023 20:01:47 - INFO - __main__ -   test: [batch: 58/196 ] | Loss: 0.691 | Acc: 81.919% (12373/15104)/ 95.935% (14490/15104)
01/04/2023 20:01:48 - INFO - __main__ -   test: [batch: 59/196 ] | Loss: 0.693 | Acc: 81.849% (12572/15360)/ 95.964% (14740/15360)
01/04/2023 20:01:48 - INFO - __main__ -   test: [batch: 60/196 ] | Loss: 0.697 | Acc: 81.737% (12764/15616)/ 95.953% (14984/15616)
01/04/2023 20:01:49 - INFO - __main__ -   test: [batch: 61/196 ] | Loss: 0.700 | Acc: 81.697% (12967/15872)/ 95.911% (15223/15872)
01/04/2023 20:01:49 - INFO - __main__ -   test: [batch: 62/196 ] | Loss: 0.694 | Acc: 81.814% (13195/16128)/ 95.951% (15475/16128)
01/04/2023 20:01:50 - INFO - __main__ -   test: [batch: 63/196 ] | Loss: 0.689 | Acc: 81.982% (13432/16384)/ 95.966% (15723/16384)
01/04/2023 20:01:51 - INFO - __main__ -   test: [batch: 64/196 ] | Loss: 0.688 | Acc: 81.989% (13643/16640)/ 95.962% (15968/16640)
01/04/2023 20:01:51 - INFO - __main__ -   test: [batch: 65/196 ] | Loss: 0.685 | Acc: 82.138% (13878/16896)/ 95.987% (16218/16896)
01/04/2023 20:01:52 - INFO - __main__ -   test: [batch: 66/196 ] | Loss: 0.684 | Acc: 82.154% (14091/17152)/ 95.983% (16463/17152)
01/04/2023 20:01:52 - INFO - __main__ -   test: [batch: 67/196 ] | Loss: 0.682 | Acc: 82.175% (14305/17408)/ 96.002% (16712/17408)
01/04/2023 20:01:53 - INFO - __main__ -   test: [batch: 68/196 ] | Loss: 0.678 | Acc: 82.195% (14519/17664)/ 96.054% (16967/17664)
01/04/2023 20:01:54 - INFO - __main__ -   test: [batch: 69/196 ] | Loss: 0.683 | Acc: 82.054% (14704/17920)/ 96.055% (17213/17920)
01/04/2023 20:01:54 - INFO - __main__ -   test: [batch: 70/196 ] | Loss: 0.684 | Acc: 82.015% (14907/18176)/ 96.061% (17460/18176)
01/04/2023 20:01:55 - INFO - __main__ -   test: [batch: 71/196 ] | Loss: 0.682 | Acc: 82.107% (15134/18432)/ 96.056% (17705/18432)
01/04/2023 20:01:55 - INFO - __main__ -   test: [batch: 72/196 ] | Loss: 0.687 | Acc: 82.069% (15337/18688)/ 95.997% (17940/18688)
01/04/2023 20:01:56 - INFO - __main__ -   test: [batch: 73/196 ] | Loss: 0.689 | Acc: 82.058% (15545/18944)/ 95.962% (18179/18944)
01/04/2023 20:01:56 - INFO - __main__ -   test: [batch: 74/196 ] | Loss: 0.694 | Acc: 81.911% (15727/19200)/ 95.927% (18418/19200)
01/04/2023 20:01:57 - INFO - __main__ -   test: [batch: 75/196 ] | Loss: 0.694 | Acc: 81.867% (15928/19456)/ 95.934% (18665/19456)
01/04/2023 20:01:58 - INFO - __main__ -   test: [batch: 76/196 ] | Loss: 0.694 | Acc: 81.899% (16144/19712)/ 95.926% (18909/19712)
01/04/2023 20:01:58 - INFO - __main__ -   test: [batch: 77/196 ] | Loss: 0.694 | Acc: 81.916% (16357/19968)/ 95.903% (19150/19968)
01/04/2023 20:01:59 - INFO - __main__ -   test: [batch: 78/196 ] | Loss: 0.698 | Acc: 81.794% (16542/20224)/ 95.856% (19386/20224)
01/04/2023 20:01:59 - INFO - __main__ -   test: [batch: 79/196 ] | Loss: 0.701 | Acc: 81.738% (16740/20480)/ 95.830% (19626/20480)
01/04/2023 20:02:00 - INFO - __main__ -   test: [batch: 80/196 ] | Loss: 0.710 | Acc: 81.491% (16898/20736)/ 95.732% (19851/20736)
01/04/2023 20:02:01 - INFO - __main__ -   test: [batch: 81/196 ] | Loss: 0.718 | Acc: 81.341% (17075/20992)/ 95.641% (20077/20992)
01/04/2023 20:02:01 - INFO - __main__ -   test: [batch: 82/196 ] | Loss: 0.720 | Acc: 81.245% (17263/21248)/ 95.651% (20324/21248)
01/04/2023 20:02:02 - INFO - __main__ -   test: [batch: 83/196 ] | Loss: 0.722 | Acc: 81.185% (17458/21504)/ 95.619% (20562/21504)
01/04/2023 20:02:02 - INFO - __main__ -   test: [batch: 84/196 ] | Loss: 0.723 | Acc: 81.149% (17658/21760)/ 95.584% (20799/21760)
01/04/2023 20:02:03 - INFO - __main__ -   test: [batch: 85/196 ] | Loss: 0.732 | Acc: 80.996% (17832/22016)/ 95.521% (21030/22016)
01/04/2023 20:02:04 - INFO - __main__ -   test: [batch: 86/196 ] | Loss: 0.735 | Acc: 80.895% (18017/22272)/ 95.506% (21271/22272)
01/04/2023 20:02:04 - INFO - __main__ -   test: [batch: 87/196 ] | Loss: 0.737 | Acc: 80.864% (18217/22528)/ 95.463% (21506/22528)
01/04/2023 20:02:05 - INFO - __main__ -   test: [batch: 88/196 ] | Loss: 0.742 | Acc: 80.780% (18405/22784)/ 95.405% (21737/22784)
01/04/2023 20:02:05 - INFO - __main__ -   test: [batch: 89/196 ] | Loss: 0.746 | Acc: 80.699% (18593/23040)/ 95.343% (21967/23040)
01/04/2023 20:02:06 - INFO - __main__ -   test: [batch: 90/196 ] | Loss: 0.759 | Acc: 80.439% (18739/23296)/ 95.252% (22190/23296)
01/04/2023 20:02:06 - INFO - __main__ -   test: [batch: 91/196 ] | Loss: 0.764 | Acc: 80.341% (18922/23552)/ 95.164% (22413/23552)
01/04/2023 20:02:07 - INFO - __main__ -   test: [batch: 92/196 ] | Loss: 0.763 | Acc: 80.376% (19136/23808)/ 95.144% (22652/23808)
01/04/2023 20:02:08 - INFO - __main__ -   test: [batch: 93/196 ] | Loss: 0.771 | Acc: 80.228% (19306/24064)/ 95.088% (22882/24064)
01/04/2023 20:02:08 - INFO - __main__ -   test: [batch: 94/196 ] | Loss: 0.776 | Acc: 80.033% (19464/24320)/ 95.062% (23119/24320)
01/04/2023 20:02:09 - INFO - __main__ -   test: [batch: 95/196 ] | Loss: 0.783 | Acc: 79.915% (19640/24576)/ 95.024% (23353/24576)
01/04/2023 20:02:09 - INFO - __main__ -   test: [batch: 96/196 ] | Loss: 0.789 | Acc: 79.820% (19821/24832)/ 94.930% (23573/24832)
01/04/2023 20:02:10 - INFO - __main__ -   test: [batch: 97/196 ] | Loss: 0.796 | Acc: 79.684% (19991/25088)/ 94.850% (23796/25088)
01/04/2023 20:02:11 - INFO - __main__ -   test: [batch: 98/196 ] | Loss: 0.804 | Acc: 79.514% (20152/25344)/ 94.788% (24023/25344)
01/04/2023 20:02:11 - INFO - __main__ -   test: [batch: 99/196 ] | Loss: 0.806 | Acc: 79.461% (20342/25600)/ 94.750% (24256/25600)
01/04/2023 20:02:12 - INFO - __main__ -   test: [batch: 100/196 ] | Loss: 0.809 | Acc: 79.332% (20512/25856)/ 94.709% (24488/25856)
01/04/2023 20:02:12 - INFO - __main__ -   test: [batch: 101/196 ] | Loss: 0.812 | Acc: 79.312% (20710/26112)/ 94.688% (24725/26112)
01/04/2023 20:02:13 - INFO - __main__ -   test: [batch: 102/196 ] | Loss: 0.817 | Acc: 79.138% (20867/26368)/ 94.641% (24955/26368)
01/04/2023 20:02:14 - INFO - __main__ -   test: [batch: 103/196 ] | Loss: 0.822 | Acc: 79.045% (21045/26624)/ 94.603% (25187/26624)
01/04/2023 20:02:14 - INFO - __main__ -   test: [batch: 104/196 ] | Loss: 0.824 | Acc: 79.010% (21238/26880)/ 94.594% (25427/26880)
01/04/2023 20:02:15 - INFO - __main__ -   test: [batch: 105/196 ] | Loss: 0.827 | Acc: 78.954% (21425/27136)/ 94.546% (25656/27136)
01/04/2023 20:02:15 - INFO - __main__ -   test: [batch: 106/196 ] | Loss: 0.829 | Acc: 78.910% (21615/27392)/ 94.531% (25894/27392)
01/04/2023 20:02:16 - INFO - __main__ -   test: [batch: 107/196 ] | Loss: 0.830 | Acc: 78.910% (21817/27648)/ 94.524% (26134/27648)
01/04/2023 20:02:16 - INFO - __main__ -   test: [batch: 108/196 ] | Loss: 0.833 | Acc: 78.881% (22011/27904)/ 94.481% (26364/27904)
01/04/2023 20:02:17 - INFO - __main__ -   test: [batch: 109/196 ] | Loss: 0.834 | Acc: 78.846% (22203/28160)/ 94.450% (26597/28160)
01/04/2023 20:02:18 - INFO - __main__ -   test: [batch: 110/196 ] | Loss: 0.834 | Acc: 78.839% (22403/28416)/ 94.457% (26841/28416)
01/04/2023 20:02:18 - INFO - __main__ -   test: [batch: 111/196 ] | Loss: 0.833 | Acc: 78.889% (22619/28672)/ 94.465% (27085/28672)
01/04/2023 20:02:19 - INFO - __main__ -   test: [batch: 112/196 ] | Loss: 0.831 | Acc: 78.913% (22828/28928)/ 94.469% (27328/28928)
01/04/2023 20:02:19 - INFO - __main__ -   test: [batch: 113/196 ] | Loss: 0.834 | Acc: 78.879% (23020/29184)/ 94.456% (27566/29184)
01/04/2023 20:02:20 - INFO - __main__ -   test: [batch: 114/196 ] | Loss: 0.841 | Acc: 78.740% (23181/29440)/ 94.355% (27778/29440)
01/04/2023 20:02:21 - INFO - __main__ -   test: [batch: 115/196 ] | Loss: 0.844 | Acc: 78.671% (23362/29696)/ 94.316% (28008/29696)
01/04/2023 20:02:21 - INFO - __main__ -   test: [batch: 116/196 ] | Loss: 0.845 | Acc: 78.642% (23555/29952)/ 94.311% (28248/29952)
01/04/2023 20:02:22 - INFO - __main__ -   test: [batch: 117/196 ] | Loss: 0.851 | Acc: 78.512% (23717/30208)/ 94.233% (28466/30208)
01/04/2023 20:02:22 - INFO - __main__ -   test: [batch: 118/196 ] | Loss: 0.850 | Acc: 78.558% (23932/30464)/ 94.210% (28700/30464)
01/04/2023 20:02:23 - INFO - __main__ -   test: [batch: 119/196 ] | Loss: 0.848 | Acc: 78.639% (24158/30720)/ 94.219% (28944/30720)
01/04/2023 20:02:24 - INFO - __main__ -   test: [batch: 120/196 ] | Loss: 0.851 | Acc: 78.571% (24338/30976)/ 94.173% (29171/30976)
01/04/2023 20:02:24 - INFO - __main__ -   test: [batch: 121/196 ] | Loss: 0.859 | Acc: 78.327% (24463/31232)/ 94.083% (29384/31232)
01/04/2023 20:02:25 - INFO - __main__ -   test: [batch: 122/196 ] | Loss: 0.859 | Acc: 78.338% (24667/31488)/ 94.071% (29621/31488)
01/04/2023 20:02:25 - INFO - __main__ -   test: [batch: 123/196 ] | Loss: 0.865 | Acc: 78.210% (24827/31744)/ 93.996% (29838/31744)
01/04/2023 20:02:26 - INFO - __main__ -   test: [batch: 124/196 ] | Loss: 0.868 | Acc: 78.056% (24978/32000)/ 93.978% (30073/32000)
01/04/2023 20:02:26 - INFO - __main__ -   test: [batch: 125/196 ] | Loss: 0.869 | Acc: 78.057% (25178/32256)/ 93.970% (30311/32256)
01/04/2023 20:02:27 - INFO - __main__ -   test: [batch: 126/196 ] | Loss: 0.869 | Acc: 78.054% (25377/32512)/ 93.947% (30544/32512)
01/04/2023 20:02:28 - INFO - __main__ -   test: [batch: 127/196 ] | Loss: 0.875 | Acc: 77.957% (25545/32768)/ 93.896% (30768/32768)
01/04/2023 20:02:28 - INFO - __main__ -   test: [batch: 128/196 ] | Loss: 0.881 | Acc: 77.816% (25698/33024)/ 93.847% (30992/33024)
01/04/2023 20:02:29 - INFO - __main__ -   test: [batch: 129/196 ] | Loss: 0.886 | Acc: 77.686% (25854/33280)/ 93.819% (31223/33280)
01/04/2023 20:02:29 - INFO - __main__ -   test: [batch: 130/196 ] | Loss: 0.884 | Acc: 77.702% (26058/33536)/ 93.839% (31470/33536)
01/04/2023 20:02:30 - INFO - __main__ -   test: [batch: 131/196 ] | Loss: 0.886 | Acc: 77.646% (26238/33792)/ 93.821% (31704/33792)
01/04/2023 20:02:31 - INFO - __main__ -   test: [batch: 132/196 ] | Loss: 0.891 | Acc: 77.614% (26426/34048)/ 93.765% (31925/34048)
01/04/2023 20:02:31 - INFO - __main__ -   test: [batch: 133/196 ] | Loss: 0.893 | Acc: 77.574% (26611/34304)/ 93.747% (32159/34304)
01/04/2023 20:02:32 - INFO - __main__ -   test: [batch: 134/196 ] | Loss: 0.894 | Acc: 77.532% (26795/34560)/ 93.724% (32391/34560)
01/04/2023 20:02:32 - INFO - __main__ -   test: [batch: 135/196 ] | Loss: 0.897 | Acc: 77.482% (26976/34816)/ 93.690% (32619/34816)
01/04/2023 20:02:33 - INFO - __main__ -   test: [batch: 136/196 ] | Loss: 0.898 | Acc: 77.449% (27163/35072)/ 93.670% (32852/35072)
01/04/2023 20:02:34 - INFO - __main__ -   test: [batch: 137/196 ] | Loss: 0.900 | Acc: 77.415% (27349/35328)/ 93.668% (33091/35328)
01/04/2023 20:02:34 - INFO - __main__ -   test: [batch: 138/196 ] | Loss: 0.902 | Acc: 77.352% (27525/35584)/ 93.652% (33325/35584)
01/04/2023 20:02:35 - INFO - __main__ -   test: [batch: 139/196 ] | Loss: 0.902 | Acc: 77.369% (27729/35840)/ 93.641% (33561/35840)
01/04/2023 20:02:35 - INFO - __main__ -   test: [batch: 140/196 ] | Loss: 0.903 | Acc: 77.358% (27923/36096)/ 93.628% (33796/36096)
01/04/2023 20:02:36 - INFO - __main__ -   test: [batch: 141/196 ] | Loss: 0.902 | Acc: 77.363% (28123/36352)/ 93.626% (34035/36352)
01/04/2023 20:02:36 - INFO - __main__ -   test: [batch: 142/196 ] | Loss: 0.908 | Acc: 77.278% (28290/36608)/ 93.548% (34246/36608)
01/04/2023 20:02:37 - INFO - __main__ -   test: [batch: 143/196 ] | Loss: 0.909 | Acc: 77.243% (28475/36864)/ 93.528% (34478/36864)
01/04/2023 20:02:38 - INFO - __main__ -   test: [batch: 144/196 ] | Loss: 0.910 | Acc: 77.223% (28665/37120)/ 93.499% (34707/37120)
01/04/2023 20:02:38 - INFO - __main__ -   test: [batch: 145/196 ] | Loss: 0.912 | Acc: 77.156% (28838/37376)/ 93.496% (34945/37376)
01/04/2023 20:02:39 - INFO - __main__ -   test: [batch: 146/196 ] | Loss: 0.914 | Acc: 77.099% (29014/37632)/ 93.479% (35178/37632)
01/04/2023 20:02:39 - INFO - __main__ -   test: [batch: 147/196 ] | Loss: 0.914 | Acc: 77.109% (29215/37888)/ 93.481% (35418/37888)
01/04/2023 20:02:40 - INFO - __main__ -   test: [batch: 148/196 ] | Loss: 0.916 | Acc: 77.087% (29404/38144)/ 93.449% (35645/38144)
01/04/2023 20:02:41 - INFO - __main__ -   test: [batch: 149/196 ] | Loss: 0.919 | Acc: 77.047% (29586/38400)/ 93.401% (35866/38400)
01/04/2023 20:02:41 - INFO - __main__ -   test: [batch: 150/196 ] | Loss: 0.920 | Acc: 77.036% (29779/38656)/ 93.385% (36099/38656)
01/04/2023 20:02:42 - INFO - __main__ -   test: [batch: 151/196 ] | Loss: 0.922 | Acc: 76.981% (29955/38912)/ 93.354% (36326/38912)
01/04/2023 20:02:42 - INFO - __main__ -   test: [batch: 152/196 ] | Loss: 0.923 | Acc: 76.953% (30141/39168)/ 93.357% (36566/39168)
01/04/2023 20:02:43 - INFO - __main__ -   test: [batch: 153/196 ] | Loss: 0.924 | Acc: 76.943% (30334/39424)/ 93.319% (36790/39424)
01/04/2023 20:02:44 - INFO - __main__ -   test: [batch: 154/196 ] | Loss: 0.926 | Acc: 76.928% (30525/39680)/ 93.299% (37021/39680)
01/04/2023 20:02:44 - INFO - __main__ -   test: [batch: 155/196 ] | Loss: 0.928 | Acc: 76.896% (30709/39936)/ 93.274% (37250/39936)
01/04/2023 20:02:45 - INFO - __main__ -   test: [batch: 156/196 ] | Loss: 0.928 | Acc: 76.918% (30915/40192)/ 93.267% (37486/40192)
01/04/2023 20:02:45 - INFO - __main__ -   test: [batch: 157/196 ] | Loss: 0.929 | Acc: 76.891% (31101/40448)/ 93.251% (37718/40448)
01/04/2023 20:02:46 - INFO - __main__ -   test: [batch: 158/196 ] | Loss: 0.933 | Acc: 76.747% (31239/40704)/ 93.197% (37935/40704)
01/04/2023 20:02:46 - INFO - __main__ -   test: [batch: 159/196 ] | Loss: 0.935 | Acc: 76.726% (31427/40960)/ 93.181% (38167/40960)
01/04/2023 20:02:47 - INFO - __main__ -   test: [batch: 160/196 ] | Loss: 0.933 | Acc: 76.781% (31646/41216)/ 93.182% (38406/41216)
01/04/2023 20:02:48 - INFO - __main__ -   test: [batch: 161/196 ] | Loss: 0.937 | Acc: 76.719% (31817/41472)/ 93.152% (38632/41472)
01/04/2023 20:02:48 - INFO - __main__ -   test: [batch: 162/196 ] | Loss: 0.937 | Acc: 76.721% (32014/41728)/ 93.141% (38866/41728)
01/04/2023 20:02:49 - INFO - __main__ -   test: [batch: 163/196 ] | Loss: 0.943 | Acc: 76.560% (32143/41984)/ 93.083% (39080/41984)
01/04/2023 20:02:49 - INFO - __main__ -   test: [batch: 164/196 ] | Loss: 0.945 | Acc: 76.499% (32313/42240)/ 93.063% (39310/42240)
01/04/2023 20:02:50 - INFO - __main__ -   test: [batch: 165/196 ] | Loss: 0.947 | Acc: 76.424% (32477/42496)/ 93.049% (39542/42496)
01/04/2023 20:02:51 - INFO - __main__ -   test: [batch: 166/196 ] | Loss: 0.946 | Acc: 76.441% (32680/42752)/ 93.053% (39782/42752)
01/04/2023 20:02:51 - INFO - __main__ -   test: [batch: 167/196 ] | Loss: 0.949 | Acc: 76.388% (32853/43008)/ 93.020% (40006/43008)
01/04/2023 20:02:52 - INFO - __main__ -   test: [batch: 168/196 ] | Loss: 0.951 | Acc: 76.336% (33026/43264)/ 92.996% (40234/43264)
01/04/2023 20:02:52 - INFO - __main__ -   test: [batch: 169/196 ] | Loss: 0.953 | Acc: 76.282% (33198/43520)/ 92.980% (40465/43520)
01/04/2023 20:02:53 - INFO - __main__ -   test: [batch: 170/196 ] | Loss: 0.951 | Acc: 76.311% (33406/43776)/ 93.008% (40715/43776)
01/04/2023 20:02:54 - INFO - __main__ -   test: [batch: 171/196 ] | Loss: 0.952 | Acc: 76.260% (33579/44032)/ 93.005% (40952/44032)
01/04/2023 20:02:54 - INFO - __main__ -   test: [batch: 172/196 ] | Loss: 0.956 | Acc: 76.199% (33747/44288)/ 92.957% (41169/44288)
01/04/2023 20:02:55 - INFO - __main__ -   test: [batch: 173/196 ] | Loss: 0.957 | Acc: 76.194% (33940/44544)/ 92.953% (41405/44544)
01/04/2023 20:02:55 - INFO - __main__ -   test: [batch: 174/196 ] | Loss: 0.958 | Acc: 76.170% (34124/44800)/ 92.938% (41636/44800)
01/04/2023 20:02:56 - INFO - __main__ -   test: [batch: 175/196 ] | Loss: 0.960 | Acc: 76.127% (34300/45056)/ 92.918% (41865/45056)
01/04/2023 20:02:56 - INFO - __main__ -   test: [batch: 176/196 ] | Loss: 0.962 | Acc: 76.088% (34477/45312)/ 92.876% (42084/45312)
01/04/2023 20:02:57 - INFO - __main__ -   test: [batch: 177/196 ] | Loss: 0.967 | Acc: 75.959% (34613/45568)/ 92.855% (42312/45568)
01/04/2023 20:02:58 - INFO - __main__ -   test: [batch: 178/196 ] | Loss: 0.966 | Acc: 75.982% (34818/45824)/ 92.868% (42556/45824)
01/04/2023 20:02:58 - INFO - __main__ -   test: [batch: 179/196 ] | Loss: 0.966 | Acc: 76.016% (35028/46080)/ 92.869% (42794/46080)
01/04/2023 20:02:59 - INFO - __main__ -   test: [batch: 180/196 ] | Loss: 0.968 | Acc: 75.986% (35209/46336)/ 92.865% (43030/46336)
01/04/2023 20:02:59 - INFO - __main__ -   test: [batch: 181/196 ] | Loss: 0.967 | Acc: 75.977% (35399/46592)/ 92.868% (43269/46592)
01/04/2023 20:03:00 - INFO - __main__ -   test: [batch: 182/196 ] | Loss: 0.966 | Acc: 76.016% (35612/46848)/ 92.886% (43515/46848)
01/04/2023 20:03:01 - INFO - __main__ -   test: [batch: 183/196 ] | Loss: 0.963 | Acc: 76.076% (35835/47104)/ 92.914% (43766/47104)
01/04/2023 20:03:01 - INFO - __main__ -   test: [batch: 184/196 ] | Loss: 0.962 | Acc: 76.102% (36042/47360)/ 92.931% (44012/47360)
01/04/2023 20:03:02 - INFO - __main__ -   test: [batch: 185/196 ] | Loss: 0.962 | Acc: 76.092% (36232/47616)/ 92.933% (44251/47616)
01/04/2023 20:03:02 - INFO - __main__ -   test: [batch: 186/196 ] | Loss: 0.959 | Acc: 76.174% (36466/47872)/ 92.956% (44500/47872)
01/04/2023 20:03:03 - INFO - __main__ -   test: [batch: 187/196 ] | Loss: 0.960 | Acc: 76.137% (36643/48128)/ 92.925% (44723/48128)
01/04/2023 20:03:04 - INFO - __main__ -   test: [batch: 188/196 ] | Loss: 0.962 | Acc: 76.120% (36830/48384)/ 92.905% (44951/48384)
01/04/2023 20:03:04 - INFO - __main__ -   test: [batch: 189/196 ] | Loss: 0.965 | Acc: 76.034% (36983/48640)/ 92.870% (45172/48640)
01/04/2023 20:03:05 - INFO - __main__ -   test: [batch: 190/196 ] | Loss: 0.966 | Acc: 75.988% (37155/48896)/ 92.877% (45413/48896)
01/04/2023 20:03:05 - INFO - __main__ -   test: [batch: 191/196 ] | Loss: 0.968 | Acc: 75.962% (37337/49152)/ 92.865% (45645/49152)
01/04/2023 20:03:06 - INFO - __main__ -   test: [batch: 192/196 ] | Loss: 0.965 | Acc: 76.012% (37556/49408)/ 92.886% (45893/49408)
01/04/2023 20:03:06 - INFO - __main__ -   test: [batch: 193/196 ] | Loss: 0.961 | Acc: 76.099% (37794/49664)/ 92.912% (46144/49664)
01/04/2023 20:03:07 - INFO - __main__ -   test: [batch: 194/196 ] | Loss: 0.959 | Acc: 76.148% (38013/49920)/ 92.925% (46388/49920)
01/04/2023 20:03:07 - INFO - __main__ -   test: [batch: 195/196 ] | Loss: 0.963 | Acc: 76.100% (38050/50000)/ 92.916% (46458/50000)
01/04/2023 20:03:07 - INFO - __main__ -   Final accuracy: 76.100
01/04/2023 20:03:07 - INFO - __main__ -   {'milestones': Counter({2: 1, 3: 1, 4: 1}), 'gamma': 0.1, 'base_lrs': [0.0005], 'last_epoch': 2, '_step_count': 3, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [5e-05]}
01/04/2023 20:03:07 - INFO - __main__ -   
Epoch: 2
01/04/2023 20:03:08 - INFO - __main__ -   test: [epoch: 2 | batch: 0/5005 ] | Loss: 0.783 | Acc: 79.688% (204/256)
01/04/2023 20:04:09 - INFO - __main__ -   test: [epoch: 2 | batch: 100/5005 ] | Loss: 0.831 | Acc: 79.413% (20533/25856)
01/04/2023 20:05:10 - INFO - __main__ -   test: [epoch: 2 | batch: 200/5005 ] | Loss: 0.841 | Acc: 79.114% (40709/51456)
01/04/2023 20:06:10 - INFO - __main__ -   test: [epoch: 2 | batch: 300/5005 ] | Loss: 0.844 | Acc: 78.987% (60864/77056)
01/04/2023 20:07:11 - INFO - __main__ -   test: [epoch: 2 | batch: 400/5005 ] | Loss: 0.847 | Acc: 78.926% (81022/102656)
01/04/2023 20:08:12 - INFO - __main__ -   test: [epoch: 2 | batch: 500/5005 ] | Loss: 0.845 | Acc: 78.930% (101233/128256)
01/04/2023 20:09:13 - INFO - __main__ -   test: [epoch: 2 | batch: 600/5005 ] | Loss: 0.848 | Acc: 78.866% (121340/153856)
01/04/2023 20:10:14 - INFO - __main__ -   test: [epoch: 2 | batch: 700/5005 ] | Loss: 0.846 | Acc: 78.947% (141675/179456)
01/04/2023 20:11:15 - INFO - __main__ -   test: [epoch: 2 | batch: 800/5005 ] | Loss: 0.847 | Acc: 78.901% (161791/205056)
01/04/2023 20:12:16 - INFO - __main__ -   test: [epoch: 2 | batch: 900/5005 ] | Loss: 0.846 | Acc: 78.899% (181985/230656)
01/04/2023 20:13:16 - INFO - __main__ -   test: [epoch: 2 | batch: 1000/5005 ] | Loss: 0.847 | Acc: 78.867% (202101/256256)
01/04/2023 20:14:17 - INFO - __main__ -   test: [epoch: 2 | batch: 1100/5005 ] | Loss: 0.848 | Acc: 78.817% (222150/281856)
01/04/2023 20:15:18 - INFO - __main__ -   test: [epoch: 2 | batch: 1200/5005 ] | Loss: 0.847 | Acc: 78.836% (242385/307456)
01/04/2023 20:16:19 - INFO - __main__ -   test: [epoch: 2 | batch: 1300/5005 ] | Loss: 0.848 | Acc: 78.832% (262556/333056)
01/04/2023 20:17:20 - INFO - __main__ -   test: [epoch: 2 | batch: 1400/5005 ] | Loss: 0.848 | Acc: 78.820% (282694/358656)
01/04/2023 20:18:21 - INFO - __main__ -   test: [epoch: 2 | batch: 1500/5005 ] | Loss: 0.848 | Acc: 78.831% (302911/384256)
01/04/2023 20:19:21 - INFO - __main__ -   test: [epoch: 2 | batch: 1600/5005 ] | Loss: 0.848 | Acc: 78.813% (323018/409856)
01/04/2023 20:20:22 - INFO - __main__ -   test: [epoch: 2 | batch: 1700/5005 ] | Loss: 0.848 | Acc: 78.812% (343191/435456)
01/04/2023 20:21:23 - INFO - __main__ -   test: [epoch: 2 | batch: 1800/5005 ] | Loss: 0.848 | Acc: 78.818% (363396/461056)
01/04/2023 20:22:24 - INFO - __main__ -   test: [epoch: 2 | batch: 1900/5005 ] | Loss: 0.848 | Acc: 78.816% (383563/486656)
01/04/2023 20:23:25 - INFO - __main__ -   test: [epoch: 2 | batch: 2000/5005 ] | Loss: 0.849 | Acc: 78.786% (403585/512256)
01/04/2023 20:24:26 - INFO - __main__ -   test: [epoch: 2 | batch: 2100/5005 ] | Loss: 0.849 | Acc: 78.792% (423786/537856)
01/04/2023 20:25:27 - INFO - __main__ -   test: [epoch: 2 | batch: 2200/5005 ] | Loss: 0.848 | Acc: 78.800% (444006/563456)
01/04/2023 20:26:27 - INFO - __main__ -   test: [epoch: 2 | batch: 2300/5005 ] | Loss: 0.848 | Acc: 78.801% (464180/589056)
01/04/2023 20:27:28 - INFO - __main__ -   test: [epoch: 2 | batch: 2400/5005 ] | Loss: 0.848 | Acc: 78.816% (484445/614656)
01/04/2023 20:28:29 - INFO - __main__ -   test: [epoch: 2 | batch: 2500/5005 ] | Loss: 0.847 | Acc: 78.840% (504777/640256)
01/04/2023 20:29:30 - INFO - __main__ -   test: [epoch: 2 | batch: 2600/5005 ] | Loss: 0.848 | Acc: 78.817% (524811/665856)
01/04/2023 20:30:31 - INFO - __main__ -   test: [epoch: 2 | batch: 2700/5005 ] | Loss: 0.848 | Acc: 78.814% (544962/691456)
01/04/2023 20:31:32 - INFO - __main__ -   test: [epoch: 2 | batch: 2800/5005 ] | Loss: 0.848 | Acc: 78.839% (565321/717056)
01/04/2023 20:32:32 - INFO - __main__ -   test: [epoch: 2 | batch: 2900/5005 ] | Loss: 0.848 | Acc: 78.835% (585471/742656)
01/04/2023 20:33:33 - INFO - __main__ -   test: [epoch: 2 | batch: 3000/5005 ] | Loss: 0.848 | Acc: 78.836% (605662/768256)
01/04/2023 20:34:34 - INFO - __main__ -   test: [epoch: 2 | batch: 3100/5005 ] | Loss: 0.848 | Acc: 78.830% (625799/793856)
01/04/2023 20:35:35 - INFO - __main__ -   test: [epoch: 2 | batch: 3200/5005 ] | Loss: 0.848 | Acc: 78.829% (645971/819456)
01/04/2023 20:36:36 - INFO - __main__ -   test: [epoch: 2 | batch: 3300/5005 ] | Loss: 0.849 | Acc: 78.820% (666072/845056)
01/04/2023 20:37:37 - INFO - __main__ -   test: [epoch: 2 | batch: 3400/5005 ] | Loss: 0.848 | Acc: 78.825% (686294/870656)
01/04/2023 20:38:37 - INFO - __main__ -   test: [epoch: 2 | batch: 3500/5005 ] | Loss: 0.849 | Acc: 78.828% (706505/896256)
01/04/2023 20:39:38 - INFO - __main__ -   test: [epoch: 2 | batch: 3600/5005 ] | Loss: 0.848 | Acc: 78.837% (726762/921856)
01/04/2023 20:40:39 - INFO - __main__ -   test: [epoch: 2 | batch: 3700/5005 ] | Loss: 0.849 | Acc: 78.829% (746868/947456)
01/04/2023 20:41:40 - INFO - __main__ -   test: [epoch: 2 | batch: 3800/5005 ] | Loss: 0.848 | Acc: 78.836% (767123/973056)
01/04/2023 20:42:41 - INFO - __main__ -   test: [epoch: 2 | batch: 3900/5005 ] | Loss: 0.849 | Acc: 78.825% (787189/998656)
01/04/2023 20:43:42 - INFO - __main__ -   test: [epoch: 2 | batch: 4000/5005 ] | Loss: 0.849 | Acc: 78.824% (807364/1024256)
01/04/2023 20:44:43 - INFO - __main__ -   test: [epoch: 2 | batch: 4100/5005 ] | Loss: 0.848 | Acc: 78.833% (827638/1049856)
01/04/2023 20:45:44 - INFO - __main__ -   test: [epoch: 2 | batch: 4200/5005 ] | Loss: 0.849 | Acc: 78.826% (847741/1075456)
01/04/2023 20:46:45 - INFO - __main__ -   test: [epoch: 2 | batch: 4300/5005 ] | Loss: 0.849 | Acc: 78.837% (868041/1101056)
01/04/2023 20:47:45 - INFO - __main__ -   test: [epoch: 2 | batch: 4400/5005 ] | Loss: 0.849 | Acc: 78.839% (888246/1126656)
01/04/2023 20:48:46 - INFO - __main__ -   test: [epoch: 2 | batch: 4500/5005 ] | Loss: 0.849 | Acc: 78.840% (908442/1152256)
01/04/2023 20:49:47 - INFO - __main__ -   test: [epoch: 2 | batch: 4600/5005 ] | Loss: 0.849 | Acc: 78.844% (928673/1177856)
01/04/2023 20:50:48 - INFO - __main__ -   test: [epoch: 2 | batch: 4700/5005 ] | Loss: 0.849 | Acc: 78.842% (948830/1203456)
01/04/2023 20:51:49 - INFO - __main__ -   test: [epoch: 2 | batch: 4800/5005 ] | Loss: 0.849 | Acc: 78.842% (969007/1229056)
01/04/2023 20:52:50 - INFO - __main__ -   test: [epoch: 2 | batch: 4900/5005 ] | Loss: 0.849 | Acc: 78.837% (989130/1254656)
01/04/2023 20:53:51 - INFO - __main__ -   test: [epoch: 2 | batch: 5000/5005 ] | Loss: 0.849 | Acc: 78.840% (1009349/1280256)
01/04/2023 20:53:53 - INFO - __main__ -   Saving Checkpoint
01/04/2023 20:53:54 - INFO - __main__ -   test: [batch: 0/196 ] | Loss: 0.445 | Acc: 87.109% (223/256)/ 98.438% (252/256)
01/04/2023 20:53:55 - INFO - __main__ -   test: [batch: 1/196 ] | Loss: 0.561 | Acc: 85.156% (436/512)/ 96.484% (494/512)
01/04/2023 20:53:55 - INFO - __main__ -   test: [batch: 2/196 ] | Loss: 0.424 | Acc: 88.672% (681/768)/ 97.656% (750/768)
01/04/2023 20:53:56 - INFO - __main__ -   test: [batch: 3/196 ] | Loss: 0.402 | Acc: 89.551% (917/1024)/ 97.656% (1000/1024)
01/04/2023 20:53:57 - INFO - __main__ -   test: [batch: 4/196 ] | Loss: 0.400 | Acc: 89.922% (1151/1280)/ 97.656% (1250/1280)
01/04/2023 20:53:57 - INFO - __main__ -   test: [batch: 5/196 ] | Loss: 0.453 | Acc: 88.802% (1364/1536)/ 97.526% (1498/1536)
01/04/2023 20:53:58 - INFO - __main__ -   test: [batch: 6/196 ] | Loss: 0.549 | Acc: 86.105% (1543/1792)/ 96.708% (1733/1792)
01/04/2023 20:53:58 - INFO - __main__ -   test: [batch: 7/196 ] | Loss: 0.578 | Acc: 84.912% (1739/2048)/ 96.875% (1984/2048)
01/04/2023 20:53:59 - INFO - __main__ -   test: [batch: 8/196 ] | Loss: 0.614 | Acc: 84.288% (1942/2304)/ 96.181% (2216/2304)
01/04/2023 20:53:59 - INFO - __main__ -   test: [batch: 9/196 ] | Loss: 0.644 | Acc: 83.438% (2136/2560)/ 95.938% (2456/2560)
01/04/2023 20:54:00 - INFO - __main__ -   test: [batch: 10/196 ] | Loss: 0.674 | Acc: 82.848% (2333/2816)/ 95.703% (2695/2816)
01/04/2023 20:54:01 - INFO - __main__ -   test: [batch: 11/196 ] | Loss: 0.717 | Acc: 81.836% (2514/3072)/ 95.443% (2932/3072)
01/04/2023 20:54:01 - INFO - __main__ -   test: [batch: 12/196 ] | Loss: 0.759 | Acc: 80.709% (2686/3328)/ 95.042% (3163/3328)
01/04/2023 20:54:02 - INFO - __main__ -   test: [batch: 13/196 ] | Loss: 0.755 | Acc: 80.469% (2884/3584)/ 95.089% (3408/3584)
01/04/2023 20:54:02 - INFO - __main__ -   test: [batch: 14/196 ] | Loss: 0.757 | Acc: 80.052% (3074/3840)/ 95.260% (3658/3840)
01/04/2023 20:54:03 - INFO - __main__ -   test: [batch: 15/196 ] | Loss: 0.766 | Acc: 80.200% (3285/4096)/ 95.215% (3900/4096)
01/04/2023 20:54:04 - INFO - __main__ -   test: [batch: 16/196 ] | Loss: 0.746 | Acc: 80.676% (3511/4352)/ 95.335% (4149/4352)
01/04/2023 20:54:04 - INFO - __main__ -   test: [batch: 17/196 ] | Loss: 0.719 | Acc: 81.402% (3751/4608)/ 95.443% (4398/4608)
01/04/2023 20:54:05 - INFO - __main__ -   test: [batch: 18/196 ] | Loss: 0.695 | Acc: 82.134% (3995/4864)/ 95.600% (4650/4864)
01/04/2023 20:54:05 - INFO - __main__ -   test: [batch: 19/196 ] | Loss: 0.683 | Acc: 82.324% (4215/5120)/ 95.684% (4899/5120)
01/04/2023 20:54:06 - INFO - __main__ -   test: [batch: 20/196 ] | Loss: 0.684 | Acc: 82.533% (4437/5376)/ 95.517% (5135/5376)
01/04/2023 20:54:06 - INFO - __main__ -   test: [batch: 21/196 ] | Loss: 0.681 | Acc: 82.564% (4650/5632)/ 95.526% (5380/5632)
01/04/2023 20:54:07 - INFO - __main__ -   test: [batch: 22/196 ] | Loss: 0.681 | Acc: 82.779% (4874/5888)/ 95.431% (5619/5888)
01/04/2023 20:54:08 - INFO - __main__ -   test: [batch: 23/196 ] | Loss: 0.685 | Acc: 82.682% (5080/6144)/ 95.508% (5868/6144)
01/04/2023 20:54:08 - INFO - __main__ -   test: [batch: 24/196 ] | Loss: 0.693 | Acc: 82.594% (5286/6400)/ 95.391% (6105/6400)
01/04/2023 20:54:09 - INFO - __main__ -   test: [batch: 25/196 ] | Loss: 0.677 | Acc: 82.963% (5522/6656)/ 95.508% (6357/6656)
01/04/2023 20:54:09 - INFO - __main__ -   test: [batch: 26/196 ] | Loss: 0.665 | Acc: 83.261% (5755/6912)/ 95.602% (6608/6912)
01/04/2023 20:54:10 - INFO - __main__ -   test: [batch: 27/196 ] | Loss: 0.652 | Acc: 83.538% (5988/7168)/ 95.661% (6857/7168)
01/04/2023 20:54:11 - INFO - __main__ -   test: [batch: 28/196 ] | Loss: 0.638 | Acc: 83.930% (6231/7424)/ 95.744% (7108/7424)
01/04/2023 20:54:11 - INFO - __main__ -   test: [batch: 29/196 ] | Loss: 0.638 | Acc: 83.893% (6443/7680)/ 95.755% (7354/7680)
01/04/2023 20:54:12 - INFO - __main__ -   test: [batch: 30/196 ] | Loss: 0.643 | Acc: 83.833% (6653/7936)/ 95.728% (7597/7936)
01/04/2023 20:54:12 - INFO - __main__ -   test: [batch: 31/196 ] | Loss: 0.651 | Acc: 83.655% (6853/8192)/ 95.691% (7839/8192)
01/04/2023 20:54:13 - INFO - __main__ -   test: [batch: 32/196 ] | Loss: 0.672 | Acc: 82.966% (7009/8448)/ 95.549% (8072/8448)
01/04/2023 20:54:14 - INFO - __main__ -   test: [batch: 33/196 ] | Loss: 0.680 | Acc: 82.870% (7213/8704)/ 95.531% (8315/8704)
01/04/2023 20:54:14 - INFO - __main__ -   test: [batch: 34/196 ] | Loss: 0.676 | Acc: 82.935% (7431/8960)/ 95.592% (8565/8960)
01/04/2023 20:54:15 - INFO - __main__ -   test: [batch: 35/196 ] | Loss: 0.676 | Acc: 82.888% (7639/9216)/ 95.605% (8811/9216)
01/04/2023 20:54:15 - INFO - __main__ -   test: [batch: 36/196 ] | Loss: 0.682 | Acc: 82.633% (7827/9472)/ 95.619% (9057/9472)
01/04/2023 20:54:16 - INFO - __main__ -   test: [batch: 37/196 ] | Loss: 0.688 | Acc: 82.442% (8020/9728)/ 95.590% (9299/9728)
01/04/2023 20:54:16 - INFO - __main__ -   test: [batch: 38/196 ] | Loss: 0.688 | Acc: 82.392% (8226/9984)/ 95.613% (9546/9984)
01/04/2023 20:54:17 - INFO - __main__ -   test: [batch: 39/196 ] | Loss: 0.687 | Acc: 82.275% (8425/10240)/ 95.674% (9797/10240)
01/04/2023 20:54:18 - INFO - __main__ -   test: [batch: 40/196 ] | Loss: 0.686 | Acc: 82.279% (8636/10496)/ 95.694% (10044/10496)
01/04/2023 20:54:18 - INFO - __main__ -   test: [batch: 41/196 ] | Loss: 0.685 | Acc: 82.347% (8854/10752)/ 95.675% (10287/10752)
01/04/2023 20:54:19 - INFO - __main__ -   test: [batch: 42/196 ] | Loss: 0.679 | Acc: 82.467% (9078/11008)/ 95.749% (10540/11008)
01/04/2023 20:54:19 - INFO - __main__ -   test: [batch: 43/196 ] | Loss: 0.681 | Acc: 82.449% (9287/11264)/ 95.739% (10784/11264)
01/04/2023 20:54:20 - INFO - __main__ -   test: [batch: 44/196 ] | Loss: 0.687 | Acc: 82.396% (9492/11520)/ 95.686% (11023/11520)
01/04/2023 20:54:21 - INFO - __main__ -   test: [batch: 45/196 ] | Loss: 0.687 | Acc: 82.371% (9700/11776)/ 95.695% (11269/11776)
01/04/2023 20:54:21 - INFO - __main__ -   test: [batch: 46/196 ] | Loss: 0.691 | Acc: 82.173% (9887/12032)/ 95.695% (11514/12032)
01/04/2023 20:54:22 - INFO - __main__ -   test: [batch: 47/196 ] | Loss: 0.691 | Acc: 82.113% (10090/12288)/ 95.711% (11761/12288)
01/04/2023 20:54:22 - INFO - __main__ -   test: [batch: 48/196 ] | Loss: 0.697 | Acc: 81.840% (10266/12544)/ 95.743% (12010/12544)
01/04/2023 20:54:23 - INFO - __main__ -   test: [batch: 49/196 ] | Loss: 0.687 | Acc: 82.086% (10507/12800)/ 95.812% (12264/12800)
01/04/2023 20:54:24 - INFO - __main__ -   test: [batch: 50/196 ] | Loss: 0.683 | Acc: 82.238% (10737/13056)/ 95.856% (12515/13056)
01/04/2023 20:54:24 - INFO - __main__ -   test: [batch: 51/196 ] | Loss: 0.681 | Acc: 82.189% (10941/13312)/ 95.913% (12768/13312)
01/04/2023 20:54:25 - INFO - __main__ -   test: [batch: 52/196 ] | Loss: 0.681 | Acc: 82.098% (11139/13568)/ 95.924% (13015/13568)
01/04/2023 20:54:25 - INFO - __main__ -   test: [batch: 53/196 ] | Loss: 0.686 | Acc: 82.111% (11351/13824)/ 95.891% (13256/13824)
01/04/2023 20:54:26 - INFO - __main__ -   test: [batch: 54/196 ] | Loss: 0.690 | Acc: 81.953% (11539/14080)/ 95.909% (13504/14080)
01/04/2023 20:54:26 - INFO - __main__ -   test: [batch: 55/196 ] | Loss: 0.696 | Acc: 81.717% (11715/14336)/ 95.884% (13746/14336)
01/04/2023 20:54:27 - INFO - __main__ -   test: [batch: 56/196 ] | Loss: 0.695 | Acc: 81.785% (11934/14592)/ 95.902% (13994/14592)
01/04/2023 20:54:28 - INFO - __main__ -   test: [batch: 57/196 ] | Loss: 0.689 | Acc: 81.971% (12171/14848)/ 95.946% (14246/14848)
01/04/2023 20:54:28 - INFO - __main__ -   test: [batch: 58/196 ] | Loss: 0.691 | Acc: 81.965% (12380/15104)/ 95.915% (14487/15104)
01/04/2023 20:54:29 - INFO - __main__ -   test: [batch: 59/196 ] | Loss: 0.692 | Acc: 81.901% (12580/15360)/ 95.951% (14738/15360)
01/04/2023 20:54:29 - INFO - __main__ -   test: [batch: 60/196 ] | Loss: 0.696 | Acc: 81.801% (12774/15616)/ 95.934% (14981/15616)
01/04/2023 20:54:30 - INFO - __main__ -   test: [batch: 61/196 ] | Loss: 0.699 | Acc: 81.754% (12976/15872)/ 95.898% (15221/15872)
01/04/2023 20:54:31 - INFO - __main__ -   test: [batch: 62/196 ] | Loss: 0.693 | Acc: 81.870% (13204/16128)/ 95.939% (15473/16128)
01/04/2023 20:54:31 - INFO - __main__ -   test: [batch: 63/196 ] | Loss: 0.688 | Acc: 82.031% (13440/16384)/ 95.959% (15722/16384)
01/04/2023 20:54:32 - INFO - __main__ -   test: [batch: 64/196 ] | Loss: 0.687 | Acc: 82.037% (13651/16640)/ 95.968% (15969/16640)
01/04/2023 20:54:32 - INFO - __main__ -   test: [batch: 65/196 ] | Loss: 0.684 | Acc: 82.173% (13884/16896)/ 95.993% (16219/16896)
01/04/2023 20:54:33 - INFO - __main__ -   test: [batch: 66/196 ] | Loss: 0.682 | Acc: 82.189% (14097/17152)/ 95.995% (16465/17152)
01/04/2023 20:54:33 - INFO - __main__ -   test: [batch: 67/196 ] | Loss: 0.681 | Acc: 82.209% (14311/17408)/ 96.019% (16715/17408)
01/04/2023 20:54:34 - INFO - __main__ -   test: [batch: 68/196 ] | Loss: 0.677 | Acc: 82.235% (14526/17664)/ 96.071% (16970/17664)
01/04/2023 20:54:35 - INFO - __main__ -   test: [batch: 69/196 ] | Loss: 0.682 | Acc: 82.098% (14712/17920)/ 96.071% (17216/17920)
01/04/2023 20:54:35 - INFO - __main__ -   test: [batch: 70/196 ] | Loss: 0.683 | Acc: 82.070% (14917/18176)/ 96.077% (17463/18176)
01/04/2023 20:54:36 - INFO - __main__ -   test: [batch: 71/196 ] | Loss: 0.681 | Acc: 82.167% (15145/18432)/ 96.077% (17709/18432)
01/04/2023 20:54:36 - INFO - __main__ -   test: [batch: 72/196 ] | Loss: 0.686 | Acc: 82.122% (15347/18688)/ 96.024% (17945/18688)
01/04/2023 20:54:37 - INFO - __main__ -   test: [batch: 73/196 ] | Loss: 0.687 | Acc: 82.121% (15557/18944)/ 95.988% (18184/18944)
01/04/2023 20:54:38 - INFO - __main__ -   test: [batch: 74/196 ] | Loss: 0.693 | Acc: 81.969% (15738/19200)/ 95.953% (18423/19200)
01/04/2023 20:54:38 - INFO - __main__ -   test: [batch: 75/196 ] | Loss: 0.693 | Acc: 81.928% (15940/19456)/ 95.960% (18670/19456)
01/04/2023 20:54:39 - INFO - __main__ -   test: [batch: 76/196 ] | Loss: 0.693 | Acc: 81.981% (16160/19712)/ 95.952% (18914/19712)
01/04/2023 20:54:39 - INFO - __main__ -   test: [batch: 77/196 ] | Loss: 0.693 | Acc: 81.991% (16372/19968)/ 95.933% (19156/19968)
01/04/2023 20:54:40 - INFO - __main__ -   test: [batch: 78/196 ] | Loss: 0.697 | Acc: 81.888% (16561/20224)/ 95.881% (19391/20224)
01/04/2023 20:54:41 - INFO - __main__ -   test: [batch: 79/196 ] | Loss: 0.699 | Acc: 81.816% (16756/20480)/ 95.854% (19631/20480)
01/04/2023 20:54:41 - INFO - __main__ -   test: [batch: 80/196 ] | Loss: 0.709 | Acc: 81.573% (16915/20736)/ 95.756% (19856/20736)
01/04/2023 20:54:42 - INFO - __main__ -   test: [batch: 81/196 ] | Loss: 0.717 | Acc: 81.426% (17093/20992)/ 95.675% (20084/20992)
01/04/2023 20:54:42 - INFO - __main__ -   test: [batch: 82/196 ] | Loss: 0.719 | Acc: 81.344% (17284/21248)/ 95.689% (20332/21248)
01/04/2023 20:54:43 - INFO - __main__ -   test: [batch: 83/196 ] | Loss: 0.721 | Acc: 81.287% (17480/21504)/ 95.657% (20570/21504)
01/04/2023 20:54:43 - INFO - __main__ -   test: [batch: 84/196 ] | Loss: 0.722 | Acc: 81.255% (17681/21760)/ 95.625% (20808/21760)
01/04/2023 20:54:44 - INFO - __main__ -   test: [batch: 85/196 ] | Loss: 0.730 | Acc: 81.082% (17851/22016)/ 95.558% (21038/22016)
01/04/2023 20:54:45 - INFO - __main__ -   test: [batch: 86/196 ] | Loss: 0.733 | Acc: 80.985% (18037/22272)/ 95.537% (21278/22272)
01/04/2023 20:54:45 - INFO - __main__ -   test: [batch: 87/196 ] | Loss: 0.736 | Acc: 80.948% (18236/22528)/ 95.494% (21513/22528)
01/04/2023 20:54:46 - INFO - __main__ -   test: [batch: 88/196 ] | Loss: 0.740 | Acc: 80.868% (18425/22784)/ 95.435% (21744/22784)
01/04/2023 20:54:46 - INFO - __main__ -   test: [batch: 89/196 ] | Loss: 0.745 | Acc: 80.794% (18615/23040)/ 95.382% (21976/23040)
01/04/2023 20:54:47 - INFO - __main__ -   test: [batch: 90/196 ] | Loss: 0.757 | Acc: 80.533% (18761/23296)/ 95.291% (22199/23296)
01/04/2023 20:54:48 - INFO - __main__ -   test: [batch: 91/196 ] | Loss: 0.763 | Acc: 80.431% (18943/23552)/ 95.202% (22422/23552)
01/04/2023 20:54:48 - INFO - __main__ -   test: [batch: 92/196 ] | Loss: 0.762 | Acc: 80.465% (19157/23808)/ 95.182% (22661/23808)
01/04/2023 20:54:49 - INFO - __main__ -   test: [batch: 93/196 ] | Loss: 0.769 | Acc: 80.311% (19326/24064)/ 95.125% (22891/24064)
01/04/2023 20:54:49 - INFO - __main__ -   test: [batch: 94/196 ] | Loss: 0.774 | Acc: 80.115% (19484/24320)/ 95.103% (23129/24320)
01/04/2023 20:54:50 - INFO - __main__ -   test: [batch: 95/196 ] | Loss: 0.781 | Acc: 79.997% (19660/24576)/ 95.056% (23361/24576)
01/04/2023 20:54:50 - INFO - __main__ -   test: [batch: 96/196 ] | Loss: 0.787 | Acc: 79.897% (19840/24832)/ 94.958% (23580/24832)
01/04/2023 20:54:51 - INFO - __main__ -   test: [batch: 97/196 ] | Loss: 0.795 | Acc: 79.767% (20012/25088)/ 94.886% (23805/25088)
01/04/2023 20:54:52 - INFO - __main__ -   test: [batch: 98/196 ] | Loss: 0.803 | Acc: 79.597% (20173/25344)/ 94.831% (24034/25344)
01/04/2023 20:54:52 - INFO - __main__ -   test: [batch: 99/196 ] | Loss: 0.805 | Acc: 79.551% (20365/25600)/ 94.797% (24268/25600)
01/04/2023 20:54:53 - INFO - __main__ -   test: [batch: 100/196 ] | Loss: 0.808 | Acc: 79.425% (20536/25856)/ 94.756% (24500/25856)
01/04/2023 20:54:53 - INFO - __main__ -   test: [batch: 101/196 ] | Loss: 0.810 | Acc: 79.400% (20733/26112)/ 94.730% (24736/26112)
01/04/2023 20:54:54 - INFO - __main__ -   test: [batch: 102/196 ] | Loss: 0.816 | Acc: 79.225% (20890/26368)/ 94.679% (24965/26368)
01/04/2023 20:54:55 - INFO - __main__ -   test: [batch: 103/196 ] | Loss: 0.822 | Acc: 79.124% (21066/26624)/ 94.629% (25194/26624)
01/04/2023 20:54:55 - INFO - __main__ -   test: [batch: 104/196 ] | Loss: 0.823 | Acc: 79.089% (21259/26880)/ 94.621% (25434/26880)
01/04/2023 20:54:56 - INFO - __main__ -   test: [batch: 105/196 ] | Loss: 0.827 | Acc: 79.028% (21445/27136)/ 94.568% (25662/27136)
01/04/2023 20:54:56 - INFO - __main__ -   test: [batch: 106/196 ] | Loss: 0.828 | Acc: 78.976% (21633/27392)/ 94.550% (25899/27392)
01/04/2023 20:54:57 - INFO - __main__ -   test: [batch: 107/196 ] | Loss: 0.829 | Acc: 78.971% (21834/27648)/ 94.542% (26139/27648)
01/04/2023 20:54:58 - INFO - __main__ -   test: [batch: 108/196 ] | Loss: 0.832 | Acc: 78.953% (22031/27904)/ 94.506% (26371/27904)
01/04/2023 20:54:58 - INFO - __main__ -   test: [batch: 109/196 ] | Loss: 0.833 | Acc: 78.917% (22223/28160)/ 94.474% (26604/28160)
01/04/2023 20:54:59 - INFO - __main__ -   test: [batch: 110/196 ] | Loss: 0.834 | Acc: 78.903% (22421/28416)/ 94.482% (26848/28416)
01/04/2023 20:54:59 - INFO - __main__ -   test: [batch: 111/196 ] | Loss: 0.832 | Acc: 78.952% (22637/28672)/ 94.482% (27090/28672)
01/04/2023 20:55:00 - INFO - __main__ -   test: [batch: 112/196 ] | Loss: 0.831 | Acc: 78.972% (22845/28928)/ 94.486% (27333/28928)
01/04/2023 20:55:00 - INFO - __main__ -   test: [batch: 113/196 ] | Loss: 0.833 | Acc: 78.930% (23035/29184)/ 94.473% (27571/29184)
01/04/2023 20:55:01 - INFO - __main__ -   test: [batch: 114/196 ] | Loss: 0.841 | Acc: 78.798% (23198/29440)/ 94.368% (27782/29440)
01/04/2023 20:55:02 - INFO - __main__ -   test: [batch: 115/196 ] | Loss: 0.844 | Acc: 78.724% (23378/29696)/ 94.319% (28009/29696)
01/04/2023 20:55:02 - INFO - __main__ -   test: [batch: 116/196 ] | Loss: 0.845 | Acc: 78.693% (23570/29952)/ 94.314% (28249/29952)
01/04/2023 20:55:03 - INFO - __main__ -   test: [batch: 117/196 ] | Loss: 0.851 | Acc: 78.562% (23732/30208)/ 94.240% (28468/30208)
01/04/2023 20:55:03 - INFO - __main__ -   test: [batch: 118/196 ] | Loss: 0.850 | Acc: 78.611% (23948/30464)/ 94.216% (28702/30464)
01/04/2023 20:55:04 - INFO - __main__ -   test: [batch: 119/196 ] | Loss: 0.848 | Acc: 78.688% (24173/30720)/ 94.229% (28947/30720)
01/04/2023 20:55:05 - INFO - __main__ -   test: [batch: 120/196 ] | Loss: 0.851 | Acc: 78.622% (24354/30976)/ 94.183% (29174/30976)
01/04/2023 20:55:05 - INFO - __main__ -   test: [batch: 121/196 ] | Loss: 0.859 | Acc: 78.391% (24483/31232)/ 94.102% (29390/31232)
01/04/2023 20:55:06 - INFO - __main__ -   test: [batch: 122/196 ] | Loss: 0.859 | Acc: 78.408% (24689/31488)/ 94.083% (29625/31488)
01/04/2023 20:55:06 - INFO - __main__ -   test: [batch: 123/196 ] | Loss: 0.865 | Acc: 78.283% (24850/31744)/ 94.008% (29842/31744)
01/04/2023 20:55:07 - INFO - __main__ -   test: [batch: 124/196 ] | Loss: 0.868 | Acc: 78.109% (24995/32000)/ 93.987% (30076/32000)
01/04/2023 20:55:08 - INFO - __main__ -   test: [batch: 125/196 ] | Loss: 0.869 | Acc: 78.103% (25193/32256)/ 93.976% (30313/32256)
01/04/2023 20:55:08 - INFO - __main__ -   test: [batch: 126/196 ] | Loss: 0.869 | Acc: 78.103% (25393/32512)/ 93.950% (30545/32512)
01/04/2023 20:55:09 - INFO - __main__ -   test: [batch: 127/196 ] | Loss: 0.875 | Acc: 78.003% (25560/32768)/ 93.896% (30768/32768)
01/04/2023 20:55:09 - INFO - __main__ -   test: [batch: 128/196 ] | Loss: 0.881 | Acc: 77.865% (25714/33024)/ 93.850% (30993/33024)
01/04/2023 20:55:10 - INFO - __main__ -   test: [batch: 129/196 ] | Loss: 0.886 | Acc: 77.737% (25871/33280)/ 93.825% (31225/33280)
01/04/2023 20:55:10 - INFO - __main__ -   test: [batch: 130/196 ] | Loss: 0.884 | Acc: 77.752% (26075/33536)/ 93.836% (31469/33536)
01/04/2023 20:55:11 - INFO - __main__ -   test: [batch: 131/196 ] | Loss: 0.886 | Acc: 77.705% (26258/33792)/ 93.818% (31703/33792)
01/04/2023 20:55:12 - INFO - __main__ -   test: [batch: 132/196 ] | Loss: 0.891 | Acc: 77.679% (26448/34048)/ 93.756% (31922/34048)
01/04/2023 20:55:12 - INFO - __main__ -   test: [batch: 133/196 ] | Loss: 0.892 | Acc: 77.638% (26633/34304)/ 93.738% (32156/34304)
01/04/2023 20:55:13 - INFO - __main__ -   test: [batch: 134/196 ] | Loss: 0.893 | Acc: 77.595% (26817/34560)/ 93.715% (32388/34560)
01/04/2023 20:55:13 - INFO - __main__ -   test: [batch: 135/196 ] | Loss: 0.897 | Acc: 77.548% (26999/34816)/ 93.678% (32615/34816)
01/04/2023 20:55:14 - INFO - __main__ -   test: [batch: 136/196 ] | Loss: 0.898 | Acc: 77.523% (27189/35072)/ 93.662% (32849/35072)
01/04/2023 20:55:15 - INFO - __main__ -   test: [batch: 137/196 ] | Loss: 0.900 | Acc: 77.491% (27376/35328)/ 93.662% (33089/35328)
01/04/2023 20:55:15 - INFO - __main__ -   test: [batch: 138/196 ] | Loss: 0.901 | Acc: 77.425% (27551/35584)/ 93.652% (33325/35584)
01/04/2023 20:55:16 - INFO - __main__ -   test: [batch: 139/196 ] | Loss: 0.901 | Acc: 77.441% (27755/35840)/ 93.647% (33563/35840)
01/04/2023 20:55:16 - INFO - __main__ -   test: [batch: 140/196 ] | Loss: 0.902 | Acc: 77.430% (27949/36096)/ 93.639% (33800/36096)
01/04/2023 20:55:17 - INFO - __main__ -   test: [batch: 141/196 ] | Loss: 0.902 | Acc: 77.432% (28148/36352)/ 93.640% (34040/36352)
01/04/2023 20:55:18 - INFO - __main__ -   test: [batch: 142/196 ] | Loss: 0.908 | Acc: 77.341% (28313/36608)/ 93.567% (34253/36608)
01/04/2023 20:55:18 - INFO - __main__ -   test: [batch: 143/196 ] | Loss: 0.909 | Acc: 77.306% (28498/36864)/ 93.547% (34485/36864)
01/04/2023 20:55:19 - INFO - __main__ -   test: [batch: 144/196 ] | Loss: 0.910 | Acc: 77.295% (28692/37120)/ 93.518% (34714/37120)
01/04/2023 20:55:19 - INFO - __main__ -   test: [batch: 145/196 ] | Loss: 0.912 | Acc: 77.231% (28866/37376)/ 93.515% (34952/37376)
01/04/2023 20:55:20 - INFO - __main__ -   test: [batch: 146/196 ] | Loss: 0.913 | Acc: 77.176% (29043/37632)/ 93.487% (35181/37632)
01/04/2023 20:55:20 - INFO - __main__ -   test: [batch: 147/196 ] | Loss: 0.913 | Acc: 77.191% (29246/37888)/ 93.489% (35421/37888)
01/04/2023 20:55:21 - INFO - __main__ -   test: [batch: 148/196 ] | Loss: 0.916 | Acc: 77.171% (29436/38144)/ 93.459% (35649/38144)
01/04/2023 20:55:22 - INFO - __main__ -   test: [batch: 149/196 ] | Loss: 0.919 | Acc: 77.128% (29617/38400)/ 93.414% (35871/38400)
01/04/2023 20:55:22 - INFO - __main__ -   test: [batch: 150/196 ] | Loss: 0.920 | Acc: 77.116% (29810/38656)/ 93.396% (36103/38656)
01/04/2023 20:55:23 - INFO - __main__ -   test: [batch: 151/196 ] | Loss: 0.922 | Acc: 77.058% (29985/38912)/ 93.372% (36333/38912)
01/04/2023 20:55:23 - INFO - __main__ -   test: [batch: 152/196 ] | Loss: 0.922 | Acc: 77.027% (30170/39168)/ 93.375% (36573/39168)
01/04/2023 20:55:24 - INFO - __main__ -   test: [batch: 153/196 ] | Loss: 0.924 | Acc: 77.006% (30359/39424)/ 93.334% (36796/39424)
01/04/2023 20:55:25 - INFO - __main__ -   test: [batch: 154/196 ] | Loss: 0.926 | Acc: 76.986% (30548/39680)/ 93.311% (37026/39680)
01/04/2023 20:55:25 - INFO - __main__ -   test: [batch: 155/196 ] | Loss: 0.928 | Acc: 76.953% (30732/39936)/ 93.287% (37255/39936)
01/04/2023 20:55:26 - INFO - __main__ -   test: [batch: 156/196 ] | Loss: 0.927 | Acc: 76.978% (30939/40192)/ 93.285% (37493/40192)
01/04/2023 20:55:26 - INFO - __main__ -   test: [batch: 157/196 ] | Loss: 0.929 | Acc: 76.948% (31124/40448)/ 93.268% (37725/40448)
01/04/2023 20:55:27 - INFO - __main__ -   test: [batch: 158/196 ] | Loss: 0.933 | Acc: 76.801% (31261/40704)/ 93.219% (37944/40704)
01/04/2023 20:55:27 - INFO - __main__ -   test: [batch: 159/196 ] | Loss: 0.935 | Acc: 76.775% (31447/40960)/ 93.203% (38176/40960)
01/04/2023 20:55:28 - INFO - __main__ -   test: [batch: 160/196 ] | Loss: 0.933 | Acc: 76.825% (31664/41216)/ 93.207% (38416/41216)
01/04/2023 20:55:29 - INFO - __main__ -   test: [batch: 161/196 ] | Loss: 0.937 | Acc: 76.758% (31833/41472)/ 93.179% (38643/41472)
01/04/2023 20:55:29 - INFO - __main__ -   test: [batch: 162/196 ] | Loss: 0.937 | Acc: 76.759% (32030/41728)/ 93.170% (38878/41728)
01/04/2023 20:55:30 - INFO - __main__ -   test: [batch: 163/196 ] | Loss: 0.943 | Acc: 76.598% (32159/41984)/ 93.114% (39093/41984)
01/04/2023 20:55:30 - INFO - __main__ -   test: [batch: 164/196 ] | Loss: 0.945 | Acc: 76.541% (32331/42240)/ 93.089% (39321/42240)
01/04/2023 20:55:31 - INFO - __main__ -   test: [batch: 165/196 ] | Loss: 0.947 | Acc: 76.464% (32494/42496)/ 93.075% (39553/42496)
01/04/2023 20:55:32 - INFO - __main__ -   test: [batch: 166/196 ] | Loss: 0.946 | Acc: 76.483% (32698/42752)/ 93.081% (39794/42752)
01/04/2023 20:55:32 - INFO - __main__ -   test: [batch: 167/196 ] | Loss: 0.949 | Acc: 76.430% (32871/43008)/ 93.045% (40017/43008)
01/04/2023 20:55:33 - INFO - __main__ -   test: [batch: 168/196 ] | Loss: 0.950 | Acc: 76.380% (33045/43264)/ 93.024% (40246/43264)
01/04/2023 20:55:33 - INFO - __main__ -   test: [batch: 169/196 ] | Loss: 0.953 | Acc: 76.324% (33216/43520)/ 93.003% (40475/43520)
01/04/2023 20:55:34 - INFO - __main__ -   test: [batch: 170/196 ] | Loss: 0.951 | Acc: 76.352% (33424/43776)/ 93.030% (40725/43776)
01/04/2023 20:55:35 - INFO - __main__ -   test: [batch: 171/196 ] | Loss: 0.952 | Acc: 76.297% (33595/44032)/ 93.028% (40962/44032)
01/04/2023 20:55:35 - INFO - __main__ -   test: [batch: 172/196 ] | Loss: 0.956 | Acc: 76.224% (33758/44288)/ 92.982% (41180/44288)
01/04/2023 20:55:36 - INFO - __main__ -   test: [batch: 173/196 ] | Loss: 0.957 | Acc: 76.221% (33952/44544)/ 92.980% (41417/44544)
01/04/2023 20:55:36 - INFO - __main__ -   test: [batch: 174/196 ] | Loss: 0.958 | Acc: 76.203% (34139/44800)/ 92.960% (41646/44800)
01/04/2023 20:55:37 - INFO - __main__ -   test: [batch: 175/196 ] | Loss: 0.960 | Acc: 76.156% (34313/45056)/ 92.942% (41876/45056)
01/04/2023 20:55:37 - INFO - __main__ -   test: [batch: 176/196 ] | Loss: 0.962 | Acc: 76.112% (34488/45312)/ 92.905% (42097/45312)
01/04/2023 20:55:38 - INFO - __main__ -   test: [batch: 177/196 ] | Loss: 0.967 | Acc: 75.983% (34624/45568)/ 92.879% (42323/45568)
01/04/2023 20:55:39 - INFO - __main__ -   test: [batch: 178/196 ] | Loss: 0.966 | Acc: 76.013% (34832/45824)/ 92.895% (42568/45824)
01/04/2023 20:55:39 - INFO - __main__ -   test: [batch: 179/196 ] | Loss: 0.966 | Acc: 76.046% (35042/46080)/ 92.895% (42806/46080)
01/04/2023 20:55:40 - INFO - __main__ -   test: [batch: 180/196 ] | Loss: 0.967 | Acc: 76.014% (35222/46336)/ 92.893% (43043/46336)
01/04/2023 20:55:40 - INFO - __main__ -   test: [batch: 181/196 ] | Loss: 0.967 | Acc: 76.002% (35411/46592)/ 92.898% (43283/46592)
01/04/2023 20:55:41 - INFO - __main__ -   test: [batch: 182/196 ] | Loss: 0.966 | Acc: 76.044% (35625/46848)/ 92.913% (43528/46848)
01/04/2023 20:55:42 - INFO - __main__ -   test: [batch: 183/196 ] | Loss: 0.963 | Acc: 76.102% (35847/47104)/ 92.943% (43780/47104)
01/04/2023 20:55:42 - INFO - __main__ -   test: [batch: 184/196 ] | Loss: 0.962 | Acc: 76.123% (36052/47360)/ 92.958% (44025/47360)
01/04/2023 20:55:43 - INFO - __main__ -   test: [batch: 185/196 ] | Loss: 0.962 | Acc: 76.117% (36244/47616)/ 92.962% (44265/47616)
01/04/2023 20:55:43 - INFO - __main__ -   test: [batch: 186/196 ] | Loss: 0.959 | Acc: 76.201% (36479/47872)/ 92.985% (44514/47872)
01/04/2023 20:55:44 - INFO - __main__ -   test: [batch: 187/196 ] | Loss: 0.960 | Acc: 76.168% (36658/48128)/ 92.956% (44738/48128)
01/04/2023 20:55:44 - INFO - __main__ -   test: [batch: 188/196 ] | Loss: 0.961 | Acc: 76.153% (36846/48384)/ 92.936% (44966/48384)
01/04/2023 20:55:45 - INFO - __main__ -   test: [batch: 189/196 ] | Loss: 0.965 | Acc: 76.063% (36997/48640)/ 92.901% (45187/48640)
01/04/2023 20:55:46 - INFO - __main__ -   test: [batch: 190/196 ] | Loss: 0.966 | Acc: 76.016% (37169/48896)/ 92.905% (45427/48896)
01/04/2023 20:55:46 - INFO - __main__ -   test: [batch: 191/196 ] | Loss: 0.968 | Acc: 75.993% (37352/49152)/ 92.891% (45658/49152)
01/04/2023 20:55:47 - INFO - __main__ -   test: [batch: 192/196 ] | Loss: 0.965 | Acc: 76.044% (37572/49408)/ 92.912% (45906/49408)
01/04/2023 20:55:47 - INFO - __main__ -   test: [batch: 193/196 ] | Loss: 0.961 | Acc: 76.132% (37810/49664)/ 92.939% (46157/49664)
01/04/2023 20:55:48 - INFO - __main__ -   test: [batch: 194/196 ] | Loss: 0.959 | Acc: 76.182% (38030/49920)/ 92.953% (46402/49920)
01/04/2023 20:55:48 - INFO - __main__ -   test: [batch: 195/196 ] | Loss: 0.963 | Acc: 76.134% (38067/50000)/ 92.942% (46471/50000)
01/04/2023 20:55:48 - INFO - __main__ -   Final accuracy: 76.134
01/04/2023 20:55:48 - INFO - __main__ -   {'milestones': Counter({2: 1, 3: 1, 4: 1}), 'gamma': 0.1, 'base_lrs': [0.0005], 'last_epoch': 3, '_step_count': 4, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [5e-06]}
01/04/2023 20:55:48 - INFO - __main__ -   
Epoch: 3
01/04/2023 20:55:49 - INFO - __main__ -   test: [epoch: 3 | batch: 0/5005 ] | Loss: 0.745 | Acc: 82.031% (210/256)
01/04/2023 20:56:50 - INFO - __main__ -   test: [epoch: 3 | batch: 100/5005 ] | Loss: 0.833 | Acc: 79.301% (20504/25856)
01/04/2023 20:57:50 - INFO - __main__ -   test: [epoch: 3 | batch: 200/5005 ] | Loss: 0.847 | Acc: 78.943% (40621/51456)
01/04/2023 20:58:51 - INFO - __main__ -   test: [epoch: 3 | batch: 300/5005 ] | Loss: 0.851 | Acc: 78.783% (60707/77056)
01/04/2023 20:59:52 - INFO - __main__ -   test: [epoch: 3 | batch: 400/5005 ] | Loss: 0.850 | Acc: 78.763% (80855/102656)
01/04/2023 21:00:52 - INFO - __main__ -   test: [epoch: 3 | batch: 500/5005 ] | Loss: 0.849 | Acc: 78.799% (101065/128256)
01/04/2023 21:01:53 - INFO - __main__ -   test: [epoch: 3 | batch: 600/5005 ] | Loss: 0.851 | Acc: 78.776% (121201/153856)
01/04/2023 21:02:54 - INFO - __main__ -   test: [epoch: 3 | batch: 700/5005 ] | Loss: 0.847 | Acc: 78.851% (141502/179456)
01/04/2023 21:03:55 - INFO - __main__ -   test: [epoch: 3 | batch: 800/5005 ] | Loss: 0.849 | Acc: 78.791% (161566/205056)
01/04/2023 21:04:55 - INFO - __main__ -   test: [epoch: 3 | batch: 900/5005 ] | Loss: 0.848 | Acc: 78.832% (181830/230656)
01/04/2023 21:05:56 - INFO - __main__ -   test: [epoch: 3 | batch: 1000/5005 ] | Loss: 0.848 | Acc: 78.840% (202033/256256)
01/04/2023 21:06:57 - INFO - __main__ -   test: [epoch: 3 | batch: 1100/5005 ] | Loss: 0.848 | Acc: 78.833% (222196/281856)
01/04/2023 21:07:58 - INFO - __main__ -   test: [epoch: 3 | batch: 1200/5005 ] | Loss: 0.848 | Acc: 78.861% (242462/307456)
01/04/2023 21:08:58 - INFO - __main__ -   test: [epoch: 3 | batch: 1300/5005 ] | Loss: 0.848 | Acc: 78.869% (262678/333056)
01/04/2023 21:09:59 - INFO - __main__ -   test: [epoch: 3 | batch: 1400/5005 ] | Loss: 0.847 | Acc: 78.871% (282874/358656)
01/04/2023 21:11:00 - INFO - __main__ -   test: [epoch: 3 | batch: 1500/5005 ] | Loss: 0.847 | Acc: 78.878% (303095/384256)
01/04/2023 21:12:00 - INFO - __main__ -   test: [epoch: 3 | batch: 1600/5005 ] | Loss: 0.848 | Acc: 78.868% (323244/409856)
01/04/2023 21:13:01 - INFO - __main__ -   test: [epoch: 3 | batch: 1700/5005 ] | Loss: 0.849 | Acc: 78.858% (343394/435456)
01/04/2023 21:14:02 - INFO - __main__ -   test: [epoch: 3 | batch: 1800/5005 ] | Loss: 0.849 | Acc: 78.865% (363610/461056)
01/04/2023 21:15:03 - INFO - __main__ -   test: [epoch: 3 | batch: 1900/5005 ] | Loss: 0.849 | Acc: 78.866% (383808/486656)
01/04/2023 21:16:03 - INFO - __main__ -   test: [epoch: 3 | batch: 2000/5005 ] | Loss: 0.848 | Acc: 78.878% (404057/512256)
01/04/2023 21:17:04 - INFO - __main__ -   test: [epoch: 3 | batch: 2100/5005 ] | Loss: 0.848 | Acc: 78.881% (424268/537856)
01/04/2023 21:18:05 - INFO - __main__ -   test: [epoch: 3 | batch: 2200/5005 ] | Loss: 0.848 | Acc: 78.877% (444435/563456)
01/04/2023 21:19:05 - INFO - __main__ -   test: [epoch: 3 | batch: 2300/5005 ] | Loss: 0.848 | Acc: 78.867% (464568/589056)
01/04/2023 21:20:06 - INFO - __main__ -   test: [epoch: 3 | batch: 2400/5005 ] | Loss: 0.848 | Acc: 78.877% (484822/614656)
01/04/2023 21:21:07 - INFO - __main__ -   test: [epoch: 3 | batch: 2500/5005 ] | Loss: 0.848 | Acc: 78.890% (505099/640256)
01/04/2023 21:22:08 - INFO - __main__ -   test: [epoch: 3 | batch: 2600/5005 ] | Loss: 0.848 | Acc: 78.883% (525244/665856)
01/04/2023 21:23:08 - INFO - __main__ -   test: [epoch: 3 | batch: 2700/5005 ] | Loss: 0.849 | Acc: 78.878% (545408/691456)
01/04/2023 21:24:09 - INFO - __main__ -   test: [epoch: 3 | batch: 2800/5005 ] | Loss: 0.849 | Acc: 78.881% (565624/717056)
01/04/2023 21:25:10 - INFO - __main__ -   test: [epoch: 3 | batch: 2900/5005 ] | Loss: 0.848 | Acc: 78.890% (585879/742656)
01/04/2023 21:26:11 - INFO - __main__ -   test: [epoch: 3 | batch: 3000/5005 ] | Loss: 0.848 | Acc: 78.893% (606101/768256)
01/04/2023 21:27:11 - INFO - __main__ -   test: [epoch: 3 | batch: 3100/5005 ] | Loss: 0.849 | Acc: 78.889% (626264/793856)
01/04/2023 21:28:12 - INFO - __main__ -   test: [epoch: 3 | batch: 3200/5005 ] | Loss: 0.849 | Acc: 78.882% (646405/819456)
01/04/2023 21:29:13 - INFO - __main__ -   test: [epoch: 3 | batch: 3300/5005 ] | Loss: 0.848 | Acc: 78.887% (666640/845056)
01/04/2023 21:30:14 - INFO - __main__ -   test: [epoch: 3 | batch: 3400/5005 ] | Loss: 0.848 | Acc: 78.881% (686781/870656)
01/04/2023 21:31:14 - INFO - __main__ -   test: [epoch: 3 | batch: 3500/5005 ] | Loss: 0.848 | Acc: 78.881% (706976/896256)
01/04/2023 21:32:15 - INFO - __main__ -   test: [epoch: 3 | batch: 3600/5005 ] | Loss: 0.848 | Acc: 78.875% (727117/921856)
01/04/2023 21:33:16 - INFO - __main__ -   test: [epoch: 3 | batch: 3700/5005 ] | Loss: 0.848 | Acc: 78.874% (747301/947456)
01/04/2023 21:34:16 - INFO - __main__ -   test: [epoch: 3 | batch: 3800/5005 ] | Loss: 0.848 | Acc: 78.876% (767512/973056)
01/04/2023 21:35:17 - INFO - __main__ -   test: [epoch: 3 | batch: 3900/5005 ] | Loss: 0.848 | Acc: 78.864% (787585/998656)
01/04/2023 21:36:18 - INFO - __main__ -   test: [epoch: 3 | batch: 4000/5005 ] | Loss: 0.848 | Acc: 78.870% (807826/1024256)
01/04/2023 21:37:18 - INFO - __main__ -   test: [epoch: 3 | batch: 4100/5005 ] | Loss: 0.848 | Acc: 78.874% (828063/1049856)
01/04/2023 21:38:19 - INFO - __main__ -   test: [epoch: 3 | batch: 4200/5005 ] | Loss: 0.848 | Acc: 78.877% (848290/1075456)
01/04/2023 21:39:20 - INFO - __main__ -   test: [epoch: 3 | batch: 4300/5005 ] | Loss: 0.848 | Acc: 78.874% (868448/1101056)
01/04/2023 21:40:21 - INFO - __main__ -   test: [epoch: 3 | batch: 4400/5005 ] | Loss: 0.848 | Acc: 78.886% (888769/1126656)
01/04/2023 21:41:21 - INFO - __main__ -   test: [epoch: 3 | batch: 4500/5005 ] | Loss: 0.848 | Acc: 78.889% (908998/1152256)
01/04/2023 21:42:22 - INFO - __main__ -   test: [epoch: 3 | batch: 4600/5005 ] | Loss: 0.848 | Acc: 78.881% (929110/1177856)
01/04/2023 21:43:23 - INFO - __main__ -   test: [epoch: 3 | batch: 4700/5005 ] | Loss: 0.848 | Acc: 78.882% (949315/1203456)
01/04/2023 21:44:23 - INFO - __main__ -   test: [epoch: 3 | batch: 4800/5005 ] | Loss: 0.848 | Acc: 78.886% (969552/1229056)
01/04/2023 21:45:24 - INFO - __main__ -   test: [epoch: 3 | batch: 4900/5005 ] | Loss: 0.848 | Acc: 78.883% (989707/1254656)
01/04/2023 21:46:25 - INFO - __main__ -   test: [epoch: 3 | batch: 5000/5005 ] | Loss: 0.848 | Acc: 78.884% (1009921/1280256)
01/04/2023 21:46:27 - INFO - __main__ -   Saving Checkpoint
01/04/2023 21:46:28 - INFO - __main__ -   test: [batch: 0/196 ] | Loss: 0.456 | Acc: 86.719% (222/256)/ 98.047% (251/256)
01/04/2023 21:46:28 - INFO - __main__ -   test: [batch: 1/196 ] | Loss: 0.564 | Acc: 84.570% (433/512)/ 96.484% (494/512)
01/04/2023 21:46:29 - INFO - __main__ -   test: [batch: 2/196 ] | Loss: 0.425 | Acc: 88.281% (678/768)/ 97.656% (750/768)
01/04/2023 21:46:30 - INFO - __main__ -   test: [batch: 3/196 ] | Loss: 0.404 | Acc: 89.355% (915/1024)/ 97.656% (1000/1024)
01/04/2023 21:46:30 - INFO - __main__ -   test: [batch: 4/196 ] | Loss: 0.401 | Acc: 89.844% (1150/1280)/ 97.656% (1250/1280)
01/04/2023 21:46:31 - INFO - __main__ -   test: [batch: 5/196 ] | Loss: 0.454 | Acc: 88.737% (1363/1536)/ 97.461% (1497/1536)
01/04/2023 21:46:31 - INFO - __main__ -   test: [batch: 6/196 ] | Loss: 0.552 | Acc: 86.049% (1542/1792)/ 96.652% (1732/1792)
01/04/2023 21:46:32 - INFO - __main__ -   test: [batch: 7/196 ] | Loss: 0.580 | Acc: 84.766% (1736/2048)/ 96.826% (1983/2048)
01/04/2023 21:46:33 - INFO - __main__ -   test: [batch: 8/196 ] | Loss: 0.616 | Acc: 84.201% (1940/2304)/ 96.224% (2217/2304)
01/04/2023 21:46:33 - INFO - __main__ -   test: [batch: 9/196 ] | Loss: 0.645 | Acc: 83.398% (2135/2560)/ 96.016% (2458/2560)
01/04/2023 21:46:34 - INFO - __main__ -   test: [batch: 10/196 ] | Loss: 0.674 | Acc: 82.777% (2331/2816)/ 95.739% (2696/2816)
01/04/2023 21:46:34 - INFO - __main__ -   test: [batch: 11/196 ] | Loss: 0.718 | Acc: 81.836% (2514/3072)/ 95.475% (2933/3072)
01/04/2023 21:46:35 - INFO - __main__ -   test: [batch: 12/196 ] | Loss: 0.760 | Acc: 80.709% (2686/3328)/ 95.012% (3162/3328)
01/04/2023 21:46:35 - INFO - __main__ -   test: [batch: 13/196 ] | Loss: 0.755 | Acc: 80.497% (2885/3584)/ 95.061% (3407/3584)
01/04/2023 21:46:36 - INFO - __main__ -   test: [batch: 14/196 ] | Loss: 0.757 | Acc: 80.052% (3074/3840)/ 95.234% (3657/3840)
01/04/2023 21:46:37 - INFO - __main__ -   test: [batch: 15/196 ] | Loss: 0.767 | Acc: 80.200% (3285/4096)/ 95.166% (3898/4096)
01/04/2023 21:46:37 - INFO - __main__ -   test: [batch: 16/196 ] | Loss: 0.746 | Acc: 80.699% (3512/4352)/ 95.312% (4148/4352)
01/04/2023 21:46:38 - INFO - __main__ -   test: [batch: 17/196 ] | Loss: 0.720 | Acc: 81.424% (3752/4608)/ 95.421% (4397/4608)
01/04/2023 21:46:38 - INFO - __main__ -   test: [batch: 18/196 ] | Loss: 0.695 | Acc: 82.155% (3996/4864)/ 95.580% (4649/4864)
01/04/2023 21:46:39 - INFO - __main__ -   test: [batch: 19/196 ] | Loss: 0.684 | Acc: 82.344% (4216/5120)/ 95.684% (4899/5120)
01/04/2023 21:46:40 - INFO - __main__ -   test: [batch: 20/196 ] | Loss: 0.685 | Acc: 82.515% (4436/5376)/ 95.517% (5135/5376)
01/04/2023 21:46:40 - INFO - __main__ -   test: [batch: 21/196 ] | Loss: 0.682 | Acc: 82.599% (4652/5632)/ 95.543% (5381/5632)
01/04/2023 21:46:41 - INFO - __main__ -   test: [batch: 22/196 ] | Loss: 0.683 | Acc: 82.812% (4876/5888)/ 95.448% (5620/5888)
01/04/2023 21:46:41 - INFO - __main__ -   test: [batch: 23/196 ] | Loss: 0.687 | Acc: 82.699% (5081/6144)/ 95.540% (5870/6144)
01/04/2023 21:46:42 - INFO - __main__ -   test: [batch: 24/196 ] | Loss: 0.694 | Acc: 82.578% (5285/6400)/ 95.438% (6108/6400)
01/04/2023 21:46:43 - INFO - __main__ -   test: [batch: 25/196 ] | Loss: 0.678 | Acc: 82.948% (5521/6656)/ 95.553% (6360/6656)
01/04/2023 21:46:43 - INFO - __main__ -   test: [batch: 26/196 ] | Loss: 0.666 | Acc: 83.247% (5754/6912)/ 95.645% (6611/6912)
01/04/2023 21:46:44 - INFO - __main__ -   test: [batch: 27/196 ] | Loss: 0.653 | Acc: 83.538% (5988/7168)/ 95.703% (6860/7168)
01/04/2023 21:46:44 - INFO - __main__ -   test: [batch: 28/196 ] | Loss: 0.638 | Acc: 83.917% (6230/7424)/ 95.797% (7112/7424)
01/04/2023 21:46:45 - INFO - __main__ -   test: [batch: 29/196 ] | Loss: 0.638 | Acc: 83.893% (6443/7680)/ 95.807% (7358/7680)
01/04/2023 21:46:45 - INFO - __main__ -   test: [batch: 30/196 ] | Loss: 0.643 | Acc: 83.795% (6650/7936)/ 95.779% (7601/7936)
01/04/2023 21:46:46 - INFO - __main__ -   test: [batch: 31/196 ] | Loss: 0.651 | Acc: 83.618% (6850/8192)/ 95.728% (7842/8192)
01/04/2023 21:46:47 - INFO - __main__ -   test: [batch: 32/196 ] | Loss: 0.672 | Acc: 82.943% (7007/8448)/ 95.608% (8077/8448)
01/04/2023 21:46:47 - INFO - __main__ -   test: [batch: 33/196 ] | Loss: 0.680 | Acc: 82.870% (7213/8704)/ 95.588% (8320/8704)
01/04/2023 21:46:48 - INFO - __main__ -   test: [batch: 34/196 ] | Loss: 0.676 | Acc: 82.879% (7426/8960)/ 95.647% (8570/8960)
01/04/2023 21:46:48 - INFO - __main__ -   test: [batch: 35/196 ] | Loss: 0.675 | Acc: 82.834% (7634/9216)/ 95.660% (8816/9216)
01/04/2023 21:46:49 - INFO - __main__ -   test: [batch: 36/196 ] | Loss: 0.681 | Acc: 82.601% (7824/9472)/ 95.682% (9063/9472)
01/04/2023 21:46:50 - INFO - __main__ -   test: [batch: 37/196 ] | Loss: 0.688 | Acc: 82.401% (8016/9728)/ 95.652% (9305/9728)
01/04/2023 21:46:50 - INFO - __main__ -   test: [batch: 38/196 ] | Loss: 0.688 | Acc: 82.352% (8222/9984)/ 95.683% (9553/9984)
01/04/2023 21:46:51 - INFO - __main__ -   test: [batch: 39/196 ] | Loss: 0.687 | Acc: 82.236% (8421/10240)/ 95.742% (9804/10240)
01/04/2023 21:46:51 - INFO - __main__ -   test: [batch: 40/196 ] | Loss: 0.685 | Acc: 82.241% (8632/10496)/ 95.770% (10052/10496)
01/04/2023 21:46:52 - INFO - __main__ -   test: [batch: 41/196 ] | Loss: 0.684 | Acc: 82.301% (8849/10752)/ 95.750% (10295/10752)
01/04/2023 21:46:52 - INFO - __main__ -   test: [batch: 42/196 ] | Loss: 0.679 | Acc: 82.404% (9071/11008)/ 95.821% (10548/11008)
01/04/2023 21:46:53 - INFO - __main__ -   test: [batch: 43/196 ] | Loss: 0.681 | Acc: 82.360% (9277/11264)/ 95.801% (10791/11264)
01/04/2023 21:46:54 - INFO - __main__ -   test: [batch: 44/196 ] | Loss: 0.686 | Acc: 82.309% (9482/11520)/ 95.747% (11030/11520)
01/04/2023 21:46:54 - INFO - __main__ -   test: [batch: 45/196 ] | Loss: 0.686 | Acc: 82.269% (9688/11776)/ 95.746% (11275/11776)
01/04/2023 21:46:55 - INFO - __main__ -   test: [batch: 46/196 ] | Loss: 0.691 | Acc: 82.073% (9875/12032)/ 95.745% (11520/12032)
01/04/2023 21:46:55 - INFO - __main__ -   test: [batch: 47/196 ] | Loss: 0.691 | Acc: 82.015% (10078/12288)/ 95.760% (11767/12288)
01/04/2023 21:46:56 - INFO - __main__ -   test: [batch: 48/196 ] | Loss: 0.696 | Acc: 81.760% (10256/12544)/ 95.791% (12016/12544)
01/04/2023 21:46:57 - INFO - __main__ -   test: [batch: 49/196 ] | Loss: 0.687 | Acc: 82.000% (10496/12800)/ 95.859% (12270/12800)
01/04/2023 21:46:57 - INFO - __main__ -   test: [batch: 50/196 ] | Loss: 0.682 | Acc: 82.146% (10725/13056)/ 95.902% (12521/13056)
01/04/2023 21:46:58 - INFO - __main__ -   test: [batch: 51/196 ] | Loss: 0.681 | Acc: 82.091% (10928/13312)/ 95.951% (12773/13312)
01/04/2023 21:46:58 - INFO - __main__ -   test: [batch: 52/196 ] | Loss: 0.681 | Acc: 82.031% (11130/13568)/ 95.961% (13020/13568)
01/04/2023 21:46:59 - INFO - __main__ -   test: [batch: 53/196 ] | Loss: 0.686 | Acc: 82.046% (11342/13824)/ 95.935% (13262/13824)
01/04/2023 21:47:00 - INFO - __main__ -   test: [batch: 54/196 ] | Loss: 0.690 | Acc: 81.896% (11531/14080)/ 95.952% (13510/14080)
01/04/2023 21:47:00 - INFO - __main__ -   test: [batch: 55/196 ] | Loss: 0.696 | Acc: 81.669% (11708/14336)/ 95.933% (13753/14336)
01/04/2023 21:47:01 - INFO - __main__ -   test: [batch: 56/196 ] | Loss: 0.695 | Acc: 81.737% (11927/14592)/ 95.950% (14001/14592)
01/04/2023 21:47:01 - INFO - __main__ -   test: [batch: 57/196 ] | Loss: 0.689 | Acc: 81.910% (12162/14848)/ 95.993% (14253/14848)
01/04/2023 21:47:02 - INFO - __main__ -   test: [batch: 58/196 ] | Loss: 0.691 | Acc: 81.905% (12371/15104)/ 95.968% (14495/15104)
01/04/2023 21:47:02 - INFO - __main__ -   test: [batch: 59/196 ] | Loss: 0.692 | Acc: 81.829% (12569/15360)/ 96.003% (14746/15360)
01/04/2023 21:47:03 - INFO - __main__ -   test: [batch: 60/196 ] | Loss: 0.696 | Acc: 81.743% (12765/15616)/ 95.985% (14989/15616)
01/04/2023 21:47:04 - INFO - __main__ -   test: [batch: 61/196 ] | Loss: 0.699 | Acc: 81.697% (12967/15872)/ 95.955% (15230/15872)
01/04/2023 21:47:04 - INFO - __main__ -   test: [batch: 62/196 ] | Loss: 0.693 | Acc: 81.814% (13195/16128)/ 95.995% (15482/16128)
01/04/2023 21:47:05 - INFO - __main__ -   test: [batch: 63/196 ] | Loss: 0.688 | Acc: 81.982% (13432/16384)/ 96.014% (15731/16384)
01/04/2023 21:47:05 - INFO - __main__ -   test: [batch: 64/196 ] | Loss: 0.687 | Acc: 81.989% (13643/16640)/ 96.010% (15976/16640)
01/04/2023 21:47:06 - INFO - __main__ -   test: [batch: 65/196 ] | Loss: 0.684 | Acc: 82.132% (13877/16896)/ 96.035% (16226/16896)
01/04/2023 21:47:07 - INFO - __main__ -   test: [batch: 66/196 ] | Loss: 0.683 | Acc: 82.148% (14090/17152)/ 96.035% (16472/17152)
01/04/2023 21:47:07 - INFO - __main__ -   test: [batch: 67/196 ] | Loss: 0.681 | Acc: 82.169% (14304/17408)/ 96.054% (16721/17408)
01/04/2023 21:47:08 - INFO - __main__ -   test: [batch: 68/196 ] | Loss: 0.677 | Acc: 82.184% (14517/17664)/ 96.105% (16976/17664)
01/04/2023 21:47:08 - INFO - __main__ -   test: [batch: 69/196 ] | Loss: 0.682 | Acc: 82.042% (14702/17920)/ 96.099% (17221/17920)
01/04/2023 21:47:09 - INFO - __main__ -   test: [batch: 70/196 ] | Loss: 0.683 | Acc: 82.015% (14907/18176)/ 96.105% (17468/18176)
01/04/2023 21:47:09 - INFO - __main__ -   test: [batch: 71/196 ] | Loss: 0.681 | Acc: 82.113% (15135/18432)/ 96.099% (17713/18432)
01/04/2023 21:47:10 - INFO - __main__ -   test: [batch: 72/196 ] | Loss: 0.686 | Acc: 82.063% (15336/18688)/ 96.035% (17947/18688)
01/04/2023 21:47:11 - INFO - __main__ -   test: [batch: 73/196 ] | Loss: 0.688 | Acc: 82.052% (15544/18944)/ 95.999% (18186/18944)
01/04/2023 21:47:11 - INFO - __main__ -   test: [batch: 74/196 ] | Loss: 0.693 | Acc: 81.901% (15725/19200)/ 95.958% (18424/19200)
01/04/2023 21:47:12 - INFO - __main__ -   test: [batch: 75/196 ] | Loss: 0.693 | Acc: 81.872% (15929/19456)/ 95.965% (18671/19456)
01/04/2023 21:47:12 - INFO - __main__ -   test: [batch: 76/196 ] | Loss: 0.693 | Acc: 81.915% (16147/19712)/ 95.962% (18916/19712)
01/04/2023 21:47:13 - INFO - __main__ -   test: [batch: 77/196 ] | Loss: 0.693 | Acc: 81.936% (16361/19968)/ 95.933% (19156/19968)
01/04/2023 21:47:14 - INFO - __main__ -   test: [batch: 78/196 ] | Loss: 0.697 | Acc: 81.838% (16551/20224)/ 95.881% (19391/20224)
01/04/2023 21:47:14 - INFO - __main__ -   test: [batch: 79/196 ] | Loss: 0.700 | Acc: 81.777% (16748/20480)/ 95.854% (19631/20480)
01/04/2023 21:47:15 - INFO - __main__ -   test: [batch: 80/196 ] | Loss: 0.709 | Acc: 81.539% (16908/20736)/ 95.756% (19856/20736)
01/04/2023 21:47:15 - INFO - __main__ -   test: [batch: 81/196 ] | Loss: 0.718 | Acc: 81.388% (17085/20992)/ 95.679% (20085/20992)
01/04/2023 21:47:16 - INFO - __main__ -   test: [batch: 82/196 ] | Loss: 0.719 | Acc: 81.292% (17273/21248)/ 95.689% (20332/21248)
01/04/2023 21:47:17 - INFO - __main__ -   test: [batch: 83/196 ] | Loss: 0.721 | Acc: 81.245% (17471/21504)/ 95.657% (20570/21504)
01/04/2023 21:47:17 - INFO - __main__ -   test: [batch: 84/196 ] | Loss: 0.723 | Acc: 81.209% (17671/21760)/ 95.616% (20806/21760)
01/04/2023 21:47:18 - INFO - __main__ -   test: [batch: 85/196 ] | Loss: 0.731 | Acc: 81.046% (17843/22016)/ 95.544% (21035/22016)
01/04/2023 21:47:18 - INFO - __main__ -   test: [batch: 86/196 ] | Loss: 0.734 | Acc: 80.940% (18027/22272)/ 95.519% (21274/22272)
01/04/2023 21:47:19 - INFO - __main__ -   test: [batch: 87/196 ] | Loss: 0.736 | Acc: 80.904% (18226/22528)/ 95.472% (21508/22528)
01/04/2023 21:47:19 - INFO - __main__ -   test: [batch: 88/196 ] | Loss: 0.741 | Acc: 80.824% (18415/22784)/ 95.409% (21738/22784)
01/04/2023 21:47:20 - INFO - __main__ -   test: [batch: 89/196 ] | Loss: 0.746 | Acc: 80.742% (18603/23040)/ 95.352% (21969/23040)
01/04/2023 21:47:21 - INFO - __main__ -   test: [batch: 90/196 ] | Loss: 0.758 | Acc: 80.486% (18750/23296)/ 95.257% (22191/23296)
01/04/2023 21:47:21 - INFO - __main__ -   test: [batch: 91/196 ] | Loss: 0.763 | Acc: 80.392% (18934/23552)/ 95.168% (22414/23552)
01/04/2023 21:47:22 - INFO - __main__ -   test: [batch: 92/196 ] | Loss: 0.762 | Acc: 80.431% (19149/23808)/ 95.149% (22653/23808)
01/04/2023 21:47:22 - INFO - __main__ -   test: [batch: 93/196 ] | Loss: 0.770 | Acc: 80.278% (19318/24064)/ 95.088% (22882/24064)
01/04/2023 21:47:23 - INFO - __main__ -   test: [batch: 94/196 ] | Loss: 0.775 | Acc: 80.074% (19474/24320)/ 95.066% (23120/24320)
01/04/2023 21:47:24 - INFO - __main__ -   test: [batch: 95/196 ] | Loss: 0.782 | Acc: 79.952% (19649/24576)/ 95.024% (23353/24576)
01/04/2023 21:47:24 - INFO - __main__ -   test: [batch: 96/196 ] | Loss: 0.788 | Acc: 79.861% (19831/24832)/ 94.942% (23576/24832)
01/04/2023 21:47:25 - INFO - __main__ -   test: [batch: 97/196 ] | Loss: 0.795 | Acc: 79.735% (20004/25088)/ 94.866% (23800/25088)
01/04/2023 21:47:25 - INFO - __main__ -   test: [batch: 98/196 ] | Loss: 0.803 | Acc: 79.553% (20162/25344)/ 94.807% (24028/25344)
01/04/2023 21:47:26 - INFO - __main__ -   test: [batch: 99/196 ] | Loss: 0.805 | Acc: 79.504% (20353/25600)/ 94.773% (24262/25600)
01/04/2023 21:47:27 - INFO - __main__ -   test: [batch: 100/196 ] | Loss: 0.808 | Acc: 79.378% (20524/25856)/ 94.736% (24495/25856)
01/04/2023 21:47:27 - INFO - __main__ -   test: [batch: 101/196 ] | Loss: 0.810 | Acc: 79.358% (20722/26112)/ 94.711% (24731/26112)
01/04/2023 21:47:28 - INFO - __main__ -   test: [batch: 102/196 ] | Loss: 0.816 | Acc: 79.183% (20879/26368)/ 94.660% (24960/26368)
01/04/2023 21:47:28 - INFO - __main__ -   test: [batch: 103/196 ] | Loss: 0.821 | Acc: 79.075% (21053/26624)/ 94.621% (25192/26624)
01/04/2023 21:47:29 - INFO - __main__ -   test: [batch: 104/196 ] | Loss: 0.823 | Acc: 79.029% (21243/26880)/ 94.609% (25431/26880)
01/04/2023 21:47:29 - INFO - __main__ -   test: [batch: 105/196 ] | Loss: 0.827 | Acc: 78.965% (21428/27136)/ 94.572% (25663/27136)
01/04/2023 21:47:30 - INFO - __main__ -   test: [batch: 106/196 ] | Loss: 0.828 | Acc: 78.921% (21618/27392)/ 94.557% (25901/27392)
01/04/2023 21:47:31 - INFO - __main__ -   test: [batch: 107/196 ] | Loss: 0.829 | Acc: 78.924% (21821/27648)/ 94.549% (26141/27648)
01/04/2023 21:47:31 - INFO - __main__ -   test: [batch: 108/196 ] | Loss: 0.832 | Acc: 78.899% (22016/27904)/ 94.506% (26371/27904)
01/04/2023 21:47:32 - INFO - __main__ -   test: [batch: 109/196 ] | Loss: 0.833 | Acc: 78.867% (22209/28160)/ 94.471% (26603/28160)
01/04/2023 21:47:32 - INFO - __main__ -   test: [batch: 110/196 ] | Loss: 0.833 | Acc: 78.861% (22409/28416)/ 94.478% (26847/28416)
01/04/2023 21:47:33 - INFO - __main__ -   test: [batch: 111/196 ] | Loss: 0.832 | Acc: 78.917% (22627/28672)/ 94.482% (27090/28672)
01/04/2023 21:47:34 - INFO - __main__ -   test: [batch: 112/196 ] | Loss: 0.831 | Acc: 78.930% (22833/28928)/ 94.486% (27333/28928)
01/04/2023 21:47:34 - INFO - __main__ -   test: [batch: 113/196 ] | Loss: 0.833 | Acc: 78.896% (23025/29184)/ 94.473% (27571/29184)
01/04/2023 21:47:35 - INFO - __main__ -   test: [batch: 114/196 ] | Loss: 0.841 | Acc: 78.764% (23188/29440)/ 94.372% (27783/29440)
01/04/2023 21:47:35 - INFO - __main__ -   test: [batch: 115/196 ] | Loss: 0.844 | Acc: 78.697% (23370/29696)/ 94.322% (28010/29696)
01/04/2023 21:47:36 - INFO - __main__ -   test: [batch: 116/196 ] | Loss: 0.845 | Acc: 78.663% (23561/29952)/ 94.318% (28250/29952)
01/04/2023 21:47:36 - INFO - __main__ -   test: [batch: 117/196 ] | Loss: 0.851 | Acc: 78.529% (23722/30208)/ 94.247% (28470/30208)
01/04/2023 21:47:37 - INFO - __main__ -   test: [batch: 118/196 ] | Loss: 0.850 | Acc: 78.575% (23937/30464)/ 94.226% (28705/30464)
01/04/2023 21:47:38 - INFO - __main__ -   test: [batch: 119/196 ] | Loss: 0.847 | Acc: 78.656% (24163/30720)/ 94.238% (28950/30720)
01/04/2023 21:47:38 - INFO - __main__ -   test: [batch: 120/196 ] | Loss: 0.851 | Acc: 78.583% (24342/30976)/ 94.189% (29176/30976)
01/04/2023 21:47:39 - INFO - __main__ -   test: [batch: 121/196 ] | Loss: 0.859 | Acc: 78.349% (24470/31232)/ 94.102% (29390/31232)
01/04/2023 21:47:39 - INFO - __main__ -   test: [batch: 122/196 ] | Loss: 0.858 | Acc: 78.363% (24675/31488)/ 94.087% (29626/31488)
01/04/2023 21:47:40 - INFO - __main__ -   test: [batch: 123/196 ] | Loss: 0.865 | Acc: 78.242% (24837/31744)/ 94.002% (29840/31744)
01/04/2023 21:47:41 - INFO - __main__ -   test: [batch: 124/196 ] | Loss: 0.867 | Acc: 78.078% (24985/32000)/ 93.984% (30075/32000)
01/04/2023 21:47:41 - INFO - __main__ -   test: [batch: 125/196 ] | Loss: 0.868 | Acc: 78.069% (25182/32256)/ 93.973% (30312/32256)
01/04/2023 21:47:42 - INFO - __main__ -   test: [batch: 126/196 ] | Loss: 0.869 | Acc: 78.070% (25382/32512)/ 93.956% (30547/32512)
01/04/2023 21:47:42 - INFO - __main__ -   test: [batch: 127/196 ] | Loss: 0.875 | Acc: 77.975% (25551/32768)/ 93.909% (30772/32768)
01/04/2023 21:47:43 - INFO - __main__ -   test: [batch: 128/196 ] | Loss: 0.881 | Acc: 77.837% (25705/33024)/ 93.862% (30997/33024)
01/04/2023 21:47:44 - INFO - __main__ -   test: [batch: 129/196 ] | Loss: 0.885 | Acc: 77.710% (25862/33280)/ 93.837% (31229/33280)
01/04/2023 21:47:44 - INFO - __main__ -   test: [batch: 130/196 ] | Loss: 0.884 | Acc: 77.731% (26068/33536)/ 93.857% (31476/33536)
01/04/2023 21:47:45 - INFO - __main__ -   test: [batch: 131/196 ] | Loss: 0.886 | Acc: 77.687% (26252/33792)/ 93.836% (31709/33792)
01/04/2023 21:47:45 - INFO - __main__ -   test: [batch: 132/196 ] | Loss: 0.890 | Acc: 77.649% (26438/34048)/ 93.773% (31928/34048)
01/04/2023 21:47:46 - INFO - __main__ -   test: [batch: 133/196 ] | Loss: 0.892 | Acc: 77.609% (26623/34304)/ 93.756% (32162/34304)
01/04/2023 21:47:46 - INFO - __main__ -   test: [batch: 134/196 ] | Loss: 0.893 | Acc: 77.569% (26808/34560)/ 93.733% (32394/34560)
01/04/2023 21:47:47 - INFO - __main__ -   test: [batch: 135/196 ] | Loss: 0.896 | Acc: 77.519% (26989/34816)/ 93.695% (32621/34816)
01/04/2023 21:47:48 - INFO - __main__ -   test: [batch: 136/196 ] | Loss: 0.898 | Acc: 77.489% (27177/35072)/ 93.679% (32855/35072)
01/04/2023 21:47:48 - INFO - __main__ -   test: [batch: 137/196 ] | Loss: 0.900 | Acc: 77.451% (27362/35328)/ 93.679% (33095/35328)
01/04/2023 21:47:49 - INFO - __main__ -   test: [batch: 138/196 ] | Loss: 0.901 | Acc: 77.386% (27537/35584)/ 93.669% (33331/35584)
01/04/2023 21:47:49 - INFO - __main__ -   test: [batch: 139/196 ] | Loss: 0.901 | Acc: 77.400% (27740/35840)/ 93.664% (33569/35840)
01/04/2023 21:47:50 - INFO - __main__ -   test: [batch: 140/196 ] | Loss: 0.902 | Acc: 77.388% (27934/36096)/ 93.656% (33806/36096)
01/04/2023 21:47:51 - INFO - __main__ -   test: [batch: 141/196 ] | Loss: 0.902 | Acc: 77.396% (28135/36352)/ 93.654% (34045/36352)
01/04/2023 21:47:51 - INFO - __main__ -   test: [batch: 142/196 ] | Loss: 0.908 | Acc: 77.303% (28299/36608)/ 93.578% (34257/36608)
01/04/2023 21:47:52 - INFO - __main__ -   test: [batch: 143/196 ] | Loss: 0.909 | Acc: 77.268% (28484/36864)/ 93.555% (34488/36864)
01/04/2023 21:47:52 - INFO - __main__ -   test: [batch: 144/196 ] | Loss: 0.910 | Acc: 77.260% (28679/37120)/ 93.529% (34718/37120)
01/04/2023 21:47:53 - INFO - __main__ -   test: [batch: 145/196 ] | Loss: 0.911 | Acc: 77.199% (28854/37376)/ 93.525% (34956/37376)
01/04/2023 21:47:54 - INFO - __main__ -   test: [batch: 146/196 ] | Loss: 0.913 | Acc: 77.144% (29031/37632)/ 93.503% (35187/37632)
01/04/2023 21:47:54 - INFO - __main__ -   test: [batch: 147/196 ] | Loss: 0.913 | Acc: 77.151% (29231/37888)/ 93.505% (35427/37888)
01/04/2023 21:47:55 - INFO - __main__ -   test: [batch: 148/196 ] | Loss: 0.916 | Acc: 77.129% (29420/38144)/ 93.469% (35653/38144)
01/04/2023 21:47:55 - INFO - __main__ -   test: [batch: 149/196 ] | Loss: 0.919 | Acc: 77.089% (29602/38400)/ 93.424% (35875/38400)
01/04/2023 21:47:56 - INFO - __main__ -   test: [batch: 150/196 ] | Loss: 0.919 | Acc: 77.077% (29795/38656)/ 93.409% (36108/38656)
01/04/2023 21:47:56 - INFO - __main__ -   test: [batch: 151/196 ] | Loss: 0.922 | Acc: 77.020% (29970/38912)/ 93.375% (36334/38912)
01/04/2023 21:47:57 - INFO - __main__ -   test: [batch: 152/196 ] | Loss: 0.922 | Acc: 76.989% (30155/39168)/ 93.377% (36574/39168)
01/04/2023 21:47:58 - INFO - __main__ -   test: [batch: 153/196 ] | Loss: 0.924 | Acc: 76.981% (30349/39424)/ 93.339% (36798/39424)
01/04/2023 21:47:58 - INFO - __main__ -   test: [batch: 154/196 ] | Loss: 0.925 | Acc: 76.961% (30538/39680)/ 93.319% (37029/39680)
01/04/2023 21:47:59 - INFO - __main__ -   test: [batch: 155/196 ] | Loss: 0.928 | Acc: 76.918% (30718/39936)/ 93.294% (37258/39936)
01/04/2023 21:47:59 - INFO - __main__ -   test: [batch: 156/196 ] | Loss: 0.927 | Acc: 76.943% (30925/40192)/ 93.287% (37494/40192)
01/04/2023 21:48:00 - INFO - __main__ -   test: [batch: 157/196 ] | Loss: 0.929 | Acc: 76.911% (31109/40448)/ 93.265% (37724/40448)
01/04/2023 21:48:01 - INFO - __main__ -   test: [batch: 158/196 ] | Loss: 0.933 | Acc: 76.769% (31248/40704)/ 93.217% (37943/40704)
01/04/2023 21:48:01 - INFO - __main__ -   test: [batch: 159/196 ] | Loss: 0.934 | Acc: 76.746% (31435/40960)/ 93.201% (38175/40960)
01/04/2023 21:48:02 - INFO - __main__ -   test: [batch: 160/196 ] | Loss: 0.933 | Acc: 76.800% (31654/41216)/ 93.204% (38415/41216)
01/04/2023 21:48:02 - INFO - __main__ -   test: [batch: 161/196 ] | Loss: 0.937 | Acc: 76.743% (31827/41472)/ 93.176% (38642/41472)
01/04/2023 21:48:03 - INFO - __main__ -   test: [batch: 162/196 ] | Loss: 0.937 | Acc: 76.749% (32026/41728)/ 93.172% (38879/41728)
01/04/2023 21:48:03 - INFO - __main__ -   test: [batch: 163/196 ] | Loss: 0.943 | Acc: 76.593% (32157/41984)/ 93.116% (39094/41984)
01/04/2023 21:48:04 - INFO - __main__ -   test: [batch: 164/196 ] | Loss: 0.945 | Acc: 76.532% (32327/42240)/ 93.097% (39324/42240)
01/04/2023 21:48:05 - INFO - __main__ -   test: [batch: 165/196 ] | Loss: 0.947 | Acc: 76.454% (32490/42496)/ 93.086% (39558/42496)
01/04/2023 21:48:05 - INFO - __main__ -   test: [batch: 166/196 ] | Loss: 0.946 | Acc: 76.471% (32693/42752)/ 93.090% (39798/42752)
01/04/2023 21:48:06 - INFO - __main__ -   test: [batch: 167/196 ] | Loss: 0.948 | Acc: 76.418% (32866/43008)/ 93.057% (40022/43008)
01/04/2023 21:48:06 - INFO - __main__ -   test: [batch: 168/196 ] | Loss: 0.950 | Acc: 76.366% (33039/43264)/ 93.036% (40251/43264)
01/04/2023 21:48:07 - INFO - __main__ -   test: [batch: 169/196 ] | Loss: 0.953 | Acc: 76.312% (33211/43520)/ 93.017% (40481/43520)
01/04/2023 21:48:08 - INFO - __main__ -   test: [batch: 170/196 ] | Loss: 0.951 | Acc: 76.341% (33419/43776)/ 93.044% (40731/43776)
01/04/2023 21:48:08 - INFO - __main__ -   test: [batch: 171/196 ] | Loss: 0.951 | Acc: 76.285% (33590/44032)/ 93.039% (40967/44032)
01/04/2023 21:48:09 - INFO - __main__ -   test: [batch: 172/196 ] | Loss: 0.955 | Acc: 76.224% (33758/44288)/ 92.991% (41184/44288)
01/04/2023 21:48:09 - INFO - __main__ -   test: [batch: 173/196 ] | Loss: 0.956 | Acc: 76.219% (33951/44544)/ 92.987% (41420/44544)
01/04/2023 21:48:10 - INFO - __main__ -   test: [batch: 174/196 ] | Loss: 0.957 | Acc: 76.196% (34136/44800)/ 92.967% (41649/44800)
01/04/2023 21:48:11 - INFO - __main__ -   test: [batch: 175/196 ] | Loss: 0.960 | Acc: 76.145% (34308/45056)/ 92.949% (41879/45056)
01/04/2023 21:48:11 - INFO - __main__ -   test: [batch: 176/196 ] | Loss: 0.962 | Acc: 76.108% (34486/45312)/ 92.907% (42098/45312)
01/04/2023 21:48:12 - INFO - __main__ -   test: [batch: 177/196 ] | Loss: 0.967 | Acc: 75.985% (34625/45568)/ 92.885% (42326/45568)
01/04/2023 21:48:12 - INFO - __main__ -   test: [batch: 178/196 ] | Loss: 0.966 | Acc: 76.013% (34832/45824)/ 92.897% (42569/45824)
01/04/2023 21:48:13 - INFO - __main__ -   test: [batch: 179/196 ] | Loss: 0.965 | Acc: 76.048% (35043/46080)/ 92.897% (42807/46080)
01/04/2023 21:48:13 - INFO - __main__ -   test: [batch: 180/196 ] | Loss: 0.967 | Acc: 76.016% (35223/46336)/ 92.895% (43044/46336)
01/04/2023 21:48:14 - INFO - __main__ -   test: [batch: 181/196 ] | Loss: 0.967 | Acc: 76.007% (35413/46592)/ 92.898% (43283/46592)
01/04/2023 21:48:15 - INFO - __main__ -   test: [batch: 182/196 ] | Loss: 0.965 | Acc: 76.046% (35626/46848)/ 92.913% (43528/46848)
01/04/2023 21:48:15 - INFO - __main__ -   test: [batch: 183/196 ] | Loss: 0.963 | Acc: 76.108% (35850/47104)/ 92.943% (43780/47104)
01/04/2023 21:48:16 - INFO - __main__ -   test: [batch: 184/196 ] | Loss: 0.961 | Acc: 76.134% (36057/47360)/ 92.962% (44027/47360)
01/04/2023 21:48:16 - INFO - __main__ -   test: [batch: 185/196 ] | Loss: 0.961 | Acc: 76.121% (36246/47616)/ 92.965% (44266/47616)
01/04/2023 21:48:17 - INFO - __main__ -   test: [batch: 186/196 ] | Loss: 0.958 | Acc: 76.203% (36480/47872)/ 92.988% (44515/47872)
01/04/2023 21:48:18 - INFO - __main__ -   test: [batch: 187/196 ] | Loss: 0.960 | Acc: 76.161% (36655/48128)/ 92.958% (44739/48128)
01/04/2023 21:48:18 - INFO - __main__ -   test: [batch: 188/196 ] | Loss: 0.961 | Acc: 76.145% (36842/48384)/ 92.940% (44968/48384)
01/04/2023 21:48:19 - INFO - __main__ -   test: [batch: 189/196 ] | Loss: 0.965 | Acc: 76.059% (36995/48640)/ 92.903% (45188/48640)
01/04/2023 21:48:19 - INFO - __main__ -   test: [batch: 190/196 ] | Loss: 0.966 | Acc: 76.014% (37168/48896)/ 92.911% (45430/48896)
01/04/2023 21:48:20 - INFO - __main__ -   test: [batch: 191/196 ] | Loss: 0.968 | Acc: 75.985% (37348/49152)/ 92.893% (45659/49152)
01/04/2023 21:48:20 - INFO - __main__ -   test: [batch: 192/196 ] | Loss: 0.965 | Acc: 76.034% (37567/49408)/ 92.914% (45907/49408)
01/04/2023 21:48:21 - INFO - __main__ -   test: [batch: 193/196 ] | Loss: 0.961 | Acc: 76.122% (37805/49664)/ 92.941% (46158/49664)
01/04/2023 21:48:22 - INFO - __main__ -   test: [batch: 194/196 ] | Loss: 0.959 | Acc: 76.170% (38024/49920)/ 92.955% (46403/49920)
01/04/2023 21:48:22 - INFO - __main__ -   test: [batch: 195/196 ] | Loss: 0.963 | Acc: 76.122% (38061/50000)/ 92.946% (46473/50000)
01/04/2023 21:48:22 - INFO - __main__ -   Final accuracy: 76.122
01/04/2023 21:48:22 - INFO - __main__ -   {'milestones': Counter({2: 1, 3: 1, 4: 1}), 'gamma': 0.1, 'base_lrs': [0.0005], 'last_epoch': 4, '_step_count': 5, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [5.000000000000001e-07]}
01/04/2023 21:48:22 - INFO - __main__ -   
Epoch: 4
01/04/2023 21:48:22 - INFO - __main__ -   test: [epoch: 4 | batch: 0/5005 ] | Loss: 1.119 | Acc: 75.781% (194/256)
01/04/2023 21:49:23 - INFO - __main__ -   test: [epoch: 4 | batch: 100/5005 ] | Loss: 0.830 | Acc: 79.328% (20511/25856)
01/04/2023 21:50:24 - INFO - __main__ -   test: [epoch: 4 | batch: 200/5005 ] | Loss: 0.839 | Acc: 79.071% (40687/51456)
01/04/2023 21:51:25 - INFO - __main__ -   test: [epoch: 4 | batch: 300/5005 ] | Loss: 0.844 | Acc: 78.967% (60849/77056)
01/04/2023 21:52:25 - INFO - __main__ -   test: [epoch: 4 | batch: 400/5005 ] | Loss: 0.846 | Acc: 78.895% (80990/102656)
01/04/2023 21:53:26 - INFO - __main__ -   test: [epoch: 4 | batch: 500/5005 ] | Loss: 0.845 | Acc: 78.922% (101222/128256)
01/04/2023 21:54:27 - INFO - __main__ -   test: [epoch: 4 | batch: 600/5005 ] | Loss: 0.848 | Acc: 78.883% (121366/153856)
01/04/2023 21:55:27 - INFO - __main__ -   test: [epoch: 4 | batch: 700/5005 ] | Loss: 0.847 | Acc: 78.927% (141640/179456)
01/04/2023 21:56:28 - INFO - __main__ -   test: [epoch: 4 | batch: 800/5005 ] | Loss: 0.848 | Acc: 78.940% (161872/205056)
01/04/2023 21:57:29 - INFO - __main__ -   test: [epoch: 4 | batch: 900/5005 ] | Loss: 0.846 | Acc: 78.976% (182162/230656)
01/04/2023 21:58:30 - INFO - __main__ -   test: [epoch: 4 | batch: 1000/5005 ] | Loss: 0.845 | Acc: 78.980% (202392/256256)
01/04/2023 21:59:30 - INFO - __main__ -   test: [epoch: 4 | batch: 1100/5005 ] | Loss: 0.845 | Acc: 78.974% (222594/281856)
01/04/2023 22:00:31 - INFO - __main__ -   test: [epoch: 4 | batch: 1200/5005 ] | Loss: 0.845 | Acc: 78.982% (242834/307456)
01/04/2023 22:01:32 - INFO - __main__ -   test: [epoch: 4 | batch: 1300/5005 ] | Loss: 0.845 | Acc: 78.966% (263002/333056)
01/04/2023 22:02:33 - INFO - __main__ -   test: [epoch: 4 | batch: 1400/5005 ] | Loss: 0.846 | Acc: 78.953% (283168/358656)
01/04/2023 22:03:33 - INFO - __main__ -   test: [epoch: 4 | batch: 1500/5005 ] | Loss: 0.846 | Acc: 78.949% (303366/384256)
01/04/2023 22:04:34 - INFO - __main__ -   test: [epoch: 4 | batch: 1600/5005 ] | Loss: 0.846 | Acc: 78.944% (323558/409856)
01/04/2023 22:05:35 - INFO - __main__ -   test: [epoch: 4 | batch: 1700/5005 ] | Loss: 0.847 | Acc: 78.908% (343610/435456)
01/04/2023 22:06:35 - INFO - __main__ -   test: [epoch: 4 | batch: 1800/5005 ] | Loss: 0.847 | Acc: 78.908% (363809/461056)
01/04/2023 22:07:36 - INFO - __main__ -   test: [epoch: 4 | batch: 1900/5005 ] | Loss: 0.847 | Acc: 78.912% (384032/486656)
01/04/2023 22:08:37 - INFO - __main__ -   test: [epoch: 4 | batch: 2000/5005 ] | Loss: 0.848 | Acc: 78.894% (404140/512256)
01/04/2023 22:09:37 - INFO - __main__ -   test: [epoch: 4 | batch: 2100/5005 ] | Loss: 0.848 | Acc: 78.900% (424367/537856)
01/04/2023 22:10:38 - INFO - __main__ -   test: [epoch: 4 | batch: 2200/5005 ] | Loss: 0.847 | Acc: 78.904% (444590/563456)
01/04/2023 22:11:39 - INFO - __main__ -   test: [epoch: 4 | batch: 2300/5005 ] | Loss: 0.847 | Acc: 78.907% (464806/589056)
01/04/2023 22:12:40 - INFO - __main__ -   test: [epoch: 4 | batch: 2400/5005 ] | Loss: 0.847 | Acc: 78.923% (485104/614656)
01/04/2023 22:13:40 - INFO - __main__ -   test: [epoch: 4 | batch: 2500/5005 ] | Loss: 0.846 | Acc: 78.944% (505445/640256)
01/04/2023 22:14:41 - INFO - __main__ -   test: [epoch: 4 | batch: 2600/5005 ] | Loss: 0.847 | Acc: 78.924% (525520/665856)
01/04/2023 22:15:42 - INFO - __main__ -   test: [epoch: 4 | batch: 2700/5005 ] | Loss: 0.847 | Acc: 78.922% (545713/691456)
01/04/2023 22:16:43 - INFO - __main__ -   test: [epoch: 4 | batch: 2800/5005 ] | Loss: 0.847 | Acc: 78.936% (566017/717056)
01/04/2023 22:17:43 - INFO - __main__ -   test: [epoch: 4 | batch: 2900/5005 ] | Loss: 0.846 | Acc: 78.940% (586256/742656)
01/04/2023 22:18:44 - INFO - __main__ -   test: [epoch: 4 | batch: 3000/5005 ] | Loss: 0.846 | Acc: 78.957% (606592/768256)
01/04/2023 22:19:45 - INFO - __main__ -   test: [epoch: 4 | batch: 3100/5005 ] | Loss: 0.846 | Acc: 78.955% (626790/793856)
01/04/2023 22:20:46 - INFO - __main__ -   test: [epoch: 4 | batch: 3200/5005 ] | Loss: 0.847 | Acc: 78.940% (646880/819456)
01/04/2023 22:21:46 - INFO - __main__ -   test: [epoch: 4 | batch: 3300/5005 ] | Loss: 0.847 | Acc: 78.940% (667088/845056)
01/04/2023 22:22:47 - INFO - __main__ -   test: [epoch: 4 | batch: 3400/5005 ] | Loss: 0.846 | Acc: 78.941% (687307/870656)
01/04/2023 22:23:48 - INFO - __main__ -   test: [epoch: 4 | batch: 3500/5005 ] | Loss: 0.846 | Acc: 78.936% (707466/896256)
01/04/2023 22:24:48 - INFO - __main__ -   test: [epoch: 4 | batch: 3600/5005 ] | Loss: 0.846 | Acc: 78.936% (727675/921856)
01/04/2023 22:25:49 - INFO - __main__ -   test: [epoch: 4 | batch: 3700/5005 ] | Loss: 0.846 | Acc: 78.925% (747782/947456)
01/04/2023 22:26:50 - INFO - __main__ -   test: [epoch: 4 | batch: 3800/5005 ] | Loss: 0.846 | Acc: 78.931% (768044/973056)
01/04/2023 22:27:51 - INFO - __main__ -   test: [epoch: 4 | batch: 3900/5005 ] | Loss: 0.847 | Acc: 78.917% (788110/998656)
01/04/2023 22:28:51 - INFO - __main__ -   test: [epoch: 4 | batch: 4000/5005 ] | Loss: 0.847 | Acc: 78.916% (808300/1024256)
01/04/2023 22:29:52 - INFO - __main__ -   test: [epoch: 4 | batch: 4100/5005 ] | Loss: 0.847 | Acc: 78.919% (828536/1049856)
01/04/2023 22:30:53 - INFO - __main__ -   test: [epoch: 4 | batch: 4200/5005 ] | Loss: 0.847 | Acc: 78.911% (848650/1075456)
01/04/2023 22:31:53 - INFO - __main__ -   test: [epoch: 4 | batch: 4300/5005 ] | Loss: 0.847 | Acc: 78.916% (868908/1101056)
01/04/2023 22:32:54 - INFO - __main__ -   test: [epoch: 4 | batch: 4400/5005 ] | Loss: 0.846 | Acc: 78.929% (889253/1126656)
01/04/2023 22:33:55 - INFO - __main__ -   test: [epoch: 4 | batch: 4500/5005 ] | Loss: 0.846 | Acc: 78.939% (909578/1152256)
01/04/2023 22:34:56 - INFO - __main__ -   test: [epoch: 4 | batch: 4600/5005 ] | Loss: 0.846 | Acc: 78.932% (929707/1177856)
01/04/2023 22:35:57 - INFO - __main__ -   test: [epoch: 4 | batch: 4700/5005 ] | Loss: 0.846 | Acc: 78.931% (949902/1203456)
01/04/2023 22:36:59 - INFO - __main__ -   test: [epoch: 4 | batch: 4800/5005 ] | Loss: 0.846 | Acc: 78.933% (970129/1229056)
01/04/2023 22:38:00 - INFO - __main__ -   test: [epoch: 4 | batch: 4900/5005 ] | Loss: 0.846 | Acc: 78.929% (990287/1254656)
01/04/2023 22:39:01 - INFO - __main__ -   test: [epoch: 4 | batch: 5000/5005 ] | Loss: 0.846 | Acc: 78.926% (1010454/1280256)
01/04/2023 22:39:03 - INFO - __main__ -   Saving Checkpoint
01/04/2023 22:39:04 - INFO - __main__ -   test: [batch: 0/196 ] | Loss: 0.459 | Acc: 87.109% (223/256)/ 98.047% (251/256)
01/04/2023 22:39:04 - INFO - __main__ -   test: [batch: 1/196 ] | Loss: 0.571 | Acc: 84.570% (433/512)/ 96.484% (494/512)
01/04/2023 22:39:05 - INFO - __main__ -   test: [batch: 2/196 ] | Loss: 0.430 | Acc: 88.281% (678/768)/ 97.656% (750/768)
01/04/2023 22:39:06 - INFO - __main__ -   test: [batch: 3/196 ] | Loss: 0.408 | Acc: 89.258% (914/1024)/ 97.559% (999/1024)
01/04/2023 22:39:06 - INFO - __main__ -   test: [batch: 4/196 ] | Loss: 0.404 | Acc: 89.766% (1149/1280)/ 97.578% (1249/1280)
01/04/2023 22:39:07 - INFO - __main__ -   test: [batch: 5/196 ] | Loss: 0.457 | Acc: 88.737% (1363/1536)/ 97.396% (1496/1536)
01/04/2023 22:39:07 - INFO - __main__ -   test: [batch: 6/196 ] | Loss: 0.554 | Acc: 86.049% (1542/1792)/ 96.652% (1732/1792)
01/04/2023 22:39:08 - INFO - __main__ -   test: [batch: 7/196 ] | Loss: 0.582 | Acc: 84.912% (1739/2048)/ 96.826% (1983/2048)
01/04/2023 22:39:09 - INFO - __main__ -   test: [batch: 8/196 ] | Loss: 0.619 | Acc: 84.288% (1942/2304)/ 96.181% (2216/2304)
01/04/2023 22:39:09 - INFO - __main__ -   test: [batch: 9/196 ] | Loss: 0.648 | Acc: 83.477% (2137/2560)/ 96.016% (2458/2560)
01/04/2023 22:39:10 - INFO - __main__ -   test: [batch: 10/196 ] | Loss: 0.677 | Acc: 82.812% (2332/2816)/ 95.739% (2696/2816)
01/04/2023 22:39:10 - INFO - __main__ -   test: [batch: 11/196 ] | Loss: 0.720 | Acc: 81.771% (2512/3072)/ 95.443% (2932/3072)
01/04/2023 22:39:11 - INFO - __main__ -   test: [batch: 12/196 ] | Loss: 0.761 | Acc: 80.739% (2687/3328)/ 95.072% (3164/3328)
01/04/2023 22:39:12 - INFO - __main__ -   test: [batch: 13/196 ] | Loss: 0.757 | Acc: 80.469% (2884/3584)/ 95.089% (3408/3584)
01/04/2023 22:39:12 - INFO - __main__ -   test: [batch: 14/196 ] | Loss: 0.759 | Acc: 80.052% (3074/3840)/ 95.260% (3658/3840)
01/04/2023 22:39:13 - INFO - __main__ -   test: [batch: 15/196 ] | Loss: 0.769 | Acc: 80.200% (3285/4096)/ 95.215% (3900/4096)
01/04/2023 22:39:13 - INFO - __main__ -   test: [batch: 16/196 ] | Loss: 0.748 | Acc: 80.653% (3510/4352)/ 95.358% (4150/4352)
01/04/2023 22:39:14 - INFO - __main__ -   test: [batch: 17/196 ] | Loss: 0.722 | Acc: 81.402% (3751/4608)/ 95.443% (4398/4608)
01/04/2023 22:39:14 - INFO - __main__ -   test: [batch: 18/196 ] | Loss: 0.697 | Acc: 82.155% (3996/4864)/ 95.600% (4650/4864)
01/04/2023 22:39:15 - INFO - __main__ -   test: [batch: 19/196 ] | Loss: 0.685 | Acc: 82.305% (4214/5120)/ 95.723% (4901/5120)
01/04/2023 22:39:16 - INFO - __main__ -   test: [batch: 20/196 ] | Loss: 0.688 | Acc: 82.459% (4433/5376)/ 95.554% (5137/5376)
01/04/2023 22:39:16 - INFO - __main__ -   test: [batch: 21/196 ] | Loss: 0.684 | Acc: 82.528% (4648/5632)/ 95.579% (5383/5632)
01/04/2023 22:39:17 - INFO - __main__ -   test: [batch: 22/196 ] | Loss: 0.685 | Acc: 82.711% (4870/5888)/ 95.482% (5622/5888)
01/04/2023 22:39:17 - INFO - __main__ -   test: [batch: 23/196 ] | Loss: 0.689 | Acc: 82.601% (5075/6144)/ 95.557% (5871/6144)
01/04/2023 22:39:18 - INFO - __main__ -   test: [batch: 24/196 ] | Loss: 0.697 | Acc: 82.516% (5281/6400)/ 95.438% (6108/6400)
01/04/2023 22:39:19 - INFO - __main__ -   test: [batch: 25/196 ] | Loss: 0.681 | Acc: 82.903% (5518/6656)/ 95.568% (6361/6656)
01/04/2023 22:39:19 - INFO - __main__ -   test: [batch: 26/196 ] | Loss: 0.668 | Acc: 83.203% (5751/6912)/ 95.660% (6612/6912)
01/04/2023 22:39:20 - INFO - __main__ -   test: [batch: 27/196 ] | Loss: 0.655 | Acc: 83.482% (5984/7168)/ 95.717% (6861/7168)
01/04/2023 22:39:20 - INFO - __main__ -   test: [batch: 28/196 ] | Loss: 0.640 | Acc: 83.863% (6226/7424)/ 95.797% (7112/7424)
01/04/2023 22:39:21 - INFO - __main__ -   test: [batch: 29/196 ] | Loss: 0.640 | Acc: 83.841% (6439/7680)/ 95.807% (7358/7680)
01/04/2023 22:39:22 - INFO - __main__ -   test: [batch: 30/196 ] | Loss: 0.645 | Acc: 83.745% (6646/7936)/ 95.779% (7601/7936)
01/04/2023 22:39:22 - INFO - __main__ -   test: [batch: 31/196 ] | Loss: 0.653 | Acc: 83.557% (6845/8192)/ 95.728% (7842/8192)
01/04/2023 22:39:23 - INFO - __main__ -   test: [batch: 32/196 ] | Loss: 0.674 | Acc: 82.872% (7001/8448)/ 95.597% (8076/8448)
01/04/2023 22:39:23 - INFO - __main__ -   test: [batch: 33/196 ] | Loss: 0.682 | Acc: 82.801% (7207/8704)/ 95.577% (8319/8704)
01/04/2023 22:39:24 - INFO - __main__ -   test: [batch: 34/196 ] | Loss: 0.678 | Acc: 82.835% (7422/8960)/ 95.636% (8569/8960)
01/04/2023 22:39:25 - INFO - __main__ -   test: [batch: 35/196 ] | Loss: 0.678 | Acc: 82.780% (7629/9216)/ 95.649% (8815/9216)
01/04/2023 22:39:25 - INFO - __main__ -   test: [batch: 36/196 ] | Loss: 0.683 | Acc: 82.559% (7820/9472)/ 95.671% (9062/9472)
01/04/2023 22:39:26 - INFO - __main__ -   test: [batch: 37/196 ] | Loss: 0.690 | Acc: 82.340% (8010/9728)/ 95.631% (9303/9728)
01/04/2023 22:39:26 - INFO - __main__ -   test: [batch: 38/196 ] | Loss: 0.690 | Acc: 82.282% (8215/9984)/ 95.653% (9550/9984)
01/04/2023 22:39:27 - INFO - __main__ -   test: [batch: 39/196 ] | Loss: 0.689 | Acc: 82.168% (8414/10240)/ 95.723% (9802/10240)
01/04/2023 22:39:27 - INFO - __main__ -   test: [batch: 40/196 ] | Loss: 0.687 | Acc: 82.165% (8624/10496)/ 95.751% (10050/10496)
01/04/2023 22:39:28 - INFO - __main__ -   test: [batch: 41/196 ] | Loss: 0.687 | Acc: 82.227% (8841/10752)/ 95.722% (10292/10752)
01/04/2023 22:39:29 - INFO - __main__ -   test: [batch: 42/196 ] | Loss: 0.681 | Acc: 82.331% (9063/11008)/ 95.794% (10545/11008)
01/04/2023 22:39:29 - INFO - __main__ -   test: [batch: 43/196 ] | Loss: 0.683 | Acc: 82.324% (9273/11264)/ 95.783% (10789/11264)
01/04/2023 22:39:30 - INFO - __main__ -   test: [batch: 44/196 ] | Loss: 0.689 | Acc: 82.292% (9480/11520)/ 95.729% (11028/11520)
01/04/2023 22:39:30 - INFO - __main__ -   test: [batch: 45/196 ] | Loss: 0.688 | Acc: 82.278% (9689/11776)/ 95.737% (11274/11776)
01/04/2023 22:39:31 - INFO - __main__ -   test: [batch: 46/196 ] | Loss: 0.693 | Acc: 82.081% (9876/12032)/ 95.745% (11520/12032)
01/04/2023 22:39:32 - INFO - __main__ -   test: [batch: 47/196 ] | Loss: 0.693 | Acc: 82.007% (10077/12288)/ 95.760% (11767/12288)
01/04/2023 22:39:32 - INFO - __main__ -   test: [batch: 48/196 ] | Loss: 0.698 | Acc: 81.744% (10254/12544)/ 95.799% (12017/12544)
01/04/2023 22:39:33 - INFO - __main__ -   test: [batch: 49/196 ] | Loss: 0.689 | Acc: 81.992% (10495/12800)/ 95.867% (12271/12800)
01/04/2023 22:39:33 - INFO - __main__ -   test: [batch: 50/196 ] | Loss: 0.684 | Acc: 82.138% (10724/13056)/ 95.918% (12523/13056)
01/04/2023 22:39:34 - INFO - __main__ -   test: [batch: 51/196 ] | Loss: 0.682 | Acc: 82.091% (10928/13312)/ 95.974% (12776/13312)
01/04/2023 22:39:35 - INFO - __main__ -   test: [batch: 52/196 ] | Loss: 0.683 | Acc: 82.009% (11127/13568)/ 95.983% (13023/13568)
01/04/2023 22:39:35 - INFO - __main__ -   test: [batch: 53/196 ] | Loss: 0.688 | Acc: 82.010% (11337/13824)/ 95.949% (13264/13824)
01/04/2023 22:39:36 - INFO - __main__ -   test: [batch: 54/196 ] | Loss: 0.692 | Acc: 81.839% (11523/14080)/ 95.966% (13512/14080)
01/04/2023 22:39:36 - INFO - __main__ -   test: [batch: 55/196 ] | Loss: 0.698 | Acc: 81.613% (11700/14336)/ 95.940% (13754/14336)
01/04/2023 22:39:37 - INFO - __main__ -   test: [batch: 56/196 ] | Loss: 0.697 | Acc: 81.689% (11920/14592)/ 95.957% (14002/14592)
01/04/2023 22:39:38 - INFO - __main__ -   test: [batch: 57/196 ] | Loss: 0.691 | Acc: 81.856% (12154/14848)/ 95.999% (14254/14848)
01/04/2023 22:39:38 - INFO - __main__ -   test: [batch: 58/196 ] | Loss: 0.692 | Acc: 81.839% (12361/15104)/ 95.975% (14496/15104)
01/04/2023 22:39:39 - INFO - __main__ -   test: [batch: 59/196 ] | Loss: 0.694 | Acc: 81.771% (12560/15360)/ 96.003% (14746/15360)
01/04/2023 22:39:39 - INFO - __main__ -   test: [batch: 60/196 ] | Loss: 0.697 | Acc: 81.679% (12755/15616)/ 95.991% (14990/15616)
01/04/2023 22:39:40 - INFO - __main__ -   test: [batch: 61/196 ] | Loss: 0.700 | Acc: 81.634% (12957/15872)/ 95.961% (15231/15872)
01/04/2023 22:39:41 - INFO - __main__ -   test: [batch: 62/196 ] | Loss: 0.695 | Acc: 81.746% (13184/16128)/ 96.001% (15483/16128)
01/04/2023 22:39:41 - INFO - __main__ -   test: [batch: 63/196 ] | Loss: 0.690 | Acc: 81.915% (13421/16384)/ 96.021% (15732/16384)
01/04/2023 22:39:42 - INFO - __main__ -   test: [batch: 64/196 ] | Loss: 0.689 | Acc: 81.923% (13632/16640)/ 96.016% (15977/16640)
01/04/2023 22:39:42 - INFO - __main__ -   test: [batch: 65/196 ] | Loss: 0.686 | Acc: 82.073% (13867/16896)/ 96.040% (16227/16896)
01/04/2023 22:39:43 - INFO - __main__ -   test: [batch: 66/196 ] | Loss: 0.685 | Acc: 82.090% (14080/17152)/ 96.035% (16472/17152)
01/04/2023 22:39:43 - INFO - __main__ -   test: [batch: 67/196 ] | Loss: 0.683 | Acc: 82.112% (14294/17408)/ 96.054% (16721/17408)
01/04/2023 22:39:44 - INFO - __main__ -   test: [batch: 68/196 ] | Loss: 0.679 | Acc: 82.139% (14509/17664)/ 96.105% (16976/17664)
01/04/2023 22:39:45 - INFO - __main__ -   test: [batch: 69/196 ] | Loss: 0.684 | Acc: 81.998% (14694/17920)/ 96.105% (17222/17920)
01/04/2023 22:39:45 - INFO - __main__ -   test: [batch: 70/196 ] | Loss: 0.685 | Acc: 81.960% (14897/18176)/ 96.110% (17469/18176)
01/04/2023 22:39:46 - INFO - __main__ -   test: [batch: 71/196 ] | Loss: 0.683 | Acc: 82.058% (15125/18432)/ 96.110% (17715/18432)
01/04/2023 22:39:46 - INFO - __main__ -   test: [batch: 72/196 ] | Loss: 0.688 | Acc: 82.010% (15326/18688)/ 96.046% (17949/18688)
01/04/2023 22:39:47 - INFO - __main__ -   test: [batch: 73/196 ] | Loss: 0.689 | Acc: 82.000% (15534/18944)/ 96.015% (18189/18944)
01/04/2023 22:39:48 - INFO - __main__ -   test: [batch: 74/196 ] | Loss: 0.695 | Acc: 81.854% (15716/19200)/ 95.974% (18427/19200)
01/04/2023 22:39:48 - INFO - __main__ -   test: [batch: 75/196 ] | Loss: 0.695 | Acc: 81.821% (15919/19456)/ 95.981% (18674/19456)
01/04/2023 22:39:49 - INFO - __main__ -   test: [batch: 76/196 ] | Loss: 0.694 | Acc: 81.864% (16137/19712)/ 95.972% (18918/19712)
01/04/2023 22:39:49 - INFO - __main__ -   test: [batch: 77/196 ] | Loss: 0.695 | Acc: 81.881% (16350/19968)/ 95.944% (19158/19968)
01/04/2023 22:39:50 - INFO - __main__ -   test: [batch: 78/196 ] | Loss: 0.699 | Acc: 81.769% (16537/20224)/ 95.891% (19393/20224)
01/04/2023 22:39:51 - INFO - __main__ -   test: [batch: 79/196 ] | Loss: 0.702 | Acc: 81.704% (16733/20480)/ 95.869% (19634/20480)
01/04/2023 22:39:51 - INFO - __main__ -   test: [batch: 80/196 ] | Loss: 0.711 | Acc: 81.457% (16891/20736)/ 95.771% (19859/20736)
01/04/2023 22:39:52 - INFO - __main__ -   test: [batch: 81/196 ] | Loss: 0.719 | Acc: 81.302% (17067/20992)/ 95.689% (20087/20992)
01/04/2023 22:39:52 - INFO - __main__ -   test: [batch: 82/196 ] | Loss: 0.721 | Acc: 81.203% (17254/21248)/ 95.698% (20334/21248)
01/04/2023 22:39:53 - INFO - __main__ -   test: [batch: 83/196 ] | Loss: 0.723 | Acc: 81.152% (17451/21504)/ 95.666% (20572/21504)
01/04/2023 22:39:54 - INFO - __main__ -   test: [batch: 84/196 ] | Loss: 0.724 | Acc: 81.117% (17651/21760)/ 95.634% (20810/21760)
01/04/2023 22:39:54 - INFO - __main__ -   test: [batch: 85/196 ] | Loss: 0.732 | Acc: 80.955% (17823/22016)/ 95.567% (21040/22016)
01/04/2023 22:39:55 - INFO - __main__ -   test: [batch: 86/196 ] | Loss: 0.736 | Acc: 80.846% (18006/22272)/ 95.533% (21277/22272)
01/04/2023 22:39:55 - INFO - __main__ -   test: [batch: 87/196 ] | Loss: 0.738 | Acc: 80.811% (18205/22528)/ 95.490% (21512/22528)
01/04/2023 22:39:56 - INFO - __main__ -   test: [batch: 88/196 ] | Loss: 0.742 | Acc: 80.728% (18393/22784)/ 95.431% (21743/22784)
01/04/2023 22:39:57 - INFO - __main__ -   test: [batch: 89/196 ] | Loss: 0.747 | Acc: 80.647% (18581/23040)/ 95.373% (21974/23040)
01/04/2023 22:39:57 - INFO - __main__ -   test: [batch: 90/196 ] | Loss: 0.760 | Acc: 80.391% (18728/23296)/ 95.278% (22196/23296)
01/04/2023 22:39:58 - INFO - __main__ -   test: [batch: 91/196 ] | Loss: 0.765 | Acc: 80.295% (18911/23552)/ 95.194% (22420/23552)
01/04/2023 22:39:58 - INFO - __main__ -   test: [batch: 92/196 ] | Loss: 0.764 | Acc: 80.330% (19125/23808)/ 95.174% (22659/23808)
01/04/2023 22:39:59 - INFO - __main__ -   test: [batch: 93/196 ] | Loss: 0.772 | Acc: 80.182% (19295/24064)/ 95.121% (22890/24064)
01/04/2023 22:40:00 - INFO - __main__ -   test: [batch: 94/196 ] | Loss: 0.777 | Acc: 79.996% (19455/24320)/ 95.095% (23127/24320)
01/04/2023 22:40:00 - INFO - __main__ -   test: [batch: 95/196 ] | Loss: 0.784 | Acc: 79.875% (19630/24576)/ 95.056% (23361/24576)
01/04/2023 22:40:01 - INFO - __main__ -   test: [batch: 96/196 ] | Loss: 0.790 | Acc: 79.776% (19810/24832)/ 94.966% (23582/24832)
01/04/2023 22:40:01 - INFO - __main__ -   test: [batch: 97/196 ] | Loss: 0.798 | Acc: 79.640% (19980/25088)/ 94.894% (23807/25088)
01/04/2023 22:40:02 - INFO - __main__ -   test: [batch: 98/196 ] | Loss: 0.806 | Acc: 79.463% (20139/25344)/ 94.835% (24035/25344)
01/04/2023 22:40:02 - INFO - __main__ -   test: [batch: 99/196 ] | Loss: 0.807 | Acc: 79.426% (20333/25600)/ 94.801% (24269/25600)
01/04/2023 22:40:03 - INFO - __main__ -   test: [batch: 100/196 ] | Loss: 0.811 | Acc: 79.305% (20505/25856)/ 94.763% (24502/25856)
01/04/2023 22:40:04 - INFO - __main__ -   test: [batch: 101/196 ] | Loss: 0.813 | Acc: 79.285% (20703/26112)/ 94.738% (24738/26112)
01/04/2023 22:40:04 - INFO - __main__ -   test: [batch: 102/196 ] | Loss: 0.818 | Acc: 79.126% (20864/26368)/ 94.687% (24967/26368)
01/04/2023 22:40:05 - INFO - __main__ -   test: [batch: 103/196 ] | Loss: 0.824 | Acc: 79.030% (21041/26624)/ 94.644% (25198/26624)
01/04/2023 22:40:05 - INFO - __main__ -   test: [batch: 104/196 ] | Loss: 0.825 | Acc: 78.992% (21233/26880)/ 94.632% (25437/26880)
01/04/2023 22:40:06 - INFO - __main__ -   test: [batch: 105/196 ] | Loss: 0.829 | Acc: 78.925% (21417/27136)/ 94.590% (25668/27136)
01/04/2023 22:40:07 - INFO - __main__ -   test: [batch: 106/196 ] | Loss: 0.830 | Acc: 78.877% (21606/27392)/ 94.571% (25905/27392)
01/04/2023 22:40:07 - INFO - __main__ -   test: [batch: 107/196 ] | Loss: 0.831 | Acc: 78.874% (21807/27648)/ 94.564% (26145/27648)
01/04/2023 22:40:08 - INFO - __main__ -   test: [batch: 108/196 ] | Loss: 0.834 | Acc: 78.849% (22002/27904)/ 94.528% (26377/27904)
01/04/2023 22:40:08 - INFO - __main__ -   test: [batch: 109/196 ] | Loss: 0.835 | Acc: 78.814% (22194/28160)/ 94.496% (26610/28160)
01/04/2023 22:40:09 - INFO - __main__ -   test: [batch: 110/196 ] | Loss: 0.835 | Acc: 78.811% (22395/28416)/ 94.503% (26854/28416)
01/04/2023 22:40:10 - INFO - __main__ -   test: [batch: 111/196 ] | Loss: 0.834 | Acc: 78.857% (22610/28672)/ 94.503% (27096/28672)
01/04/2023 22:40:10 - INFO - __main__ -   test: [batch: 112/196 ] | Loss: 0.833 | Acc: 78.875% (22817/28928)/ 94.504% (27338/28928)
01/04/2023 22:40:11 - INFO - __main__ -   test: [batch: 113/196 ] | Loss: 0.835 | Acc: 78.834% (23007/29184)/ 94.490% (27576/29184)
01/04/2023 22:40:11 - INFO - __main__ -   test: [batch: 114/196 ] | Loss: 0.842 | Acc: 78.709% (23172/29440)/ 94.385% (27787/29440)
01/04/2023 22:40:12 - INFO - __main__ -   test: [batch: 115/196 ] | Loss: 0.845 | Acc: 78.637% (23352/29696)/ 94.343% (28016/29696)
01/04/2023 22:40:13 - INFO - __main__ -   test: [batch: 116/196 ] | Loss: 0.846 | Acc: 78.616% (23547/29952)/ 94.338% (28256/29952)
01/04/2023 22:40:13 - INFO - __main__ -   test: [batch: 117/196 ] | Loss: 0.852 | Acc: 78.479% (23707/30208)/ 94.266% (28476/30208)
01/04/2023 22:40:14 - INFO - __main__ -   test: [batch: 118/196 ] | Loss: 0.851 | Acc: 78.529% (23923/30464)/ 94.246% (28711/30464)
01/04/2023 22:40:14 - INFO - __main__ -   test: [batch: 119/196 ] | Loss: 0.849 | Acc: 78.610% (24149/30720)/ 94.261% (28957/30720)
01/04/2023 22:40:15 - INFO - __main__ -   test: [batch: 120/196 ] | Loss: 0.852 | Acc: 78.541% (24329/30976)/ 94.202% (29180/30976)
01/04/2023 22:40:15 - INFO - __main__ -   test: [batch: 121/196 ] | Loss: 0.860 | Acc: 78.308% (24457/31232)/ 94.112% (29393/31232)
01/04/2023 22:40:16 - INFO - __main__ -   test: [batch: 122/196 ] | Loss: 0.860 | Acc: 78.325% (24663/31488)/ 94.093% (29628/31488)
01/04/2023 22:40:17 - INFO - __main__ -   test: [batch: 123/196 ] | Loss: 0.866 | Acc: 78.204% (24825/31744)/ 94.008% (29842/31744)
01/04/2023 22:40:17 - INFO - __main__ -   test: [batch: 124/196 ] | Loss: 0.869 | Acc: 78.037% (24972/32000)/ 93.991% (30077/32000)
01/04/2023 22:40:18 - INFO - __main__ -   test: [batch: 125/196 ] | Loss: 0.870 | Acc: 78.026% (25168/32256)/ 93.979% (30314/32256)
01/04/2023 22:40:18 - INFO - __main__ -   test: [batch: 126/196 ] | Loss: 0.871 | Acc: 78.027% (25368/32512)/ 93.956% (30547/32512)
01/04/2023 22:40:19 - INFO - __main__ -   test: [batch: 127/196 ] | Loss: 0.877 | Acc: 77.930% (25536/32768)/ 93.909% (30772/32768)
01/04/2023 22:40:20 - INFO - __main__ -   test: [batch: 128/196 ] | Loss: 0.882 | Acc: 77.792% (25690/33024)/ 93.859% (30996/33024)
01/04/2023 22:40:20 - INFO - __main__ -   test: [batch: 129/196 ] | Loss: 0.887 | Acc: 77.665% (25847/33280)/ 93.834% (31228/33280)
01/04/2023 22:40:21 - INFO - __main__ -   test: [batch: 130/196 ] | Loss: 0.886 | Acc: 77.681% (26051/33536)/ 93.854% (31475/33536)
01/04/2023 22:40:21 - INFO - __main__ -   test: [batch: 131/196 ] | Loss: 0.887 | Acc: 77.631% (26233/33792)/ 93.836% (31709/33792)
01/04/2023 22:40:22 - INFO - __main__ -   test: [batch: 132/196 ] | Loss: 0.892 | Acc: 77.602% (26422/34048)/ 93.773% (31928/34048)
01/04/2023 22:40:23 - INFO - __main__ -   test: [batch: 133/196 ] | Loss: 0.894 | Acc: 77.562% (26607/34304)/ 93.756% (32162/34304)
01/04/2023 22:40:23 - INFO - __main__ -   test: [batch: 134/196 ] | Loss: 0.895 | Acc: 77.520% (26791/34560)/ 93.730% (32393/34560)
01/04/2023 22:40:24 - INFO - __main__ -   test: [batch: 135/196 ] | Loss: 0.898 | Acc: 77.464% (26970/34816)/ 93.695% (32621/34816)
01/04/2023 22:40:24 - INFO - __main__ -   test: [batch: 136/196 ] | Loss: 0.900 | Acc: 77.438% (27159/35072)/ 93.676% (32854/35072)
01/04/2023 22:40:25 - INFO - __main__ -   test: [batch: 137/196 ] | Loss: 0.901 | Acc: 77.406% (27346/35328)/ 93.674% (33093/35328)
01/04/2023 22:40:26 - INFO - __main__ -   test: [batch: 138/196 ] | Loss: 0.903 | Acc: 77.347% (27523/35584)/ 93.660% (33328/35584)
01/04/2023 22:40:26 - INFO - __main__ -   test: [batch: 139/196 ] | Loss: 0.903 | Acc: 77.363% (27727/35840)/ 93.650% (33564/35840)
01/04/2023 22:40:27 - INFO - __main__ -   test: [batch: 140/196 ] | Loss: 0.904 | Acc: 77.352% (27921/36096)/ 93.636% (33799/36096)
01/04/2023 22:40:27 - INFO - __main__ -   test: [batch: 141/196 ] | Loss: 0.903 | Acc: 77.360% (28122/36352)/ 93.632% (34037/36352)
01/04/2023 22:40:28 - INFO - __main__ -   test: [batch: 142/196 ] | Loss: 0.909 | Acc: 77.270% (28287/36608)/ 93.553% (34248/36608)
01/04/2023 22:40:29 - INFO - __main__ -   test: [batch: 143/196 ] | Loss: 0.910 | Acc: 77.230% (28470/36864)/ 93.530% (34479/36864)
01/04/2023 22:40:29 - INFO - __main__ -   test: [batch: 144/196 ] | Loss: 0.911 | Acc: 77.212% (28661/37120)/ 93.502% (34708/37120)
01/04/2023 22:40:30 - INFO - __main__ -   test: [batch: 145/196 ] | Loss: 0.913 | Acc: 77.148% (28835/37376)/ 93.499% (34946/37376)
01/04/2023 22:40:30 - INFO - __main__ -   test: [batch: 146/196 ] | Loss: 0.915 | Acc: 77.089% (29010/37632)/ 93.474% (35176/37632)
01/04/2023 22:40:31 - INFO - __main__ -   test: [batch: 147/196 ] | Loss: 0.915 | Acc: 77.098% (29211/37888)/ 93.473% (35415/37888)
01/04/2023 22:40:31 - INFO - __main__ -   test: [batch: 148/196 ] | Loss: 0.917 | Acc: 77.082% (29402/38144)/ 93.438% (35641/38144)
01/04/2023 22:40:32 - INFO - __main__ -   test: [batch: 149/196 ] | Loss: 0.920 | Acc: 77.044% (29585/38400)/ 93.393% (35863/38400)
01/04/2023 22:40:33 - INFO - __main__ -   test: [batch: 150/196 ] | Loss: 0.921 | Acc: 77.036% (29779/38656)/ 93.372% (36094/38656)
01/04/2023 22:40:33 - INFO - __main__ -   test: [batch: 151/196 ] | Loss: 0.924 | Acc: 76.976% (29953/38912)/ 93.344% (36322/38912)
01/04/2023 22:40:34 - INFO - __main__ -   test: [batch: 152/196 ] | Loss: 0.924 | Acc: 76.945% (30138/39168)/ 93.347% (36562/39168)
01/04/2023 22:40:34 - INFO - __main__ -   test: [batch: 153/196 ] | Loss: 0.926 | Acc: 76.933% (30330/39424)/ 93.311% (36787/39424)
01/04/2023 22:40:35 - INFO - __main__ -   test: [batch: 154/196 ] | Loss: 0.927 | Acc: 76.913% (30519/39680)/ 93.291% (37018/39680)
01/04/2023 22:40:36 - INFO - __main__ -   test: [batch: 155/196 ] | Loss: 0.929 | Acc: 76.873% (30700/39936)/ 93.272% (37249/39936)
01/04/2023 22:40:36 - INFO - __main__ -   test: [batch: 156/196 ] | Loss: 0.929 | Acc: 76.896% (30906/40192)/ 93.265% (37485/40192)
01/04/2023 22:40:37 - INFO - __main__ -   test: [batch: 157/196 ] | Loss: 0.930 | Acc: 76.864% (31090/40448)/ 93.246% (37716/40448)
01/04/2023 22:40:37 - INFO - __main__ -   test: [batch: 158/196 ] | Loss: 0.935 | Acc: 76.722% (31229/40704)/ 93.197% (37935/40704)
01/04/2023 22:40:38 - INFO - __main__ -   test: [batch: 159/196 ] | Loss: 0.936 | Acc: 76.697% (31415/40960)/ 93.181% (38167/40960)
01/04/2023 22:40:39 - INFO - __main__ -   test: [batch: 160/196 ] | Loss: 0.935 | Acc: 76.749% (31633/41216)/ 93.182% (38406/41216)
01/04/2023 22:40:39 - INFO - __main__ -   test: [batch: 161/196 ] | Loss: 0.938 | Acc: 76.693% (31806/41472)/ 93.152% (38632/41472)
01/04/2023 22:40:40 - INFO - __main__ -   test: [batch: 162/196 ] | Loss: 0.938 | Acc: 76.697% (32004/41728)/ 93.141% (38866/41728)
01/04/2023 22:40:40 - INFO - __main__ -   test: [batch: 163/196 ] | Loss: 0.945 | Acc: 76.541% (32135/41984)/ 93.085% (39081/41984)
01/04/2023 22:40:41 - INFO - __main__ -   test: [batch: 164/196 ] | Loss: 0.947 | Acc: 76.487% (32308/42240)/ 93.066% (39311/42240)
01/04/2023 22:40:41 - INFO - __main__ -   test: [batch: 165/196 ] | Loss: 0.949 | Acc: 76.412% (32472/42496)/ 93.056% (39545/42496)
01/04/2023 22:40:42 - INFO - __main__ -   test: [batch: 166/196 ] | Loss: 0.948 | Acc: 76.429% (32675/42752)/ 93.062% (39786/42752)
01/04/2023 22:40:43 - INFO - __main__ -   test: [batch: 167/196 ] | Loss: 0.950 | Acc: 76.372% (32846/43008)/ 93.034% (40012/43008)
01/04/2023 22:40:43 - INFO - __main__ -   test: [batch: 168/196 ] | Loss: 0.952 | Acc: 76.317% (33018/43264)/ 93.015% (40242/43264)
01/04/2023 22:40:44 - INFO - __main__ -   test: [batch: 169/196 ] | Loss: 0.954 | Acc: 76.266% (33191/43520)/ 92.999% (40473/43520)
01/04/2023 22:40:44 - INFO - __main__ -   test: [batch: 170/196 ] | Loss: 0.952 | Acc: 76.293% (33398/43776)/ 93.024% (40722/43776)
01/04/2023 22:40:45 - INFO - __main__ -   test: [batch: 171/196 ] | Loss: 0.953 | Acc: 76.245% (33572/44032)/ 93.021% (40959/44032)
01/04/2023 22:40:46 - INFO - __main__ -   test: [batch: 172/196 ] | Loss: 0.957 | Acc: 76.176% (33737/44288)/ 92.976% (41177/44288)
01/04/2023 22:40:46 - INFO - __main__ -   test: [batch: 173/196 ] | Loss: 0.958 | Acc: 76.170% (33929/44544)/ 92.971% (41413/44544)
01/04/2023 22:40:47 - INFO - __main__ -   test: [batch: 174/196 ] | Loss: 0.959 | Acc: 76.147% (34114/44800)/ 92.953% (41643/44800)
01/04/2023 22:40:47 - INFO - __main__ -   test: [batch: 175/196 ] | Loss: 0.961 | Acc: 76.108% (34291/45056)/ 92.933% (41872/45056)
01/04/2023 22:40:48 - INFO - __main__ -   test: [batch: 176/196 ] | Loss: 0.964 | Acc: 76.066% (34467/45312)/ 92.889% (42090/45312)
01/04/2023 22:40:49 - INFO - __main__ -   test: [batch: 177/196 ] | Loss: 0.968 | Acc: 75.939% (34604/45568)/ 92.870% (42319/45568)
01/04/2023 22:40:49 - INFO - __main__ -   test: [batch: 178/196 ] | Loss: 0.967 | Acc: 75.967% (34811/45824)/ 92.881% (42562/45824)
01/04/2023 22:40:50 - INFO - __main__ -   test: [batch: 179/196 ] | Loss: 0.967 | Acc: 76.000% (35021/46080)/ 92.880% (42799/46080)
01/04/2023 22:40:50 - INFO - __main__ -   test: [batch: 180/196 ] | Loss: 0.969 | Acc: 75.967% (35200/46336)/ 92.874% (43034/46336)
01/04/2023 22:40:51 - INFO - __main__ -   test: [batch: 181/196 ] | Loss: 0.969 | Acc: 75.957% (35390/46592)/ 92.879% (43274/46592)
01/04/2023 22:40:52 - INFO - __main__ -   test: [batch: 182/196 ] | Loss: 0.967 | Acc: 76.001% (35605/46848)/ 92.894% (43519/46848)
01/04/2023 22:40:52 - INFO - __main__ -   test: [batch: 183/196 ] | Loss: 0.964 | Acc: 76.061% (35828/47104)/ 92.924% (43771/47104)
01/04/2023 22:40:53 - INFO - __main__ -   test: [batch: 184/196 ] | Loss: 0.963 | Acc: 76.087% (36035/47360)/ 92.941% (44017/47360)
01/04/2023 22:40:53 - INFO - __main__ -   test: [batch: 185/196 ] | Loss: 0.963 | Acc: 76.075% (36224/47616)/ 92.944% (44256/47616)
01/04/2023 22:40:54 - INFO - __main__ -   test: [batch: 186/196 ] | Loss: 0.960 | Acc: 76.159% (36459/47872)/ 92.965% (44504/47872)
01/04/2023 22:40:54 - INFO - __main__ -   test: [batch: 187/196 ] | Loss: 0.962 | Acc: 76.118% (36634/48128)/ 92.936% (44728/48128)
01/04/2023 22:40:55 - INFO - __main__ -   test: [batch: 188/196 ] | Loss: 0.963 | Acc: 76.100% (36820/48384)/ 92.915% (44956/48384)
01/04/2023 22:40:56 - INFO - __main__ -   test: [batch: 189/196 ] | Loss: 0.966 | Acc: 76.009% (36971/48640)/ 92.876% (45175/48640)
01/04/2023 22:40:56 - INFO - __main__ -   test: [batch: 190/196 ] | Loss: 0.967 | Acc: 75.963% (37143/48896)/ 92.883% (45416/48896)
01/04/2023 22:40:57 - INFO - __main__ -   test: [batch: 191/196 ] | Loss: 0.969 | Acc: 75.938% (37325/49152)/ 92.865% (45645/49152)
01/04/2023 22:40:57 - INFO - __main__ -   test: [batch: 192/196 ] | Loss: 0.966 | Acc: 75.990% (37545/49408)/ 92.886% (45893/49408)
01/04/2023 22:40:58 - INFO - __main__ -   test: [batch: 193/196 ] | Loss: 0.963 | Acc: 76.077% (37783/49664)/ 92.912% (46144/49664)
01/04/2023 22:40:59 - INFO - __main__ -   test: [batch: 194/196 ] | Loss: 0.960 | Acc: 76.126% (38002/49920)/ 92.927% (46389/49920)
01/04/2023 22:40:59 - INFO - __main__ -   test: [batch: 195/196 ] | Loss: 0.965 | Acc: 76.076% (38038/50000)/ 92.918% (46459/50000)
01/04/2023 22:40:59 - INFO - __main__ -   Final accuracy: 76.076
