/home/jjc/miniconda3/envs/ant_quant/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
Namespace(a_low=75, a_up=150, abit=8, batch_size=128, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='csd_eb2', epoch=3, layer_4bit_l='28,40,18,0,39,27,33,40,27,33,39,13,43', layer_8bit_l=None, layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='resnet50', percent=100, ptq=False, resume=False, search=False, tag='', train=True, w_low=75, w_up=150, wbit=8)
01/15/2023 02:49:40 - INFO - __main__ -   output/resnet50_imagenet/int_W8A8_26595/gpu_0
01/15/2023 02:49:40 - INFO - __main__ -   Namespace(a_low=75, a_up=150, abit=8, batch_size=128, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='csd_eb2', epoch=3, layer_4bit_l='28,40,18,0,39,27,33,40,27,33,39,13,43', layer_8bit_l=None, layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='resnet50', percent=100, ptq=False, resume=False, search=False, tag='', train=True, w_low=75, w_up=150, wbit=8)
01/15/2023 02:49:40 - INFO - __main__ -   ==> Preparing data..
01/15/2023 02:49:42 - INFO - __main__ -   ==> Setting quantizer..
Namespace(a_low=75, a_up=150, abit=8, batch_size=128, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='csd_eb2', epoch=3, layer_4bit_l='28,40,18,0,39,27,33,40,27,33,39,13,43', layer_8bit_l=None, layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='resnet50', percent=100, ptq=False, resume=False, search=False, tag='', train=True, w_low=75, w_up=150, wbit=8)
01/15/2023 02:49:42 - INFO - __main__ -   Namespace(a_low=75, a_up=150, abit=8, batch_size=128, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='csd_eb2', epoch=3, layer_4bit_l='28,40,18,0,39,27,33,40,27,33,39,13,43', layer_8bit_l=None, layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='resnet50', percent=100, ptq=False, resume=False, search=False, tag='', train=True, w_low=75, w_up=150, wbit=8)
01/15/2023 02:49:42 - INFO - __main__ -   ==> Building model..
ResNet(
  (conv1): Conv2dQuantizer(
    (quant_weight): TensorQuantizer()
    (quant_input): TensorQuantizer()
  )
  (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (1): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (1): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): LinearQuantizer(
    (quant_weight): TensorQuantizer()
    (quant_input): TensorQuantizer()
  )
)
01/15/2023 02:49:43 - INFO - __main__ -   {'milestones': Counter({2: 1, 3: 1, 4: 1}), 'gamma': 0.1, 'base_lrs': [0.00025], 'last_epoch': 0, '_step_count': 1, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.00025]}
01/15/2023 02:49:43 - INFO - __main__ -   
Epoch: 0
Layer quant EB csd_eb2
int	8-bit 	 conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.3.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.3.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.3.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.3.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.3.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.3.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.4.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.4.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.4.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.5.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.5.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.5.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 fc.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 fc.quant_input,
set init to 1
------------- 8-bit EB1 Re-SET -------------
13
conv1.quant_weight 0
conv1.quant_input 0
layer2.0.conv3.quant_weight 13
layer2.0.conv3.quant_input 13
layer2.2.conv1.quant_weight 18
layer2.2.conv1.quant_input 18
layer3.0.downsample.0.quant_weight 27
layer3.0.downsample.0.quant_input 27
layer3.1.conv1.quant_weight 28
layer3.1.conv1.quant_input 28
layer3.2.conv3.quant_weight 33
layer3.2.conv3.quant_input 33
layer3.4.conv3.quant_weight 39
layer3.4.conv3.quant_input 39
layer3.5.conv1.quant_weight 40
layer3.5.conv1.quant_input 40
layer4.0.conv1.quant_weight 43
layer4.0.conv1.quant_input 43
------------- 8-bit EB1 Re-SET -------------
Layer quant EB csd_eb1
int	8-bit 	 conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer2.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer2.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.3.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.3.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.3.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer3.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer3.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer3.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.3.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.3.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.3.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.4.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.4.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv2.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer3.4.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv3.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer3.5.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.5.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.5.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv3.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer4.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 fc.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 fc.quant_input,
set init to 1
01/15/2023 02:50:38 - INFO - __main__ -   test: [epoch: 0 | batch: 0/10010 ] | Loss: 0.738 | Acc: 80.469% (103/128)
01/15/2023 02:54:06 - INFO - __main__ -   test: [epoch: 0 | batch: 100/10010 ] | Loss: 0.909 | Acc: 77.607% (10033/12928)
01/15/2023 02:57:38 - INFO - __main__ -   test: [epoch: 0 | batch: 200/10010 ] | Loss: 0.893 | Acc: 77.771% (20009/25728)
01/15/2023 03:01:10 - INFO - __main__ -   test: [epoch: 0 | batch: 300/10010 ] | Loss: 0.909 | Acc: 77.354% (29803/38528)
01/15/2023 03:04:41 - INFO - __main__ -   test: [epoch: 0 | batch: 400/10010 ] | Loss: 0.920 | Acc: 77.108% (39578/51328)
01/15/2023 03:08:12 - INFO - __main__ -   test: [epoch: 0 | batch: 500/10010 ] | Loss: 0.923 | Acc: 77.010% (49385/64128)
01/15/2023 03:11:43 - INFO - __main__ -   test: [epoch: 0 | batch: 600/10010 ] | Loss: 0.924 | Acc: 76.998% (59233/76928)
01/15/2023 03:15:14 - INFO - __main__ -   test: [epoch: 0 | batch: 700/10010 ] | Loss: 0.924 | Acc: 77.055% (69140/89728)
01/15/2023 03:18:45 - INFO - __main__ -   test: [epoch: 0 | batch: 800/10010 ] | Loss: 0.926 | Acc: 76.982% (78928/102528)
01/15/2023 03:22:16 - INFO - __main__ -   test: [epoch: 0 | batch: 900/10010 ] | Loss: 0.927 | Acc: 76.997% (88799/115328)
01/15/2023 03:25:47 - INFO - __main__ -   test: [epoch: 0 | batch: 1000/10010 ] | Loss: 0.927 | Acc: 76.975% (98626/128128)
01/15/2023 03:29:19 - INFO - __main__ -   test: [epoch: 0 | batch: 1100/10010 ] | Loss: 0.929 | Acc: 76.968% (108470/140928)
01/15/2023 03:32:49 - INFO - __main__ -   test: [epoch: 0 | batch: 1200/10010 ] | Loss: 0.931 | Acc: 76.950% (118294/153728)
01/15/2023 03:36:21 - INFO - __main__ -   test: [epoch: 0 | batch: 1300/10010 ] | Loss: 0.929 | Acc: 77.000% (128226/166528)
01/15/2023 03:39:52 - INFO - __main__ -   test: [epoch: 0 | batch: 1400/10010 ] | Loss: 0.930 | Acc: 77.001% (138084/179328)
01/15/2023 03:43:23 - INFO - __main__ -   test: [epoch: 0 | batch: 1500/10010 ] | Loss: 0.928 | Acc: 77.025% (147987/192128)
01/15/2023 03:46:54 - INFO - __main__ -   test: [epoch: 0 | batch: 1600/10010 ] | Loss: 0.929 | Acc: 77.015% (157826/204928)
01/15/2023 03:50:26 - INFO - __main__ -   test: [epoch: 0 | batch: 1700/10010 ] | Loss: 0.929 | Acc: 77.019% (167693/217728)
01/15/2023 03:53:58 - INFO - __main__ -   test: [epoch: 0 | batch: 1800/10010 ] | Loss: 0.927 | Acc: 77.072% (177673/230528)
01/15/2023 03:57:29 - INFO - __main__ -   test: [epoch: 0 | batch: 1900/10010 ] | Loss: 0.926 | Acc: 77.090% (187582/243328)
01/15/2023 04:00:59 - INFO - __main__ -   test: [epoch: 0 | batch: 2000/10010 ] | Loss: 0.926 | Acc: 77.091% (197452/256128)
01/15/2023 04:04:30 - INFO - __main__ -   test: [epoch: 0 | batch: 2100/10010 ] | Loss: 0.927 | Acc: 77.075% (207275/268928)
01/15/2023 04:08:02 - INFO - __main__ -   test: [epoch: 0 | batch: 2200/10010 ] | Loss: 0.927 | Acc: 77.059% (217096/281728)
01/15/2023 04:11:32 - INFO - __main__ -   test: [epoch: 0 | batch: 2300/10010 ] | Loss: 0.928 | Acc: 77.040% (226903/294528)
01/15/2023 04:15:04 - INFO - __main__ -   test: [epoch: 0 | batch: 2400/10010 ] | Loss: 0.928 | Acc: 77.042% (236771/307328)
01/15/2023 04:18:36 - INFO - __main__ -   test: [epoch: 0 | batch: 2500/10010 ] | Loss: 0.928 | Acc: 77.048% (246653/320128)
01/15/2023 04:22:08 - INFO - __main__ -   test: [epoch: 0 | batch: 2600/10010 ] | Loss: 0.929 | Acc: 77.037% (256477/332928)
01/15/2023 04:25:40 - INFO - __main__ -   test: [epoch: 0 | batch: 2700/10010 ] | Loss: 0.928 | Acc: 77.053% (266395/345728)
01/15/2023 04:29:11 - INFO - __main__ -   test: [epoch: 0 | batch: 2800/10010 ] | Loss: 0.928 | Acc: 77.039% (276208/358528)
01/15/2023 04:32:42 - INFO - __main__ -   test: [epoch: 0 | batch: 2900/10010 ] | Loss: 0.928 | Acc: 77.049% (286104/371328)
01/15/2023 04:36:13 - INFO - __main__ -   test: [epoch: 0 | batch: 3000/10010 ] | Loss: 0.927 | Acc: 77.052% (295980/384128)
01/15/2023 04:39:46 - INFO - __main__ -   test: [epoch: 0 | batch: 3100/10010 ] | Loss: 0.927 | Acc: 77.056% (305857/396928)
01/15/2023 04:43:17 - INFO - __main__ -   test: [epoch: 0 | batch: 3200/10010 ] | Loss: 0.928 | Acc: 77.046% (315681/409728)
01/15/2023 04:46:50 - INFO - __main__ -   test: [epoch: 0 | batch: 3300/10010 ] | Loss: 0.928 | Acc: 77.037% (325501/422528)
01/15/2023 04:50:22 - INFO - __main__ -   test: [epoch: 0 | batch: 3400/10010 ] | Loss: 0.928 | Acc: 77.039% (335374/435328)
01/15/2023 04:53:54 - INFO - __main__ -   test: [epoch: 0 | batch: 3500/10010 ] | Loss: 0.928 | Acc: 77.040% (345236/448128)
01/15/2023 04:57:24 - INFO - __main__ -   test: [epoch: 0 | batch: 3600/10010 ] | Loss: 0.928 | Acc: 77.030% (355051/460928)
01/15/2023 05:00:54 - INFO - __main__ -   test: [epoch: 0 | batch: 3700/10010 ] | Loss: 0.929 | Acc: 77.010% (364817/473728)
01/15/2023 05:04:25 - INFO - __main__ -   test: [epoch: 0 | batch: 3800/10010 ] | Loss: 0.928 | Acc: 77.015% (374700/486528)
01/15/2023 05:07:55 - INFO - __main__ -   test: [epoch: 0 | batch: 3900/10010 ] | Loss: 0.929 | Acc: 77.002% (384492/499328)
01/15/2023 05:11:29 - INFO - __main__ -   test: [epoch: 0 | batch: 4000/10010 ] | Loss: 0.929 | Acc: 76.993% (394305/512128)
01/15/2023 05:14:59 - INFO - __main__ -   test: [epoch: 0 | batch: 4100/10010 ] | Loss: 0.929 | Acc: 76.995% (404168/524928)
01/15/2023 05:18:30 - INFO - __main__ -   test: [epoch: 0 | batch: 4200/10010 ] | Loss: 0.929 | Acc: 76.997% (414032/537728)
01/15/2023 05:22:01 - INFO - __main__ -   test: [epoch: 0 | batch: 4300/10010 ] | Loss: 0.929 | Acc: 77.002% (423919/550528)
01/15/2023 05:25:33 - INFO - __main__ -   test: [epoch: 0 | batch: 4400/10010 ] | Loss: 0.929 | Acc: 77.008% (433805/563328)
01/15/2023 05:29:05 - INFO - __main__ -   test: [epoch: 0 | batch: 4500/10010 ] | Loss: 0.929 | Acc: 76.996% (443594/576128)
01/15/2023 05:32:37 - INFO - __main__ -   test: [epoch: 0 | batch: 4600/10010 ] | Loss: 0.929 | Acc: 77.009% (453528/588928)
01/15/2023 05:36:07 - INFO - __main__ -   test: [epoch: 0 | batch: 4700/10010 ] | Loss: 0.929 | Acc: 77.007% (463371/601728)
01/15/2023 05:39:39 - INFO - __main__ -   test: [epoch: 0 | batch: 4800/10010 ] | Loss: 0.929 | Acc: 77.006% (473223/614528)
01/15/2023 05:43:11 - INFO - __main__ -   test: [epoch: 0 | batch: 4900/10010 ] | Loss: 0.929 | Acc: 77.003% (483060/627328)
01/15/2023 05:46:43 - INFO - __main__ -   test: [epoch: 0 | batch: 5000/10010 ] | Loss: 0.929 | Acc: 77.015% (492995/640128)
01/15/2023 05:50:14 - INFO - __main__ -   test: [epoch: 0 | batch: 5100/10010 ] | Loss: 0.928 | Acc: 77.019% (502876/652928)
01/15/2023 05:53:46 - INFO - __main__ -   test: [epoch: 0 | batch: 5200/10010 ] | Loss: 0.928 | Acc: 77.016% (512720/665728)
01/15/2023 05:57:17 - INFO - __main__ -   test: [epoch: 0 | batch: 5300/10010 ] | Loss: 0.929 | Acc: 77.016% (522575/678528)
01/15/2023 06:00:48 - INFO - __main__ -   test: [epoch: 0 | batch: 5400/10010 ] | Loss: 0.929 | Acc: 77.009% (532388/691328)
01/15/2023 06:04:19 - INFO - __main__ -   test: [epoch: 0 | batch: 5500/10010 ] | Loss: 0.929 | Acc: 77.001% (542186/704128)
01/15/2023 06:07:50 - INFO - __main__ -   test: [epoch: 0 | batch: 5600/10010 ] | Loss: 0.929 | Acc: 77.008% (552094/716928)
01/15/2023 06:11:21 - INFO - __main__ -   test: [epoch: 0 | batch: 5700/10010 ] | Loss: 0.929 | Acc: 77.009% (561958/729728)
01/15/2023 06:14:52 - INFO - __main__ -   test: [epoch: 0 | batch: 5800/10010 ] | Loss: 0.928 | Acc: 77.011% (571829/742528)
01/15/2023 06:18:23 - INFO - __main__ -   test: [epoch: 0 | batch: 5900/10010 ] | Loss: 0.929 | Acc: 77.012% (581695/755328)
01/15/2023 06:21:55 - INFO - __main__ -   test: [epoch: 0 | batch: 6000/10010 ] | Loss: 0.929 | Acc: 77.007% (591509/768128)
01/15/2023 06:25:26 - INFO - __main__ -   test: [epoch: 0 | batch: 6100/10010 ] | Loss: 0.929 | Acc: 77.003% (601336/780928)
01/15/2023 06:28:58 - INFO - __main__ -   test: [epoch: 0 | batch: 6200/10010 ] | Loss: 0.929 | Acc: 76.999% (611162/793728)
01/15/2023 06:32:31 - INFO - __main__ -   test: [epoch: 0 | batch: 6300/10010 ] | Loss: 0.930 | Acc: 76.993% (620971/806528)
01/15/2023 06:36:02 - INFO - __main__ -   test: [epoch: 0 | batch: 6400/10010 ] | Loss: 0.930 | Acc: 76.988% (630782/819328)
01/15/2023 06:39:33 - INFO - __main__ -   test: [epoch: 0 | batch: 6500/10010 ] | Loss: 0.930 | Acc: 76.988% (640640/832128)
01/15/2023 06:43:03 - INFO - __main__ -   test: [epoch: 0 | batch: 6600/10010 ] | Loss: 0.930 | Acc: 76.986% (650480/844928)
01/15/2023 06:46:35 - INFO - __main__ -   test: [epoch: 0 | batch: 6700/10010 ] | Loss: 0.929 | Acc: 76.990% (660364/857728)
01/15/2023 06:50:06 - INFO - __main__ -   test: [epoch: 0 | batch: 6800/10010 ] | Loss: 0.930 | Acc: 76.987% (670191/870528)
01/15/2023 06:53:36 - INFO - __main__ -   test: [epoch: 0 | batch: 6900/10010 ] | Loss: 0.929 | Acc: 76.995% (680122/883328)
01/15/2023 06:57:08 - INFO - __main__ -   test: [epoch: 0 | batch: 7000/10010 ] | Loss: 0.929 | Acc: 76.990% (689930/896128)
01/15/2023 07:00:39 - INFO - __main__ -   test: [epoch: 0 | batch: 7100/10010 ] | Loss: 0.929 | Acc: 76.998% (699856/908928)
01/15/2023 07:04:11 - INFO - __main__ -   test: [epoch: 0 | batch: 7200/10010 ] | Loss: 0.929 | Acc: 77.007% (709795/921728)
01/15/2023 07:07:44 - INFO - __main__ -   test: [epoch: 0 | batch: 7300/10010 ] | Loss: 0.929 | Acc: 76.998% (719566/934528)
01/15/2023 07:11:13 - INFO - __main__ -   test: [epoch: 0 | batch: 7400/10010 ] | Loss: 0.929 | Acc: 76.989% (729338/947328)
01/15/2023 07:14:46 - INFO - __main__ -   test: [epoch: 0 | batch: 7500/10010 ] | Loss: 0.929 | Acc: 76.992% (739223/960128)
01/15/2023 07:18:17 - INFO - __main__ -   test: [epoch: 0 | batch: 7600/10010 ] | Loss: 0.929 | Acc: 76.991% (749071/972928)
01/15/2023 07:21:49 - INFO - __main__ -   test: [epoch: 0 | batch: 7700/10010 ] | Loss: 0.930 | Acc: 76.983% (758847/985728)
01/15/2023 07:25:22 - INFO - __main__ -   test: [epoch: 0 | batch: 7800/10010 ] | Loss: 0.930 | Acc: 76.980% (768663/998528)
01/15/2023 07:28:54 - INFO - __main__ -   test: [epoch: 0 | batch: 7900/10010 ] | Loss: 0.930 | Acc: 76.982% (778544/1011328)
01/15/2023 07:32:26 - INFO - __main__ -   test: [epoch: 0 | batch: 8000/10010 ] | Loss: 0.930 | Acc: 76.977% (788339/1024128)
01/15/2023 07:35:58 - INFO - __main__ -   test: [epoch: 0 | batch: 8100/10010 ] | Loss: 0.930 | Acc: 76.981% (798241/1036928)
01/15/2023 07:39:29 - INFO - __main__ -   test: [epoch: 0 | batch: 8200/10010 ] | Loss: 0.930 | Acc: 76.977% (808049/1049728)
01/15/2023 07:43:02 - INFO - __main__ -   test: [epoch: 0 | batch: 8300/10010 ] | Loss: 0.930 | Acc: 76.979% (817924/1062528)
01/15/2023 07:46:32 - INFO - __main__ -   test: [epoch: 0 | batch: 8400/10010 ] | Loss: 0.929 | Acc: 76.985% (827836/1075328)
01/15/2023 07:50:04 - INFO - __main__ -   test: [epoch: 0 | batch: 8500/10010 ] | Loss: 0.929 | Acc: 76.990% (837753/1088128)
01/15/2023 07:53:36 - INFO - __main__ -   test: [epoch: 0 | batch: 8600/10010 ] | Loss: 0.930 | Acc: 76.985% (847554/1100928)
01/15/2023 07:57:08 - INFO - __main__ -   test: [epoch: 0 | batch: 8700/10010 ] | Loss: 0.930 | Acc: 76.987% (857424/1113728)
01/15/2023 08:00:38 - INFO - __main__ -   test: [epoch: 0 | batch: 8800/10010 ] | Loss: 0.930 | Acc: 76.990% (867311/1126528)
01/15/2023 08:04:08 - INFO - __main__ -   test: [epoch: 0 | batch: 8900/10010 ] | Loss: 0.930 | Acc: 76.990% (877164/1139328)
01/15/2023 08:07:38 - INFO - __main__ -   test: [epoch: 0 | batch: 9000/10010 ] | Loss: 0.929 | Acc: 76.995% (887081/1152128)
01/15/2023 08:11:11 - INFO - __main__ -   test: [epoch: 0 | batch: 9100/10010 ] | Loss: 0.929 | Acc: 76.993% (896913/1164928)
01/15/2023 08:14:43 - INFO - __main__ -   test: [epoch: 0 | batch: 9200/10010 ] | Loss: 0.929 | Acc: 76.991% (906744/1177728)
01/15/2023 08:18:13 - INFO - __main__ -   test: [epoch: 0 | batch: 9300/10010 ] | Loss: 0.930 | Acc: 76.994% (916636/1190528)
01/15/2023 08:21:45 - INFO - __main__ -   test: [epoch: 0 | batch: 9400/10010 ] | Loss: 0.930 | Acc: 76.990% (926441/1203328)
01/15/2023 08:25:16 - INFO - __main__ -   test: [epoch: 0 | batch: 9500/10010 ] | Loss: 0.929 | Acc: 76.995% (936363/1216128)
01/15/2023 08:28:49 - INFO - __main__ -   test: [epoch: 0 | batch: 9600/10010 ] | Loss: 0.930 | Acc: 76.992% (946177/1228928)
01/15/2023 08:32:19 - INFO - __main__ -   test: [epoch: 0 | batch: 9700/10010 ] | Loss: 0.930 | Acc: 76.990% (956006/1241728)
01/15/2023 08:35:51 - INFO - __main__ -   test: [epoch: 0 | batch: 9800/10010 ] | Loss: 0.930 | Acc: 76.987% (965824/1254528)
01/15/2023 08:39:24 - INFO - __main__ -   test: [epoch: 0 | batch: 9900/10010 ] | Loss: 0.930 | Acc: 76.991% (975732/1267328)
01/15/2023 08:42:57 - INFO - __main__ -   test: [epoch: 0 | batch: 10000/10010 ] | Loss: 0.930 | Acc: 76.989% (985554/1280128)
01/15/2023 08:43:16 - INFO - __main__ -   Saving Checkpoint
01/15/2023 08:43:19 - INFO - __main__ -   test: [batch: 0/391 ] | Loss: 0.537 | Acc: 86.719% (111/128)/ 95.312% (122/128)
01/15/2023 08:43:21 - INFO - __main__ -   test: [batch: 1/391 ] | Loss: 0.536 | Acc: 85.938% (220/256)/ 96.875% (248/256)
01/15/2023 08:43:23 - INFO - __main__ -   test: [batch: 2/391 ] | Loss: 0.735 | Acc: 80.469% (309/384)/ 94.531% (363/384)
01/15/2023 08:43:25 - INFO - __main__ -   test: [batch: 3/391 ] | Loss: 0.672 | Acc: 82.812% (424/512)/ 95.117% (487/512)
01/15/2023 08:43:27 - INFO - __main__ -   test: [batch: 4/391 ] | Loss: 0.575 | Acc: 85.000% (544/640)/ 96.094% (615/640)
01/15/2023 08:43:29 - INFO - __main__ -   test: [batch: 5/391 ] | Loss: 0.510 | Acc: 86.328% (663/768)/ 96.745% (743/768)
01/15/2023 08:43:31 - INFO - __main__ -   test: [batch: 6/391 ] | Loss: 0.497 | Acc: 86.830% (778/896)/ 96.763% (867/896)
01/15/2023 08:43:33 - INFO - __main__ -   test: [batch: 7/391 ] | Loss: 0.474 | Acc: 87.793% (899/1024)/ 96.875% (992/1024)
01/15/2023 08:43:35 - INFO - __main__ -   test: [batch: 8/391 ] | Loss: 0.482 | Acc: 87.847% (1012/1152)/ 96.875% (1116/1152)
01/15/2023 08:43:38 - INFO - __main__ -   test: [batch: 9/391 ] | Loss: 0.458 | Acc: 88.516% (1133/1280)/ 97.031% (1242/1280)
01/15/2023 08:43:40 - INFO - __main__ -   test: [batch: 10/391 ] | Loss: 0.504 | Acc: 87.287% (1229/1408)/ 96.875% (1364/1408)
01/15/2023 08:43:42 - INFO - __main__ -   test: [batch: 11/391 ] | Loss: 0.507 | Acc: 87.370% (1342/1536)/ 96.745% (1486/1536)
01/15/2023 08:43:44 - INFO - __main__ -   test: [batch: 12/391 ] | Loss: 0.562 | Acc: 85.998% (1431/1664)/ 96.334% (1603/1664)
01/15/2023 08:43:46 - INFO - __main__ -   test: [batch: 13/391 ] | Loss: 0.619 | Acc: 84.431% (1513/1792)/ 95.592% (1713/1792)
01/15/2023 08:43:48 - INFO - __main__ -   test: [batch: 14/391 ] | Loss: 0.636 | Acc: 83.698% (1607/1920)/ 95.781% (1839/1920)
01/15/2023 08:43:50 - INFO - __main__ -   test: [batch: 15/391 ] | Loss: 0.641 | Acc: 83.203% (1704/2048)/ 95.947% (1965/2048)
01/15/2023 08:43:53 - INFO - __main__ -   test: [batch: 16/391 ] | Loss: 0.648 | Acc: 83.180% (1810/2176)/ 95.818% (2085/2176)
01/15/2023 08:43:55 - INFO - __main__ -   test: [batch: 17/391 ] | Loss: 0.675 | Acc: 82.682% (1905/2304)/ 95.399% (2198/2304)
01/15/2023 08:43:57 - INFO - __main__ -   test: [batch: 18/391 ] | Loss: 0.700 | Acc: 82.196% (1999/2432)/ 95.189% (2315/2432)
01/15/2023 08:43:59 - INFO - __main__ -   test: [batch: 19/391 ] | Loss: 0.706 | Acc: 81.992% (2099/2560)/ 95.117% (2435/2560)
01/15/2023 08:44:01 - INFO - __main__ -   test: [batch: 20/391 ] | Loss: 0.701 | Acc: 82.106% (2207/2688)/ 95.126% (2557/2688)
01/15/2023 08:44:03 - INFO - __main__ -   test: [batch: 21/391 ] | Loss: 0.737 | Acc: 81.179% (2286/2816)/ 94.957% (2674/2816)
01/15/2023 08:44:05 - INFO - __main__ -   test: [batch: 22/391 ] | Loss: 0.736 | Acc: 80.978% (2384/2944)/ 94.973% (2796/2944)
01/15/2023 08:44:07 - INFO - __main__ -   test: [batch: 23/391 ] | Loss: 0.778 | Acc: 80.078% (2460/3072)/ 94.661% (2908/3072)
01/15/2023 08:44:09 - INFO - __main__ -   test: [batch: 24/391 ] | Loss: 0.795 | Acc: 79.625% (2548/3200)/ 94.562% (3026/3200)
01/15/2023 08:44:11 - INFO - __main__ -   test: [batch: 25/391 ] | Loss: 0.822 | Acc: 78.906% (2626/3328)/ 94.291% (3138/3328)
01/15/2023 08:44:13 - INFO - __main__ -   test: [batch: 26/391 ] | Loss: 0.838 | Acc: 78.154% (2701/3456)/ 94.155% (3254/3456)
01/15/2023 08:44:15 - INFO - __main__ -   test: [batch: 27/391 ] | Loss: 0.820 | Acc: 78.655% (2819/3584)/ 94.224% (3377/3584)
01/15/2023 08:44:17 - INFO - __main__ -   test: [batch: 28/391 ] | Loss: 0.825 | Acc: 78.098% (2899/3712)/ 94.370% (3503/3712)
01/15/2023 08:44:19 - INFO - __main__ -   test: [batch: 29/391 ] | Loss: 0.817 | Acc: 78.255% (3005/3840)/ 94.505% (3629/3840)
01/15/2023 08:44:22 - INFO - __main__ -   test: [batch: 30/391 ] | Loss: 0.830 | Acc: 78.125% (3100/3968)/ 94.355% (3744/3968)
01/15/2023 08:44:24 - INFO - __main__ -   test: [batch: 31/391 ] | Loss: 0.825 | Acc: 78.369% (3210/4096)/ 94.434% (3868/4096)
01/15/2023 08:44:26 - INFO - __main__ -   test: [batch: 32/391 ] | Loss: 0.811 | Acc: 78.670% (3323/4224)/ 94.555% (3994/4224)
01/15/2023 08:44:28 - INFO - __main__ -   test: [batch: 33/391 ] | Loss: 0.804 | Acc: 78.906% (3434/4352)/ 94.600% (4117/4352)
01/15/2023 08:44:30 - INFO - __main__ -   test: [batch: 34/391 ] | Loss: 0.788 | Acc: 79.353% (3555/4480)/ 94.688% (4242/4480)
01/15/2023 08:44:32 - INFO - __main__ -   test: [batch: 35/391 ] | Loss: 0.775 | Acc: 79.731% (3674/4608)/ 94.705% (4364/4608)
01/15/2023 08:44:35 - INFO - __main__ -   test: [batch: 36/391 ] | Loss: 0.760 | Acc: 80.152% (3796/4736)/ 94.827% (4491/4736)
01/15/2023 08:44:37 - INFO - __main__ -   test: [batch: 37/391 ] | Loss: 0.749 | Acc: 80.428% (3912/4864)/ 94.901% (4616/4864)
01/15/2023 08:44:39 - INFO - __main__ -   test: [batch: 38/391 ] | Loss: 0.741 | Acc: 80.609% (4024/4992)/ 95.012% (4743/4992)
01/15/2023 08:44:41 - INFO - __main__ -   test: [batch: 39/391 ] | Loss: 0.735 | Acc: 80.781% (4136/5120)/ 95.039% (4866/5120)
01/15/2023 08:44:43 - INFO - __main__ -   test: [batch: 40/391 ] | Loss: 0.732 | Acc: 80.907% (4246/5248)/ 95.008% (4986/5248)
01/15/2023 08:44:46 - INFO - __main__ -   test: [batch: 41/391 ] | Loss: 0.735 | Acc: 81.064% (4358/5376)/ 94.922% (5103/5376)
01/15/2023 08:44:48 - INFO - __main__ -   test: [batch: 42/391 ] | Loss: 0.735 | Acc: 81.086% (4463/5504)/ 95.004% (5229/5504)
01/15/2023 08:44:50 - INFO - __main__ -   test: [batch: 43/391 ] | Loss: 0.734 | Acc: 81.126% (4569/5632)/ 94.975% (5349/5632)
01/15/2023 08:44:52 - INFO - __main__ -   test: [batch: 44/391 ] | Loss: 0.739 | Acc: 81.146% (4674/5760)/ 94.826% (5462/5760)
01/15/2023 08:44:54 - INFO - __main__ -   test: [batch: 45/391 ] | Loss: 0.735 | Acc: 81.267% (4785/5888)/ 94.871% (5586/5888)
01/15/2023 08:44:56 - INFO - __main__ -   test: [batch: 46/391 ] | Loss: 0.739 | Acc: 81.167% (4883/6016)/ 94.897% (5709/6016)
01/15/2023 08:44:58 - INFO - __main__ -   test: [batch: 47/391 ] | Loss: 0.742 | Acc: 81.055% (4980/6144)/ 94.938% (5833/6144)
01/15/2023 08:45:00 - INFO - __main__ -   test: [batch: 48/391 ] | Loss: 0.746 | Acc: 80.963% (5078/6272)/ 94.914% (5953/6272)
01/15/2023 08:45:03 - INFO - __main__ -   test: [batch: 49/391 ] | Loss: 0.751 | Acc: 80.953% (5181/6400)/ 94.797% (6067/6400)
01/15/2023 08:45:05 - INFO - __main__ -   test: [batch: 50/391 ] | Loss: 0.742 | Acc: 81.127% (5296/6528)/ 94.884% (6194/6528)
01/15/2023 08:45:07 - INFO - __main__ -   test: [batch: 51/391 ] | Loss: 0.733 | Acc: 81.340% (5414/6656)/ 94.967% (6321/6656)
01/15/2023 08:45:09 - INFO - __main__ -   test: [batch: 52/391 ] | Loss: 0.730 | Acc: 81.427% (5524/6784)/ 95.003% (6445/6784)
01/15/2023 08:45:11 - INFO - __main__ -   test: [batch: 53/391 ] | Loss: 0.720 | Acc: 81.655% (5644/6912)/ 95.081% (6572/6912)
01/15/2023 08:45:13 - INFO - __main__ -   test: [batch: 54/391 ] | Loss: 0.713 | Acc: 81.818% (5760/7040)/ 95.114% (6696/7040)
01/15/2023 08:45:15 - INFO - __main__ -   test: [batch: 55/391 ] | Loss: 0.706 | Acc: 81.961% (5875/7168)/ 95.159% (6821/7168)
01/15/2023 08:45:17 - INFO - __main__ -   test: [batch: 56/391 ] | Loss: 0.698 | Acc: 82.209% (5998/7296)/ 95.217% (6947/7296)
01/15/2023 08:45:19 - INFO - __main__ -   test: [batch: 57/391 ] | Loss: 0.691 | Acc: 82.422% (6119/7424)/ 95.259% (7072/7424)
01/15/2023 08:45:21 - INFO - __main__ -   test: [batch: 58/391 ] | Loss: 0.686 | Acc: 82.508% (6231/7552)/ 95.273% (7195/7552)
01/15/2023 08:45:24 - INFO - __main__ -   test: [batch: 59/391 ] | Loss: 0.688 | Acc: 82.448% (6332/7680)/ 95.260% (7316/7680)
01/15/2023 08:45:26 - INFO - __main__ -   test: [batch: 60/391 ] | Loss: 0.691 | Acc: 82.377% (6432/7808)/ 95.248% (7437/7808)
01/15/2023 08:45:28 - INFO - __main__ -   test: [batch: 61/391 ] | Loss: 0.692 | Acc: 82.346% (6535/7936)/ 95.262% (7560/7936)
01/15/2023 08:45:30 - INFO - __main__ -   test: [batch: 62/391 ] | Loss: 0.690 | Acc: 82.341% (6640/8064)/ 95.288% (7684/8064)
01/15/2023 08:45:32 - INFO - __main__ -   test: [batch: 63/391 ] | Loss: 0.697 | Acc: 82.239% (6737/8192)/ 95.215% (7800/8192)
01/15/2023 08:45:34 - INFO - __main__ -   test: [batch: 64/391 ] | Loss: 0.706 | Acc: 82.031% (6825/8320)/ 95.168% (7918/8320)
01/15/2023 08:45:36 - INFO - __main__ -   test: [batch: 65/391 ] | Loss: 0.718 | Acc: 81.593% (6893/8448)/ 95.099% (8034/8448)
01/15/2023 08:45:39 - INFO - __main__ -   test: [batch: 66/391 ] | Loss: 0.724 | Acc: 81.576% (6996/8576)/ 95.056% (8152/8576)
01/15/2023 08:45:41 - INFO - __main__ -   test: [batch: 67/391 ] | Loss: 0.727 | Acc: 81.514% (7095/8704)/ 95.060% (8274/8704)
01/15/2023 08:45:43 - INFO - __main__ -   test: [batch: 68/391 ] | Loss: 0.727 | Acc: 81.454% (7194/8832)/ 95.086% (8398/8832)
01/15/2023 08:45:45 - INFO - __main__ -   test: [batch: 69/391 ] | Loss: 0.722 | Acc: 81.551% (7307/8960)/ 95.134% (8524/8960)
01/15/2023 08:45:47 - INFO - __main__ -   test: [batch: 70/391 ] | Loss: 0.722 | Acc: 81.492% (7406/9088)/ 95.136% (8646/9088)
01/15/2023 08:45:49 - INFO - __main__ -   test: [batch: 71/391 ] | Loss: 0.721 | Acc: 81.554% (7516/9216)/ 95.117% (8766/9216)
01/15/2023 08:45:52 - INFO - __main__ -   test: [batch: 72/391 ] | Loss: 0.724 | Acc: 81.443% (7610/9344)/ 95.120% (8888/9344)
01/15/2023 08:45:54 - INFO - __main__ -   test: [batch: 73/391 ] | Loss: 0.726 | Acc: 81.377% (7708/9472)/ 95.122% (9010/9472)
01/15/2023 08:45:56 - INFO - __main__ -   test: [batch: 74/391 ] | Loss: 0.726 | Acc: 81.375% (7812/9600)/ 95.115% (9131/9600)
01/15/2023 08:45:58 - INFO - __main__ -   test: [batch: 75/391 ] | Loss: 0.733 | Acc: 81.209% (7900/9728)/ 95.086% (9250/9728)
01/15/2023 08:46:00 - INFO - __main__ -   test: [batch: 76/391 ] | Loss: 0.732 | Acc: 81.209% (8004/9856)/ 95.110% (9374/9856)
01/15/2023 08:46:02 - INFO - __main__ -   test: [batch: 77/391 ] | Loss: 0.732 | Acc: 81.160% (8103/9984)/ 95.142% (9499/9984)
01/15/2023 08:46:04 - INFO - __main__ -   test: [batch: 78/391 ] | Loss: 0.731 | Acc: 81.082% (8199/10112)/ 95.184% (9625/10112)
01/15/2023 08:46:06 - INFO - __main__ -   test: [batch: 79/391 ] | Loss: 0.730 | Acc: 81.064% (8301/10240)/ 95.195% (9748/10240)
01/15/2023 08:46:09 - INFO - __main__ -   test: [batch: 80/391 ] | Loss: 0.730 | Acc: 81.038% (8402/10368)/ 95.168% (9867/10368)
01/15/2023 08:46:11 - INFO - __main__ -   test: [batch: 81/391 ] | Loss: 0.729 | Acc: 81.050% (8507/10496)/ 95.179% (9990/10496)
01/15/2023 08:46:13 - INFO - __main__ -   test: [batch: 82/391 ] | Loss: 0.728 | Acc: 81.062% (8612/10624)/ 95.181% (10112/10624)
01/15/2023 08:46:15 - INFO - __main__ -   test: [batch: 83/391 ] | Loss: 0.728 | Acc: 81.110% (8721/10752)/ 95.154% (10231/10752)
01/15/2023 08:46:17 - INFO - __main__ -   test: [batch: 84/391 ] | Loss: 0.724 | Acc: 81.250% (8840/10880)/ 95.193% (10357/10880)
01/15/2023 08:46:19 - INFO - __main__ -   test: [batch: 85/391 ] | Loss: 0.722 | Acc: 81.259% (8945/11008)/ 95.240% (10484/11008)
01/15/2023 08:46:21 - INFO - __main__ -   test: [batch: 86/391 ] | Loss: 0.726 | Acc: 81.178% (9040/11136)/ 95.223% (10604/11136)
01/15/2023 08:46:23 - INFO - __main__ -   test: [batch: 87/391 ] | Loss: 0.724 | Acc: 81.232% (9150/11264)/ 95.241% (10728/11264)
01/15/2023 08:46:26 - INFO - __main__ -   test: [batch: 88/391 ] | Loss: 0.732 | Acc: 81.153% (9245/11392)/ 95.172% (10842/11392)
01/15/2023 08:46:28 - INFO - __main__ -   test: [batch: 89/391 ] | Loss: 0.730 | Acc: 81.189% (9353/11520)/ 95.191% (10966/11520)
01/15/2023 08:46:30 - INFO - __main__ -   test: [batch: 90/391 ] | Loss: 0.731 | Acc: 81.070% (9443/11648)/ 95.192% (11088/11648)
01/15/2023 08:46:32 - INFO - __main__ -   test: [batch: 91/391 ] | Loss: 0.729 | Acc: 81.123% (9553/11776)/ 95.211% (11212/11776)
01/15/2023 08:46:34 - INFO - __main__ -   test: [batch: 92/391 ] | Loss: 0.731 | Acc: 81.090% (9653/11904)/ 95.178% (11330/11904)
01/15/2023 08:46:36 - INFO - __main__ -   test: [batch: 93/391 ] | Loss: 0.734 | Acc: 80.893% (9733/12032)/ 95.221% (11457/12032)
01/15/2023 08:46:38 - INFO - __main__ -   test: [batch: 94/391 ] | Loss: 0.737 | Acc: 80.765% (9821/12160)/ 95.230% (11580/12160)
01/15/2023 08:46:40 - INFO - __main__ -   test: [batch: 95/391 ] | Loss: 0.734 | Acc: 80.835% (9933/12288)/ 95.247% (11704/12288)
01/15/2023 08:46:42 - INFO - __main__ -   test: [batch: 96/391 ] | Loss: 0.738 | Acc: 80.694% (10019/12416)/ 95.256% (11827/12416)
01/15/2023 08:46:45 - INFO - __main__ -   test: [batch: 97/391 ] | Loss: 0.740 | Acc: 80.517% (10100/12544)/ 95.281% (11952/12544)
01/15/2023 08:46:47 - INFO - __main__ -   test: [batch: 98/391 ] | Loss: 0.736 | Acc: 80.619% (10216/12672)/ 95.320% (12079/12672)
01/15/2023 08:46:49 - INFO - __main__ -   test: [batch: 99/391 ] | Loss: 0.730 | Acc: 80.773% (10339/12800)/ 95.367% (12207/12800)
01/15/2023 08:46:51 - INFO - __main__ -   test: [batch: 100/391 ] | Loss: 0.729 | Acc: 80.817% (10448/12928)/ 95.390% (12332/12928)
01/15/2023 08:46:53 - INFO - __main__ -   test: [batch: 101/391 ] | Loss: 0.724 | Acc: 80.928% (10566/13056)/ 95.420% (12458/13056)
01/15/2023 08:46:55 - INFO - __main__ -   test: [batch: 102/391 ] | Loss: 0.721 | Acc: 81.030% (10683/13184)/ 95.449% (12584/13184)
01/15/2023 08:46:58 - INFO - __main__ -   test: [batch: 103/391 ] | Loss: 0.723 | Acc: 80.889% (10768/13312)/ 95.448% (12706/13312)
01/15/2023 08:47:00 - INFO - __main__ -   test: [batch: 104/391 ] | Loss: 0.722 | Acc: 80.818% (10862/13440)/ 95.446% (12828/13440)
01/15/2023 08:47:02 - INFO - __main__ -   test: [batch: 105/391 ] | Loss: 0.723 | Acc: 80.800% (10963/13568)/ 95.445% (12950/13568)
01/15/2023 08:47:04 - INFO - __main__ -   test: [batch: 106/391 ] | Loss: 0.731 | Acc: 80.702% (11053/13696)/ 95.378% (13063/13696)
01/15/2023 08:47:06 - INFO - __main__ -   test: [batch: 107/391 ] | Loss: 0.727 | Acc: 80.830% (11174/13824)/ 95.414% (13190/13824)
01/15/2023 08:47:08 - INFO - __main__ -   test: [batch: 108/391 ] | Loss: 0.731 | Acc: 80.698% (11259/13952)/ 95.420% (13313/13952)
01/15/2023 08:47:10 - INFO - __main__ -   test: [batch: 109/391 ] | Loss: 0.731 | Acc: 80.703% (11363/14080)/ 95.419% (13435/14080)
01/15/2023 08:47:12 - INFO - __main__ -   test: [batch: 110/391 ] | Loss: 0.734 | Acc: 80.532% (11442/14208)/ 95.439% (13560/14208)
01/15/2023 08:47:15 - INFO - __main__ -   test: [batch: 111/391 ] | Loss: 0.737 | Acc: 80.483% (11538/14336)/ 95.389% (13675/14336)
01/15/2023 08:47:17 - INFO - __main__ -   test: [batch: 112/391 ] | Loss: 0.737 | Acc: 80.503% (11644/14464)/ 95.409% (13800/14464)
01/15/2023 08:47:19 - INFO - __main__ -   test: [batch: 113/391 ] | Loss: 0.736 | Acc: 80.551% (11754/14592)/ 95.415% (13923/14592)
01/15/2023 08:47:21 - INFO - __main__ -   test: [batch: 114/391 ] | Loss: 0.732 | Acc: 80.652% (11872/14720)/ 95.448% (14050/14720)
01/15/2023 08:47:23 - INFO - __main__ -   test: [batch: 115/391 ] | Loss: 0.730 | Acc: 80.745% (11989/14848)/ 95.467% (14175/14848)
01/15/2023 08:47:25 - INFO - __main__ -   test: [batch: 116/391 ] | Loss: 0.732 | Acc: 80.722% (12089/14976)/ 95.446% (14294/14976)
01/15/2023 08:47:28 - INFO - __main__ -   test: [batch: 117/391 ] | Loss: 0.731 | Acc: 80.740% (12195/15104)/ 95.465% (14419/15104)
01/15/2023 08:47:30 - INFO - __main__ -   test: [batch: 118/391 ] | Loss: 0.735 | Acc: 80.593% (12276/15232)/ 95.477% (14543/15232)
01/15/2023 08:47:32 - INFO - __main__ -   test: [batch: 119/391 ] | Loss: 0.733 | Acc: 80.638% (12386/15360)/ 95.501% (14669/15360)
01/15/2023 08:47:34 - INFO - __main__ -   test: [batch: 120/391 ] | Loss: 0.732 | Acc: 80.682% (12496/15488)/ 95.519% (14794/15488)
01/15/2023 08:47:36 - INFO - __main__ -   test: [batch: 121/391 ] | Loss: 0.737 | Acc: 80.533% (12576/15616)/ 95.485% (14911/15616)
01/15/2023 08:47:38 - INFO - __main__ -   test: [batch: 122/391 ] | Loss: 0.740 | Acc: 80.475% (12670/15744)/ 95.459% (15029/15744)
01/15/2023 08:47:40 - INFO - __main__ -   test: [batch: 123/391 ] | Loss: 0.741 | Acc: 80.494% (12776/15872)/ 95.439% (15148/15872)
01/15/2023 08:47:43 - INFO - __main__ -   test: [batch: 124/391 ] | Loss: 0.740 | Acc: 80.525% (12884/16000)/ 95.450% (15272/16000)
01/15/2023 08:47:45 - INFO - __main__ -   test: [batch: 125/391 ] | Loss: 0.736 | Acc: 80.630% (13004/16128)/ 95.480% (15399/16128)
01/15/2023 08:47:47 - INFO - __main__ -   test: [batch: 126/391 ] | Loss: 0.733 | Acc: 80.721% (13122/16256)/ 95.485% (15522/16256)
01/15/2023 08:47:49 - INFO - __main__ -   test: [batch: 127/391 ] | Loss: 0.731 | Acc: 80.798% (13238/16384)/ 95.496% (15646/16384)
01/15/2023 08:47:51 - INFO - __main__ -   test: [batch: 128/391 ] | Loss: 0.731 | Acc: 80.802% (13342/16512)/ 95.476% (15765/16512)
01/15/2023 08:47:53 - INFO - __main__ -   test: [batch: 129/391 ] | Loss: 0.730 | Acc: 80.829% (13450/16640)/ 95.493% (15890/16640)
01/15/2023 08:47:56 - INFO - __main__ -   test: [batch: 130/391 ] | Loss: 0.726 | Acc: 80.940% (13572/16768)/ 95.515% (16016/16768)
01/15/2023 08:47:58 - INFO - __main__ -   test: [batch: 131/391 ] | Loss: 0.726 | Acc: 80.984% (13683/16896)/ 95.526% (16140/16896)
01/15/2023 08:48:00 - INFO - __main__ -   test: [batch: 132/391 ] | Loss: 0.723 | Acc: 81.044% (13797/17024)/ 95.536% (16264/17024)
01/15/2023 08:48:02 - INFO - __main__ -   test: [batch: 133/391 ] | Loss: 0.725 | Acc: 80.999% (13893/17152)/ 95.522% (16384/17152)
01/15/2023 08:48:04 - INFO - __main__ -   test: [batch: 134/391 ] | Loss: 0.723 | Acc: 81.007% (13998/17280)/ 95.544% (16510/17280)
01/15/2023 08:48:06 - INFO - __main__ -   test: [batch: 135/391 ] | Loss: 0.723 | Acc: 80.997% (14100/17408)/ 95.548% (16633/17408)
01/15/2023 08:48:08 - INFO - __main__ -   test: [batch: 136/391 ] | Loss: 0.722 | Acc: 80.971% (14199/17536)/ 95.569% (16759/17536)
01/15/2023 08:48:10 - INFO - __main__ -   test: [batch: 137/391 ] | Loss: 0.720 | Acc: 81.012% (14310/17664)/ 95.596% (16886/17664)
01/15/2023 08:48:12 - INFO - __main__ -   test: [batch: 138/391 ] | Loss: 0.719 | Acc: 81.070% (14424/17792)/ 95.610% (17011/17792)
01/15/2023 08:48:15 - INFO - __main__ -   test: [batch: 139/391 ] | Loss: 0.724 | Acc: 80.938% (14504/17920)/ 95.597% (17131/17920)
01/15/2023 08:48:17 - INFO - __main__ -   test: [batch: 140/391 ] | Loss: 0.726 | Acc: 80.873% (14596/18048)/ 95.595% (17253/18048)
01/15/2023 08:48:19 - INFO - __main__ -   test: [batch: 141/391 ] | Loss: 0.724 | Acc: 80.903% (14705/18176)/ 95.610% (17378/18176)
01/15/2023 08:48:21 - INFO - __main__ -   test: [batch: 142/391 ] | Loss: 0.722 | Acc: 80.988% (14824/18304)/ 95.618% (17502/18304)
01/15/2023 08:48:23 - INFO - __main__ -   test: [batch: 143/391 ] | Loss: 0.722 | Acc: 81.017% (14933/18432)/ 95.605% (17622/18432)
01/15/2023 08:48:25 - INFO - __main__ -   test: [batch: 144/391 ] | Loss: 0.724 | Acc: 80.997% (15033/18560)/ 95.577% (17739/18560)
01/15/2023 08:48:27 - INFO - __main__ -   test: [batch: 145/391 ] | Loss: 0.727 | Acc: 80.972% (15132/18688)/ 95.553% (17857/18688)
01/15/2023 08:48:29 - INFO - __main__ -   test: [batch: 146/391 ] | Loss: 0.728 | Acc: 80.958% (15233/18816)/ 95.530% (17975/18816)
01/15/2023 08:48:31 - INFO - __main__ -   test: [batch: 147/391 ] | Loss: 0.728 | Acc: 80.944% (15334/18944)/ 95.508% (18093/18944)
01/15/2023 08:48:33 - INFO - __main__ -   test: [batch: 148/391 ] | Loss: 0.730 | Acc: 80.867% (15423/19072)/ 95.512% (18216/19072)
01/15/2023 08:48:35 - INFO - __main__ -   test: [batch: 149/391 ] | Loss: 0.734 | Acc: 80.781% (15510/19200)/ 95.479% (18332/19200)
01/15/2023 08:48:37 - INFO - __main__ -   test: [batch: 150/391 ] | Loss: 0.734 | Acc: 80.722% (15602/19328)/ 95.488% (18456/19328)
01/15/2023 08:48:39 - INFO - __main__ -   test: [batch: 151/391 ] | Loss: 0.733 | Acc: 80.751% (15711/19456)/ 95.487% (18578/19456)
01/15/2023 08:48:42 - INFO - __main__ -   test: [batch: 152/391 ] | Loss: 0.735 | Acc: 80.734% (15811/19584)/ 95.466% (18696/19584)
01/15/2023 08:48:44 - INFO - __main__ -   test: [batch: 153/391 ] | Loss: 0.733 | Acc: 80.783% (15924/19712)/ 95.470% (18819/19712)
01/15/2023 08:48:46 - INFO - __main__ -   test: [batch: 154/391 ] | Loss: 0.735 | Acc: 80.751% (16021/19840)/ 95.433% (18934/19840)
01/15/2023 08:48:48 - INFO - __main__ -   test: [batch: 155/391 ] | Loss: 0.736 | Acc: 80.774% (16129/19968)/ 95.423% (19054/19968)
01/15/2023 08:48:50 - INFO - __main__ -   test: [batch: 156/391 ] | Loss: 0.737 | Acc: 80.693% (16216/20096)/ 95.402% (19172/20096)
01/15/2023 08:48:52 - INFO - __main__ -   test: [batch: 157/391 ] | Loss: 0.740 | Acc: 80.647% (16310/20224)/ 95.387% (19291/20224)
01/15/2023 08:48:54 - INFO - __main__ -   test: [batch: 158/391 ] | Loss: 0.741 | Acc: 80.636% (16411/20352)/ 95.352% (19406/20352)
01/15/2023 08:48:57 - INFO - __main__ -   test: [batch: 159/391 ] | Loss: 0.742 | Acc: 80.610% (16509/20480)/ 95.356% (19529/20480)
01/15/2023 08:48:59 - INFO - __main__ -   test: [batch: 160/391 ] | Loss: 0.742 | Acc: 80.595% (16609/20608)/ 95.342% (19648/20608)
01/15/2023 08:49:01 - INFO - __main__ -   test: [batch: 161/391 ] | Loss: 0.752 | Acc: 80.353% (16662/20736)/ 95.240% (19749/20736)
01/15/2023 08:49:03 - INFO - __main__ -   test: [batch: 162/391 ] | Loss: 0.758 | Acc: 80.248% (16743/20864)/ 95.169% (19856/20864)
01/15/2023 08:49:05 - INFO - __main__ -   test: [batch: 163/391 ] | Loss: 0.761 | Acc: 80.169% (16829/20992)/ 95.155% (19975/20992)
01/15/2023 08:49:07 - INFO - __main__ -   test: [batch: 164/391 ] | Loss: 0.761 | Acc: 80.147% (16927/21120)/ 95.175% (20101/21120)
01/15/2023 08:49:10 - INFO - __main__ -   test: [batch: 165/391 ] | Loss: 0.763 | Acc: 80.073% (17014/21248)/ 95.162% (20220/21248)
01/15/2023 08:49:12 - INFO - __main__ -   test: [batch: 166/391 ] | Loss: 0.763 | Acc: 80.099% (17122/21376)/ 95.153% (20340/21376)
01/15/2023 08:49:14 - INFO - __main__ -   test: [batch: 167/391 ] | Loss: 0.765 | Acc: 80.046% (17213/21504)/ 95.136% (20458/21504)
01/15/2023 08:49:16 - INFO - __main__ -   test: [batch: 168/391 ] | Loss: 0.764 | Acc: 80.057% (17318/21632)/ 95.137% (20580/21632)
01/15/2023 08:49:18 - INFO - __main__ -   test: [batch: 169/391 ] | Loss: 0.767 | Acc: 80.005% (17409/21760)/ 95.092% (20692/21760)
01/15/2023 08:49:20 - INFO - __main__ -   test: [batch: 170/391 ] | Loss: 0.771 | Acc: 79.902% (17489/21888)/ 95.057% (20806/21888)
01/15/2023 08:49:22 - INFO - __main__ -   test: [batch: 171/391 ] | Loss: 0.775 | Acc: 79.856% (17581/22016)/ 95.026% (20921/22016)
01/15/2023 08:49:24 - INFO - __main__ -   test: [batch: 172/391 ] | Loss: 0.775 | Acc: 79.809% (17673/22144)/ 95.023% (21042/22144)
01/15/2023 08:49:26 - INFO - __main__ -   test: [batch: 173/391 ] | Loss: 0.778 | Acc: 79.773% (17767/22272)/ 95.007% (21160/22272)
01/15/2023 08:49:28 - INFO - __main__ -   test: [batch: 174/391 ] | Loss: 0.782 | Acc: 79.679% (17848/22400)/ 94.951% (21269/22400)
01/15/2023 08:49:30 - INFO - __main__ -   test: [batch: 175/391 ] | Loss: 0.780 | Acc: 79.741% (17964/22528)/ 94.962% (21393/22528)
01/15/2023 08:49:33 - INFO - __main__ -   test: [batch: 176/391 ] | Loss: 0.781 | Acc: 79.727% (18063/22656)/ 94.928% (21507/22656)
01/15/2023 08:49:35 - INFO - __main__ -   test: [batch: 177/391 ] | Loss: 0.784 | Acc: 79.652% (18148/22784)/ 94.896% (21621/22784)
01/15/2023 08:49:37 - INFO - __main__ -   test: [batch: 178/391 ] | Loss: 0.786 | Acc: 79.644% (18248/22912)/ 94.876% (21738/22912)
01/15/2023 08:49:39 - INFO - __main__ -   test: [batch: 179/391 ] | Loss: 0.789 | Acc: 79.592% (18338/23040)/ 94.831% (21849/23040)
01/15/2023 08:49:41 - INFO - __main__ -   test: [batch: 180/391 ] | Loss: 0.795 | Acc: 79.463% (18410/23168)/ 94.786% (21960/23168)
01/15/2023 08:49:43 - INFO - __main__ -   test: [batch: 181/391 ] | Loss: 0.802 | Acc: 79.340% (18483/23296)/ 94.737% (22070/23296)
01/15/2023 08:49:45 - INFO - __main__ -   test: [batch: 182/391 ] | Loss: 0.801 | Acc: 79.363% (18590/23424)/ 94.740% (22192/23424)
01/15/2023 08:49:47 - INFO - __main__ -   test: [batch: 183/391 ] | Loss: 0.807 | Acc: 79.233% (18661/23552)/ 94.637% (22289/23552)
01/15/2023 08:49:50 - INFO - __main__ -   test: [batch: 184/391 ] | Loss: 0.807 | Acc: 79.248% (18766/23680)/ 94.628% (22408/23680)
01/15/2023 08:49:52 - INFO - __main__ -   test: [batch: 185/391 ] | Loss: 0.807 | Acc: 79.263% (18871/23808)/ 94.615% (22526/23808)
01/15/2023 08:49:54 - INFO - __main__ -   test: [batch: 186/391 ] | Loss: 0.809 | Acc: 79.228% (18964/23936)/ 94.573% (22637/23936)
01/15/2023 08:49:56 - INFO - __main__ -   test: [batch: 187/391 ] | Loss: 0.814 | Acc: 79.114% (19038/24064)/ 94.548% (22752/24064)
01/15/2023 08:49:58 - INFO - __main__ -   test: [batch: 188/391 ] | Loss: 0.817 | Acc: 78.997% (19111/24192)/ 94.519% (22866/24192)
01/15/2023 08:50:01 - INFO - __main__ -   test: [batch: 189/391 ] | Loss: 0.819 | Acc: 78.927% (19195/24320)/ 94.519% (22987/24320)
01/15/2023 08:50:03 - INFO - __main__ -   test: [batch: 190/391 ] | Loss: 0.823 | Acc: 78.845% (19276/24448)/ 94.494% (23102/24448)
01/15/2023 08:50:05 - INFO - __main__ -   test: [batch: 191/391 ] | Loss: 0.826 | Acc: 78.805% (19367/24576)/ 94.462% (23215/24576)
01/15/2023 08:50:07 - INFO - __main__ -   test: [batch: 192/391 ] | Loss: 0.833 | Acc: 78.692% (19440/24704)/ 94.361% (23311/24704)
01/15/2023 08:50:09 - INFO - __main__ -   test: [batch: 193/391 ] | Loss: 0.833 | Acc: 78.701% (19543/24832)/ 94.350% (23429/24832)
01/15/2023 08:50:11 - INFO - __main__ -   test: [batch: 194/391 ] | Loss: 0.836 | Acc: 78.630% (19626/24960)/ 94.327% (23544/24960)
01/15/2023 08:50:14 - INFO - __main__ -   test: [batch: 195/391 ] | Loss: 0.842 | Acc: 78.551% (19707/25088)/ 94.260% (23648/25088)
01/15/2023 08:50:16 - INFO - __main__ -   test: [batch: 196/391 ] | Loss: 0.845 | Acc: 78.458% (19784/25216)/ 94.214% (23757/25216)
01/15/2023 08:50:18 - INFO - __main__ -   test: [batch: 197/391 ] | Loss: 0.849 | Acc: 78.381% (19865/25344)/ 94.200% (23874/25344)
01/15/2023 08:50:20 - INFO - __main__ -   test: [batch: 198/391 ] | Loss: 0.851 | Acc: 78.345% (19956/25472)/ 94.170% (23987/25472)
01/15/2023 08:50:22 - INFO - __main__ -   test: [batch: 199/391 ] | Loss: 0.851 | Acc: 78.316% (20049/25600)/ 94.180% (24110/25600)
01/15/2023 08:50:24 - INFO - __main__ -   test: [batch: 200/391 ] | Loss: 0.851 | Acc: 78.292% (20143/25728)/ 94.185% (24232/25728)
01/15/2023 08:50:26 - INFO - __main__ -   test: [batch: 201/391 ] | Loss: 0.854 | Acc: 78.195% (20218/25856)/ 94.144% (24342/25856)
01/15/2023 08:50:29 - INFO - __main__ -   test: [batch: 202/391 ] | Loss: 0.855 | Acc: 78.175% (20313/25984)/ 94.139% (24461/25984)
01/15/2023 08:50:31 - INFO - __main__ -   test: [batch: 203/391 ] | Loss: 0.856 | Acc: 78.159% (20409/26112)/ 94.129% (24579/26112)
01/15/2023 08:50:33 - INFO - __main__ -   test: [batch: 204/391 ] | Loss: 0.859 | Acc: 78.079% (20488/26240)/ 94.101% (24692/26240)
01/15/2023 08:50:35 - INFO - __main__ -   test: [batch: 205/391 ] | Loss: 0.862 | Acc: 78.007% (20569/26368)/ 94.072% (24805/26368)
01/15/2023 08:50:37 - INFO - __main__ -   test: [batch: 206/391 ] | Loss: 0.864 | Acc: 77.985% (20663/26496)/ 94.067% (24924/26496)
01/15/2023 08:50:39 - INFO - __main__ -   test: [batch: 207/391 ] | Loss: 0.867 | Acc: 77.903% (20741/26624)/ 94.039% (25037/26624)
01/15/2023 08:50:41 - INFO - __main__ -   test: [batch: 208/391 ] | Loss: 0.869 | Acc: 77.867% (20831/26752)/ 94.000% (25147/26752)
01/15/2023 08:50:44 - INFO - __main__ -   test: [batch: 209/391 ] | Loss: 0.869 | Acc: 77.868% (20931/26880)/ 94.022% (25273/26880)
01/15/2023 08:50:46 - INFO - __main__ -   test: [batch: 210/391 ] | Loss: 0.870 | Acc: 77.844% (21024/27008)/ 94.005% (25389/27008)
01/15/2023 08:50:48 - INFO - __main__ -   test: [batch: 211/391 ] | Loss: 0.873 | Acc: 77.782% (21107/27136)/ 93.978% (25502/27136)
01/15/2023 08:50:50 - INFO - __main__ -   test: [batch: 212/391 ] | Loss: 0.875 | Acc: 77.714% (21188/27264)/ 93.966% (25619/27264)
01/15/2023 08:50:52 - INFO - __main__ -   test: [batch: 213/391 ] | Loss: 0.874 | Acc: 77.731% (21292/27392)/ 93.976% (25742/27392)
01/15/2023 08:50:54 - INFO - __main__ -   test: [batch: 214/391 ] | Loss: 0.874 | Acc: 77.722% (21389/27520)/ 93.983% (25864/27520)
01/15/2023 08:50:56 - INFO - __main__ -   test: [batch: 215/391 ] | Loss: 0.876 | Acc: 77.709% (21485/27648)/ 93.963% (25979/27648)
01/15/2023 08:50:58 - INFO - __main__ -   test: [batch: 216/391 ] | Loss: 0.873 | Acc: 77.758% (21598/27776)/ 93.980% (26104/27776)
01/15/2023 08:51:01 - INFO - __main__ -   test: [batch: 217/391 ] | Loss: 0.878 | Acc: 77.695% (21680/27904)/ 93.908% (26204/27904)
01/15/2023 08:51:03 - INFO - __main__ -   test: [batch: 218/391 ] | Loss: 0.881 | Acc: 77.633% (21762/28032)/ 93.871% (26314/28032)
01/15/2023 08:51:05 - INFO - __main__ -   test: [batch: 219/391 ] | Loss: 0.879 | Acc: 77.667% (21871/28160)/ 93.881% (26437/28160)
01/15/2023 08:51:07 - INFO - __main__ -   test: [batch: 220/391 ] | Loss: 0.878 | Acc: 77.701% (21980/28288)/ 93.891% (26560/28288)
01/15/2023 08:51:09 - INFO - __main__ -   test: [batch: 221/391 ] | Loss: 0.880 | Acc: 77.668% (22070/28416)/ 93.873% (26675/28416)
01/15/2023 08:51:11 - INFO - __main__ -   test: [batch: 222/391 ] | Loss: 0.878 | Acc: 77.719% (22184/28544)/ 93.887% (26799/28544)
01/15/2023 08:51:13 - INFO - __main__ -   test: [batch: 223/391 ] | Loss: 0.878 | Acc: 77.706% (22280/28672)/ 93.879% (26917/28672)
01/15/2023 08:51:16 - INFO - __main__ -   test: [batch: 224/391 ] | Loss: 0.878 | Acc: 77.726% (22385/28800)/ 93.875% (27036/28800)
01/15/2023 08:51:18 - INFO - __main__ -   test: [batch: 225/391 ] | Loss: 0.877 | Acc: 77.721% (22483/28928)/ 93.878% (27157/28928)
01/15/2023 08:51:20 - INFO - __main__ -   test: [batch: 226/391 ] | Loss: 0.877 | Acc: 77.691% (22574/29056)/ 93.884% (27279/29056)
01/15/2023 08:51:22 - INFO - __main__ -   test: [batch: 227/391 ] | Loss: 0.879 | Acc: 77.693% (22674/29184)/ 93.877% (27397/29184)
01/15/2023 08:51:24 - INFO - __main__ -   test: [batch: 228/391 ] | Loss: 0.883 | Acc: 77.610% (22749/29312)/ 93.815% (27499/29312)
01/15/2023 08:51:26 - INFO - __main__ -   test: [batch: 229/391 ] | Loss: 0.887 | Acc: 77.531% (22825/29440)/ 93.767% (27605/29440)
01/15/2023 08:51:28 - INFO - __main__ -   test: [batch: 230/391 ] | Loss: 0.890 | Acc: 77.462% (22904/29568)/ 93.726% (27713/29568)
01/15/2023 08:51:30 - INFO - __main__ -   test: [batch: 231/391 ] | Loss: 0.890 | Acc: 77.445% (22998/29696)/ 93.713% (27829/29696)
01/15/2023 08:51:32 - INFO - __main__ -   test: [batch: 232/391 ] | Loss: 0.889 | Acc: 77.478% (23107/29824)/ 93.727% (27953/29824)
01/15/2023 08:51:34 - INFO - __main__ -   test: [batch: 233/391 ] | Loss: 0.891 | Acc: 77.431% (23192/29952)/ 93.700% (28065/29952)
01/15/2023 08:51:36 - INFO - __main__ -   test: [batch: 234/391 ] | Loss: 0.897 | Acc: 77.330% (23261/30080)/ 93.630% (28164/30080)
01/15/2023 08:51:39 - INFO - __main__ -   test: [batch: 235/391 ] | Loss: 0.897 | Acc: 77.324% (23358/30208)/ 93.628% (28283/30208)
01/15/2023 08:51:41 - INFO - __main__ -   test: [batch: 236/391 ] | Loss: 0.896 | Acc: 77.373% (23472/30336)/ 93.638% (28406/30336)
01/15/2023 08:51:43 - INFO - __main__ -   test: [batch: 237/391 ] | Loss: 0.896 | Acc: 77.370% (23570/30464)/ 93.612% (28518/30464)
01/15/2023 08:51:45 - INFO - __main__ -   test: [batch: 238/391 ] | Loss: 0.895 | Acc: 77.406% (23680/30592)/ 93.623% (28641/30592)
01/15/2023 08:51:47 - INFO - __main__ -   test: [batch: 239/391 ] | Loss: 0.894 | Acc: 77.438% (23789/30720)/ 93.633% (28764/30720)
01/15/2023 08:51:49 - INFO - __main__ -   test: [batch: 240/391 ] | Loss: 0.895 | Acc: 77.447% (23891/30848)/ 93.624% (28881/30848)
01/15/2023 08:51:51 - INFO - __main__ -   test: [batch: 241/391 ] | Loss: 0.898 | Acc: 77.373% (23967/30976)/ 93.579% (28987/30976)
01/15/2023 08:51:54 - INFO - __main__ -   test: [batch: 242/391 ] | Loss: 0.900 | Acc: 77.273% (24035/31104)/ 93.564% (29102/31104)
01/15/2023 08:51:56 - INFO - __main__ -   test: [batch: 243/391 ] | Loss: 0.905 | Acc: 77.152% (24096/31232)/ 93.494% (29200/31232)
01/15/2023 08:51:58 - INFO - __main__ -   test: [batch: 244/391 ] | Loss: 0.906 | Acc: 77.149% (24194/31360)/ 93.485% (29317/31360)
01/15/2023 08:52:00 - INFO - __main__ -   test: [batch: 245/391 ] | Loss: 0.905 | Acc: 77.156% (24295/31488)/ 93.483% (29436/31488)
01/15/2023 08:52:02 - INFO - __main__ -   test: [batch: 246/391 ] | Loss: 0.906 | Acc: 77.144% (24390/31616)/ 93.475% (29553/31616)
01/15/2023 08:52:04 - INFO - __main__ -   test: [batch: 247/391 ] | Loss: 0.911 | Acc: 77.070% (24465/31744)/ 93.403% (29650/31744)
01/15/2023 08:52:06 - INFO - __main__ -   test: [batch: 248/391 ] | Loss: 0.912 | Acc: 77.030% (24551/31872)/ 93.399% (29768/31872)
01/15/2023 08:52:08 - INFO - __main__ -   test: [batch: 249/391 ] | Loss: 0.914 | Acc: 76.881% (24602/32000)/ 93.394% (29886/32000)
01/15/2023 08:52:11 - INFO - __main__ -   test: [batch: 250/391 ] | Loss: 0.912 | Acc: 76.914% (24711/32128)/ 93.408% (30010/32128)
01/15/2023 08:52:13 - INFO - __main__ -   test: [batch: 251/391 ] | Loss: 0.915 | Acc: 76.863% (24793/32256)/ 93.375% (30119/32256)
01/15/2023 08:52:15 - INFO - __main__ -   test: [batch: 252/391 ] | Loss: 0.914 | Acc: 76.890% (24900/32384)/ 93.370% (30237/32384)
01/15/2023 08:52:17 - INFO - __main__ -   test: [batch: 253/391 ] | Loss: 0.915 | Acc: 76.861% (24989/32512)/ 93.356% (30352/32512)
01/15/2023 08:52:19 - INFO - __main__ -   test: [batch: 254/391 ] | Loss: 0.920 | Acc: 76.783% (25062/32640)/ 93.309% (30456/32640)
01/15/2023 08:52:21 - INFO - __main__ -   test: [batch: 255/391 ] | Loss: 0.921 | Acc: 76.755% (25151/32768)/ 93.298% (30572/32768)
01/15/2023 08:52:23 - INFO - __main__ -   test: [batch: 256/391 ] | Loss: 0.927 | Acc: 76.623% (25206/32896)/ 93.251% (30676/32896)
01/15/2023 08:52:26 - INFO - __main__ -   test: [batch: 257/391 ] | Loss: 0.928 | Acc: 76.614% (25301/33024)/ 93.241% (30792/33024)
01/15/2023 08:52:28 - INFO - __main__ -   test: [batch: 258/391 ] | Loss: 0.928 | Acc: 76.611% (25398/33152)/ 93.228% (30907/33152)
01/15/2023 08:52:30 - INFO - __main__ -   test: [batch: 259/391 ] | Loss: 0.932 | Acc: 76.487% (25455/33280)/ 93.224% (31025/33280)
01/15/2023 08:52:32 - INFO - __main__ -   test: [batch: 260/391 ] | Loss: 0.932 | Acc: 76.473% (25548/33408)/ 93.223% (31144/33408)
01/15/2023 08:52:34 - INFO - __main__ -   test: [batch: 261/391 ] | Loss: 0.930 | Acc: 76.509% (25658/33536)/ 93.246% (31271/33536)
01/15/2023 08:52:36 - INFO - __main__ -   test: [batch: 262/391 ] | Loss: 0.929 | Acc: 76.536% (25765/33664)/ 93.257% (31394/33664)
01/15/2023 08:52:38 - INFO - __main__ -   test: [batch: 263/391 ] | Loss: 0.932 | Acc: 76.471% (25841/33792)/ 93.232% (31505/33792)
01/15/2023 08:52:40 - INFO - __main__ -   test: [batch: 264/391 ] | Loss: 0.937 | Acc: 76.412% (25919/33920)/ 93.172% (31604/33920)
01/15/2023 08:52:42 - INFO - __main__ -   test: [batch: 265/391 ] | Loss: 0.936 | Acc: 76.445% (26028/34048)/ 93.166% (31721/34048)
01/15/2023 08:52:45 - INFO - __main__ -   test: [batch: 266/391 ] | Loss: 0.938 | Acc: 76.369% (26100/34176)/ 93.150% (31835/34176)
01/15/2023 08:52:47 - INFO - __main__ -   test: [batch: 267/391 ] | Loss: 0.938 | Acc: 76.405% (26210/34304)/ 93.149% (31954/34304)
01/15/2023 08:52:49 - INFO - __main__ -   test: [batch: 268/391 ] | Loss: 0.937 | Acc: 76.429% (26316/34432)/ 93.149% (32073/34432)
01/15/2023 08:52:51 - INFO - __main__ -   test: [batch: 269/391 ] | Loss: 0.939 | Acc: 76.372% (26394/34560)/ 93.128% (32185/34560)
01/15/2023 08:52:53 - INFO - __main__ -   test: [batch: 270/391 ] | Loss: 0.942 | Acc: 76.320% (26474/34688)/ 93.104% (32296/34688)
01/15/2023 08:52:55 - INFO - __main__ -   test: [batch: 271/391 ] | Loss: 0.942 | Acc: 76.324% (26573/34816)/ 93.098% (32413/34816)
01/15/2023 08:52:57 - INFO - __main__ -   test: [batch: 272/391 ] | Loss: 0.943 | Acc: 76.291% (26659/34944)/ 93.098% (32532/34944)
01/15/2023 08:52:59 - INFO - __main__ -   test: [batch: 273/391 ] | Loss: 0.943 | Acc: 76.303% (26761/35072)/ 93.071% (32642/35072)
01/15/2023 08:53:01 - INFO - __main__ -   test: [batch: 274/391 ] | Loss: 0.943 | Acc: 76.301% (26858/35200)/ 93.074% (32762/35200)
01/15/2023 08:53:04 - INFO - __main__ -   test: [batch: 275/391 ] | Loss: 0.945 | Acc: 76.265% (26943/35328)/ 93.065% (32878/35328)
01/15/2023 08:53:06 - INFO - __main__ -   test: [batch: 276/391 ] | Loss: 0.946 | Acc: 76.216% (27023/35456)/ 93.053% (32993/35456)
01/15/2023 08:53:08 - INFO - __main__ -   test: [batch: 277/391 ] | Loss: 0.947 | Acc: 76.186% (27110/35584)/ 93.042% (33108/35584)
01/15/2023 08:53:10 - INFO - __main__ -   test: [batch: 278/391 ] | Loss: 0.946 | Acc: 76.201% (27213/35712)/ 93.042% (33227/35712)
01/15/2023 08:53:12 - INFO - __main__ -   test: [batch: 279/391 ] | Loss: 0.946 | Acc: 76.219% (27317/35840)/ 93.036% (33344/35840)
01/15/2023 08:53:14 - INFO - __main__ -   test: [batch: 280/391 ] | Loss: 0.947 | Acc: 76.196% (27406/35968)/ 93.030% (33461/35968)
01/15/2023 08:53:16 - INFO - __main__ -   test: [batch: 281/391 ] | Loss: 0.948 | Acc: 76.202% (27506/36096)/ 93.019% (33576/36096)
01/15/2023 08:53:18 - INFO - __main__ -   test: [batch: 282/391 ] | Loss: 0.947 | Acc: 76.240% (27617/36224)/ 93.018% (33695/36224)
01/15/2023 08:53:20 - INFO - __main__ -   test: [batch: 283/391 ] | Loss: 0.947 | Acc: 76.227% (27710/36352)/ 93.013% (33812/36352)
01/15/2023 08:53:23 - INFO - __main__ -   test: [batch: 284/391 ] | Loss: 0.951 | Acc: 76.181% (27791/36480)/ 92.958% (33911/36480)
01/15/2023 08:53:25 - INFO - __main__ -   test: [batch: 285/391 ] | Loss: 0.954 | Acc: 76.142% (27874/36608)/ 92.914% (34014/36608)
01/15/2023 08:53:27 - INFO - __main__ -   test: [batch: 286/391 ] | Loss: 0.955 | Acc: 76.102% (27957/36736)/ 92.898% (34127/36736)
01/15/2023 08:53:29 - INFO - __main__ -   test: [batch: 287/391 ] | Loss: 0.955 | Acc: 76.099% (28053/36864)/ 92.890% (34243/36864)
01/15/2023 08:53:31 - INFO - __main__ -   test: [batch: 288/391 ] | Loss: 0.954 | Acc: 76.106% (28153/36992)/ 92.893% (34363/36992)
01/15/2023 08:53:33 - INFO - __main__ -   test: [batch: 289/391 ] | Loss: 0.956 | Acc: 76.072% (28238/37120)/ 92.856% (34468/37120)
01/15/2023 08:53:35 - INFO - __main__ -   test: [batch: 290/391 ] | Loss: 0.958 | Acc: 75.991% (28305/37248)/ 92.856% (34587/37248)
01/15/2023 08:53:37 - INFO - __main__ -   test: [batch: 291/391 ] | Loss: 0.958 | Acc: 75.990% (28402/37376)/ 92.856% (34706/37376)
01/15/2023 08:53:40 - INFO - __main__ -   test: [batch: 292/391 ] | Loss: 0.960 | Acc: 75.923% (28474/37504)/ 92.838% (34818/37504)
01/15/2023 08:53:42 - INFO - __main__ -   test: [batch: 293/391 ] | Loss: 0.960 | Acc: 75.906% (28565/37632)/ 92.831% (34934/37632)
01/15/2023 08:53:44 - INFO - __main__ -   test: [batch: 294/391 ] | Loss: 0.962 | Acc: 75.874% (28650/37760)/ 92.807% (35044/37760)
01/15/2023 08:53:46 - INFO - __main__ -   test: [batch: 295/391 ] | Loss: 0.961 | Acc: 75.903% (28758/37888)/ 92.805% (35162/37888)
01/15/2023 08:53:48 - INFO - __main__ -   test: [batch: 296/391 ] | Loss: 0.961 | Acc: 75.894% (28852/38016)/ 92.800% (35279/38016)
01/15/2023 08:53:50 - INFO - __main__ -   test: [batch: 297/391 ] | Loss: 0.963 | Acc: 75.868% (28939/38144)/ 92.775% (35388/38144)
01/15/2023 08:53:53 - INFO - __main__ -   test: [batch: 298/391 ] | Loss: 0.965 | Acc: 75.828% (29021/38272)/ 92.752% (35498/38272)
01/15/2023 08:53:55 - INFO - __main__ -   test: [batch: 299/391 ] | Loss: 0.967 | Acc: 75.815% (29113/38400)/ 92.727% (35607/38400)
01/15/2023 08:53:57 - INFO - __main__ -   test: [batch: 300/391 ] | Loss: 0.967 | Acc: 75.828% (29215/38528)/ 92.717% (35722/38528)
01/15/2023 08:53:59 - INFO - __main__ -   test: [batch: 301/391 ] | Loss: 0.968 | Acc: 75.823% (29310/38656)/ 92.702% (35835/38656)
01/15/2023 08:54:01 - INFO - __main__ -   test: [batch: 302/391 ] | Loss: 0.970 | Acc: 75.774% (29388/38784)/ 92.683% (35946/38784)
01/15/2023 08:54:03 - INFO - __main__ -   test: [batch: 303/391 ] | Loss: 0.971 | Acc: 75.748% (29475/38912)/ 92.678% (36063/38912)
01/15/2023 08:54:05 - INFO - __main__ -   test: [batch: 304/391 ] | Loss: 0.970 | Acc: 75.753% (29574/39040)/ 92.677% (36181/39040)
01/15/2023 08:54:07 - INFO - __main__ -   test: [batch: 305/391 ] | Loss: 0.971 | Acc: 75.710% (29654/39168)/ 92.665% (36295/39168)
01/15/2023 08:54:09 - INFO - __main__ -   test: [batch: 306/391 ] | Loss: 0.972 | Acc: 75.702% (29748/39296)/ 92.638% (36403/39296)
01/15/2023 08:54:11 - INFO - __main__ -   test: [batch: 307/391 ] | Loss: 0.973 | Acc: 75.687% (29839/39424)/ 92.621% (36515/39424)
01/15/2023 08:54:14 - INFO - __main__ -   test: [batch: 308/391 ] | Loss: 0.974 | Acc: 75.678% (29932/39552)/ 92.612% (36630/39552)
01/15/2023 08:54:16 - INFO - __main__ -   test: [batch: 309/391 ] | Loss: 0.975 | Acc: 75.668% (30025/39680)/ 92.591% (36740/39680)
01/15/2023 08:54:18 - INFO - __main__ -   test: [batch: 310/391 ] | Loss: 0.975 | Acc: 75.651% (30115/39808)/ 92.582% (36855/39808)
01/15/2023 08:54:20 - INFO - __main__ -   test: [batch: 311/391 ] | Loss: 0.977 | Acc: 75.629% (30203/39936)/ 92.568% (36968/39936)
01/15/2023 08:54:22 - INFO - __main__ -   test: [batch: 312/391 ] | Loss: 0.978 | Acc: 75.617% (30295/40064)/ 92.547% (37078/40064)
01/15/2023 08:54:24 - INFO - __main__ -   test: [batch: 313/391 ] | Loss: 0.976 | Acc: 75.664% (30411/40192)/ 92.571% (37206/40192)
01/15/2023 08:54:26 - INFO - __main__ -   test: [batch: 314/391 ] | Loss: 0.977 | Acc: 75.647% (30501/40320)/ 92.564% (37322/40320)
01/15/2023 08:54:29 - INFO - __main__ -   test: [batch: 315/391 ] | Loss: 0.978 | Acc: 75.630% (30591/40448)/ 92.546% (37433/40448)
01/15/2023 08:54:31 - INFO - __main__ -   test: [batch: 316/391 ] | Loss: 0.980 | Acc: 75.535% (30649/40576)/ 92.515% (37539/40576)
01/15/2023 08:54:33 - INFO - __main__ -   test: [batch: 317/391 ] | Loss: 0.982 | Acc: 75.496% (30730/40704)/ 92.482% (37644/40704)
01/15/2023 08:54:35 - INFO - __main__ -   test: [batch: 318/391 ] | Loss: 0.981 | Acc: 75.529% (30840/40832)/ 92.501% (37770/40832)
01/15/2023 08:54:37 - INFO - __main__ -   test: [batch: 319/391 ] | Loss: 0.983 | Acc: 75.474% (30914/40960)/ 92.471% (37876/40960)
01/15/2023 08:54:40 - INFO - __main__ -   test: [batch: 320/391 ] | Loss: 0.982 | Acc: 75.509% (31025/41088)/ 92.480% (37998/41088)
01/15/2023 08:54:42 - INFO - __main__ -   test: [batch: 321/391 ] | Loss: 0.982 | Acc: 75.522% (31127/41216)/ 92.479% (38116/41216)
01/15/2023 08:54:44 - INFO - __main__ -   test: [batch: 322/391 ] | Loss: 0.984 | Acc: 75.484% (31208/41344)/ 92.468% (38230/41344)
01/15/2023 08:54:46 - INFO - __main__ -   test: [batch: 323/391 ] | Loss: 0.986 | Acc: 75.451% (31291/41472)/ 92.441% (38337/41472)
01/15/2023 08:54:48 - INFO - __main__ -   test: [batch: 324/391 ] | Loss: 0.986 | Acc: 75.447% (31386/41600)/ 92.435% (38453/41600)
01/15/2023 08:54:50 - INFO - __main__ -   test: [batch: 325/391 ] | Loss: 0.986 | Acc: 75.460% (31488/41728)/ 92.437% (38572/41728)
01/15/2023 08:54:52 - INFO - __main__ -   test: [batch: 326/391 ] | Loss: 0.989 | Acc: 75.377% (31550/41856)/ 92.391% (38671/41856)
01/15/2023 08:54:54 - INFO - __main__ -   test: [batch: 327/391 ] | Loss: 0.993 | Acc: 75.291% (31610/41984)/ 92.345% (38770/41984)
01/15/2023 08:54:56 - INFO - __main__ -   test: [batch: 328/391 ] | Loss: 0.994 | Acc: 75.247% (31688/42112)/ 92.320% (38878/42112)
01/15/2023 08:54:59 - INFO - __main__ -   test: [batch: 329/391 ] | Loss: 0.994 | Acc: 75.251% (31786/42240)/ 92.322% (38997/42240)
01/15/2023 08:55:01 - INFO - __main__ -   test: [batch: 330/391 ] | Loss: 0.996 | Acc: 75.205% (31863/42368)/ 92.301% (39106/42368)
01/15/2023 08:55:03 - INFO - __main__ -   test: [batch: 331/391 ] | Loss: 0.996 | Acc: 75.167% (31943/42496)/ 92.317% (39231/42496)
01/15/2023 08:55:05 - INFO - __main__ -   test: [batch: 332/391 ] | Loss: 0.997 | Acc: 75.145% (32030/42624)/ 92.314% (39348/42624)
01/15/2023 08:55:07 - INFO - __main__ -   test: [batch: 333/391 ] | Loss: 0.996 | Acc: 75.178% (32140/42752)/ 92.321% (39469/42752)
01/15/2023 08:55:09 - INFO - __main__ -   test: [batch: 334/391 ] | Loss: 0.997 | Acc: 75.147% (32223/42880)/ 92.306% (39581/42880)
01/15/2023 08:55:11 - INFO - __main__ -   test: [batch: 335/391 ] | Loss: 0.999 | Acc: 75.130% (32312/43008)/ 92.287% (39691/43008)
01/15/2023 08:55:13 - INFO - __main__ -   test: [batch: 336/391 ] | Loss: 1.000 | Acc: 75.093% (32392/43136)/ 92.273% (39803/43136)
01/15/2023 08:55:15 - INFO - __main__ -   test: [batch: 337/391 ] | Loss: 1.000 | Acc: 75.086% (32485/43264)/ 92.264% (39917/43264)
01/15/2023 08:55:17 - INFO - __main__ -   test: [batch: 338/391 ] | Loss: 1.000 | Acc: 75.083% (32580/43392)/ 92.270% (40038/43392)
01/15/2023 08:55:19 - INFO - __main__ -   test: [batch: 339/391 ] | Loss: 1.003 | Acc: 75.044% (32659/43520)/ 92.236% (40141/43520)
01/15/2023 08:55:21 - INFO - __main__ -   test: [batch: 340/391 ] | Loss: 1.003 | Acc: 75.041% (32754/43648)/ 92.245% (40263/43648)
01/15/2023 08:55:24 - INFO - __main__ -   test: [batch: 341/391 ] | Loss: 1.001 | Acc: 75.080% (32867/43776)/ 92.263% (40389/43776)
01/15/2023 08:55:26 - INFO - __main__ -   test: [batch: 342/391 ] | Loss: 1.002 | Acc: 75.025% (32939/43904)/ 92.254% (40503/43904)
01/15/2023 08:55:28 - INFO - __main__ -   test: [batch: 343/391 ] | Loss: 1.002 | Acc: 75.027% (33036/44032)/ 92.258% (40623/44032)
01/15/2023 08:55:30 - INFO - __main__ -   test: [batch: 344/391 ] | Loss: 1.002 | Acc: 75.027% (33132/44160)/ 92.244% (40735/44160)
01/15/2023 08:55:32 - INFO - __main__ -   test: [batch: 345/391 ] | Loss: 1.006 | Acc: 74.957% (33197/44288)/ 92.210% (40838/44288)
01/15/2023 08:55:34 - INFO - __main__ -   test: [batch: 346/391 ] | Loss: 1.007 | Acc: 74.941% (33286/44416)/ 92.199% (40951/44416)
01/15/2023 08:55:36 - INFO - __main__ -   test: [batch: 347/391 ] | Loss: 1.006 | Acc: 74.951% (33386/44544)/ 92.208% (41073/44544)
01/15/2023 08:55:39 - INFO - __main__ -   test: [batch: 348/391 ] | Loss: 1.008 | Acc: 74.933% (33474/44672)/ 92.190% (41183/44672)
01/15/2023 08:55:41 - INFO - __main__ -   test: [batch: 349/391 ] | Loss: 1.007 | Acc: 74.938% (33572/44800)/ 92.196% (41304/44800)
01/15/2023 08:55:43 - INFO - __main__ -   test: [batch: 350/391 ] | Loss: 1.007 | Acc: 74.942% (33670/44928)/ 92.192% (41420/44928)
01/15/2023 08:55:45 - INFO - __main__ -   test: [batch: 351/391 ] | Loss: 1.009 | Acc: 74.889% (33742/45056)/ 92.174% (41530/45056)
01/15/2023 08:55:47 - INFO - __main__ -   test: [batch: 352/391 ] | Loss: 1.010 | Acc: 74.885% (33836/45184)/ 92.172% (41647/45184)
01/15/2023 08:55:49 - INFO - __main__ -   test: [batch: 353/391 ] | Loss: 1.012 | Acc: 74.834% (33909/45312)/ 92.132% (41747/45312)
01/15/2023 08:55:51 - INFO - __main__ -   test: [batch: 354/391 ] | Loss: 1.015 | Acc: 74.776% (33978/45440)/ 92.117% (41858/45440)
01/15/2023 08:55:53 - INFO - __main__ -   test: [batch: 355/391 ] | Loss: 1.017 | Acc: 74.710% (34044/45568)/ 92.100% (41968/45568)
01/15/2023 08:55:55 - INFO - __main__ -   test: [batch: 356/391 ] | Loss: 1.018 | Acc: 74.709% (34139/45696)/ 92.102% (42087/45696)
01/15/2023 08:55:57 - INFO - __main__ -   test: [batch: 357/391 ] | Loss: 1.016 | Acc: 74.742% (34250/45824)/ 92.115% (42211/45824)
01/15/2023 08:56:00 - INFO - __main__ -   test: [batch: 358/391 ] | Loss: 1.016 | Acc: 74.763% (34355/45952)/ 92.118% (42330/45952)
01/15/2023 08:56:02 - INFO - __main__ -   test: [batch: 359/391 ] | Loss: 1.016 | Acc: 74.763% (34451/46080)/ 92.107% (42443/46080)
01/15/2023 08:56:04 - INFO - __main__ -   test: [batch: 360/391 ] | Loss: 1.017 | Acc: 74.727% (34530/46208)/ 92.105% (42560/46208)
01/15/2023 08:56:06 - INFO - __main__ -   test: [batch: 361/391 ] | Loss: 1.018 | Acc: 74.726% (34625/46336)/ 92.110% (42680/46336)
01/15/2023 08:56:08 - INFO - __main__ -   test: [batch: 362/391 ] | Loss: 1.017 | Acc: 74.725% (34720/46464)/ 92.123% (42804/46464)
01/15/2023 08:56:10 - INFO - __main__ -   test: [batch: 363/391 ] | Loss: 1.018 | Acc: 74.717% (34812/46592)/ 92.112% (42917/46592)
01/15/2023 08:56:12 - INFO - __main__ -   test: [batch: 364/391 ] | Loss: 1.016 | Acc: 74.745% (34921/46720)/ 92.125% (43041/46720)
01/15/2023 08:56:14 - INFO - __main__ -   test: [batch: 365/391 ] | Loss: 1.016 | Acc: 74.750% (35019/46848)/ 92.128% (43160/46848)
01/15/2023 08:56:16 - INFO - __main__ -   test: [batch: 366/391 ] | Loss: 1.015 | Acc: 74.785% (35131/46976)/ 92.145% (43286/46976)
01/15/2023 08:56:18 - INFO - __main__ -   test: [batch: 367/391 ] | Loss: 1.014 | Acc: 74.805% (35236/47104)/ 92.156% (43409/47104)
01/15/2023 08:56:20 - INFO - __main__ -   test: [batch: 368/391 ] | Loss: 1.014 | Acc: 74.801% (35330/47232)/ 92.162% (43530/47232)
01/15/2023 08:56:23 - INFO - __main__ -   test: [batch: 369/391 ] | Loss: 1.013 | Acc: 74.823% (35436/47360)/ 92.173% (43653/47360)
01/15/2023 08:56:25 - INFO - __main__ -   test: [batch: 370/391 ] | Loss: 1.013 | Acc: 74.798% (35520/47488)/ 92.177% (43773/47488)
01/15/2023 08:56:27 - INFO - __main__ -   test: [batch: 371/391 ] | Loss: 1.013 | Acc: 74.809% (35621/47616)/ 92.177% (43891/47616)
01/15/2023 08:56:29 - INFO - __main__ -   test: [batch: 372/391 ] | Loss: 1.012 | Acc: 74.858% (35740/47744)/ 92.196% (44018/47744)
01/15/2023 08:56:31 - INFO - __main__ -   test: [batch: 373/391 ] | Loss: 1.010 | Acc: 74.891% (35852/47872)/ 92.206% (44141/47872)
01/15/2023 08:56:33 - INFO - __main__ -   test: [batch: 374/391 ] | Loss: 1.009 | Acc: 74.921% (35962/48000)/ 92.210% (44261/48000)
01/15/2023 08:56:35 - INFO - __main__ -   test: [batch: 375/391 ] | Loss: 1.013 | Acc: 74.846% (36022/48128)/ 92.179% (44364/48128)
01/15/2023 08:56:37 - INFO - __main__ -   test: [batch: 376/391 ] | Loss: 1.013 | Acc: 74.828% (36109/48256)/ 92.163% (44474/48256)
01/15/2023 08:56:40 - INFO - __main__ -   test: [batch: 377/391 ] | Loss: 1.014 | Acc: 74.820% (36201/48384)/ 92.152% (44587/48384)
01/15/2023 08:56:42 - INFO - __main__ -   test: [batch: 378/391 ] | Loss: 1.017 | Acc: 74.746% (36261/48512)/ 92.115% (44687/48512)
01/15/2023 08:56:44 - INFO - __main__ -   test: [batch: 379/391 ] | Loss: 1.017 | Acc: 74.733% (36350/48640)/ 92.128% (44811/48640)
01/15/2023 08:56:46 - INFO - __main__ -   test: [batch: 380/391 ] | Loss: 1.017 | Acc: 74.740% (36449/48768)/ 92.142% (44936/48768)
01/15/2023 08:56:48 - INFO - __main__ -   test: [batch: 381/391 ] | Loss: 1.018 | Acc: 74.697% (36524/48896)/ 92.143% (45054/48896)
01/15/2023 08:56:50 - INFO - __main__ -   test: [batch: 382/391 ] | Loss: 1.020 | Acc: 74.661% (36602/49024)/ 92.132% (45167/49024)
01/15/2023 08:56:53 - INFO - __main__ -   test: [batch: 383/391 ] | Loss: 1.020 | Acc: 74.664% (36699/49152)/ 92.126% (45282/49152)
01/15/2023 08:56:55 - INFO - __main__ -   test: [batch: 384/391 ] | Loss: 1.018 | Acc: 74.702% (36813/49280)/ 92.141% (45407/49280)
01/15/2023 08:56:57 - INFO - __main__ -   test: [batch: 385/391 ] | Loss: 1.017 | Acc: 74.715% (36915/49408)/ 92.151% (45530/49408)
01/15/2023 08:56:59 - INFO - __main__ -   test: [batch: 386/391 ] | Loss: 1.015 | Acc: 74.760% (37033/49536)/ 92.167% (45656/49536)
01/15/2023 08:57:01 - INFO - __main__ -   test: [batch: 387/391 ] | Loss: 1.013 | Acc: 74.803% (37150/49664)/ 92.177% (45779/49664)
01/15/2023 08:57:03 - INFO - __main__ -   test: [batch: 388/391 ] | Loss: 1.011 | Acc: 74.853% (37271/49792)/ 92.192% (45904/49792)
01/15/2023 08:57:05 - INFO - __main__ -   test: [batch: 389/391 ] | Loss: 1.011 | Acc: 74.854% (37367/49920)/ 92.198% (46025/49920)
01/15/2023 08:57:08 - INFO - __main__ -   test: [batch: 390/391 ] | Loss: 1.013 | Acc: 74.810% (37405/50000)/ 92.188% (46094/50000)
01/15/2023 08:57:08 - INFO - __main__ -   Final accuracy: 74.810
01/15/2023 08:57:08 - INFO - __main__ -   {'milestones': Counter({2: 1, 3: 1, 4: 1}), 'gamma': 0.1, 'base_lrs': [0.00025], 'last_epoch': 1, '_step_count': 2, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.00025]}
01/15/2023 08:57:08 - INFO - __main__ -   
Epoch: 1
01/15/2023 08:57:10 - INFO - __main__ -   test: [epoch: 1 | batch: 0/10010 ] | Loss: 0.674 | Acc: 83.594% (107/128)
01/15/2023 09:00:41 - INFO - __main__ -   test: [epoch: 1 | batch: 100/10010 ] | Loss: 0.894 | Acc: 77.908% (10072/12928)
01/15/2023 09:04:12 - INFO - __main__ -   test: [epoch: 1 | batch: 200/10010 ] | Loss: 0.895 | Acc: 77.795% (20015/25728)
01/15/2023 09:07:42 - INFO - __main__ -   test: [epoch: 1 | batch: 300/10010 ] | Loss: 0.910 | Acc: 77.512% (29864/38528)
01/15/2023 09:11:14 - INFO - __main__ -   test: [epoch: 1 | batch: 400/10010 ] | Loss: 0.921 | Acc: 77.264% (39658/51328)
01/15/2023 09:14:46 - INFO - __main__ -   test: [epoch: 1 | batch: 500/10010 ] | Loss: 0.923 | Acc: 77.147% (49473/64128)
01/15/2023 09:18:17 - INFO - __main__ -   test: [epoch: 1 | batch: 600/10010 ] | Loss: 0.926 | Acc: 77.103% (59314/76928)
01/15/2023 09:21:49 - INFO - __main__ -   test: [epoch: 1 | batch: 700/10010 ] | Loss: 0.926 | Acc: 77.086% (69168/89728)
01/15/2023 09:25:22 - INFO - __main__ -   test: [epoch: 1 | batch: 800/10010 ] | Loss: 0.925 | Acc: 77.083% (79032/102528)
01/15/2023 09:28:55 - INFO - __main__ -   test: [epoch: 1 | batch: 900/10010 ] | Loss: 0.925 | Acc: 77.097% (88914/115328)
01/15/2023 09:32:27 - INFO - __main__ -   test: [epoch: 1 | batch: 1000/10010 ] | Loss: 0.925 | Acc: 77.050% (98722/128128)
01/15/2023 09:35:59 - INFO - __main__ -   test: [epoch: 1 | batch: 1100/10010 ] | Loss: 0.928 | Acc: 77.024% (108548/140928)
01/15/2023 09:39:31 - INFO - __main__ -   test: [epoch: 1 | batch: 1200/10010 ] | Loss: 0.928 | Acc: 77.028% (118414/153728)
01/15/2023 09:43:01 - INFO - __main__ -   test: [epoch: 1 | batch: 1300/10010 ] | Loss: 0.925 | Acc: 77.099% (128391/166528)
01/15/2023 09:46:34 - INFO - __main__ -   test: [epoch: 1 | batch: 1400/10010 ] | Loss: 0.925 | Acc: 77.081% (138227/179328)
01/15/2023 09:50:06 - INFO - __main__ -   test: [epoch: 1 | batch: 1500/10010 ] | Loss: 0.925 | Acc: 77.085% (148101/192128)
01/15/2023 09:53:37 - INFO - __main__ -   test: [epoch: 1 | batch: 1600/10010 ] | Loss: 0.927 | Acc: 77.049% (157895/204928)
01/15/2023 09:57:09 - INFO - __main__ -   test: [epoch: 1 | batch: 1700/10010 ] | Loss: 0.927 | Acc: 77.073% (167809/217728)
01/15/2023 10:00:42 - INFO - __main__ -   test: [epoch: 1 | batch: 1800/10010 ] | Loss: 0.927 | Acc: 77.079% (177688/230528)
01/15/2023 10:04:14 - INFO - __main__ -   test: [epoch: 1 | batch: 1900/10010 ] | Loss: 0.926 | Acc: 77.089% (187579/243328)
01/15/2023 10:07:46 - INFO - __main__ -   test: [epoch: 1 | batch: 2000/10010 ] | Loss: 0.925 | Acc: 77.111% (197502/256128)
01/15/2023 10:11:17 - INFO - __main__ -   test: [epoch: 1 | batch: 2100/10010 ] | Loss: 0.925 | Acc: 77.098% (207337/268928)
01/15/2023 10:14:48 - INFO - __main__ -   test: [epoch: 1 | batch: 2200/10010 ] | Loss: 0.927 | Acc: 77.080% (217156/281728)
01/15/2023 10:18:21 - INFO - __main__ -   test: [epoch: 1 | batch: 2300/10010 ] | Loss: 0.927 | Acc: 77.082% (227027/294528)
01/15/2023 10:21:55 - INFO - __main__ -   test: [epoch: 1 | batch: 2400/10010 ] | Loss: 0.926 | Acc: 77.113% (236989/307328)
01/15/2023 10:25:27 - INFO - __main__ -   test: [epoch: 1 | batch: 2500/10010 ] | Loss: 0.927 | Acc: 77.093% (246796/320128)
01/15/2023 10:28:58 - INFO - __main__ -   test: [epoch: 1 | batch: 2600/10010 ] | Loss: 0.928 | Acc: 77.079% (256618/332928)
01/15/2023 10:32:30 - INFO - __main__ -   test: [epoch: 1 | batch: 2700/10010 ] | Loss: 0.928 | Acc: 77.085% (266505/345728)
01/15/2023 10:36:03 - INFO - __main__ -   test: [epoch: 1 | batch: 2800/10010 ] | Loss: 0.928 | Acc: 77.074% (276332/358528)
01/15/2023 10:39:34 - INFO - __main__ -   test: [epoch: 1 | batch: 2900/10010 ] | Loss: 0.928 | Acc: 77.076% (286205/371328)
01/15/2023 10:43:04 - INFO - __main__ -   test: [epoch: 1 | batch: 3000/10010 ] | Loss: 0.927 | Acc: 77.091% (296127/384128)
01/15/2023 10:46:35 - INFO - __main__ -   test: [epoch: 1 | batch: 3100/10010 ] | Loss: 0.928 | Acc: 77.094% (306009/396928)
01/15/2023 10:50:06 - INFO - __main__ -   test: [epoch: 1 | batch: 3200/10010 ] | Loss: 0.928 | Acc: 77.074% (315795/409728)
01/15/2023 10:53:37 - INFO - __main__ -   test: [epoch: 1 | batch: 3300/10010 ] | Loss: 0.928 | Acc: 77.063% (325613/422528)
01/15/2023 10:57:08 - INFO - __main__ -   test: [epoch: 1 | batch: 3400/10010 ] | Loss: 0.928 | Acc: 77.068% (335498/435328)
01/15/2023 11:00:40 - INFO - __main__ -   test: [epoch: 1 | batch: 3500/10010 ] | Loss: 0.928 | Acc: 77.072% (345379/448128)
01/15/2023 11:04:11 - INFO - __main__ -   test: [epoch: 1 | batch: 3600/10010 ] | Loss: 0.928 | Acc: 77.073% (355252/460928)
01/15/2023 11:07:44 - INFO - __main__ -   test: [epoch: 1 | batch: 3700/10010 ] | Loss: 0.929 | Acc: 77.052% (365015/473728)
01/15/2023 11:11:15 - INFO - __main__ -   test: [epoch: 1 | batch: 3800/10010 ] | Loss: 0.929 | Acc: 77.042% (374830/486528)
01/15/2023 11:14:46 - INFO - __main__ -   test: [epoch: 1 | batch: 3900/10010 ] | Loss: 0.929 | Acc: 77.044% (384701/499328)
01/15/2023 11:18:18 - INFO - __main__ -   test: [epoch: 1 | batch: 4000/10010 ] | Loss: 0.929 | Acc: 77.044% (394562/512128)
01/15/2023 11:21:49 - INFO - __main__ -   test: [epoch: 1 | batch: 4100/10010 ] | Loss: 0.929 | Acc: 77.055% (404483/524928)
01/15/2023 11:25:21 - INFO - __main__ -   test: [epoch: 1 | batch: 4200/10010 ] | Loss: 0.929 | Acc: 77.057% (414358/537728)
01/15/2023 11:28:52 - INFO - __main__ -   test: [epoch: 1 | batch: 4300/10010 ] | Loss: 0.928 | Acc: 77.064% (424257/550528)
01/15/2023 11:32:23 - INFO - __main__ -   test: [epoch: 1 | batch: 4400/10010 ] | Loss: 0.928 | Acc: 77.062% (434114/563328)
01/15/2023 11:35:53 - INFO - __main__ -   test: [epoch: 1 | batch: 4500/10010 ] | Loss: 0.928 | Acc: 77.055% (443936/576128)
01/15/2023 11:39:25 - INFO - __main__ -   test: [epoch: 1 | batch: 4600/10010 ] | Loss: 0.928 | Acc: 77.052% (453779/588928)
01/15/2023 11:42:58 - INFO - __main__ -   test: [epoch: 1 | batch: 4700/10010 ] | Loss: 0.928 | Acc: 77.062% (463706/601728)
01/15/2023 11:46:29 - INFO - __main__ -   test: [epoch: 1 | batch: 4800/10010 ] | Loss: 0.927 | Acc: 77.070% (473615/614528)
01/15/2023 11:50:00 - INFO - __main__ -   test: [epoch: 1 | batch: 4900/10010 ] | Loss: 0.928 | Acc: 77.067% (483466/627328)
01/15/2023 11:53:32 - INFO - __main__ -   test: [epoch: 1 | batch: 5000/10010 ] | Loss: 0.927 | Acc: 77.076% (493388/640128)
01/15/2023 11:57:03 - INFO - __main__ -   test: [epoch: 1 | batch: 5100/10010 ] | Loss: 0.928 | Acc: 77.074% (503236/652928)
01/15/2023 12:00:36 - INFO - __main__ -   test: [epoch: 1 | batch: 5200/10010 ] | Loss: 0.928 | Acc: 77.065% (513043/665728)
01/15/2023 12:04:07 - INFO - __main__ -   test: [epoch: 1 | batch: 5300/10010 ] | Loss: 0.928 | Acc: 77.062% (522888/678528)
01/15/2023 12:07:38 - INFO - __main__ -   test: [epoch: 1 | batch: 5400/10010 ] | Loss: 0.928 | Acc: 77.060% (532738/691328)
01/15/2023 12:11:09 - INFO - __main__ -   test: [epoch: 1 | batch: 5500/10010 ] | Loss: 0.928 | Acc: 77.057% (542577/704128)
01/15/2023 12:14:41 - INFO - __main__ -   test: [epoch: 1 | batch: 5600/10010 ] | Loss: 0.928 | Acc: 77.067% (552515/716928)
01/15/2023 12:18:13 - INFO - __main__ -   test: [epoch: 1 | batch: 5700/10010 ] | Loss: 0.928 | Acc: 77.064% (562361/729728)
01/15/2023 12:21:46 - INFO - __main__ -   test: [epoch: 1 | batch: 5800/10010 ] | Loss: 0.927 | Acc: 77.068% (572249/742528)
01/15/2023 12:25:16 - INFO - __main__ -   test: [epoch: 1 | batch: 5900/10010 ] | Loss: 0.927 | Acc: 77.067% (582110/755328)
01/15/2023 12:28:49 - INFO - __main__ -   test: [epoch: 1 | batch: 6000/10010 ] | Loss: 0.927 | Acc: 77.070% (591996/768128)
01/15/2023 12:32:19 - INFO - __main__ -   test: [epoch: 1 | batch: 6100/10010 ] | Loss: 0.927 | Acc: 77.056% (601754/780928)
01/15/2023 12:35:51 - INFO - __main__ -   test: [epoch: 1 | batch: 6200/10010 ] | Loss: 0.927 | Acc: 77.061% (611654/793728)
01/15/2023 12:39:23 - INFO - __main__ -   test: [epoch: 1 | batch: 6300/10010 ] | Loss: 0.927 | Acc: 77.061% (621518/806528)
01/15/2023 12:42:56 - INFO - __main__ -   test: [epoch: 1 | batch: 6400/10010 ] | Loss: 0.928 | Acc: 77.046% (631262/819328)
01/15/2023 12:46:26 - INFO - __main__ -   test: [epoch: 1 | batch: 6500/10010 ] | Loss: 0.927 | Acc: 77.053% (641183/832128)
01/15/2023 12:49:57 - INFO - __main__ -   test: [epoch: 1 | batch: 6600/10010 ] | Loss: 0.927 | Acc: 77.061% (651110/844928)
01/15/2023 12:53:30 - INFO - __main__ -   test: [epoch: 1 | batch: 6700/10010 ] | Loss: 0.927 | Acc: 77.059% (660955/857728)
01/15/2023 12:57:03 - INFO - __main__ -   test: [epoch: 1 | batch: 6800/10010 ] | Loss: 0.928 | Acc: 77.051% (670753/870528)
01/15/2023 13:00:36 - INFO - __main__ -   test: [epoch: 1 | batch: 6900/10010 ] | Loss: 0.927 | Acc: 77.056% (680656/883328)
01/15/2023 13:04:08 - INFO - __main__ -   test: [epoch: 1 | batch: 7000/10010 ] | Loss: 0.928 | Acc: 77.047% (690438/896128)
01/15/2023 13:07:39 - INFO - __main__ -   test: [epoch: 1 | batch: 7100/10010 ] | Loss: 0.927 | Acc: 77.054% (700362/908928)
01/15/2023 13:11:11 - INFO - __main__ -   test: [epoch: 1 | batch: 7200/10010 ] | Loss: 0.927 | Acc: 77.056% (710243/921728)
01/15/2023 13:14:42 - INFO - __main__ -   test: [epoch: 1 | batch: 7300/10010 ] | Loss: 0.927 | Acc: 77.049% (720045/934528)
01/15/2023 13:18:14 - INFO - __main__ -   test: [epoch: 1 | batch: 7400/10010 ] | Loss: 0.928 | Acc: 77.037% (729793/947328)
01/15/2023 13:21:46 - INFO - __main__ -   test: [epoch: 1 | batch: 7500/10010 ] | Loss: 0.928 | Acc: 77.039% (739673/960128)
01/15/2023 13:25:18 - INFO - __main__ -   test: [epoch: 1 | batch: 7600/10010 ] | Loss: 0.928 | Acc: 77.034% (749489/972928)
01/15/2023 13:28:50 - INFO - __main__ -   test: [epoch: 1 | batch: 7700/10010 ] | Loss: 0.928 | Acc: 77.037% (759373/985728)
01/15/2023 13:32:23 - INFO - __main__ -   test: [epoch: 1 | batch: 7800/10010 ] | Loss: 0.928 | Acc: 77.029% (769160/998528)
01/15/2023 13:35:54 - INFO - __main__ -   test: [epoch: 1 | batch: 7900/10010 ] | Loss: 0.928 | Acc: 77.033% (779054/1011328)
01/15/2023 13:39:27 - INFO - __main__ -   test: [epoch: 1 | batch: 8000/10010 ] | Loss: 0.928 | Acc: 77.039% (788977/1024128)
01/15/2023 13:42:57 - INFO - __main__ -   test: [epoch: 1 | batch: 8100/10010 ] | Loss: 0.928 | Acc: 77.047% (798927/1036928)
01/15/2023 13:46:28 - INFO - __main__ -   test: [epoch: 1 | batch: 8200/10010 ] | Loss: 0.928 | Acc: 77.042% (808730/1049728)
01/15/2023 13:49:59 - INFO - __main__ -   test: [epoch: 1 | batch: 8300/10010 ] | Loss: 0.928 | Acc: 77.039% (818562/1062528)
01/15/2023 13:53:30 - INFO - __main__ -   test: [epoch: 1 | batch: 8400/10010 ] | Loss: 0.928 | Acc: 77.040% (828430/1075328)
01/15/2023 13:57:01 - INFO - __main__ -   test: [epoch: 1 | batch: 8500/10010 ] | Loss: 0.928 | Acc: 77.041% (838309/1088128)
01/15/2023 14:00:32 - INFO - __main__ -   test: [epoch: 1 | batch: 8600/10010 ] | Loss: 0.928 | Acc: 77.039% (848146/1100928)
01/15/2023 14:04:02 - INFO - __main__ -   test: [epoch: 1 | batch: 8700/10010 ] | Loss: 0.928 | Acc: 77.049% (858118/1113728)
01/15/2023 14:07:32 - INFO - __main__ -   test: [epoch: 1 | batch: 8800/10010 ] | Loss: 0.928 | Acc: 77.054% (868032/1126528)
01/15/2023 14:11:05 - INFO - __main__ -   test: [epoch: 1 | batch: 8900/10010 ] | Loss: 0.928 | Acc: 77.058% (877944/1139328)
01/15/2023 14:14:35 - INFO - __main__ -   test: [epoch: 1 | batch: 9000/10010 ] | Loss: 0.928 | Acc: 77.062% (887856/1152128)
01/15/2023 14:18:05 - INFO - __main__ -   test: [epoch: 1 | batch: 9100/10010 ] | Loss: 0.927 | Acc: 77.067% (897778/1164928)
01/15/2023 14:21:35 - INFO - __main__ -   test: [epoch: 1 | batch: 9200/10010 ] | Loss: 0.927 | Acc: 77.072% (907696/1177728)
01/15/2023 14:25:08 - INFO - __main__ -   test: [epoch: 1 | batch: 9300/10010 ] | Loss: 0.927 | Acc: 77.078% (917639/1190528)
01/15/2023 14:28:38 - INFO - __main__ -   test: [epoch: 1 | batch: 9400/10010 ] | Loss: 0.928 | Acc: 77.073% (927440/1203328)
01/15/2023 14:32:10 - INFO - __main__ -   test: [epoch: 1 | batch: 9500/10010 ] | Loss: 0.927 | Acc: 77.073% (937304/1216128)
01/15/2023 14:35:42 - INFO - __main__ -   test: [epoch: 1 | batch: 9600/10010 ] | Loss: 0.927 | Acc: 77.070% (947132/1228928)
01/15/2023 14:39:14 - INFO - __main__ -   test: [epoch: 1 | batch: 9700/10010 ] | Loss: 0.928 | Acc: 77.068% (956980/1241728)
01/15/2023 14:42:48 - INFO - __main__ -   test: [epoch: 1 | batch: 9800/10010 ] | Loss: 0.928 | Acc: 77.064% (966793/1254528)
01/15/2023 14:46:20 - INFO - __main__ -   test: [epoch: 1 | batch: 9900/10010 ] | Loss: 0.928 | Acc: 77.064% (976653/1267328)
01/15/2023 14:49:52 - INFO - __main__ -   test: [epoch: 1 | batch: 10000/10010 ] | Loss: 0.928 | Acc: 77.061% (986476/1280128)
01/15/2023 14:50:11 - INFO - __main__ -   Saving Checkpoint
01/15/2023 14:50:13 - INFO - __main__ -   test: [batch: 0/391 ] | Loss: 0.494 | Acc: 87.500% (112/128)/ 96.875% (124/128)
01/15/2023 14:50:15 - INFO - __main__ -   test: [batch: 1/391 ] | Loss: 0.506 | Acc: 85.938% (220/256)/ 97.656% (250/256)
01/15/2023 14:50:17 - INFO - __main__ -   test: [batch: 2/391 ] | Loss: 0.717 | Acc: 79.688% (306/384)/ 95.052% (365/384)
01/15/2023 14:50:19 - INFO - __main__ -   test: [batch: 3/391 ] | Loss: 0.658 | Acc: 82.227% (421/512)/ 95.508% (489/512)
01/15/2023 14:50:21 - INFO - __main__ -   test: [batch: 4/391 ] | Loss: 0.567 | Acc: 84.844% (543/640)/ 96.250% (616/640)
01/15/2023 14:50:23 - INFO - __main__ -   test: [batch: 5/391 ] | Loss: 0.502 | Acc: 86.328% (663/768)/ 96.875% (744/768)
01/15/2023 14:50:26 - INFO - __main__ -   test: [batch: 6/391 ] | Loss: 0.489 | Acc: 86.830% (778/896)/ 96.763% (867/896)
01/15/2023 14:50:28 - INFO - __main__ -   test: [batch: 7/391 ] | Loss: 0.467 | Acc: 87.695% (898/1024)/ 96.875% (992/1024)
01/15/2023 14:50:30 - INFO - __main__ -   test: [batch: 8/391 ] | Loss: 0.475 | Acc: 87.674% (1010/1152)/ 96.788% (1115/1152)
01/15/2023 14:50:32 - INFO - __main__ -   test: [batch: 9/391 ] | Loss: 0.451 | Acc: 88.438% (1132/1280)/ 96.953% (1241/1280)
01/15/2023 14:50:34 - INFO - __main__ -   test: [batch: 10/391 ] | Loss: 0.496 | Acc: 87.145% (1227/1408)/ 96.875% (1364/1408)
01/15/2023 14:50:36 - INFO - __main__ -   test: [batch: 11/391 ] | Loss: 0.499 | Acc: 87.370% (1342/1536)/ 96.810% (1487/1536)
01/15/2023 14:50:38 - INFO - __main__ -   test: [batch: 12/391 ] | Loss: 0.554 | Acc: 86.058% (1432/1664)/ 96.394% (1604/1664)
01/15/2023 14:50:40 - INFO - __main__ -   test: [batch: 13/391 ] | Loss: 0.614 | Acc: 84.542% (1515/1792)/ 95.592% (1713/1792)
01/15/2023 14:50:42 - INFO - __main__ -   test: [batch: 14/391 ] | Loss: 0.631 | Acc: 83.698% (1607/1920)/ 95.781% (1839/1920)
01/15/2023 14:50:44 - INFO - __main__ -   test: [batch: 15/391 ] | Loss: 0.636 | Acc: 83.154% (1703/2048)/ 95.947% (1965/2048)
01/15/2023 14:50:47 - INFO - __main__ -   test: [batch: 16/391 ] | Loss: 0.643 | Acc: 83.134% (1809/2176)/ 95.818% (2085/2176)
01/15/2023 14:50:49 - INFO - __main__ -   test: [batch: 17/391 ] | Loss: 0.671 | Acc: 82.595% (1903/2304)/ 95.443% (2199/2304)
01/15/2023 14:50:51 - INFO - __main__ -   test: [batch: 18/391 ] | Loss: 0.695 | Acc: 82.155% (1998/2432)/ 95.230% (2316/2432)
01/15/2023 14:50:53 - INFO - __main__ -   test: [batch: 19/391 ] | Loss: 0.704 | Acc: 81.836% (2095/2560)/ 95.117% (2435/2560)
01/15/2023 14:50:55 - INFO - __main__ -   test: [batch: 20/391 ] | Loss: 0.699 | Acc: 81.957% (2203/2688)/ 95.126% (2557/2688)
01/15/2023 14:50:57 - INFO - __main__ -   test: [batch: 21/391 ] | Loss: 0.735 | Acc: 81.037% (2282/2816)/ 94.922% (2673/2816)
01/15/2023 14:50:59 - INFO - __main__ -   test: [batch: 22/391 ] | Loss: 0.733 | Acc: 80.910% (2382/2944)/ 94.905% (2794/2944)
01/15/2023 14:51:02 - INFO - __main__ -   test: [batch: 23/391 ] | Loss: 0.776 | Acc: 79.980% (2457/3072)/ 94.564% (2905/3072)
01/15/2023 14:51:04 - INFO - __main__ -   test: [batch: 24/391 ] | Loss: 0.793 | Acc: 79.562% (2546/3200)/ 94.500% (3024/3200)
01/15/2023 14:51:06 - INFO - __main__ -   test: [batch: 25/391 ] | Loss: 0.820 | Acc: 78.756% (2621/3328)/ 94.261% (3137/3328)
01/15/2023 14:51:08 - INFO - __main__ -   test: [batch: 26/391 ] | Loss: 0.839 | Acc: 77.980% (2695/3456)/ 94.097% (3252/3456)
01/15/2023 14:51:10 - INFO - __main__ -   test: [batch: 27/391 ] | Loss: 0.820 | Acc: 78.488% (2813/3584)/ 94.169% (3375/3584)
01/15/2023 14:51:12 - INFO - __main__ -   test: [batch: 28/391 ] | Loss: 0.826 | Acc: 77.963% (2894/3712)/ 94.289% (3500/3712)
01/15/2023 14:51:14 - INFO - __main__ -   test: [batch: 29/391 ] | Loss: 0.818 | Acc: 78.151% (3001/3840)/ 94.427% (3626/3840)
01/15/2023 14:51:16 - INFO - __main__ -   test: [batch: 30/391 ] | Loss: 0.830 | Acc: 78.125% (3100/3968)/ 94.304% (3742/3968)
01/15/2023 14:51:19 - INFO - __main__ -   test: [batch: 31/391 ] | Loss: 0.825 | Acc: 78.369% (3210/4096)/ 94.385% (3866/4096)
01/15/2023 14:51:21 - INFO - __main__ -   test: [batch: 32/391 ] | Loss: 0.811 | Acc: 78.670% (3323/4224)/ 94.508% (3992/4224)
01/15/2023 14:51:23 - INFO - __main__ -   test: [batch: 33/391 ] | Loss: 0.804 | Acc: 78.814% (3430/4352)/ 94.554% (4115/4352)
01/15/2023 14:51:25 - INFO - __main__ -   test: [batch: 34/391 ] | Loss: 0.790 | Acc: 79.241% (3550/4480)/ 94.621% (4239/4480)
01/15/2023 14:51:27 - INFO - __main__ -   test: [batch: 35/391 ] | Loss: 0.776 | Acc: 79.644% (3670/4608)/ 94.661% (4362/4608)
01/15/2023 14:51:29 - INFO - __main__ -   test: [batch: 36/391 ] | Loss: 0.761 | Acc: 80.089% (3793/4736)/ 94.785% (4489/4736)
01/15/2023 14:51:31 - INFO - __main__ -   test: [batch: 37/391 ] | Loss: 0.751 | Acc: 80.407% (3911/4864)/ 94.860% (4614/4864)
01/15/2023 14:51:33 - INFO - __main__ -   test: [batch: 38/391 ] | Loss: 0.742 | Acc: 80.609% (4024/4992)/ 94.972% (4741/4992)
01/15/2023 14:51:36 - INFO - __main__ -   test: [batch: 39/391 ] | Loss: 0.736 | Acc: 80.762% (4135/5120)/ 95.000% (4864/5120)
01/15/2023 14:51:38 - INFO - __main__ -   test: [batch: 40/391 ] | Loss: 0.734 | Acc: 80.888% (4245/5248)/ 94.931% (4982/5248)
01/15/2023 14:51:40 - INFO - __main__ -   test: [batch: 41/391 ] | Loss: 0.736 | Acc: 81.064% (4358/5376)/ 94.847% (5099/5376)
01/15/2023 14:51:42 - INFO - __main__ -   test: [batch: 42/391 ] | Loss: 0.737 | Acc: 81.086% (4463/5504)/ 94.931% (5225/5504)
01/15/2023 14:51:44 - INFO - __main__ -   test: [batch: 43/391 ] | Loss: 0.735 | Acc: 81.143% (4570/5632)/ 94.904% (5345/5632)
01/15/2023 14:51:46 - INFO - __main__ -   test: [batch: 44/391 ] | Loss: 0.739 | Acc: 81.163% (4675/5760)/ 94.809% (5461/5760)
01/15/2023 14:51:49 - INFO - __main__ -   test: [batch: 45/391 ] | Loss: 0.736 | Acc: 81.301% (4787/5888)/ 94.854% (5585/5888)
01/15/2023 14:51:50 - INFO - __main__ -   test: [batch: 46/391 ] | Loss: 0.739 | Acc: 81.200% (4885/6016)/ 94.864% (5707/6016)
01/15/2023 14:51:53 - INFO - __main__ -   test: [batch: 47/391 ] | Loss: 0.743 | Acc: 81.022% (4978/6144)/ 94.906% (5831/6144)
01/15/2023 14:51:55 - INFO - __main__ -   test: [batch: 48/391 ] | Loss: 0.748 | Acc: 80.915% (5075/6272)/ 94.882% (5951/6272)
01/15/2023 14:51:57 - INFO - __main__ -   test: [batch: 49/391 ] | Loss: 0.752 | Acc: 80.891% (5177/6400)/ 94.781% (6066/6400)
01/15/2023 14:51:59 - INFO - __main__ -   test: [batch: 50/391 ] | Loss: 0.744 | Acc: 81.081% (5293/6528)/ 94.853% (6192/6528)
01/15/2023 14:52:01 - INFO - __main__ -   test: [batch: 51/391 ] | Loss: 0.735 | Acc: 81.295% (5411/6656)/ 94.937% (6319/6656)
01/15/2023 14:52:03 - INFO - __main__ -   test: [batch: 52/391 ] | Loss: 0.731 | Acc: 81.397% (5522/6784)/ 94.973% (6443/6784)
01/15/2023 14:52:05 - INFO - __main__ -   test: [batch: 53/391 ] | Loss: 0.721 | Acc: 81.626% (5642/6912)/ 95.052% (6570/6912)
01/15/2023 14:52:08 - INFO - __main__ -   test: [batch: 54/391 ] | Loss: 0.714 | Acc: 81.790% (5758/7040)/ 95.085% (6694/7040)
01/15/2023 14:52:10 - INFO - __main__ -   test: [batch: 55/391 ] | Loss: 0.708 | Acc: 81.934% (5873/7168)/ 95.131% (6819/7168)
01/15/2023 14:52:12 - INFO - __main__ -   test: [batch: 56/391 ] | Loss: 0.699 | Acc: 82.196% (5997/7296)/ 95.189% (6945/7296)
01/15/2023 14:52:14 - INFO - __main__ -   test: [batch: 57/391 ] | Loss: 0.692 | Acc: 82.422% (6119/7424)/ 95.232% (7070/7424)
01/15/2023 14:52:16 - INFO - __main__ -   test: [batch: 58/391 ] | Loss: 0.688 | Acc: 82.508% (6231/7552)/ 95.233% (7192/7552)
01/15/2023 14:52:18 - INFO - __main__ -   test: [batch: 59/391 ] | Loss: 0.689 | Acc: 82.448% (6332/7680)/ 95.208% (7312/7680)
01/15/2023 14:52:20 - INFO - __main__ -   test: [batch: 60/391 ] | Loss: 0.692 | Acc: 82.390% (6433/7808)/ 95.197% (7433/7808)
01/15/2023 14:52:22 - INFO - __main__ -   test: [batch: 61/391 ] | Loss: 0.692 | Acc: 82.359% (6536/7936)/ 95.212% (7556/7936)
01/15/2023 14:52:25 - INFO - __main__ -   test: [batch: 62/391 ] | Loss: 0.691 | Acc: 82.341% (6640/8064)/ 95.263% (7682/8064)
01/15/2023 14:52:27 - INFO - __main__ -   test: [batch: 63/391 ] | Loss: 0.698 | Acc: 82.214% (6735/8192)/ 95.203% (7799/8192)
01/15/2023 14:52:29 - INFO - __main__ -   test: [batch: 64/391 ] | Loss: 0.707 | Acc: 82.007% (6823/8320)/ 95.156% (7917/8320)
01/15/2023 14:52:31 - INFO - __main__ -   test: [batch: 65/391 ] | Loss: 0.720 | Acc: 81.534% (6888/8448)/ 95.088% (8033/8448)
01/15/2023 14:52:33 - INFO - __main__ -   test: [batch: 66/391 ] | Loss: 0.725 | Acc: 81.542% (6993/8576)/ 95.044% (8151/8576)
01/15/2023 14:52:35 - INFO - __main__ -   test: [batch: 67/391 ] | Loss: 0.728 | Acc: 81.491% (7093/8704)/ 95.037% (8272/8704)
01/15/2023 14:52:37 - INFO - __main__ -   test: [batch: 68/391 ] | Loss: 0.729 | Acc: 81.431% (7192/8832)/ 95.052% (8395/8832)
01/15/2023 14:52:39 - INFO - __main__ -   test: [batch: 69/391 ] | Loss: 0.724 | Acc: 81.529% (7305/8960)/ 95.100% (8521/8960)
01/15/2023 14:52:41 - INFO - __main__ -   test: [batch: 70/391 ] | Loss: 0.724 | Acc: 81.459% (7403/9088)/ 95.125% (8645/9088)
01/15/2023 14:52:43 - INFO - __main__ -   test: [batch: 71/391 ] | Loss: 0.722 | Acc: 81.521% (7513/9216)/ 95.117% (8766/9216)
01/15/2023 14:52:46 - INFO - __main__ -   test: [batch: 72/391 ] | Loss: 0.725 | Acc: 81.421% (7608/9344)/ 95.131% (8889/9344)
01/15/2023 14:52:48 - INFO - __main__ -   test: [batch: 73/391 ] | Loss: 0.728 | Acc: 81.334% (7704/9472)/ 95.122% (9010/9472)
01/15/2023 14:52:50 - INFO - __main__ -   test: [batch: 74/391 ] | Loss: 0.729 | Acc: 81.333% (7808/9600)/ 95.115% (9131/9600)
01/15/2023 14:52:52 - INFO - __main__ -   test: [batch: 75/391 ] | Loss: 0.735 | Acc: 81.157% (7895/9728)/ 95.076% (9249/9728)
01/15/2023 14:52:54 - INFO - __main__ -   test: [batch: 76/391 ] | Loss: 0.734 | Acc: 81.169% (8000/9856)/ 95.099% (9373/9856)
01/15/2023 14:52:56 - INFO - __main__ -   test: [batch: 77/391 ] | Loss: 0.734 | Acc: 81.090% (8096/9984)/ 95.132% (9498/9984)
01/15/2023 14:52:58 - INFO - __main__ -   test: [batch: 78/391 ] | Loss: 0.733 | Acc: 81.023% (8193/10112)/ 95.174% (9624/10112)
01/15/2023 14:53:00 - INFO - __main__ -   test: [batch: 79/391 ] | Loss: 0.732 | Acc: 81.016% (8296/10240)/ 95.186% (9747/10240)
01/15/2023 14:53:03 - INFO - __main__ -   test: [batch: 80/391 ] | Loss: 0.732 | Acc: 80.970% (8395/10368)/ 95.168% (9867/10368)
01/15/2023 14:53:05 - INFO - __main__ -   test: [batch: 81/391 ] | Loss: 0.731 | Acc: 81.002% (8502/10496)/ 95.198% (9992/10496)
01/15/2023 14:53:07 - INFO - __main__ -   test: [batch: 82/391 ] | Loss: 0.730 | Acc: 81.034% (8609/10624)/ 95.200% (10114/10624)
01/15/2023 14:53:09 - INFO - __main__ -   test: [batch: 83/391 ] | Loss: 0.730 | Acc: 81.073% (8717/10752)/ 95.173% (10233/10752)
01/15/2023 14:53:11 - INFO - __main__ -   test: [batch: 84/391 ] | Loss: 0.726 | Acc: 81.204% (8835/10880)/ 95.211% (10359/10880)
01/15/2023 14:53:13 - INFO - __main__ -   test: [batch: 85/391 ] | Loss: 0.724 | Acc: 81.223% (8941/11008)/ 95.267% (10487/11008)
01/15/2023 14:53:15 - INFO - __main__ -   test: [batch: 86/391 ] | Loss: 0.728 | Acc: 81.142% (9036/11136)/ 95.232% (10605/11136)
01/15/2023 14:53:17 - INFO - __main__ -   test: [batch: 87/391 ] | Loss: 0.726 | Acc: 81.214% (9148/11264)/ 95.250% (10729/11264)
01/15/2023 14:53:20 - INFO - __main__ -   test: [batch: 88/391 ] | Loss: 0.733 | Acc: 81.145% (9244/11392)/ 95.198% (10845/11392)
01/15/2023 14:53:22 - INFO - __main__ -   test: [batch: 89/391 ] | Loss: 0.732 | Acc: 81.189% (9353/11520)/ 95.208% (10968/11520)
01/15/2023 14:53:24 - INFO - __main__ -   test: [batch: 90/391 ] | Loss: 0.733 | Acc: 81.087% (9445/11648)/ 95.209% (11090/11648)
01/15/2023 14:53:26 - INFO - __main__ -   test: [batch: 91/391 ] | Loss: 0.731 | Acc: 81.131% (9554/11776)/ 95.228% (11214/11776)
01/15/2023 14:53:28 - INFO - __main__ -   test: [batch: 92/391 ] | Loss: 0.732 | Acc: 81.099% (9654/11904)/ 95.195% (11332/11904)
01/15/2023 14:53:30 - INFO - __main__ -   test: [batch: 93/391 ] | Loss: 0.735 | Acc: 80.893% (9733/12032)/ 95.238% (11459/12032)
01/15/2023 14:53:32 - INFO - __main__ -   test: [batch: 94/391 ] | Loss: 0.739 | Acc: 80.773% (9822/12160)/ 95.255% (11583/12160)
01/15/2023 14:53:34 - INFO - __main__ -   test: [batch: 95/391 ] | Loss: 0.736 | Acc: 80.843% (9934/12288)/ 95.264% (11706/12288)
01/15/2023 14:53:36 - INFO - __main__ -   test: [batch: 96/391 ] | Loss: 0.740 | Acc: 80.694% (10019/12416)/ 95.272% (11829/12416)
01/15/2023 14:53:38 - INFO - __main__ -   test: [batch: 97/391 ] | Loss: 0.742 | Acc: 80.517% (10100/12544)/ 95.289% (11953/12544)
01/15/2023 14:53:40 - INFO - __main__ -   test: [batch: 98/391 ] | Loss: 0.738 | Acc: 80.619% (10216/12672)/ 95.328% (12080/12672)
01/15/2023 14:53:42 - INFO - __main__ -   test: [batch: 99/391 ] | Loss: 0.732 | Acc: 80.773% (10339/12800)/ 95.375% (12208/12800)
01/15/2023 14:53:44 - INFO - __main__ -   test: [batch: 100/391 ] | Loss: 0.731 | Acc: 80.817% (10448/12928)/ 95.398% (12333/12928)
01/15/2023 14:53:46 - INFO - __main__ -   test: [batch: 101/391 ] | Loss: 0.726 | Acc: 80.936% (10567/13056)/ 95.427% (12459/13056)
01/15/2023 14:53:49 - INFO - __main__ -   test: [batch: 102/391 ] | Loss: 0.723 | Acc: 81.030% (10683/13184)/ 95.464% (12586/13184)
01/15/2023 14:53:51 - INFO - __main__ -   test: [batch: 103/391 ] | Loss: 0.725 | Acc: 80.874% (10766/13312)/ 95.470% (12709/13312)
01/15/2023 14:53:53 - INFO - __main__ -   test: [batch: 104/391 ] | Loss: 0.724 | Acc: 80.811% (10861/13440)/ 95.476% (12832/13440)
01/15/2023 14:53:55 - INFO - __main__ -   test: [batch: 105/391 ] | Loss: 0.725 | Acc: 80.808% (10964/13568)/ 95.475% (12954/13568)
01/15/2023 14:53:57 - INFO - __main__ -   test: [batch: 106/391 ] | Loss: 0.734 | Acc: 80.702% (11053/13696)/ 95.407% (13067/13696)
01/15/2023 14:53:59 - INFO - __main__ -   test: [batch: 107/391 ] | Loss: 0.730 | Acc: 80.830% (11174/13824)/ 95.443% (13194/13824)
01/15/2023 14:54:01 - INFO - __main__ -   test: [batch: 108/391 ] | Loss: 0.733 | Acc: 80.698% (11259/13952)/ 95.449% (13317/13952)
01/15/2023 14:54:03 - INFO - __main__ -   test: [batch: 109/391 ] | Loss: 0.733 | Acc: 80.682% (11360/14080)/ 95.455% (13440/14080)
01/15/2023 14:54:06 - INFO - __main__ -   test: [batch: 110/391 ] | Loss: 0.736 | Acc: 80.504% (11438/14208)/ 95.474% (13565/14208)
01/15/2023 14:54:08 - INFO - __main__ -   test: [batch: 111/391 ] | Loss: 0.739 | Acc: 80.469% (11536/14336)/ 95.424% (13680/14336)
01/15/2023 14:54:10 - INFO - __main__ -   test: [batch: 112/391 ] | Loss: 0.739 | Acc: 80.503% (11644/14464)/ 95.444% (13805/14464)
01/15/2023 14:54:12 - INFO - __main__ -   test: [batch: 113/391 ] | Loss: 0.738 | Acc: 80.551% (11754/14592)/ 95.443% (13927/14592)
01/15/2023 14:54:14 - INFO - __main__ -   test: [batch: 114/391 ] | Loss: 0.735 | Acc: 80.639% (11870/14720)/ 95.469% (14053/14720)
01/15/2023 14:54:16 - INFO - __main__ -   test: [batch: 115/391 ] | Loss: 0.732 | Acc: 80.718% (11985/14848)/ 95.488% (14178/14848)
01/15/2023 14:54:18 - INFO - __main__ -   test: [batch: 116/391 ] | Loss: 0.735 | Acc: 80.696% (12085/14976)/ 95.466% (14297/14976)
01/15/2023 14:54:20 - INFO - __main__ -   test: [batch: 117/391 ] | Loss: 0.734 | Acc: 80.720% (12192/15104)/ 95.485% (14422/15104)
01/15/2023 14:54:23 - INFO - __main__ -   test: [batch: 118/391 ] | Loss: 0.737 | Acc: 80.580% (12274/15232)/ 95.496% (14546/15232)
01/15/2023 14:54:25 - INFO - __main__ -   test: [batch: 119/391 ] | Loss: 0.735 | Acc: 80.632% (12385/15360)/ 95.521% (14672/15360)
01/15/2023 14:54:27 - INFO - __main__ -   test: [batch: 120/391 ] | Loss: 0.734 | Acc: 80.675% (12495/15488)/ 95.532% (14796/15488)
01/15/2023 14:54:29 - INFO - __main__ -   test: [batch: 121/391 ] | Loss: 0.739 | Acc: 80.546% (12578/15616)/ 95.505% (14914/15616)
01/15/2023 14:54:31 - INFO - __main__ -   test: [batch: 122/391 ] | Loss: 0.742 | Acc: 80.494% (12673/15744)/ 95.490% (15034/15744)
01/15/2023 14:54:33 - INFO - __main__ -   test: [batch: 123/391 ] | Loss: 0.742 | Acc: 80.519% (12780/15872)/ 95.476% (15154/15872)
01/15/2023 14:54:35 - INFO - __main__ -   test: [batch: 124/391 ] | Loss: 0.742 | Acc: 80.544% (12887/16000)/ 95.481% (15277/16000)
01/15/2023 14:54:37 - INFO - __main__ -   test: [batch: 125/391 ] | Loss: 0.737 | Acc: 80.655% (13008/16128)/ 95.511% (15404/16128)
01/15/2023 14:54:39 - INFO - __main__ -   test: [batch: 126/391 ] | Loss: 0.734 | Acc: 80.746% (13126/16256)/ 95.516% (15527/16256)
01/15/2023 14:54:42 - INFO - __main__ -   test: [batch: 127/391 ] | Loss: 0.732 | Acc: 80.823% (13242/16384)/ 95.526% (15651/16384)
01/15/2023 14:54:44 - INFO - __main__ -   test: [batch: 128/391 ] | Loss: 0.733 | Acc: 80.820% (13345/16512)/ 95.506% (15770/16512)
01/15/2023 14:54:46 - INFO - __main__ -   test: [batch: 129/391 ] | Loss: 0.731 | Acc: 80.853% (13454/16640)/ 95.529% (15896/16640)
01/15/2023 14:54:48 - INFO - __main__ -   test: [batch: 130/391 ] | Loss: 0.727 | Acc: 80.964% (13576/16768)/ 95.551% (16022/16768)
01/15/2023 14:54:50 - INFO - __main__ -   test: [batch: 131/391 ] | Loss: 0.727 | Acc: 81.013% (13688/16896)/ 95.561% (16146/16896)
01/15/2023 14:54:52 - INFO - __main__ -   test: [batch: 132/391 ] | Loss: 0.725 | Acc: 81.080% (13803/17024)/ 95.577% (16271/17024)
01/15/2023 14:54:54 - INFO - __main__ -   test: [batch: 133/391 ] | Loss: 0.727 | Acc: 81.028% (13898/17152)/ 95.569% (16392/17152)
01/15/2023 14:54:56 - INFO - __main__ -   test: [batch: 134/391 ] | Loss: 0.725 | Acc: 81.042% (14004/17280)/ 95.584% (16517/17280)
01/15/2023 14:54:59 - INFO - __main__ -   test: [batch: 135/391 ] | Loss: 0.725 | Acc: 81.026% (14105/17408)/ 95.582% (16639/17408)
01/15/2023 14:55:01 - INFO - __main__ -   test: [batch: 136/391 ] | Loss: 0.723 | Acc: 81.005% (14205/17536)/ 95.603% (16765/17536)
01/15/2023 14:55:03 - INFO - __main__ -   test: [batch: 137/391 ] | Loss: 0.721 | Acc: 81.041% (14315/17664)/ 95.635% (16893/17664)
01/15/2023 14:55:05 - INFO - __main__ -   test: [batch: 138/391 ] | Loss: 0.720 | Acc: 81.081% (14426/17792)/ 95.650% (17018/17792)
01/15/2023 14:55:07 - INFO - __main__ -   test: [batch: 139/391 ] | Loss: 0.725 | Acc: 80.949% (14506/17920)/ 95.642% (17139/17920)
01/15/2023 14:55:09 - INFO - __main__ -   test: [batch: 140/391 ] | Loss: 0.727 | Acc: 80.868% (14595/18048)/ 95.639% (17261/18048)
01/15/2023 14:55:11 - INFO - __main__ -   test: [batch: 141/391 ] | Loss: 0.725 | Acc: 80.898% (14704/18176)/ 95.654% (17386/18176)
01/15/2023 14:55:14 - INFO - __main__ -   test: [batch: 142/391 ] | Loss: 0.723 | Acc: 80.982% (14823/18304)/ 95.662% (17510/18304)
01/15/2023 14:55:16 - INFO - __main__ -   test: [batch: 143/391 ] | Loss: 0.723 | Acc: 81.011% (14932/18432)/ 95.649% (17630/18432)
01/15/2023 14:55:18 - INFO - __main__ -   test: [batch: 144/391 ] | Loss: 0.725 | Acc: 80.981% (15030/18560)/ 95.614% (17746/18560)
01/15/2023 14:55:20 - INFO - __main__ -   test: [batch: 145/391 ] | Loss: 0.728 | Acc: 80.966% (15131/18688)/ 95.591% (17864/18688)
01/15/2023 14:55:22 - INFO - __main__ -   test: [batch: 146/391 ] | Loss: 0.729 | Acc: 80.947% (15231/18816)/ 95.568% (17982/18816)
01/15/2023 14:55:24 - INFO - __main__ -   test: [batch: 147/391 ] | Loss: 0.729 | Acc: 80.939% (15333/18944)/ 95.561% (18103/18944)
01/15/2023 14:55:26 - INFO - __main__ -   test: [batch: 148/391 ] | Loss: 0.731 | Acc: 80.872% (15424/19072)/ 95.569% (18227/19072)
01/15/2023 14:55:29 - INFO - __main__ -   test: [batch: 149/391 ] | Loss: 0.735 | Acc: 80.792% (15512/19200)/ 95.542% (18344/19200)
01/15/2023 14:55:31 - INFO - __main__ -   test: [batch: 150/391 ] | Loss: 0.735 | Acc: 80.717% (15601/19328)/ 95.550% (18468/19328)
01/15/2023 14:55:33 - INFO - __main__ -   test: [batch: 151/391 ] | Loss: 0.734 | Acc: 80.757% (15712/19456)/ 95.549% (18590/19456)
01/15/2023 14:55:35 - INFO - __main__ -   test: [batch: 152/391 ] | Loss: 0.736 | Acc: 80.739% (15812/19584)/ 95.522% (18707/19584)
01/15/2023 14:55:37 - INFO - __main__ -   test: [batch: 153/391 ] | Loss: 0.734 | Acc: 80.783% (15924/19712)/ 95.526% (18830/19712)
01/15/2023 14:55:39 - INFO - __main__ -   test: [batch: 154/391 ] | Loss: 0.736 | Acc: 80.761% (16023/19840)/ 95.499% (18947/19840)
01/15/2023 14:55:41 - INFO - __main__ -   test: [batch: 155/391 ] | Loss: 0.736 | Acc: 80.784% (16131/19968)/ 95.488% (19067/19968)
01/15/2023 14:55:43 - INFO - __main__ -   test: [batch: 156/391 ] | Loss: 0.738 | Acc: 80.713% (16220/20096)/ 95.457% (19183/20096)
01/15/2023 14:55:46 - INFO - __main__ -   test: [batch: 157/391 ] | Loss: 0.741 | Acc: 80.667% (16314/20224)/ 95.441% (19302/20224)
01/15/2023 14:55:48 - INFO - __main__ -   test: [batch: 158/391 ] | Loss: 0.741 | Acc: 80.660% (16416/20352)/ 95.416% (19419/20352)
01/15/2023 14:55:50 - INFO - __main__ -   test: [batch: 159/391 ] | Loss: 0.743 | Acc: 80.640% (16515/20480)/ 95.415% (19541/20480)
01/15/2023 14:55:52 - INFO - __main__ -   test: [batch: 160/391 ] | Loss: 0.743 | Acc: 80.639% (16618/20608)/ 95.390% (19658/20608)
01/15/2023 14:55:54 - INFO - __main__ -   test: [batch: 161/391 ] | Loss: 0.753 | Acc: 80.401% (16672/20736)/ 95.288% (19759/20736)
01/15/2023 14:55:56 - INFO - __main__ -   test: [batch: 162/391 ] | Loss: 0.759 | Acc: 80.301% (16754/20864)/ 95.207% (19864/20864)
01/15/2023 14:55:58 - INFO - __main__ -   test: [batch: 163/391 ] | Loss: 0.761 | Acc: 80.231% (16842/20992)/ 95.193% (19983/20992)
01/15/2023 14:56:00 - INFO - __main__ -   test: [batch: 164/391 ] | Loss: 0.762 | Acc: 80.213% (16941/21120)/ 95.208% (20108/21120)
01/15/2023 14:56:02 - INFO - __main__ -   test: [batch: 165/391 ] | Loss: 0.764 | Acc: 80.139% (17028/21248)/ 95.195% (20227/21248)
01/15/2023 14:56:05 - INFO - __main__ -   test: [batch: 166/391 ] | Loss: 0.764 | Acc: 80.160% (17135/21376)/ 95.186% (20347/21376)
01/15/2023 14:56:07 - INFO - __main__ -   test: [batch: 167/391 ] | Loss: 0.766 | Acc: 80.106% (17226/21504)/ 95.159% (20463/21504)
01/15/2023 14:56:09 - INFO - __main__ -   test: [batch: 168/391 ] | Loss: 0.765 | Acc: 80.122% (17332/21632)/ 95.155% (20584/21632)
01/15/2023 14:56:11 - INFO - __main__ -   test: [batch: 169/391 ] | Loss: 0.768 | Acc: 80.064% (17422/21760)/ 95.110% (20696/21760)
01/15/2023 14:56:13 - INFO - __main__ -   test: [batch: 170/391 ] | Loss: 0.772 | Acc: 79.966% (17503/21888)/ 95.079% (20811/21888)
01/15/2023 14:56:15 - INFO - __main__ -   test: [batch: 171/391 ] | Loss: 0.776 | Acc: 79.919% (17595/22016)/ 95.049% (20926/22016)
01/15/2023 14:56:17 - INFO - __main__ -   test: [batch: 172/391 ] | Loss: 0.776 | Acc: 79.873% (17687/22144)/ 95.046% (21047/22144)
01/15/2023 14:56:19 - INFO - __main__ -   test: [batch: 173/391 ] | Loss: 0.779 | Acc: 79.831% (17780/22272)/ 95.025% (21164/22272)
01/15/2023 14:56:21 - INFO - __main__ -   test: [batch: 174/391 ] | Loss: 0.783 | Acc: 79.728% (17859/22400)/ 94.964% (21272/22400)
01/15/2023 14:56:23 - INFO - __main__ -   test: [batch: 175/391 ] | Loss: 0.782 | Acc: 79.785% (17974/22528)/ 94.975% (21396/22528)
01/15/2023 14:56:26 - INFO - __main__ -   test: [batch: 176/391 ] | Loss: 0.782 | Acc: 79.785% (18076/22656)/ 94.937% (21509/22656)
01/15/2023 14:56:28 - INFO - __main__ -   test: [batch: 177/391 ] | Loss: 0.786 | Acc: 79.709% (18161/22784)/ 94.904% (21623/22784)
01/15/2023 14:56:30 - INFO - __main__ -   test: [batch: 178/391 ] | Loss: 0.787 | Acc: 79.696% (18260/22912)/ 94.889% (21741/22912)
01/15/2023 14:56:32 - INFO - __main__ -   test: [batch: 179/391 ] | Loss: 0.791 | Acc: 79.631% (18347/23040)/ 94.835% (21850/23040)
01/15/2023 14:56:34 - INFO - __main__ -   test: [batch: 180/391 ] | Loss: 0.797 | Acc: 79.498% (18418/23168)/ 94.790% (21961/23168)
01/15/2023 14:56:36 - INFO - __main__ -   test: [batch: 181/391 ] | Loss: 0.804 | Acc: 79.370% (18490/23296)/ 94.733% (22069/23296)
01/15/2023 14:56:38 - INFO - __main__ -   test: [batch: 182/391 ] | Loss: 0.803 | Acc: 79.389% (18596/23424)/ 94.732% (22190/23424)
01/15/2023 14:56:40 - INFO - __main__ -   test: [batch: 183/391 ] | Loss: 0.809 | Acc: 79.259% (18667/23552)/ 94.629% (22287/23552)
01/15/2023 14:56:42 - INFO - __main__ -   test: [batch: 184/391 ] | Loss: 0.809 | Acc: 79.274% (18772/23680)/ 94.616% (22405/23680)
01/15/2023 14:56:44 - INFO - __main__ -   test: [batch: 185/391 ] | Loss: 0.808 | Acc: 79.280% (18875/23808)/ 94.607% (22524/23808)
01/15/2023 14:56:47 - INFO - __main__ -   test: [batch: 186/391 ] | Loss: 0.811 | Acc: 79.240% (18967/23936)/ 94.569% (22636/23936)
01/15/2023 14:56:49 - INFO - __main__ -   test: [batch: 187/391 ] | Loss: 0.816 | Acc: 79.131% (19042/24064)/ 94.548% (22752/24064)
01/15/2023 14:56:51 - INFO - __main__ -   test: [batch: 188/391 ] | Loss: 0.819 | Acc: 78.997% (19111/24192)/ 94.523% (22867/24192)
01/15/2023 14:56:53 - INFO - __main__ -   test: [batch: 189/391 ] | Loss: 0.821 | Acc: 78.923% (19194/24320)/ 94.527% (22989/24320)
01/15/2023 14:56:55 - INFO - __main__ -   test: [batch: 190/391 ] | Loss: 0.826 | Acc: 78.833% (19273/24448)/ 94.503% (23104/24448)
01/15/2023 14:56:58 - INFO - __main__ -   test: [batch: 191/391 ] | Loss: 0.828 | Acc: 78.792% (19364/24576)/ 94.474% (23218/24576)
01/15/2023 14:57:00 - INFO - __main__ -   test: [batch: 192/391 ] | Loss: 0.835 | Acc: 78.684% (19438/24704)/ 94.381% (23316/24704)
01/15/2023 14:57:02 - INFO - __main__ -   test: [batch: 193/391 ] | Loss: 0.835 | Acc: 78.697% (19542/24832)/ 94.374% (23435/24832)
01/15/2023 14:57:04 - INFO - __main__ -   test: [batch: 194/391 ] | Loss: 0.838 | Acc: 78.626% (19625/24960)/ 94.351% (23550/24960)
01/15/2023 14:57:06 - INFO - __main__ -   test: [batch: 195/391 ] | Loss: 0.844 | Acc: 78.540% (19704/25088)/ 94.284% (23654/25088)
01/15/2023 14:57:08 - INFO - __main__ -   test: [batch: 196/391 ] | Loss: 0.847 | Acc: 78.446% (19781/25216)/ 94.246% (23765/25216)
01/15/2023 14:57:10 - INFO - __main__ -   test: [batch: 197/391 ] | Loss: 0.851 | Acc: 78.362% (19860/25344)/ 94.231% (23882/25344)
01/15/2023 14:57:13 - INFO - __main__ -   test: [batch: 198/391 ] | Loss: 0.853 | Acc: 78.317% (19949/25472)/ 94.194% (23993/25472)
01/15/2023 14:57:15 - INFO - __main__ -   test: [batch: 199/391 ] | Loss: 0.853 | Acc: 78.293% (20043/25600)/ 94.203% (24116/25600)
01/15/2023 14:57:17 - INFO - __main__ -   test: [batch: 200/391 ] | Loss: 0.853 | Acc: 78.277% (20139/25728)/ 94.205% (24237/25728)
01/15/2023 14:57:19 - INFO - __main__ -   test: [batch: 201/391 ] | Loss: 0.856 | Acc: 78.175% (20213/25856)/ 94.164% (24347/25856)
01/15/2023 14:57:21 - INFO - __main__ -   test: [batch: 202/391 ] | Loss: 0.858 | Acc: 78.152% (20307/25984)/ 94.150% (24464/25984)
01/15/2023 14:57:23 - INFO - __main__ -   test: [batch: 203/391 ] | Loss: 0.858 | Acc: 78.144% (20405/26112)/ 94.148% (24584/26112)
01/15/2023 14:57:25 - INFO - __main__ -   test: [batch: 204/391 ] | Loss: 0.861 | Acc: 78.060% (20483/26240)/ 94.120% (24697/26240)
01/15/2023 14:57:27 - INFO - __main__ -   test: [batch: 205/391 ] | Loss: 0.864 | Acc: 77.988% (20564/26368)/ 94.095% (24811/26368)
01/15/2023 14:57:29 - INFO - __main__ -   test: [batch: 206/391 ] | Loss: 0.866 | Acc: 77.963% (20657/26496)/ 94.093% (24931/26496)
01/15/2023 14:57:32 - INFO - __main__ -   test: [batch: 207/391 ] | Loss: 0.869 | Acc: 77.892% (20738/26624)/ 94.062% (25043/26624)
01/15/2023 14:57:34 - INFO - __main__ -   test: [batch: 208/391 ] | Loss: 0.871 | Acc: 77.856% (20828/26752)/ 94.027% (25154/26752)
01/15/2023 14:57:36 - INFO - __main__ -   test: [batch: 209/391 ] | Loss: 0.871 | Acc: 77.857% (20928/26880)/ 94.048% (25280/26880)
01/15/2023 14:57:38 - INFO - __main__ -   test: [batch: 210/391 ] | Loss: 0.872 | Acc: 77.832% (21021/27008)/ 94.028% (25395/27008)
01/15/2023 14:57:40 - INFO - __main__ -   test: [batch: 211/391 ] | Loss: 0.874 | Acc: 77.768% (21103/27136)/ 93.993% (25506/27136)
01/15/2023 14:57:42 - INFO - __main__ -   test: [batch: 212/391 ] | Loss: 0.876 | Acc: 77.703% (21185/27264)/ 93.981% (25623/27264)
01/15/2023 14:57:44 - INFO - __main__ -   test: [batch: 213/391 ] | Loss: 0.876 | Acc: 77.712% (21287/27392)/ 93.987% (25745/27392)
01/15/2023 14:57:46 - INFO - __main__ -   test: [batch: 214/391 ] | Loss: 0.876 | Acc: 77.703% (21384/27520)/ 93.990% (25866/27520)
01/15/2023 14:57:48 - INFO - __main__ -   test: [batch: 215/391 ] | Loss: 0.877 | Acc: 77.691% (21480/27648)/ 93.971% (25981/27648)
01/15/2023 14:57:51 - INFO - __main__ -   test: [batch: 216/391 ] | Loss: 0.875 | Acc: 77.740% (21593/27776)/ 93.977% (26103/27776)
01/15/2023 14:57:53 - INFO - __main__ -   test: [batch: 217/391 ] | Loss: 0.880 | Acc: 77.677% (21675/27904)/ 93.908% (26204/27904)
01/15/2023 14:57:55 - INFO - __main__ -   test: [batch: 218/391 ] | Loss: 0.883 | Acc: 77.622% (21759/28032)/ 93.871% (26314/28032)
01/15/2023 14:57:57 - INFO - __main__ -   test: [batch: 219/391 ] | Loss: 0.881 | Acc: 77.660% (21869/28160)/ 93.878% (26436/28160)
01/15/2023 14:57:59 - INFO - __main__ -   test: [batch: 220/391 ] | Loss: 0.879 | Acc: 77.697% (21979/28288)/ 93.888% (26559/28288)
01/15/2023 14:58:01 - INFO - __main__ -   test: [batch: 221/391 ] | Loss: 0.881 | Acc: 77.664% (22069/28416)/ 93.866% (26673/28416)
01/15/2023 14:58:03 - INFO - __main__ -   test: [batch: 222/391 ] | Loss: 0.879 | Acc: 77.712% (22182/28544)/ 93.880% (26797/28544)
01/15/2023 14:58:06 - INFO - __main__ -   test: [batch: 223/391 ] | Loss: 0.880 | Acc: 77.706% (22280/28672)/ 93.865% (26913/28672)
01/15/2023 14:58:08 - INFO - __main__ -   test: [batch: 224/391 ] | Loss: 0.879 | Acc: 77.729% (22386/28800)/ 93.861% (27032/28800)
01/15/2023 14:58:10 - INFO - __main__ -   test: [batch: 225/391 ] | Loss: 0.879 | Acc: 77.721% (22483/28928)/ 93.864% (27153/28928)
01/15/2023 14:58:12 - INFO - __main__ -   test: [batch: 226/391 ] | Loss: 0.879 | Acc: 77.691% (22574/29056)/ 93.870% (27275/29056)
01/15/2023 14:58:14 - INFO - __main__ -   test: [batch: 227/391 ] | Loss: 0.881 | Acc: 77.693% (22674/29184)/ 93.863% (27393/29184)
01/15/2023 14:58:16 - INFO - __main__ -   test: [batch: 228/391 ] | Loss: 0.885 | Acc: 77.600% (22746/29312)/ 93.801% (27495/29312)
01/15/2023 14:58:18 - INFO - __main__ -   test: [batch: 229/391 ] | Loss: 0.889 | Acc: 77.524% (22823/29440)/ 93.753% (27601/29440)
01/15/2023 14:58:20 - INFO - __main__ -   test: [batch: 230/391 ] | Loss: 0.892 | Acc: 77.449% (22900/29568)/ 93.709% (27708/29568)
01/15/2023 14:58:22 - INFO - __main__ -   test: [batch: 231/391 ] | Loss: 0.892 | Acc: 77.435% (22995/29696)/ 93.699% (27825/29696)
01/15/2023 14:58:25 - INFO - __main__ -   test: [batch: 232/391 ] | Loss: 0.891 | Acc: 77.468% (23104/29824)/ 93.710% (27948/29824)
01/15/2023 14:58:27 - INFO - __main__ -   test: [batch: 233/391 ] | Loss: 0.893 | Acc: 77.424% (23190/29952)/ 93.687% (28061/29952)
01/15/2023 14:58:29 - INFO - __main__ -   test: [batch: 234/391 ] | Loss: 0.900 | Acc: 77.320% (23258/30080)/ 93.620% (28161/30080)
01/15/2023 14:58:31 - INFO - __main__ -   test: [batch: 235/391 ] | Loss: 0.900 | Acc: 77.311% (23354/30208)/ 93.614% (28279/30208)
01/15/2023 14:58:33 - INFO - __main__ -   test: [batch: 236/391 ] | Loss: 0.898 | Acc: 77.357% (23467/30336)/ 93.625% (28402/30336)
01/15/2023 14:58:35 - INFO - __main__ -   test: [batch: 237/391 ] | Loss: 0.899 | Acc: 77.354% (23565/30464)/ 93.592% (28512/30464)
01/15/2023 14:58:37 - INFO - __main__ -   test: [batch: 238/391 ] | Loss: 0.898 | Acc: 77.386% (23674/30592)/ 93.606% (28636/30592)
01/15/2023 14:58:40 - INFO - __main__ -   test: [batch: 239/391 ] | Loss: 0.897 | Acc: 77.419% (23783/30720)/ 93.610% (28757/30720)
01/15/2023 14:58:42 - INFO - __main__ -   test: [batch: 240/391 ] | Loss: 0.897 | Acc: 77.435% (23887/30848)/ 93.601% (28874/30848)
01/15/2023 14:58:44 - INFO - __main__ -   test: [batch: 241/391 ] | Loss: 0.900 | Acc: 77.363% (23964/30976)/ 93.560% (28981/30976)
01/15/2023 14:58:46 - INFO - __main__ -   test: [batch: 242/391 ] | Loss: 0.902 | Acc: 77.263% (24032/31104)/ 93.544% (29096/31104)
01/15/2023 14:58:48 - INFO - __main__ -   test: [batch: 243/391 ] | Loss: 0.908 | Acc: 77.142% (24093/31232)/ 93.478% (29195/31232)
01/15/2023 14:58:50 - INFO - __main__ -   test: [batch: 244/391 ] | Loss: 0.908 | Acc: 77.140% (24191/31360)/ 93.482% (29316/31360)
01/15/2023 14:58:52 - INFO - __main__ -   test: [batch: 245/391 ] | Loss: 0.908 | Acc: 77.147% (24292/31488)/ 93.483% (29436/31488)
01/15/2023 14:58:54 - INFO - __main__ -   test: [batch: 246/391 ] | Loss: 0.908 | Acc: 77.132% (24386/31616)/ 93.478% (29554/31616)
01/15/2023 14:58:56 - INFO - __main__ -   test: [batch: 247/391 ] | Loss: 0.913 | Acc: 77.051% (24459/31744)/ 93.407% (29651/31744)
01/15/2023 14:58:59 - INFO - __main__ -   test: [batch: 248/391 ] | Loss: 0.914 | Acc: 77.017% (24547/31872)/ 93.402% (29769/31872)
01/15/2023 14:59:01 - INFO - __main__ -   test: [batch: 249/391 ] | Loss: 0.917 | Acc: 76.869% (24598/32000)/ 93.400% (29888/32000)
01/15/2023 14:59:03 - INFO - __main__ -   test: [batch: 250/391 ] | Loss: 0.915 | Acc: 76.908% (24709/32128)/ 93.414% (30012/32128)
01/15/2023 14:59:05 - INFO - __main__ -   test: [batch: 251/391 ] | Loss: 0.918 | Acc: 76.869% (24795/32256)/ 93.381% (30121/32256)
01/15/2023 14:59:07 - INFO - __main__ -   test: [batch: 252/391 ] | Loss: 0.917 | Acc: 76.899% (24903/32384)/ 93.383% (30241/32384)
01/15/2023 14:59:09 - INFO - __main__ -   test: [batch: 253/391 ] | Loss: 0.918 | Acc: 76.870% (24992/32512)/ 93.369% (30356/32512)
01/15/2023 14:59:11 - INFO - __main__ -   test: [batch: 254/391 ] | Loss: 0.922 | Acc: 76.792% (25065/32640)/ 93.327% (30462/32640)
01/15/2023 14:59:13 - INFO - __main__ -   test: [batch: 255/391 ] | Loss: 0.924 | Acc: 76.767% (25155/32768)/ 93.320% (30579/32768)
01/15/2023 14:59:15 - INFO - __main__ -   test: [batch: 256/391 ] | Loss: 0.929 | Acc: 76.632% (25209/32896)/ 93.270% (30682/32896)
01/15/2023 14:59:18 - INFO - __main__ -   test: [batch: 257/391 ] | Loss: 0.930 | Acc: 76.623% (25304/33024)/ 93.259% (30798/33024)
01/15/2023 14:59:20 - INFO - __main__ -   test: [batch: 258/391 ] | Loss: 0.931 | Acc: 76.617% (25400/33152)/ 93.249% (30914/33152)
01/15/2023 14:59:22 - INFO - __main__ -   test: [batch: 259/391 ] | Loss: 0.934 | Acc: 76.496% (25458/33280)/ 93.242% (31031/33280)
01/15/2023 14:59:24 - INFO - __main__ -   test: [batch: 260/391 ] | Loss: 0.935 | Acc: 76.476% (25549/33408)/ 93.241% (31150/33408)
01/15/2023 14:59:26 - INFO - __main__ -   test: [batch: 261/391 ] | Loss: 0.933 | Acc: 76.515% (25660/33536)/ 93.264% (31277/33536)
01/15/2023 14:59:28 - INFO - __main__ -   test: [batch: 262/391 ] | Loss: 0.932 | Acc: 76.533% (25764/33664)/ 93.275% (31400/33664)
01/15/2023 14:59:30 - INFO - __main__ -   test: [batch: 263/391 ] | Loss: 0.934 | Acc: 76.462% (25838/33792)/ 93.250% (31511/33792)
01/15/2023 14:59:32 - INFO - __main__ -   test: [batch: 264/391 ] | Loss: 0.940 | Acc: 76.394% (25913/33920)/ 93.181% (31607/33920)
01/15/2023 14:59:34 - INFO - __main__ -   test: [batch: 265/391 ] | Loss: 0.939 | Acc: 76.424% (26021/34048)/ 93.177% (31725/34048)
01/15/2023 14:59:37 - INFO - __main__ -   test: [batch: 266/391 ] | Loss: 0.941 | Acc: 76.349% (26093/34176)/ 93.162% (31839/34176)
01/15/2023 14:59:39 - INFO - __main__ -   test: [batch: 267/391 ] | Loss: 0.941 | Acc: 76.382% (26202/34304)/ 93.161% (31958/34304)
01/15/2023 14:59:41 - INFO - __main__ -   test: [batch: 268/391 ] | Loss: 0.940 | Acc: 76.400% (26306/34432)/ 93.158% (32076/34432)
01/15/2023 14:59:43 - INFO - __main__ -   test: [batch: 269/391 ] | Loss: 0.942 | Acc: 76.345% (26385/34560)/ 93.134% (32187/34560)
01/15/2023 14:59:45 - INFO - __main__ -   test: [batch: 270/391 ] | Loss: 0.945 | Acc: 76.297% (26466/34688)/ 93.107% (32297/34688)
01/15/2023 14:59:47 - INFO - __main__ -   test: [batch: 271/391 ] | Loss: 0.945 | Acc: 76.310% (26568/34816)/ 93.101% (32414/34816)
01/15/2023 14:59:50 - INFO - __main__ -   test: [batch: 272/391 ] | Loss: 0.946 | Acc: 76.279% (26655/34944)/ 93.095% (32531/34944)
01/15/2023 14:59:52 - INFO - __main__ -   test: [batch: 273/391 ] | Loss: 0.946 | Acc: 76.292% (26757/35072)/ 93.066% (32640/35072)
01/15/2023 14:59:54 - INFO - __main__ -   test: [batch: 274/391 ] | Loss: 0.946 | Acc: 76.298% (26857/35200)/ 93.071% (32761/35200)
01/15/2023 14:59:56 - INFO - __main__ -   test: [batch: 275/391 ] | Loss: 0.948 | Acc: 76.265% (26943/35328)/ 93.062% (32877/35328)
01/15/2023 14:59:58 - INFO - __main__ -   test: [batch: 276/391 ] | Loss: 0.949 | Acc: 76.216% (27023/35456)/ 93.056% (32994/35456)
01/15/2023 15:00:00 - INFO - __main__ -   test: [batch: 277/391 ] | Loss: 0.950 | Acc: 76.186% (27110/35584)/ 93.047% (33110/35584)
01/15/2023 15:00:02 - INFO - __main__ -   test: [batch: 278/391 ] | Loss: 0.949 | Acc: 76.201% (27213/35712)/ 93.042% (33227/35712)
01/15/2023 15:00:05 - INFO - __main__ -   test: [batch: 279/391 ] | Loss: 0.949 | Acc: 76.222% (27318/35840)/ 93.036% (33344/35840)
01/15/2023 15:00:07 - INFO - __main__ -   test: [batch: 280/391 ] | Loss: 0.950 | Acc: 76.190% (27404/35968)/ 93.030% (33461/35968)
01/15/2023 15:00:09 - INFO - __main__ -   test: [batch: 281/391 ] | Loss: 0.950 | Acc: 76.191% (27502/36096)/ 93.019% (33576/36096)
01/15/2023 15:00:11 - INFO - __main__ -   test: [batch: 282/391 ] | Loss: 0.949 | Acc: 76.228% (27613/36224)/ 93.021% (33696/36224)
01/15/2023 15:00:13 - INFO - __main__ -   test: [batch: 283/391 ] | Loss: 0.950 | Acc: 76.205% (27702/36352)/ 93.016% (33813/36352)
01/15/2023 15:00:15 - INFO - __main__ -   test: [batch: 284/391 ] | Loss: 0.954 | Acc: 76.165% (27785/36480)/ 92.963% (33913/36480)
01/15/2023 15:00:17 - INFO - __main__ -   test: [batch: 285/391 ] | Loss: 0.956 | Acc: 76.123% (27867/36608)/ 92.914% (34014/36608)
01/15/2023 15:00:20 - INFO - __main__ -   test: [batch: 286/391 ] | Loss: 0.958 | Acc: 76.092% (27953/36736)/ 92.898% (34127/36736)
01/15/2023 15:00:21 - INFO - __main__ -   test: [batch: 287/391 ] | Loss: 0.958 | Acc: 76.085% (28048/36864)/ 92.890% (34243/36864)
01/15/2023 15:00:24 - INFO - __main__ -   test: [batch: 288/391 ] | Loss: 0.957 | Acc: 76.095% (28149/36992)/ 92.898% (34365/36992)
01/15/2023 15:00:26 - INFO - __main__ -   test: [batch: 289/391 ] | Loss: 0.959 | Acc: 76.059% (28233/37120)/ 92.864% (34471/37120)
01/15/2023 15:00:28 - INFO - __main__ -   test: [batch: 290/391 ] | Loss: 0.960 | Acc: 75.975% (28299/37248)/ 92.864% (34590/37248)
01/15/2023 15:00:30 - INFO - __main__ -   test: [batch: 291/391 ] | Loss: 0.961 | Acc: 75.977% (28397/37376)/ 92.864% (34709/37376)
01/15/2023 15:00:32 - INFO - __main__ -   test: [batch: 292/391 ] | Loss: 0.963 | Acc: 75.917% (28472/37504)/ 92.846% (34821/37504)
01/15/2023 15:00:34 - INFO - __main__ -   test: [batch: 293/391 ] | Loss: 0.963 | Acc: 75.903% (28564/37632)/ 92.844% (34939/37632)
01/15/2023 15:00:36 - INFO - __main__ -   test: [batch: 294/391 ] | Loss: 0.964 | Acc: 75.879% (28652/37760)/ 92.828% (35052/37760)
01/15/2023 15:00:38 - INFO - __main__ -   test: [batch: 295/391 ] | Loss: 0.963 | Acc: 75.903% (28758/37888)/ 92.832% (35172/37888)
01/15/2023 15:00:41 - INFO - __main__ -   test: [batch: 296/391 ] | Loss: 0.964 | Acc: 75.894% (28852/38016)/ 92.824% (35288/38016)
01/15/2023 15:00:43 - INFO - __main__ -   test: [batch: 297/391 ] | Loss: 0.966 | Acc: 75.876% (28942/38144)/ 92.796% (35396/38144)
01/15/2023 15:00:45 - INFO - __main__ -   test: [batch: 298/391 ] | Loss: 0.968 | Acc: 75.841% (29026/38272)/ 92.770% (35505/38272)
01/15/2023 15:00:47 - INFO - __main__ -   test: [batch: 299/391 ] | Loss: 0.969 | Acc: 75.823% (29116/38400)/ 92.742% (35613/38400)
01/15/2023 15:00:49 - INFO - __main__ -   test: [batch: 300/391 ] | Loss: 0.969 | Acc: 75.833% (29217/38528)/ 92.740% (35731/38528)
01/15/2023 15:00:51 - INFO - __main__ -   test: [batch: 301/391 ] | Loss: 0.970 | Acc: 75.825% (29311/38656)/ 92.728% (35845/38656)
01/15/2023 15:00:53 - INFO - __main__ -   test: [batch: 302/391 ] | Loss: 0.972 | Acc: 75.776% (29389/38784)/ 92.703% (35954/38784)
01/15/2023 15:00:55 - INFO - __main__ -   test: [batch: 303/391 ] | Loss: 0.973 | Acc: 75.748% (29475/38912)/ 92.694% (36069/38912)
01/15/2023 15:00:58 - INFO - __main__ -   test: [batch: 304/391 ] | Loss: 0.972 | Acc: 75.758% (29576/39040)/ 92.697% (36189/39040)
01/15/2023 15:01:00 - INFO - __main__ -   test: [batch: 305/391 ] | Loss: 0.973 | Acc: 75.717% (29657/39168)/ 92.688% (36304/39168)
01/15/2023 15:01:02 - INFO - __main__ -   test: [batch: 306/391 ] | Loss: 0.974 | Acc: 75.710% (29751/39296)/ 92.663% (36413/39296)
01/15/2023 15:01:04 - INFO - __main__ -   test: [batch: 307/391 ] | Loss: 0.975 | Acc: 75.700% (29844/39424)/ 92.649% (36526/39424)
01/15/2023 15:01:06 - INFO - __main__ -   test: [batch: 308/391 ] | Loss: 0.976 | Acc: 75.690% (29937/39552)/ 92.638% (36640/39552)
01/15/2023 15:01:08 - INFO - __main__ -   test: [batch: 309/391 ] | Loss: 0.977 | Acc: 75.680% (30030/39680)/ 92.616% (36750/39680)
01/15/2023 15:01:11 - INFO - __main__ -   test: [batch: 310/391 ] | Loss: 0.978 | Acc: 75.668% (30122/39808)/ 92.605% (36864/39808)
01/15/2023 15:01:13 - INFO - __main__ -   test: [batch: 311/391 ] | Loss: 0.979 | Acc: 75.651% (30212/39936)/ 92.588% (36976/39936)
01/15/2023 15:01:15 - INFO - __main__ -   test: [batch: 312/391 ] | Loss: 0.980 | Acc: 75.639% (30304/40064)/ 92.569% (37087/40064)
01/15/2023 15:01:17 - INFO - __main__ -   test: [batch: 313/391 ] | Loss: 0.978 | Acc: 75.684% (30419/40192)/ 92.593% (37215/40192)
01/15/2023 15:01:19 - INFO - __main__ -   test: [batch: 314/391 ] | Loss: 0.979 | Acc: 75.670% (30510/40320)/ 92.587% (37331/40320)
01/15/2023 15:01:21 - INFO - __main__ -   test: [batch: 315/391 ] | Loss: 0.980 | Acc: 75.655% (30601/40448)/ 92.571% (37443/40448)
01/15/2023 15:01:23 - INFO - __main__ -   test: [batch: 316/391 ] | Loss: 0.982 | Acc: 75.567% (30662/40576)/ 92.542% (37550/40576)
01/15/2023 15:01:26 - INFO - __main__ -   test: [batch: 317/391 ] | Loss: 0.984 | Acc: 75.526% (30742/40704)/ 92.512% (37656/40704)
01/15/2023 15:01:28 - INFO - __main__ -   test: [batch: 318/391 ] | Loss: 0.983 | Acc: 75.553% (30850/40832)/ 92.530% (37782/40832)
01/15/2023 15:01:30 - INFO - __main__ -   test: [batch: 319/391 ] | Loss: 0.985 | Acc: 75.500% (30925/40960)/ 92.498% (37887/40960)
01/15/2023 15:01:32 - INFO - __main__ -   test: [batch: 320/391 ] | Loss: 0.984 | Acc: 75.538% (31037/41088)/ 92.506% (38009/41088)
01/15/2023 15:01:34 - INFO - __main__ -   test: [batch: 321/391 ] | Loss: 0.984 | Acc: 75.553% (31140/41216)/ 92.503% (38126/41216)
01/15/2023 15:01:36 - INFO - __main__ -   test: [batch: 322/391 ] | Loss: 0.985 | Acc: 75.513% (31220/41344)/ 92.490% (38239/41344)
01/15/2023 15:01:38 - INFO - __main__ -   test: [batch: 323/391 ] | Loss: 0.988 | Acc: 75.482% (31304/41472)/ 92.467% (38348/41472)
01/15/2023 15:01:40 - INFO - __main__ -   test: [batch: 324/391 ] | Loss: 0.988 | Acc: 75.478% (31399/41600)/ 92.464% (38465/41600)
01/15/2023 15:01:43 - INFO - __main__ -   test: [batch: 325/391 ] | Loss: 0.987 | Acc: 75.484% (31498/41728)/ 92.470% (38586/41728)
01/15/2023 15:01:45 - INFO - __main__ -   test: [batch: 326/391 ] | Loss: 0.991 | Acc: 75.404% (31561/41856)/ 92.424% (38685/41856)
01/15/2023 15:01:47 - INFO - __main__ -   test: [batch: 327/391 ] | Loss: 0.994 | Acc: 75.314% (31620/41984)/ 92.385% (38787/41984)
01/15/2023 15:01:49 - INFO - __main__ -   test: [batch: 328/391 ] | Loss: 0.996 | Acc: 75.266% (31696/42112)/ 92.354% (38892/42112)
01/15/2023 15:01:51 - INFO - __main__ -   test: [batch: 329/391 ] | Loss: 0.996 | Acc: 75.268% (31793/42240)/ 92.358% (39012/42240)
01/15/2023 15:01:53 - INFO - __main__ -   test: [batch: 330/391 ] | Loss: 0.998 | Acc: 75.229% (31873/42368)/ 92.339% (39122/42368)
01/15/2023 15:01:55 - INFO - __main__ -   test: [batch: 331/391 ] | Loss: 0.998 | Acc: 75.186% (31951/42496)/ 92.355% (39247/42496)
01/15/2023 15:01:57 - INFO - __main__ -   test: [batch: 332/391 ] | Loss: 0.999 | Acc: 75.171% (32041/42624)/ 92.356% (39366/42624)
01/15/2023 15:01:59 - INFO - __main__ -   test: [batch: 333/391 ] | Loss: 0.998 | Acc: 75.203% (32151/42752)/ 92.363% (39487/42752)
01/15/2023 15:02:01 - INFO - __main__ -   test: [batch: 334/391 ] | Loss: 0.999 | Acc: 75.173% (32234/42880)/ 92.348% (39599/42880)
01/15/2023 15:02:03 - INFO - __main__ -   test: [batch: 335/391 ] | Loss: 1.000 | Acc: 75.146% (32319/43008)/ 92.327% (39708/43008)
01/15/2023 15:02:05 - INFO - __main__ -   test: [batch: 336/391 ] | Loss: 1.002 | Acc: 75.109% (32399/43136)/ 92.310% (39819/43136)
01/15/2023 15:02:07 - INFO - __main__ -   test: [batch: 337/391 ] | Loss: 1.002 | Acc: 75.102% (32492/43264)/ 92.301% (39933/43264)
01/15/2023 15:02:09 - INFO - __main__ -   test: [batch: 338/391 ] | Loss: 1.002 | Acc: 75.094% (32585/43392)/ 92.310% (40055/43392)
01/15/2023 15:02:11 - INFO - __main__ -   test: [batch: 339/391 ] | Loss: 1.005 | Acc: 75.060% (32666/43520)/ 92.279% (40160/43520)
01/15/2023 15:02:14 - INFO - __main__ -   test: [batch: 340/391 ] | Loss: 1.004 | Acc: 75.062% (32763/43648)/ 92.291% (40283/43648)
01/15/2023 15:02:16 - INFO - __main__ -   test: [batch: 341/391 ] | Loss: 1.003 | Acc: 75.103% (32877/43776)/ 92.309% (40409/43776)
01/15/2023 15:02:18 - INFO - __main__ -   test: [batch: 342/391 ] | Loss: 1.003 | Acc: 75.048% (32949/43904)/ 92.297% (40522/43904)
01/15/2023 15:02:20 - INFO - __main__ -   test: [batch: 343/391 ] | Loss: 1.003 | Acc: 75.050% (33046/44032)/ 92.303% (40643/44032)
01/15/2023 15:02:22 - INFO - __main__ -   test: [batch: 344/391 ] | Loss: 1.004 | Acc: 75.054% (33144/44160)/ 92.292% (40756/44160)
01/15/2023 15:02:24 - INFO - __main__ -   test: [batch: 345/391 ] | Loss: 1.007 | Acc: 74.982% (33208/44288)/ 92.248% (40855/44288)
01/15/2023 15:02:26 - INFO - __main__ -   test: [batch: 346/391 ] | Loss: 1.009 | Acc: 74.964% (33296/44416)/ 92.239% (40969/44416)
01/15/2023 15:02:29 - INFO - __main__ -   test: [batch: 347/391 ] | Loss: 1.008 | Acc: 74.978% (33398/44544)/ 92.248% (41091/44544)
01/15/2023 15:02:31 - INFO - __main__ -   test: [batch: 348/391 ] | Loss: 1.010 | Acc: 74.960% (33486/44672)/ 92.232% (41202/44672)
01/15/2023 15:02:33 - INFO - __main__ -   test: [batch: 349/391 ] | Loss: 1.009 | Acc: 74.964% (33584/44800)/ 92.241% (41324/44800)
01/15/2023 15:02:35 - INFO - __main__ -   test: [batch: 350/391 ] | Loss: 1.009 | Acc: 74.969% (33682/44928)/ 92.234% (41439/44928)
01/15/2023 15:02:37 - INFO - __main__ -   test: [batch: 351/391 ] | Loss: 1.011 | Acc: 74.911% (33752/45056)/ 92.221% (41551/45056)
01/15/2023 15:02:39 - INFO - __main__ -   test: [batch: 352/391 ] | Loss: 1.012 | Acc: 74.909% (33847/45184)/ 92.216% (41667/45184)
01/15/2023 15:02:41 - INFO - __main__ -   test: [batch: 353/391 ] | Loss: 1.014 | Acc: 74.854% (33918/45312)/ 92.174% (41766/45312)
01/15/2023 15:02:43 - INFO - __main__ -   test: [batch: 354/391 ] | Loss: 1.017 | Acc: 74.798% (33988/45440)/ 92.163% (41879/45440)
01/15/2023 15:02:46 - INFO - __main__ -   test: [batch: 355/391 ] | Loss: 1.019 | Acc: 74.734% (34055/45568)/ 92.152% (41992/45568)
01/15/2023 15:02:48 - INFO - __main__ -   test: [batch: 356/391 ] | Loss: 1.020 | Acc: 74.735% (34151/45696)/ 92.159% (42113/45696)
01/15/2023 15:02:50 - INFO - __main__ -   test: [batch: 357/391 ] | Loss: 1.018 | Acc: 74.766% (34261/45824)/ 92.172% (42237/45824)
01/15/2023 15:02:52 - INFO - __main__ -   test: [batch: 358/391 ] | Loss: 1.017 | Acc: 74.789% (34367/45952)/ 92.174% (42356/45952)
01/15/2023 15:02:54 - INFO - __main__ -   test: [batch: 359/391 ] | Loss: 1.018 | Acc: 74.794% (34465/46080)/ 92.164% (42469/46080)
01/15/2023 15:02:56 - INFO - __main__ -   test: [batch: 360/391 ] | Loss: 1.019 | Acc: 74.755% (34543/46208)/ 92.164% (42587/46208)
01/15/2023 15:02:58 - INFO - __main__ -   test: [batch: 361/391 ] | Loss: 1.019 | Acc: 74.754% (34638/46336)/ 92.170% (42708/46336)
01/15/2023 15:03:00 - INFO - __main__ -   test: [batch: 362/391 ] | Loss: 1.019 | Acc: 74.757% (34735/46464)/ 92.181% (42831/46464)
01/15/2023 15:03:02 - INFO - __main__ -   test: [batch: 363/391 ] | Loss: 1.019 | Acc: 74.747% (34826/46592)/ 92.172% (42945/46592)
01/15/2023 15:03:05 - INFO - __main__ -   test: [batch: 364/391 ] | Loss: 1.018 | Acc: 74.769% (34932/46720)/ 92.181% (43067/46720)
01/15/2023 15:03:07 - INFO - __main__ -   test: [batch: 365/391 ] | Loss: 1.018 | Acc: 74.778% (35032/46848)/ 92.185% (43187/46848)
01/15/2023 15:03:09 - INFO - __main__ -   test: [batch: 366/391 ] | Loss: 1.017 | Acc: 74.813% (35144/46976)/ 92.202% (43313/46976)
01/15/2023 15:03:11 - INFO - __main__ -   test: [batch: 367/391 ] | Loss: 1.016 | Acc: 74.826% (35246/47104)/ 92.215% (43437/47104)
01/15/2023 15:03:13 - INFO - __main__ -   test: [batch: 368/391 ] | Loss: 1.016 | Acc: 74.824% (35341/47232)/ 92.221% (43558/47232)
01/15/2023 15:03:15 - INFO - __main__ -   test: [batch: 369/391 ] | Loss: 1.015 | Acc: 74.846% (35447/47360)/ 92.230% (43680/47360)
01/15/2023 15:03:17 - INFO - __main__ -   test: [batch: 370/391 ] | Loss: 1.015 | Acc: 74.823% (35532/47488)/ 92.234% (43800/47488)
01/15/2023 15:03:19 - INFO - __main__ -   test: [batch: 371/391 ] | Loss: 1.015 | Acc: 74.832% (35632/47616)/ 92.234% (43918/47616)
01/15/2023 15:03:21 - INFO - __main__ -   test: [batch: 372/391 ] | Loss: 1.013 | Acc: 74.876% (35749/47744)/ 92.252% (44045/47744)
01/15/2023 15:03:23 - INFO - __main__ -   test: [batch: 373/391 ] | Loss: 1.012 | Acc: 74.908% (35860/47872)/ 92.263% (44168/47872)
01/15/2023 15:03:26 - INFO - __main__ -   test: [batch: 374/391 ] | Loss: 1.011 | Acc: 74.933% (35968/48000)/ 92.269% (44289/48000)
01/15/2023 15:03:28 - INFO - __main__ -   test: [batch: 375/391 ] | Loss: 1.014 | Acc: 74.861% (36029/48128)/ 92.237% (44392/48128)
01/15/2023 15:03:30 - INFO - __main__ -   test: [batch: 376/391 ] | Loss: 1.015 | Acc: 74.845% (36117/48256)/ 92.221% (44502/48256)
01/15/2023 15:03:32 - INFO - __main__ -   test: [batch: 377/391 ] | Loss: 1.015 | Acc: 74.837% (36209/48384)/ 92.212% (44616/48384)
01/15/2023 15:03:34 - INFO - __main__ -   test: [batch: 378/391 ] | Loss: 1.019 | Acc: 74.763% (36269/48512)/ 92.169% (44713/48512)
01/15/2023 15:03:36 - INFO - __main__ -   test: [batch: 379/391 ] | Loss: 1.019 | Acc: 74.745% (36356/48640)/ 92.181% (44837/48640)
01/15/2023 15:03:38 - INFO - __main__ -   test: [batch: 380/391 ] | Loss: 1.018 | Acc: 74.754% (36456/48768)/ 92.196% (44962/48768)
01/15/2023 15:03:41 - INFO - __main__ -   test: [batch: 381/391 ] | Loss: 1.020 | Acc: 74.703% (36527/48896)/ 92.194% (45079/48896)
01/15/2023 15:03:43 - INFO - __main__ -   test: [batch: 382/391 ] | Loss: 1.021 | Acc: 74.668% (36605/49024)/ 92.181% (45191/49024)
01/15/2023 15:03:45 - INFO - __main__ -   test: [batch: 383/391 ] | Loss: 1.022 | Acc: 74.672% (36703/49152)/ 92.177% (45307/49152)
01/15/2023 15:03:47 - INFO - __main__ -   test: [batch: 384/391 ] | Loss: 1.020 | Acc: 74.710% (36817/49280)/ 92.190% (45431/49280)
01/15/2023 15:03:49 - INFO - __main__ -   test: [batch: 385/391 ] | Loss: 1.018 | Acc: 74.721% (36918/49408)/ 92.202% (45555/49408)
01/15/2023 15:03:51 - INFO - __main__ -   test: [batch: 386/391 ] | Loss: 1.017 | Acc: 74.760% (37033/49536)/ 92.218% (45681/49536)
01/15/2023 15:03:53 - INFO - __main__ -   test: [batch: 387/391 ] | Loss: 1.015 | Acc: 74.803% (37150/49664)/ 92.230% (45805/49664)
01/15/2023 15:03:56 - INFO - __main__ -   test: [batch: 388/391 ] | Loss: 1.013 | Acc: 74.851% (37270/49792)/ 92.244% (45930/49792)
01/15/2023 15:03:58 - INFO - __main__ -   test: [batch: 389/391 ] | Loss: 1.013 | Acc: 74.856% (37368/49920)/ 92.250% (46051/49920)
01/15/2023 15:04:00 - INFO - __main__ -   test: [batch: 390/391 ] | Loss: 1.015 | Acc: 74.810% (37405/50000)/ 92.240% (46120/50000)
01/15/2023 15:04:00 - INFO - __main__ -   Final accuracy: 74.810
01/15/2023 15:04:00 - INFO - __main__ -   {'milestones': Counter({2: 1, 3: 1, 4: 1}), 'gamma': 0.1, 'base_lrs': [0.00025], 'last_epoch': 2, '_step_count': 3, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [2.5e-05]}
01/15/2023 15:04:00 - INFO - __main__ -   
Epoch: 2
01/15/2023 15:04:02 - INFO - __main__ -   test: [epoch: 2 | batch: 0/10010 ] | Loss: 1.088 | Acc: 74.219% (95/128)
01/15/2023 15:07:34 - INFO - __main__ -   test: [epoch: 2 | batch: 100/10010 ] | Loss: 0.899 | Acc: 78.055% (10091/12928)
01/15/2023 15:11:06 - INFO - __main__ -   test: [epoch: 2 | batch: 200/10010 ] | Loss: 0.906 | Acc: 77.515% (19943/25728)
01/15/2023 15:14:37 - INFO - __main__ -   test: [epoch: 2 | batch: 300/10010 ] | Loss: 0.911 | Acc: 77.492% (29856/38528)
01/15/2023 15:18:08 - INFO - __main__ -   test: [epoch: 2 | batch: 400/10010 ] | Loss: 0.917 | Acc: 77.371% (39713/51328)
01/15/2023 15:21:43 - INFO - __main__ -   test: [epoch: 2 | batch: 500/10010 ] | Loss: 0.923 | Acc: 77.247% (49537/64128)
01/15/2023 15:25:15 - INFO - __main__ -   test: [epoch: 2 | batch: 600/10010 ] | Loss: 0.925 | Acc: 77.171% (59366/76928)
01/15/2023 15:28:47 - INFO - __main__ -   test: [epoch: 2 | batch: 700/10010 ] | Loss: 0.926 | Acc: 77.133% (69210/89728)
01/15/2023 15:32:19 - INFO - __main__ -   test: [epoch: 2 | batch: 800/10010 ] | Loss: 0.927 | Acc: 77.087% (79036/102528)
01/15/2023 15:35:49 - INFO - __main__ -   test: [epoch: 2 | batch: 900/10010 ] | Loss: 0.926 | Acc: 77.093% (88910/115328)
01/15/2023 15:39:21 - INFO - __main__ -   test: [epoch: 2 | batch: 1000/10010 ] | Loss: 0.927 | Acc: 77.063% (98739/128128)
01/15/2023 15:42:51 - INFO - __main__ -   test: [epoch: 2 | batch: 1100/10010 ] | Loss: 0.929 | Acc: 77.058% (108597/140928)
01/15/2023 15:46:23 - INFO - __main__ -   test: [epoch: 2 | batch: 1200/10010 ] | Loss: 0.930 | Acc: 77.021% (118403/153728)
01/15/2023 15:49:55 - INFO - __main__ -   test: [epoch: 2 | batch: 1300/10010 ] | Loss: 0.929 | Acc: 77.057% (128322/166528)
01/15/2023 15:53:26 - INFO - __main__ -   test: [epoch: 2 | batch: 1400/10010 ] | Loss: 0.929 | Acc: 77.067% (138203/179328)
01/15/2023 15:56:56 - INFO - __main__ -   test: [epoch: 2 | batch: 1500/10010 ] | Loss: 0.929 | Acc: 77.050% (148035/192128)
01/15/2023 16:00:28 - INFO - __main__ -   test: [epoch: 2 | batch: 1600/10010 ] | Loss: 0.930 | Acc: 77.011% (157818/204928)
01/15/2023 16:03:59 - INFO - __main__ -   test: [epoch: 2 | batch: 1700/10010 ] | Loss: 0.930 | Acc: 77.025% (167706/217728)
01/15/2023 16:07:31 - INFO - __main__ -   test: [epoch: 2 | batch: 1800/10010 ] | Loss: 0.929 | Acc: 77.029% (177573/230528)
01/15/2023 16:11:03 - INFO - __main__ -   test: [epoch: 2 | batch: 1900/10010 ] | Loss: 0.928 | Acc: 77.047% (187476/243328)
01/15/2023 16:14:34 - INFO - __main__ -   test: [epoch: 2 | batch: 2000/10010 ] | Loss: 0.927 | Acc: 77.069% (197395/256128)
01/15/2023 16:18:05 - INFO - __main__ -   test: [epoch: 2 | batch: 2100/10010 ] | Loss: 0.927 | Acc: 77.068% (207257/268928)
01/15/2023 16:21:38 - INFO - __main__ -   test: [epoch: 2 | batch: 2200/10010 ] | Loss: 0.928 | Acc: 77.051% (217075/281728)
01/15/2023 16:25:11 - INFO - __main__ -   test: [epoch: 2 | batch: 2300/10010 ] | Loss: 0.929 | Acc: 77.023% (226855/294528)
01/15/2023 16:28:42 - INFO - __main__ -   test: [epoch: 2 | batch: 2400/10010 ] | Loss: 0.929 | Acc: 77.027% (236727/307328)
01/15/2023 16:32:15 - INFO - __main__ -   test: [epoch: 2 | batch: 2500/10010 ] | Loss: 0.929 | Acc: 77.028% (246587/320128)
01/15/2023 16:35:46 - INFO - __main__ -   test: [epoch: 2 | batch: 2600/10010 ] | Loss: 0.930 | Acc: 77.031% (256458/332928)
01/15/2023 16:39:17 - INFO - __main__ -   test: [epoch: 2 | batch: 2700/10010 ] | Loss: 0.929 | Acc: 77.060% (266418/345728)
01/15/2023 16:42:48 - INFO - __main__ -   test: [epoch: 2 | batch: 2800/10010 ] | Loss: 0.929 | Acc: 77.063% (276292/358528)
01/15/2023 16:46:19 - INFO - __main__ -   test: [epoch: 2 | batch: 2900/10010 ] | Loss: 0.928 | Acc: 77.056% (286130/371328)
01/15/2023 16:49:49 - INFO - __main__ -   test: [epoch: 2 | batch: 3000/10010 ] | Loss: 0.928 | Acc: 77.053% (295984/384128)
01/15/2023 16:53:20 - INFO - __main__ -   test: [epoch: 2 | batch: 3100/10010 ] | Loss: 0.928 | Acc: 77.070% (305911/396928)
01/15/2023 16:56:50 - INFO - __main__ -   test: [epoch: 2 | batch: 3200/10010 ] | Loss: 0.928 | Acc: 77.065% (315757/409728)
01/15/2023 17:00:21 - INFO - __main__ -   test: [epoch: 2 | batch: 3300/10010 ] | Loss: 0.928 | Acc: 77.050% (325559/422528)
01/15/2023 17:03:53 - INFO - __main__ -   test: [epoch: 2 | batch: 3400/10010 ] | Loss: 0.928 | Acc: 77.053% (335432/435328)
01/15/2023 17:07:24 - INFO - __main__ -   test: [epoch: 2 | batch: 3500/10010 ] | Loss: 0.928 | Acc: 77.058% (345317/448128)
01/15/2023 17:10:55 - INFO - __main__ -   test: [epoch: 2 | batch: 3600/10010 ] | Loss: 0.928 | Acc: 77.065% (355212/460928)
01/15/2023 17:14:27 - INFO - __main__ -   test: [epoch: 2 | batch: 3700/10010 ] | Loss: 0.928 | Acc: 77.054% (365026/473728)
01/15/2023 17:17:58 - INFO - __main__ -   test: [epoch: 2 | batch: 3800/10010 ] | Loss: 0.927 | Acc: 77.085% (375039/486528)
01/15/2023 17:21:28 - INFO - __main__ -   test: [epoch: 2 | batch: 3900/10010 ] | Loss: 0.927 | Acc: 77.074% (384850/499328)
01/15/2023 17:25:00 - INFO - __main__ -   test: [epoch: 2 | batch: 4000/10010 ] | Loss: 0.928 | Acc: 77.057% (394632/512128)
01/15/2023 17:28:32 - INFO - __main__ -   test: [epoch: 2 | batch: 4100/10010 ] | Loss: 0.928 | Acc: 77.058% (404497/524928)
01/15/2023 17:32:02 - INFO - __main__ -   test: [epoch: 2 | batch: 4200/10010 ] | Loss: 0.927 | Acc: 77.057% (414357/537728)
01/15/2023 17:35:33 - INFO - __main__ -   test: [epoch: 2 | batch: 4300/10010 ] | Loss: 0.927 | Acc: 77.058% (424227/550528)
01/15/2023 17:39:05 - INFO - __main__ -   test: [epoch: 2 | batch: 4400/10010 ] | Loss: 0.927 | Acc: 77.071% (434165/563328)
01/15/2023 17:42:37 - INFO - __main__ -   test: [epoch: 2 | batch: 4500/10010 ] | Loss: 0.927 | Acc: 77.059% (443958/576128)
01/15/2023 17:46:08 - INFO - __main__ -   test: [epoch: 2 | batch: 4600/10010 ] | Loss: 0.928 | Acc: 77.053% (453789/588928)
01/15/2023 17:49:39 - INFO - __main__ -   test: [epoch: 2 | batch: 4700/10010 ] | Loss: 0.928 | Acc: 77.054% (463655/601728)
01/15/2023 17:53:11 - INFO - __main__ -   test: [epoch: 2 | batch: 4800/10010 ] | Loss: 0.927 | Acc: 77.059% (473551/614528)
01/15/2023 17:56:43 - INFO - __main__ -   test: [epoch: 2 | batch: 4900/10010 ] | Loss: 0.928 | Acc: 77.056% (483395/627328)
01/15/2023 18:00:16 - INFO - __main__ -   test: [epoch: 2 | batch: 5000/10010 ] | Loss: 0.927 | Acc: 77.066% (493323/640128)
01/15/2023 18:03:48 - INFO - __main__ -   test: [epoch: 2 | batch: 5100/10010 ] | Loss: 0.928 | Acc: 77.053% (503102/652928)
01/15/2023 18:07:19 - INFO - __main__ -   test: [epoch: 2 | batch: 5200/10010 ] | Loss: 0.928 | Acc: 77.046% (512920/665728)
01/15/2023 18:10:51 - INFO - __main__ -   test: [epoch: 2 | batch: 5300/10010 ] | Loss: 0.928 | Acc: 77.040% (522736/678528)
01/15/2023 18:14:23 - INFO - __main__ -   test: [epoch: 2 | batch: 5400/10010 ] | Loss: 0.928 | Acc: 77.031% (532537/691328)
01/15/2023 18:17:53 - INFO - __main__ -   test: [epoch: 2 | batch: 5500/10010 ] | Loss: 0.929 | Acc: 77.032% (542402/704128)
01/15/2023 18:21:24 - INFO - __main__ -   test: [epoch: 2 | batch: 5600/10010 ] | Loss: 0.928 | Acc: 77.037% (552301/716928)
01/15/2023 18:24:56 - INFO - __main__ -   test: [epoch: 2 | batch: 5700/10010 ] | Loss: 0.928 | Acc: 77.038% (562167/729728)
01/15/2023 18:28:28 - INFO - __main__ -   test: [epoch: 2 | batch: 5800/10010 ] | Loss: 0.928 | Acc: 77.041% (572048/742528)
01/15/2023 18:31:59 - INFO - __main__ -   test: [epoch: 2 | batch: 5900/10010 ] | Loss: 0.928 | Acc: 77.049% (581974/755328)
01/15/2023 18:35:31 - INFO - __main__ -   test: [epoch: 2 | batch: 6000/10010 ] | Loss: 0.928 | Acc: 77.047% (591818/768128)
01/15/2023 18:39:03 - INFO - __main__ -   test: [epoch: 2 | batch: 6100/10010 ] | Loss: 0.928 | Acc: 77.036% (601595/780928)
01/15/2023 18:42:36 - INFO - __main__ -   test: [epoch: 2 | batch: 6200/10010 ] | Loss: 0.929 | Acc: 77.035% (611451/793728)
01/15/2023 18:46:08 - INFO - __main__ -   test: [epoch: 2 | batch: 6300/10010 ] | Loss: 0.928 | Acc: 77.036% (621318/806528)
01/15/2023 18:49:38 - INFO - __main__ -   test: [epoch: 2 | batch: 6400/10010 ] | Loss: 0.929 | Acc: 77.030% (631129/819328)
01/15/2023 18:53:08 - INFO - __main__ -   test: [epoch: 2 | batch: 6500/10010 ] | Loss: 0.928 | Acc: 77.035% (641031/832128)
01/15/2023 18:56:40 - INFO - __main__ -   test: [epoch: 2 | batch: 6600/10010 ] | Loss: 0.929 | Acc: 77.029% (650842/844928)
01/15/2023 19:00:11 - INFO - __main__ -   test: [epoch: 2 | batch: 6700/10010 ] | Loss: 0.929 | Acc: 77.025% (660664/857728)
01/15/2023 19:03:42 - INFO - __main__ -   test: [epoch: 2 | batch: 6800/10010 ] | Loss: 0.929 | Acc: 77.022% (670502/870528)
01/15/2023 19:07:13 - INFO - __main__ -   test: [epoch: 2 | batch: 6900/10010 ] | Loss: 0.928 | Acc: 77.025% (680380/883328)
01/15/2023 19:10:45 - INFO - __main__ -   test: [epoch: 2 | batch: 7000/10010 ] | Loss: 0.929 | Acc: 77.016% (690158/896128)
01/15/2023 19:14:15 - INFO - __main__ -   test: [epoch: 2 | batch: 7100/10010 ] | Loss: 0.928 | Acc: 77.024% (700089/908928)
01/15/2023 19:17:44 - INFO - __main__ -   test: [epoch: 2 | batch: 7200/10010 ] | Loss: 0.928 | Acc: 77.029% (709998/921728)
01/15/2023 19:21:15 - INFO - __main__ -   test: [epoch: 2 | batch: 7300/10010 ] | Loss: 0.928 | Acc: 77.026% (719830/934528)
01/15/2023 19:24:46 - INFO - __main__ -   test: [epoch: 2 | batch: 7400/10010 ] | Loss: 0.928 | Acc: 77.028% (729711/947328)
01/15/2023 19:28:19 - INFO - __main__ -   test: [epoch: 2 | batch: 7500/10010 ] | Loss: 0.928 | Acc: 77.033% (739620/960128)
01/15/2023 19:31:49 - INFO - __main__ -   test: [epoch: 2 | batch: 7600/10010 ] | Loss: 0.928 | Acc: 77.040% (749539/972928)
01/15/2023 19:35:21 - INFO - __main__ -   test: [epoch: 2 | batch: 7700/10010 ] | Loss: 0.928 | Acc: 77.034% (759344/985728)
01/15/2023 19:38:54 - INFO - __main__ -   test: [epoch: 2 | batch: 7800/10010 ] | Loss: 0.928 | Acc: 77.023% (769099/998528)
01/15/2023 19:42:25 - INFO - __main__ -   test: [epoch: 2 | batch: 7900/10010 ] | Loss: 0.928 | Acc: 77.028% (779002/1011328)
01/15/2023 19:45:57 - INFO - __main__ -   test: [epoch: 2 | batch: 8000/10010 ] | Loss: 0.928 | Acc: 77.024% (788822/1024128)
01/15/2023 19:49:29 - INFO - __main__ -   test: [epoch: 2 | batch: 8100/10010 ] | Loss: 0.928 | Acc: 77.029% (798734/1036928)
01/15/2023 19:53:00 - INFO - __main__ -   test: [epoch: 2 | batch: 8200/10010 ] | Loss: 0.928 | Acc: 77.030% (808604/1049728)
01/15/2023 19:56:31 - INFO - __main__ -   test: [epoch: 2 | batch: 8300/10010 ] | Loss: 0.928 | Acc: 77.026% (818424/1062528)
01/15/2023 20:00:03 - INFO - __main__ -   test: [epoch: 2 | batch: 8400/10010 ] | Loss: 0.928 | Acc: 77.027% (828291/1075328)
01/15/2023 20:03:35 - INFO - __main__ -   test: [epoch: 2 | batch: 8500/10010 ] | Loss: 0.928 | Acc: 77.034% (838225/1088128)
01/15/2023 20:07:07 - INFO - __main__ -   test: [epoch: 2 | batch: 8600/10010 ] | Loss: 0.928 | Acc: 77.034% (848093/1100928)
01/15/2023 20:10:38 - INFO - __main__ -   test: [epoch: 2 | batch: 8700/10010 ] | Loss: 0.928 | Acc: 77.037% (857979/1113728)
01/15/2023 20:14:10 - INFO - __main__ -   test: [epoch: 2 | batch: 8800/10010 ] | Loss: 0.928 | Acc: 77.044% (867919/1126528)
01/15/2023 20:17:42 - INFO - __main__ -   test: [epoch: 2 | batch: 8900/10010 ] | Loss: 0.928 | Acc: 77.046% (877802/1139328)
01/15/2023 20:21:14 - INFO - __main__ -   test: [epoch: 2 | batch: 9000/10010 ] | Loss: 0.928 | Acc: 77.042% (887622/1152128)
01/15/2023 20:24:45 - INFO - __main__ -   test: [epoch: 2 | batch: 9100/10010 ] | Loss: 0.928 | Acc: 77.039% (897449/1164928)
01/15/2023 20:28:17 - INFO - __main__ -   test: [epoch: 2 | batch: 9200/10010 ] | Loss: 0.928 | Acc: 77.045% (907376/1177728)
01/15/2023 20:31:47 - INFO - __main__ -   test: [epoch: 2 | batch: 9300/10010 ] | Loss: 0.928 | Acc: 77.038% (917155/1190528)
01/15/2023 20:35:20 - INFO - __main__ -   test: [epoch: 2 | batch: 9400/10010 ] | Loss: 0.928 | Acc: 77.039% (927033/1203328)
01/15/2023 20:38:50 - INFO - __main__ -   test: [epoch: 2 | batch: 9500/10010 ] | Loss: 0.928 | Acc: 77.037% (936870/1216128)
01/15/2023 20:42:22 - INFO - __main__ -   test: [epoch: 2 | batch: 9600/10010 ] | Loss: 0.928 | Acc: 77.037% (946726/1228928)
01/15/2023 20:45:56 - INFO - __main__ -   test: [epoch: 2 | batch: 9700/10010 ] | Loss: 0.928 | Acc: 77.033% (956541/1241728)
01/15/2023 20:49:28 - INFO - __main__ -   test: [epoch: 2 | batch: 9800/10010 ] | Loss: 0.928 | Acc: 77.031% (966370/1254528)
01/15/2023 20:52:52 - INFO - __main__ -   test: [epoch: 2 | batch: 9900/10010 ] | Loss: 0.929 | Acc: 77.033% (976262/1267328)
01/15/2023 20:55:26 - INFO - __main__ -   test: [epoch: 2 | batch: 10000/10010 ] | Loss: 0.929 | Acc: 77.030% (986083/1280128)
01/15/2023 20:55:40 - INFO - __main__ -   Saving Checkpoint
01/15/2023 20:55:42 - INFO - __main__ -   test: [batch: 0/391 ] | Loss: 0.537 | Acc: 86.719% (111/128)/ 95.312% (122/128)
01/15/2023 20:55:43 - INFO - __main__ -   test: [batch: 1/391 ] | Loss: 0.536 | Acc: 85.547% (219/256)/ 96.484% (247/256)
01/15/2023 20:55:45 - INFO - __main__ -   test: [batch: 2/391 ] | Loss: 0.736 | Acc: 79.167% (304/384)/ 94.271% (362/384)
01/15/2023 20:55:47 - INFO - __main__ -   test: [batch: 3/391 ] | Loss: 0.678 | Acc: 81.641% (418/512)/ 95.117% (487/512)
01/15/2023 20:55:48 - INFO - __main__ -   test: [batch: 4/391 ] | Loss: 0.582 | Acc: 84.062% (538/640)/ 96.094% (615/640)
01/15/2023 20:55:50 - INFO - __main__ -   test: [batch: 5/391 ] | Loss: 0.515 | Acc: 85.547% (657/768)/ 96.745% (743/768)
01/15/2023 20:55:51 - INFO - __main__ -   test: [batch: 6/391 ] | Loss: 0.501 | Acc: 86.161% (772/896)/ 96.652% (866/896)
01/15/2023 20:55:53 - INFO - __main__ -   test: [batch: 7/391 ] | Loss: 0.476 | Acc: 87.305% (894/1024)/ 96.777% (991/1024)
01/15/2023 20:55:55 - INFO - __main__ -   test: [batch: 8/391 ] | Loss: 0.486 | Acc: 87.326% (1006/1152)/ 96.788% (1115/1152)
01/15/2023 20:55:56 - INFO - __main__ -   test: [batch: 9/391 ] | Loss: 0.462 | Acc: 88.047% (1127/1280)/ 96.953% (1241/1280)
01/15/2023 20:55:57 - INFO - __main__ -   test: [batch: 10/391 ] | Loss: 0.507 | Acc: 86.861% (1223/1408)/ 96.875% (1364/1408)
01/15/2023 20:55:59 - INFO - __main__ -   test: [batch: 11/391 ] | Loss: 0.509 | Acc: 87.109% (1338/1536)/ 96.745% (1486/1536)
01/15/2023 20:56:01 - INFO - __main__ -   test: [batch: 12/391 ] | Loss: 0.558 | Acc: 86.058% (1432/1664)/ 96.394% (1604/1664)
01/15/2023 20:56:02 - INFO - __main__ -   test: [batch: 13/391 ] | Loss: 0.614 | Acc: 84.542% (1515/1792)/ 95.536% (1712/1792)
01/15/2023 20:56:04 - INFO - __main__ -   test: [batch: 14/391 ] | Loss: 0.631 | Acc: 83.750% (1608/1920)/ 95.729% (1838/1920)
01/15/2023 20:56:05 - INFO - __main__ -   test: [batch: 15/391 ] | Loss: 0.637 | Acc: 83.154% (1703/2048)/ 95.850% (1963/2048)
01/15/2023 20:56:07 - INFO - __main__ -   test: [batch: 16/391 ] | Loss: 0.642 | Acc: 83.134% (1809/2176)/ 95.726% (2083/2176)
01/15/2023 20:56:08 - INFO - __main__ -   test: [batch: 17/391 ] | Loss: 0.667 | Acc: 82.639% (1904/2304)/ 95.356% (2197/2304)
01/15/2023 20:56:10 - INFO - __main__ -   test: [batch: 18/391 ] | Loss: 0.693 | Acc: 82.113% (1997/2432)/ 95.148% (2314/2432)
01/15/2023 20:56:12 - INFO - __main__ -   test: [batch: 19/391 ] | Loss: 0.700 | Acc: 81.758% (2093/2560)/ 95.078% (2434/2560)
01/15/2023 20:56:13 - INFO - __main__ -   test: [batch: 20/391 ] | Loss: 0.695 | Acc: 81.882% (2201/2688)/ 95.089% (2556/2688)
01/15/2023 20:56:14 - INFO - __main__ -   test: [batch: 21/391 ] | Loss: 0.732 | Acc: 80.930% (2279/2816)/ 94.886% (2672/2816)
01/15/2023 20:56:16 - INFO - __main__ -   test: [batch: 22/391 ] | Loss: 0.731 | Acc: 80.707% (2376/2944)/ 94.871% (2793/2944)
01/15/2023 20:56:17 - INFO - __main__ -   test: [batch: 23/391 ] | Loss: 0.773 | Acc: 79.785% (2451/3072)/ 94.596% (2906/3072)
01/15/2023 20:56:19 - INFO - __main__ -   test: [batch: 24/391 ] | Loss: 0.790 | Acc: 79.312% (2538/3200)/ 94.500% (3024/3200)
01/15/2023 20:56:20 - INFO - __main__ -   test: [batch: 25/391 ] | Loss: 0.816 | Acc: 78.636% (2617/3328)/ 94.291% (3138/3328)
01/15/2023 20:56:22 - INFO - __main__ -   test: [batch: 26/391 ] | Loss: 0.833 | Acc: 77.894% (2692/3456)/ 94.242% (3257/3456)
01/15/2023 20:56:23 - INFO - __main__ -   test: [batch: 27/391 ] | Loss: 0.814 | Acc: 78.432% (2811/3584)/ 94.308% (3380/3584)
01/15/2023 20:56:25 - INFO - __main__ -   test: [batch: 28/391 ] | Loss: 0.820 | Acc: 77.856% (2890/3712)/ 94.423% (3505/3712)
01/15/2023 20:56:27 - INFO - __main__ -   test: [batch: 29/391 ] | Loss: 0.813 | Acc: 78.021% (2996/3840)/ 94.557% (3631/3840)
01/15/2023 20:56:28 - INFO - __main__ -   test: [batch: 30/391 ] | Loss: 0.824 | Acc: 77.949% (3093/3968)/ 94.430% (3747/3968)
01/15/2023 20:56:29 - INFO - __main__ -   test: [batch: 31/391 ] | Loss: 0.818 | Acc: 78.198% (3203/4096)/ 94.507% (3871/4096)
01/15/2023 20:56:31 - INFO - __main__ -   test: [batch: 32/391 ] | Loss: 0.804 | Acc: 78.527% (3317/4224)/ 94.626% (3997/4224)
01/15/2023 20:56:33 - INFO - __main__ -   test: [batch: 33/391 ] | Loss: 0.797 | Acc: 78.745% (3427/4352)/ 94.669% (4120/4352)
01/15/2023 20:56:34 - INFO - __main__ -   test: [batch: 34/391 ] | Loss: 0.784 | Acc: 79.152% (3546/4480)/ 94.754% (4245/4480)
01/15/2023 20:56:36 - INFO - __main__ -   test: [batch: 35/391 ] | Loss: 0.771 | Acc: 79.536% (3665/4608)/ 94.792% (4368/4608)
01/15/2023 20:56:37 - INFO - __main__ -   test: [batch: 36/391 ] | Loss: 0.755 | Acc: 79.962% (3787/4736)/ 94.911% (4495/4736)
01/15/2023 20:56:39 - INFO - __main__ -   test: [batch: 37/391 ] | Loss: 0.746 | Acc: 80.243% (3903/4864)/ 94.984% (4620/4864)
01/15/2023 20:56:41 - INFO - __main__ -   test: [batch: 38/391 ] | Loss: 0.737 | Acc: 80.429% (4015/4992)/ 95.072% (4746/4992)
01/15/2023 20:56:42 - INFO - __main__ -   test: [batch: 39/391 ] | Loss: 0.732 | Acc: 80.566% (4125/5120)/ 95.098% (4869/5120)
01/15/2023 20:56:44 - INFO - __main__ -   test: [batch: 40/391 ] | Loss: 0.730 | Acc: 80.697% (4235/5248)/ 95.046% (4988/5248)
01/15/2023 20:56:45 - INFO - __main__ -   test: [batch: 41/391 ] | Loss: 0.732 | Acc: 80.859% (4347/5376)/ 94.978% (5106/5376)
01/15/2023 20:56:46 - INFO - __main__ -   test: [batch: 42/391 ] | Loss: 0.733 | Acc: 80.850% (4450/5504)/ 95.058% (5232/5504)
01/15/2023 20:56:48 - INFO - __main__ -   test: [batch: 43/391 ] | Loss: 0.732 | Acc: 80.895% (4556/5632)/ 95.028% (5352/5632)
01/15/2023 20:56:49 - INFO - __main__ -   test: [batch: 44/391 ] | Loss: 0.737 | Acc: 80.903% (4660/5760)/ 94.896% (5466/5760)
01/15/2023 20:56:51 - INFO - __main__ -   test: [batch: 45/391 ] | Loss: 0.734 | Acc: 81.063% (4773/5888)/ 94.939% (5590/5888)
01/15/2023 20:56:53 - INFO - __main__ -   test: [batch: 46/391 ] | Loss: 0.737 | Acc: 81.034% (4875/6016)/ 94.963% (5713/6016)
01/15/2023 20:56:54 - INFO - __main__ -   test: [batch: 47/391 ] | Loss: 0.741 | Acc: 80.892% (4970/6144)/ 95.003% (5837/6144)
01/15/2023 20:56:56 - INFO - __main__ -   test: [batch: 48/391 ] | Loss: 0.745 | Acc: 80.804% (5068/6272)/ 94.978% (5957/6272)
01/15/2023 20:56:57 - INFO - __main__ -   test: [batch: 49/391 ] | Loss: 0.750 | Acc: 80.781% (5170/6400)/ 94.891% (6073/6400)
01/15/2023 20:56:59 - INFO - __main__ -   test: [batch: 50/391 ] | Loss: 0.742 | Acc: 80.974% (5286/6528)/ 94.945% (6198/6528)
01/15/2023 20:57:00 - INFO - __main__ -   test: [batch: 51/391 ] | Loss: 0.733 | Acc: 81.190% (5404/6656)/ 95.027% (6325/6656)
01/15/2023 20:57:02 - INFO - __main__ -   test: [batch: 52/391 ] | Loss: 0.730 | Acc: 81.265% (5513/6784)/ 95.062% (6449/6784)
01/15/2023 20:57:03 - INFO - __main__ -   test: [batch: 53/391 ] | Loss: 0.721 | Acc: 81.496% (5633/6912)/ 95.139% (6576/6912)
01/15/2023 20:57:05 - INFO - __main__ -   test: [batch: 54/391 ] | Loss: 0.714 | Acc: 81.648% (5748/7040)/ 95.170% (6700/7040)
01/15/2023 20:57:06 - INFO - __main__ -   test: [batch: 55/391 ] | Loss: 0.707 | Acc: 81.794% (5863/7168)/ 95.215% (6825/7168)
01/15/2023 20:57:08 - INFO - __main__ -   test: [batch: 56/391 ] | Loss: 0.698 | Acc: 82.045% (5986/7296)/ 95.271% (6951/7296)
01/15/2023 20:57:09 - INFO - __main__ -   test: [batch: 57/391 ] | Loss: 0.691 | Acc: 82.260% (6107/7424)/ 95.312% (7076/7424)
01/15/2023 20:57:11 - INFO - __main__ -   test: [batch: 58/391 ] | Loss: 0.688 | Acc: 82.349% (6219/7552)/ 95.312% (7198/7552)
01/15/2023 20:57:12 - INFO - __main__ -   test: [batch: 59/391 ] | Loss: 0.689 | Acc: 82.292% (6320/7680)/ 95.299% (7319/7680)
01/15/2023 20:57:14 - INFO - __main__ -   test: [batch: 60/391 ] | Loss: 0.692 | Acc: 82.223% (6420/7808)/ 95.287% (7440/7808)
01/15/2023 20:57:15 - INFO - __main__ -   test: [batch: 61/391 ] | Loss: 0.692 | Acc: 82.220% (6525/7936)/ 95.312% (7564/7936)
01/15/2023 20:57:17 - INFO - __main__ -   test: [batch: 62/391 ] | Loss: 0.690 | Acc: 82.180% (6627/8064)/ 95.362% (7690/8064)
01/15/2023 20:57:18 - INFO - __main__ -   test: [batch: 63/391 ] | Loss: 0.698 | Acc: 82.056% (6722/8192)/ 95.300% (7807/8192)
01/15/2023 20:57:20 - INFO - __main__ -   test: [batch: 64/391 ] | Loss: 0.706 | Acc: 81.851% (6810/8320)/ 95.240% (7924/8320)
01/15/2023 20:57:21 - INFO - __main__ -   test: [batch: 65/391 ] | Loss: 0.719 | Acc: 81.368% (6874/8448)/ 95.182% (8041/8448)
01/15/2023 20:57:22 - INFO - __main__ -   test: [batch: 66/391 ] | Loss: 0.724 | Acc: 81.355% (6977/8576)/ 95.138% (8159/8576)
01/15/2023 20:57:24 - INFO - __main__ -   test: [batch: 67/391 ] | Loss: 0.727 | Acc: 81.284% (7075/8704)/ 95.129% (8280/8704)
01/15/2023 20:57:26 - INFO - __main__ -   test: [batch: 68/391 ] | Loss: 0.728 | Acc: 81.227% (7174/8832)/ 95.154% (8404/8832)
01/15/2023 20:57:27 - INFO - __main__ -   test: [batch: 69/391 ] | Loss: 0.723 | Acc: 81.317% (7286/8960)/ 95.201% (8530/8960)
01/15/2023 20:57:29 - INFO - __main__ -   test: [batch: 70/391 ] | Loss: 0.723 | Acc: 81.261% (7385/9088)/ 95.202% (8652/9088)
01/15/2023 20:57:30 - INFO - __main__ -   test: [batch: 71/391 ] | Loss: 0.721 | Acc: 81.315% (7494/9216)/ 95.193% (8773/9216)
01/15/2023 20:57:32 - INFO - __main__ -   test: [batch: 72/391 ] | Loss: 0.724 | Acc: 81.196% (7587/9344)/ 95.195% (8895/9344)
01/15/2023 20:57:33 - INFO - __main__ -   test: [batch: 73/391 ] | Loss: 0.727 | Acc: 81.113% (7683/9472)/ 95.175% (9015/9472)
01/15/2023 20:57:34 - INFO - __main__ -   test: [batch: 74/391 ] | Loss: 0.728 | Acc: 81.115% (7787/9600)/ 95.156% (9135/9600)
01/15/2023 20:57:36 - INFO - __main__ -   test: [batch: 75/391 ] | Loss: 0.734 | Acc: 80.931% (7873/9728)/ 95.117% (9253/9728)
01/15/2023 20:57:37 - INFO - __main__ -   test: [batch: 76/391 ] | Loss: 0.734 | Acc: 80.946% (7978/9856)/ 95.140% (9377/9856)
01/15/2023 20:57:39 - INFO - __main__ -   test: [batch: 77/391 ] | Loss: 0.734 | Acc: 80.879% (8075/9984)/ 95.162% (9501/9984)
01/15/2023 20:57:40 - INFO - __main__ -   test: [batch: 78/391 ] | Loss: 0.733 | Acc: 80.785% (8169/10112)/ 95.204% (9627/10112)
01/15/2023 20:57:42 - INFO - __main__ -   test: [batch: 79/391 ] | Loss: 0.732 | Acc: 80.771% (8271/10240)/ 95.215% (9750/10240)
01/15/2023 20:57:44 - INFO - __main__ -   test: [batch: 80/391 ] | Loss: 0.732 | Acc: 80.729% (8370/10368)/ 95.197% (9870/10368)
01/15/2023 20:57:45 - INFO - __main__ -   test: [batch: 81/391 ] | Loss: 0.731 | Acc: 80.755% (8476/10496)/ 95.227% (9995/10496)
01/15/2023 20:57:47 - INFO - __main__ -   test: [batch: 82/391 ] | Loss: 0.731 | Acc: 80.770% (8581/10624)/ 95.228% (10117/10624)
01/15/2023 20:57:48 - INFO - __main__ -   test: [batch: 83/391 ] | Loss: 0.731 | Acc: 80.822% (8690/10752)/ 95.192% (10235/10752)
01/15/2023 20:57:49 - INFO - __main__ -   test: [batch: 84/391 ] | Loss: 0.727 | Acc: 80.947% (8807/10880)/ 95.230% (10361/10880)
01/15/2023 20:57:51 - INFO - __main__ -   test: [batch: 85/391 ] | Loss: 0.724 | Acc: 80.968% (8913/11008)/ 95.276% (10488/11008)
01/15/2023 20:57:53 - INFO - __main__ -   test: [batch: 86/391 ] | Loss: 0.728 | Acc: 80.891% (9008/11136)/ 95.250% (10607/11136)
01/15/2023 20:57:54 - INFO - __main__ -   test: [batch: 87/391 ] | Loss: 0.726 | Acc: 80.966% (9120/11264)/ 95.250% (10729/11264)
01/15/2023 20:57:56 - INFO - __main__ -   test: [batch: 88/391 ] | Loss: 0.734 | Acc: 80.881% (9214/11392)/ 95.181% (10843/11392)
01/15/2023 20:57:57 - INFO - __main__ -   test: [batch: 89/391 ] | Loss: 0.732 | Acc: 80.929% (9323/11520)/ 95.200% (10967/11520)
01/15/2023 20:57:59 - INFO - __main__ -   test: [batch: 90/391 ] | Loss: 0.733 | Acc: 80.821% (9414/11648)/ 95.201% (11089/11648)
01/15/2023 20:58:00 - INFO - __main__ -   test: [batch: 91/391 ] | Loss: 0.732 | Acc: 80.876% (9524/11776)/ 95.211% (11212/11776)
01/15/2023 20:58:02 - INFO - __main__ -   test: [batch: 92/391 ] | Loss: 0.733 | Acc: 80.847% (9624/11904)/ 95.178% (11330/11904)
01/15/2023 20:58:04 - INFO - __main__ -   test: [batch: 93/391 ] | Loss: 0.736 | Acc: 80.652% (9704/12032)/ 95.221% (11457/12032)
01/15/2023 20:58:05 - INFO - __main__ -   test: [batch: 94/391 ] | Loss: 0.740 | Acc: 80.535% (9793/12160)/ 95.230% (11580/12160)
01/15/2023 20:58:07 - INFO - __main__ -   test: [batch: 95/391 ] | Loss: 0.737 | Acc: 80.615% (9906/12288)/ 95.239% (11703/12288)
01/15/2023 20:58:08 - INFO - __main__ -   test: [batch: 96/391 ] | Loss: 0.741 | Acc: 80.477% (9992/12416)/ 95.248% (11826/12416)
01/15/2023 20:58:10 - INFO - __main__ -   test: [batch: 97/391 ] | Loss: 0.743 | Acc: 80.293% (10072/12544)/ 95.273% (11951/12544)
01/15/2023 20:58:11 - INFO - __main__ -   test: [batch: 98/391 ] | Loss: 0.739 | Acc: 80.398% (10188/12672)/ 95.312% (12078/12672)
01/15/2023 20:58:12 - INFO - __main__ -   test: [batch: 99/391 ] | Loss: 0.733 | Acc: 80.555% (10311/12800)/ 95.359% (12206/12800)
01/15/2023 20:58:14 - INFO - __main__ -   test: [batch: 100/391 ] | Loss: 0.731 | Acc: 80.600% (10420/12928)/ 95.374% (12330/12928)
01/15/2023 20:58:15 - INFO - __main__ -   test: [batch: 101/391 ] | Loss: 0.727 | Acc: 80.714% (10538/13056)/ 95.404% (12456/13056)
01/15/2023 20:58:17 - INFO - __main__ -   test: [batch: 102/391 ] | Loss: 0.724 | Acc: 80.825% (10656/13184)/ 95.434% (12582/13184)
01/15/2023 20:58:18 - INFO - __main__ -   test: [batch: 103/391 ] | Loss: 0.725 | Acc: 80.664% (10738/13312)/ 95.440% (12705/13312)
01/15/2023 20:58:20 - INFO - __main__ -   test: [batch: 104/391 ] | Loss: 0.725 | Acc: 80.595% (10832/13440)/ 95.439% (12827/13440)
01/15/2023 20:58:21 - INFO - __main__ -   test: [batch: 105/391 ] | Loss: 0.725 | Acc: 80.594% (10935/13568)/ 95.445% (12950/13568)
01/15/2023 20:58:23 - INFO - __main__ -   test: [batch: 106/391 ] | Loss: 0.734 | Acc: 80.491% (11024/13696)/ 95.378% (13063/13696)
01/15/2023 20:58:25 - INFO - __main__ -   test: [batch: 107/391 ] | Loss: 0.729 | Acc: 80.621% (11145/13824)/ 95.414% (13190/13824)
01/15/2023 20:58:26 - INFO - __main__ -   test: [batch: 108/391 ] | Loss: 0.733 | Acc: 80.476% (11228/13952)/ 95.420% (13313/13952)
01/15/2023 20:58:28 - INFO - __main__ -   test: [batch: 109/391 ] | Loss: 0.733 | Acc: 80.483% (11332/14080)/ 95.426% (13436/14080)
01/15/2023 20:58:29 - INFO - __main__ -   test: [batch: 110/391 ] | Loss: 0.736 | Acc: 80.307% (11410/14208)/ 95.446% (13561/14208)
01/15/2023 20:58:31 - INFO - __main__ -   test: [batch: 111/391 ] | Loss: 0.739 | Acc: 80.273% (11508/14336)/ 95.396% (13676/14336)
01/15/2023 20:58:32 - INFO - __main__ -   test: [batch: 112/391 ] | Loss: 0.739 | Acc: 80.296% (11614/14464)/ 95.416% (13801/14464)
01/15/2023 20:58:34 - INFO - __main__ -   test: [batch: 113/391 ] | Loss: 0.738 | Acc: 80.345% (11724/14592)/ 95.422% (13924/14592)
01/15/2023 20:58:35 - INFO - __main__ -   test: [batch: 114/391 ] | Loss: 0.735 | Acc: 80.448% (11842/14720)/ 95.448% (14050/14720)
01/15/2023 20:58:37 - INFO - __main__ -   test: [batch: 115/391 ] | Loss: 0.732 | Acc: 80.543% (11959/14848)/ 95.467% (14175/14848)
01/15/2023 20:58:39 - INFO - __main__ -   test: [batch: 116/391 ] | Loss: 0.735 | Acc: 80.529% (12060/14976)/ 95.453% (14295/14976)
01/15/2023 20:58:40 - INFO - __main__ -   test: [batch: 117/391 ] | Loss: 0.734 | Acc: 80.555% (12167/15104)/ 95.471% (14420/15104)
01/15/2023 20:58:42 - INFO - __main__ -   test: [batch: 118/391 ] | Loss: 0.737 | Acc: 80.423% (12250/15232)/ 95.483% (14544/15232)
01/15/2023 20:58:43 - INFO - __main__ -   test: [batch: 119/391 ] | Loss: 0.735 | Acc: 80.475% (12361/15360)/ 95.508% (14670/15360)
01/15/2023 20:58:45 - INFO - __main__ -   test: [batch: 120/391 ] | Loss: 0.734 | Acc: 80.527% (12472/15488)/ 95.526% (14795/15488)
01/15/2023 20:58:47 - INFO - __main__ -   test: [batch: 121/391 ] | Loss: 0.739 | Acc: 80.392% (12554/15616)/ 95.498% (14913/15616)
01/15/2023 20:58:48 - INFO - __main__ -   test: [batch: 122/391 ] | Loss: 0.741 | Acc: 80.348% (12650/15744)/ 95.484% (15033/15744)
01/15/2023 20:58:50 - INFO - __main__ -   test: [batch: 123/391 ] | Loss: 0.742 | Acc: 80.368% (12756/15872)/ 95.476% (15154/15872)
01/15/2023 20:58:52 - INFO - __main__ -   test: [batch: 124/391 ] | Loss: 0.741 | Acc: 80.394% (12863/16000)/ 95.481% (15277/16000)
01/15/2023 20:58:53 - INFO - __main__ -   test: [batch: 125/391 ] | Loss: 0.737 | Acc: 80.506% (12984/16128)/ 95.511% (15404/16128)
01/15/2023 20:58:55 - INFO - __main__ -   test: [batch: 126/391 ] | Loss: 0.734 | Acc: 80.598% (13102/16256)/ 95.516% (15527/16256)
01/15/2023 20:58:56 - INFO - __main__ -   test: [batch: 127/391 ] | Loss: 0.732 | Acc: 80.670% (13217/16384)/ 95.526% (15651/16384)
01/15/2023 20:58:58 - INFO - __main__ -   test: [batch: 128/391 ] | Loss: 0.732 | Acc: 80.663% (13319/16512)/ 95.506% (15770/16512)
01/15/2023 20:58:59 - INFO - __main__ -   test: [batch: 129/391 ] | Loss: 0.731 | Acc: 80.685% (13426/16640)/ 95.523% (15895/16640)
01/15/2023 20:59:01 - INFO - __main__ -   test: [batch: 130/391 ] | Loss: 0.727 | Acc: 80.797% (13548/16768)/ 95.545% (16021/16768)
01/15/2023 20:59:02 - INFO - __main__ -   test: [batch: 131/391 ] | Loss: 0.727 | Acc: 80.848% (13660/16896)/ 95.555% (16145/16896)
01/15/2023 20:59:04 - INFO - __main__ -   test: [batch: 132/391 ] | Loss: 0.724 | Acc: 80.909% (13774/17024)/ 95.571% (16270/17024)
01/15/2023 20:59:05 - INFO - __main__ -   test: [batch: 133/391 ] | Loss: 0.726 | Acc: 80.865% (13870/17152)/ 95.563% (16391/17152)
01/15/2023 20:59:07 - INFO - __main__ -   test: [batch: 134/391 ] | Loss: 0.724 | Acc: 80.880% (13976/17280)/ 95.579% (16516/17280)
01/15/2023 20:59:08 - INFO - __main__ -   test: [batch: 135/391 ] | Loss: 0.724 | Acc: 80.859% (14076/17408)/ 95.577% (16638/17408)
01/15/2023 20:59:10 - INFO - __main__ -   test: [batch: 136/391 ] | Loss: 0.723 | Acc: 80.839% (14176/17536)/ 95.598% (16764/17536)
01/15/2023 20:59:11 - INFO - __main__ -   test: [batch: 137/391 ] | Loss: 0.721 | Acc: 80.882% (14287/17664)/ 95.624% (16891/17664)
01/15/2023 20:59:13 - INFO - __main__ -   test: [batch: 138/391 ] | Loss: 0.719 | Acc: 80.941% (14401/17792)/ 95.633% (17015/17792)
01/15/2023 20:59:15 - INFO - __main__ -   test: [batch: 139/391 ] | Loss: 0.724 | Acc: 80.820% (14483/17920)/ 95.619% (17135/17920)
01/15/2023 20:59:16 - INFO - __main__ -   test: [batch: 140/391 ] | Loss: 0.726 | Acc: 80.757% (14575/18048)/ 95.617% (17257/18048)
01/15/2023 20:59:18 - INFO - __main__ -   test: [batch: 141/391 ] | Loss: 0.724 | Acc: 80.799% (14686/18176)/ 95.632% (17382/18176)
01/15/2023 20:59:19 - INFO - __main__ -   test: [batch: 142/391 ] | Loss: 0.722 | Acc: 80.884% (14805/18304)/ 95.640% (17506/18304)
01/15/2023 20:59:21 - INFO - __main__ -   test: [batch: 143/391 ] | Loss: 0.723 | Acc: 80.914% (14914/18432)/ 95.627% (17626/18432)
01/15/2023 20:59:23 - INFO - __main__ -   test: [batch: 144/391 ] | Loss: 0.724 | Acc: 80.884% (15012/18560)/ 95.587% (17741/18560)
01/15/2023 20:59:24 - INFO - __main__ -   test: [batch: 145/391 ] | Loss: 0.727 | Acc: 80.870% (15113/18688)/ 95.564% (17859/18688)
01/15/2023 20:59:26 - INFO - __main__ -   test: [batch: 146/391 ] | Loss: 0.729 | Acc: 80.851% (15213/18816)/ 95.541% (17977/18816)
01/15/2023 20:59:27 - INFO - __main__ -   test: [batch: 147/391 ] | Loss: 0.729 | Acc: 80.838% (15314/18944)/ 95.529% (18097/18944)
01/15/2023 20:59:29 - INFO - __main__ -   test: [batch: 148/391 ] | Loss: 0.730 | Acc: 80.768% (15404/19072)/ 95.517% (18217/19072)
01/15/2023 20:59:30 - INFO - __main__ -   test: [batch: 149/391 ] | Loss: 0.734 | Acc: 80.677% (15490/19200)/ 95.490% (18334/19200)
01/15/2023 20:59:31 - INFO - __main__ -   test: [batch: 150/391 ] | Loss: 0.734 | Acc: 80.603% (15579/19328)/ 95.499% (18458/19328)
01/15/2023 20:59:33 - INFO - __main__ -   test: [batch: 151/391 ] | Loss: 0.734 | Acc: 80.644% (15690/19456)/ 95.498% (18580/19456)
01/15/2023 20:59:35 - INFO - __main__ -   test: [batch: 152/391 ] | Loss: 0.736 | Acc: 80.617% (15788/19584)/ 95.471% (18697/19584)
01/15/2023 20:59:36 - INFO - __main__ -   test: [batch: 153/391 ] | Loss: 0.734 | Acc: 80.672% (15902/19712)/ 95.470% (18819/19712)
01/15/2023 20:59:38 - INFO - __main__ -   test: [batch: 154/391 ] | Loss: 0.736 | Acc: 80.655% (16002/19840)/ 95.439% (18935/19840)
01/15/2023 20:59:39 - INFO - __main__ -   test: [batch: 155/391 ] | Loss: 0.736 | Acc: 80.679% (16110/19968)/ 95.428% (19055/19968)
01/15/2023 20:59:41 - INFO - __main__ -   test: [batch: 156/391 ] | Loss: 0.738 | Acc: 80.598% (16197/20096)/ 95.402% (19172/20096)
01/15/2023 20:59:43 - INFO - __main__ -   test: [batch: 157/391 ] | Loss: 0.741 | Acc: 80.568% (16294/20224)/ 95.387% (19291/20224)
01/15/2023 20:59:44 - INFO - __main__ -   test: [batch: 158/391 ] | Loss: 0.742 | Acc: 80.552% (16394/20352)/ 95.357% (19407/20352)
01/15/2023 20:59:46 - INFO - __main__ -   test: [batch: 159/391 ] | Loss: 0.743 | Acc: 80.532% (16493/20480)/ 95.356% (19529/20480)
01/15/2023 20:59:48 - INFO - __main__ -   test: [batch: 160/391 ] | Loss: 0.743 | Acc: 80.537% (16597/20608)/ 95.337% (19647/20608)
01/15/2023 20:59:49 - INFO - __main__ -   test: [batch: 161/391 ] | Loss: 0.753 | Acc: 80.295% (16650/20736)/ 95.231% (19747/20736)
01/15/2023 20:59:51 - INFO - __main__ -   test: [batch: 162/391 ] | Loss: 0.759 | Acc: 80.186% (16730/20864)/ 95.159% (19854/20864)
01/15/2023 20:59:52 - INFO - __main__ -   test: [batch: 163/391 ] | Loss: 0.762 | Acc: 80.116% (16818/20992)/ 95.146% (19973/20992)
01/15/2023 20:59:54 - INFO - __main__ -   test: [batch: 164/391 ] | Loss: 0.762 | Acc: 80.099% (16917/21120)/ 95.161% (20098/21120)
01/15/2023 20:59:55 - INFO - __main__ -   test: [batch: 165/391 ] | Loss: 0.764 | Acc: 80.026% (17004/21248)/ 95.148% (20217/21248)
01/15/2023 20:59:57 - INFO - __main__ -   test: [batch: 166/391 ] | Loss: 0.764 | Acc: 80.048% (17111/21376)/ 95.139% (20337/21376)
01/15/2023 20:59:58 - INFO - __main__ -   test: [batch: 167/391 ] | Loss: 0.766 | Acc: 79.990% (17201/21504)/ 95.108% (20452/21504)
01/15/2023 21:00:00 - INFO - __main__ -   test: [batch: 168/391 ] | Loss: 0.766 | Acc: 80.002% (17306/21632)/ 95.104% (20573/21632)
01/15/2023 21:00:01 - INFO - __main__ -   test: [batch: 169/391 ] | Loss: 0.768 | Acc: 79.949% (17397/21760)/ 95.064% (20686/21760)
01/15/2023 21:00:02 - INFO - __main__ -   test: [batch: 170/391 ] | Loss: 0.772 | Acc: 79.838% (17475/21888)/ 95.034% (20801/21888)
01/15/2023 21:00:04 - INFO - __main__ -   test: [batch: 171/391 ] | Loss: 0.776 | Acc: 79.792% (17567/22016)/ 94.995% (20914/22016)
01/15/2023 21:00:05 - INFO - __main__ -   test: [batch: 172/391 ] | Loss: 0.777 | Acc: 79.751% (17660/22144)/ 94.987% (21034/22144)
01/15/2023 21:00:07 - INFO - __main__ -   test: [batch: 173/391 ] | Loss: 0.779 | Acc: 79.714% (17754/22272)/ 94.971% (21152/22272)
01/15/2023 21:00:08 - INFO - __main__ -   test: [batch: 174/391 ] | Loss: 0.783 | Acc: 79.625% (17836/22400)/ 94.911% (21260/22400)
01/15/2023 21:00:10 - INFO - __main__ -   test: [batch: 175/391 ] | Loss: 0.781 | Acc: 79.683% (17951/22528)/ 94.922% (21384/22528)
01/15/2023 21:00:12 - INFO - __main__ -   test: [batch: 176/391 ] | Loss: 0.782 | Acc: 79.679% (18052/22656)/ 94.889% (21498/22656)
01/15/2023 21:00:13 - INFO - __main__ -   test: [batch: 177/391 ] | Loss: 0.786 | Acc: 79.600% (18136/22784)/ 94.856% (21612/22784)
01/15/2023 21:00:15 - INFO - __main__ -   test: [batch: 178/391 ] | Loss: 0.787 | Acc: 79.591% (18236/22912)/ 94.841% (21730/22912)
01/15/2023 21:00:16 - INFO - __main__ -   test: [batch: 179/391 ] | Loss: 0.790 | Acc: 79.531% (18324/23040)/ 94.792% (21840/23040)
01/15/2023 21:00:18 - INFO - __main__ -   test: [batch: 180/391 ] | Loss: 0.796 | Acc: 79.390% (18393/23168)/ 94.747% (21951/23168)
01/15/2023 21:00:19 - INFO - __main__ -   test: [batch: 181/391 ] | Loss: 0.802 | Acc: 79.275% (18468/23296)/ 94.690% (22059/23296)
01/15/2023 21:00:21 - INFO - __main__ -   test: [batch: 182/391 ] | Loss: 0.802 | Acc: 79.290% (18573/23424)/ 94.693% (22181/23424)
01/15/2023 21:00:22 - INFO - __main__ -   test: [batch: 183/391 ] | Loss: 0.808 | Acc: 79.169% (18646/23552)/ 94.586% (22277/23552)
01/15/2023 21:00:24 - INFO - __main__ -   test: [batch: 184/391 ] | Loss: 0.808 | Acc: 79.181% (18750/23680)/ 94.573% (22395/23680)
01/15/2023 21:00:26 - INFO - __main__ -   test: [batch: 185/391 ] | Loss: 0.807 | Acc: 79.196% (18855/23808)/ 94.569% (22515/23808)
01/15/2023 21:00:27 - INFO - __main__ -   test: [batch: 186/391 ] | Loss: 0.810 | Acc: 79.165% (18949/23936)/ 94.523% (22625/23936)
01/15/2023 21:00:29 - INFO - __main__ -   test: [batch: 187/391 ] | Loss: 0.815 | Acc: 79.060% (19025/24064)/ 94.498% (22740/24064)
01/15/2023 21:00:30 - INFO - __main__ -   test: [batch: 188/391 ] | Loss: 0.818 | Acc: 78.948% (19099/24192)/ 94.469% (22854/24192)
01/15/2023 21:00:32 - INFO - __main__ -   test: [batch: 189/391 ] | Loss: 0.820 | Acc: 78.886% (19185/24320)/ 94.470% (22975/24320)
01/15/2023 21:00:33 - INFO - __main__ -   test: [batch: 190/391 ] | Loss: 0.825 | Acc: 78.800% (19265/24448)/ 94.445% (23090/24448)
01/15/2023 21:00:35 - INFO - __main__ -   test: [batch: 191/391 ] | Loss: 0.827 | Acc: 78.764% (19357/24576)/ 94.413% (23203/24576)
01/15/2023 21:00:37 - INFO - __main__ -   test: [batch: 192/391 ] | Loss: 0.834 | Acc: 78.655% (19431/24704)/ 94.317% (23300/24704)
01/15/2023 21:00:38 - INFO - __main__ -   test: [batch: 193/391 ] | Loss: 0.834 | Acc: 78.665% (19534/24832)/ 94.310% (23419/24832)
01/15/2023 21:00:40 - INFO - __main__ -   test: [batch: 194/391 ] | Loss: 0.837 | Acc: 78.594% (19617/24960)/ 94.295% (23536/24960)
01/15/2023 21:00:41 - INFO - __main__ -   test: [batch: 195/391 ] | Loss: 0.842 | Acc: 78.504% (19695/25088)/ 94.224% (23639/25088)
01/15/2023 21:00:43 - INFO - __main__ -   test: [batch: 196/391 ] | Loss: 0.846 | Acc: 78.414% (19773/25216)/ 94.182% (23749/25216)
01/15/2023 21:00:45 - INFO - __main__ -   test: [batch: 197/391 ] | Loss: 0.850 | Acc: 78.334% (19853/25344)/ 94.168% (23866/25344)
01/15/2023 21:00:46 - INFO - __main__ -   test: [batch: 198/391 ] | Loss: 0.852 | Acc: 78.298% (19944/25472)/ 94.135% (23978/25472)
01/15/2023 21:00:48 - INFO - __main__ -   test: [batch: 199/391 ] | Loss: 0.851 | Acc: 78.277% (20039/25600)/ 94.145% (24101/25600)
01/15/2023 21:00:50 - INFO - __main__ -   test: [batch: 200/391 ] | Loss: 0.851 | Acc: 78.261% (20135/25728)/ 94.150% (24223/25728)
01/15/2023 21:00:51 - INFO - __main__ -   test: [batch: 201/391 ] | Loss: 0.854 | Acc: 78.168% (20211/25856)/ 94.114% (24334/25856)
01/15/2023 21:00:53 - INFO - __main__ -   test: [batch: 202/391 ] | Loss: 0.856 | Acc: 78.144% (20305/25984)/ 94.100% (24451/25984)
01/15/2023 21:00:54 - INFO - __main__ -   test: [batch: 203/391 ] | Loss: 0.856 | Acc: 78.125% (20400/26112)/ 94.091% (24569/26112)
01/15/2023 21:00:56 - INFO - __main__ -   test: [batch: 204/391 ] | Loss: 0.860 | Acc: 78.045% (20479/26240)/ 94.062% (24682/26240)
01/15/2023 21:00:57 - INFO - __main__ -   test: [batch: 205/391 ] | Loss: 0.862 | Acc: 77.973% (20560/26368)/ 94.034% (24795/26368)
01/15/2023 21:00:59 - INFO - __main__ -   test: [batch: 206/391 ] | Loss: 0.864 | Acc: 77.944% (20652/26496)/ 94.033% (24915/26496)
01/15/2023 21:01:00 - INFO - __main__ -   test: [batch: 207/391 ] | Loss: 0.868 | Acc: 77.873% (20733/26624)/ 94.005% (25028/26624)
01/15/2023 21:01:02 - INFO - __main__ -   test: [batch: 208/391 ] | Loss: 0.869 | Acc: 77.833% (20822/26752)/ 93.967% (25138/26752)
01/15/2023 21:01:03 - INFO - __main__ -   test: [batch: 209/391 ] | Loss: 0.869 | Acc: 77.835% (20922/26880)/ 93.988% (25264/26880)
01/15/2023 21:01:05 - INFO - __main__ -   test: [batch: 210/391 ] | Loss: 0.871 | Acc: 77.810% (21015/27008)/ 93.972% (25380/27008)
01/15/2023 21:01:06 - INFO - __main__ -   test: [batch: 211/391 ] | Loss: 0.873 | Acc: 77.745% (21097/27136)/ 93.942% (25492/27136)
01/15/2023 21:01:08 - INFO - __main__ -   test: [batch: 212/391 ] | Loss: 0.875 | Acc: 77.681% (21179/27264)/ 93.933% (25610/27264)
01/15/2023 21:01:09 - INFO - __main__ -   test: [batch: 213/391 ] | Loss: 0.874 | Acc: 77.687% (21280/27392)/ 93.943% (25733/27392)
01/15/2023 21:01:11 - INFO - __main__ -   test: [batch: 214/391 ] | Loss: 0.875 | Acc: 77.678% (21377/27520)/ 93.950% (25855/27520)
01/15/2023 21:01:13 - INFO - __main__ -   test: [batch: 215/391 ] | Loss: 0.876 | Acc: 77.658% (21471/27648)/ 93.931% (25970/27648)
01/15/2023 21:01:14 - INFO - __main__ -   test: [batch: 216/391 ] | Loss: 0.874 | Acc: 77.707% (21584/27776)/ 93.941% (26093/27776)
01/15/2023 21:01:16 - INFO - __main__ -   test: [batch: 217/391 ] | Loss: 0.879 | Acc: 77.645% (21666/27904)/ 93.883% (26197/27904)
01/15/2023 21:01:17 - INFO - __main__ -   test: [batch: 218/391 ] | Loss: 0.882 | Acc: 77.586% (21749/28032)/ 93.850% (26308/28032)
01/15/2023 21:01:19 - INFO - __main__ -   test: [batch: 219/391 ] | Loss: 0.880 | Acc: 77.624% (21859/28160)/ 93.857% (26430/28160)
01/15/2023 21:01:20 - INFO - __main__ -   test: [batch: 220/391 ] | Loss: 0.878 | Acc: 77.658% (21968/28288)/ 93.863% (26552/28288)
01/15/2023 21:01:22 - INFO - __main__ -   test: [batch: 221/391 ] | Loss: 0.880 | Acc: 77.625% (22058/28416)/ 93.841% (26666/28416)
01/15/2023 21:01:23 - INFO - __main__ -   test: [batch: 222/391 ] | Loss: 0.878 | Acc: 77.680% (22173/28544)/ 93.855% (26790/28544)
01/15/2023 21:01:25 - INFO - __main__ -   test: [batch: 223/391 ] | Loss: 0.879 | Acc: 77.672% (22270/28672)/ 93.841% (26906/28672)
01/15/2023 21:01:26 - INFO - __main__ -   test: [batch: 224/391 ] | Loss: 0.878 | Acc: 77.698% (22377/28800)/ 93.837% (27025/28800)
01/15/2023 21:01:27 - INFO - __main__ -   test: [batch: 225/391 ] | Loss: 0.878 | Acc: 77.693% (22475/28928)/ 93.840% (27146/28928)
01/15/2023 21:01:29 - INFO - __main__ -   test: [batch: 226/391 ] | Loss: 0.878 | Acc: 77.667% (22567/29056)/ 93.846% (27268/29056)
01/15/2023 21:01:31 - INFO - __main__ -   test: [batch: 227/391 ] | Loss: 0.879 | Acc: 77.673% (22668/29184)/ 93.839% (27386/29184)
01/15/2023 21:01:32 - INFO - __main__ -   test: [batch: 228/391 ] | Loss: 0.884 | Acc: 77.576% (22739/29312)/ 93.781% (27489/29312)
01/15/2023 21:01:34 - INFO - __main__ -   test: [batch: 229/391 ] | Loss: 0.887 | Acc: 77.500% (22816/29440)/ 93.740% (27597/29440)
01/15/2023 21:01:35 - INFO - __main__ -   test: [batch: 230/391 ] | Loss: 0.891 | Acc: 77.432% (22895/29568)/ 93.699% (27705/29568)
01/15/2023 21:01:37 - INFO - __main__ -   test: [batch: 231/391 ] | Loss: 0.891 | Acc: 77.421% (22991/29696)/ 93.686% (27821/29696)
01/15/2023 21:01:39 - INFO - __main__ -   test: [batch: 232/391 ] | Loss: 0.890 | Acc: 77.451% (23099/29824)/ 93.703% (27946/29824)
01/15/2023 21:01:40 - INFO - __main__ -   test: [batch: 233/391 ] | Loss: 0.892 | Acc: 77.411% (23186/29952)/ 93.680% (28059/29952)
01/15/2023 21:01:41 - INFO - __main__ -   test: [batch: 234/391 ] | Loss: 0.898 | Acc: 77.311% (23255/30080)/ 93.604% (28156/30080)
01/15/2023 21:01:43 - INFO - __main__ -   test: [batch: 235/391 ] | Loss: 0.898 | Acc: 77.297% (23350/30208)/ 93.601% (28275/30208)
01/15/2023 21:01:44 - INFO - __main__ -   test: [batch: 236/391 ] | Loss: 0.896 | Acc: 77.347% (23464/30336)/ 93.608% (28397/30336)
01/15/2023 21:01:46 - INFO - __main__ -   test: [batch: 237/391 ] | Loss: 0.897 | Acc: 77.340% (23561/30464)/ 93.579% (28508/30464)
01/15/2023 21:01:48 - INFO - __main__ -   test: [batch: 238/391 ] | Loss: 0.896 | Acc: 77.380% (23672/30592)/ 93.590% (28631/30592)
01/15/2023 21:01:49 - INFO - __main__ -   test: [batch: 239/391 ] | Loss: 0.895 | Acc: 77.412% (23781/30720)/ 93.597% (28753/30720)
01/15/2023 21:01:51 - INFO - __main__ -   test: [batch: 240/391 ] | Loss: 0.895 | Acc: 77.425% (23884/30848)/ 93.591% (28871/30848)
01/15/2023 21:01:52 - INFO - __main__ -   test: [batch: 241/391 ] | Loss: 0.898 | Acc: 77.350% (23960/30976)/ 93.547% (28977/30976)
01/15/2023 21:01:54 - INFO - __main__ -   test: [batch: 242/391 ] | Loss: 0.900 | Acc: 77.251% (24028/31104)/ 93.535% (29093/31104)
01/15/2023 21:01:56 - INFO - __main__ -   test: [batch: 243/391 ] | Loss: 0.906 | Acc: 77.132% (24090/31232)/ 93.471% (29193/31232)
01/15/2023 21:01:57 - INFO - __main__ -   test: [batch: 244/391 ] | Loss: 0.906 | Acc: 77.130% (24188/31360)/ 93.473% (29313/31360)
01/15/2023 21:01:59 - INFO - __main__ -   test: [batch: 245/391 ] | Loss: 0.906 | Acc: 77.137% (24289/31488)/ 93.471% (29432/31488)
01/15/2023 21:02:00 - INFO - __main__ -   test: [batch: 246/391 ] | Loss: 0.906 | Acc: 77.116% (24381/31616)/ 93.462% (29549/31616)
01/15/2023 21:02:02 - INFO - __main__ -   test: [batch: 247/391 ] | Loss: 0.911 | Acc: 77.041% (24456/31744)/ 93.394% (29647/31744)
01/15/2023 21:02:03 - INFO - __main__ -   test: [batch: 248/391 ] | Loss: 0.912 | Acc: 77.005% (24543/31872)/ 93.389% (29765/31872)
01/15/2023 21:02:05 - INFO - __main__ -   test: [batch: 249/391 ] | Loss: 0.915 | Acc: 76.850% (24592/32000)/ 93.388% (29884/32000)
01/15/2023 21:02:06 - INFO - __main__ -   test: [batch: 250/391 ] | Loss: 0.913 | Acc: 76.892% (24704/32128)/ 93.401% (30008/32128)
01/15/2023 21:02:08 - INFO - __main__ -   test: [batch: 251/391 ] | Loss: 0.916 | Acc: 76.851% (24789/32256)/ 93.369% (30117/32256)
01/15/2023 21:02:09 - INFO - __main__ -   test: [batch: 252/391 ] | Loss: 0.915 | Acc: 76.881% (24897/32384)/ 93.364% (30235/32384)
01/15/2023 21:02:11 - INFO - __main__ -   test: [batch: 253/391 ] | Loss: 0.916 | Acc: 76.855% (24987/32512)/ 93.350% (30350/32512)
01/15/2023 21:02:12 - INFO - __main__ -   test: [batch: 254/391 ] | Loss: 0.920 | Acc: 76.774% (25059/32640)/ 93.303% (30454/32640)
01/15/2023 21:02:14 - INFO - __main__ -   test: [batch: 255/391 ] | Loss: 0.922 | Acc: 76.752% (25150/32768)/ 93.295% (30571/32768)
01/15/2023 21:02:16 - INFO - __main__ -   test: [batch: 256/391 ] | Loss: 0.928 | Acc: 76.620% (25205/32896)/ 93.248% (30675/32896)
01/15/2023 21:02:17 - INFO - __main__ -   test: [batch: 257/391 ] | Loss: 0.928 | Acc: 76.611% (25300/33024)/ 93.238% (30791/33024)
01/15/2023 21:02:19 - INFO - __main__ -   test: [batch: 258/391 ] | Loss: 0.929 | Acc: 76.608% (25397/33152)/ 93.228% (30907/33152)
01/15/2023 21:02:20 - INFO - __main__ -   test: [batch: 259/391 ] | Loss: 0.932 | Acc: 76.487% (25455/33280)/ 93.224% (31025/33280)
01/15/2023 21:02:22 - INFO - __main__ -   test: [batch: 260/391 ] | Loss: 0.933 | Acc: 76.467% (25546/33408)/ 93.220% (31143/33408)
01/15/2023 21:02:23 - INFO - __main__ -   test: [batch: 261/391 ] | Loss: 0.931 | Acc: 76.503% (25656/33536)/ 93.243% (31270/33536)
01/15/2023 21:02:25 - INFO - __main__ -   test: [batch: 262/391 ] | Loss: 0.930 | Acc: 76.521% (25760/33664)/ 93.254% (31393/33664)
01/15/2023 21:02:26 - INFO - __main__ -   test: [batch: 263/391 ] | Loss: 0.932 | Acc: 76.453% (25835/33792)/ 93.223% (31502/33792)
01/15/2023 21:02:28 - INFO - __main__ -   test: [batch: 264/391 ] | Loss: 0.937 | Acc: 76.392% (25912/33920)/ 93.160% (31600/33920)
01/15/2023 21:02:29 - INFO - __main__ -   test: [batch: 265/391 ] | Loss: 0.937 | Acc: 76.424% (26021/34048)/ 93.160% (31719/34048)
01/15/2023 21:02:31 - INFO - __main__ -   test: [batch: 266/391 ] | Loss: 0.939 | Acc: 76.346% (26092/34176)/ 93.144% (31833/34176)
01/15/2023 21:02:32 - INFO - __main__ -   test: [batch: 267/391 ] | Loss: 0.938 | Acc: 76.373% (26199/34304)/ 93.144% (31952/34304)
01/15/2023 21:02:34 - INFO - __main__ -   test: [batch: 268/391 ] | Loss: 0.938 | Acc: 76.385% (26301/34432)/ 93.143% (32071/34432)
01/15/2023 21:02:35 - INFO - __main__ -   test: [batch: 269/391 ] | Loss: 0.940 | Acc: 76.334% (26381/34560)/ 93.122% (32183/34560)
01/15/2023 21:02:37 - INFO - __main__ -   test: [batch: 270/391 ] | Loss: 0.943 | Acc: 76.289% (26463/34688)/ 93.096% (32293/34688)
01/15/2023 21:02:38 - INFO - __main__ -   test: [batch: 271/391 ] | Loss: 0.943 | Acc: 76.298% (26564/34816)/ 93.089% (32410/34816)
01/15/2023 21:02:40 - INFO - __main__ -   test: [batch: 272/391 ] | Loss: 0.944 | Acc: 76.271% (26652/34944)/ 93.083% (32527/34944)
01/15/2023 21:02:41 - INFO - __main__ -   test: [batch: 273/391 ] | Loss: 0.944 | Acc: 76.283% (26754/35072)/ 93.057% (32637/35072)
01/15/2023 21:02:43 - INFO - __main__ -   test: [batch: 274/391 ] | Loss: 0.944 | Acc: 76.290% (26854/35200)/ 93.062% (32758/35200)
01/15/2023 21:02:45 - INFO - __main__ -   test: [batch: 275/391 ] | Loss: 0.946 | Acc: 76.254% (26939/35328)/ 93.054% (32874/35328)
01/15/2023 21:02:46 - INFO - __main__ -   test: [batch: 276/391 ] | Loss: 0.947 | Acc: 76.204% (27019/35456)/ 93.048% (32991/35456)
01/15/2023 21:02:48 - INFO - __main__ -   test: [batch: 277/391 ] | Loss: 0.948 | Acc: 76.175% (27106/35584)/ 93.028% (33103/35584)
01/15/2023 21:02:50 - INFO - __main__ -   test: [batch: 278/391 ] | Loss: 0.947 | Acc: 76.187% (27208/35712)/ 93.025% (33221/35712)
01/15/2023 21:02:51 - INFO - __main__ -   test: [batch: 279/391 ] | Loss: 0.947 | Acc: 76.208% (27313/35840)/ 93.019% (33338/35840)
01/15/2023 21:02:53 - INFO - __main__ -   test: [batch: 280/391 ] | Loss: 0.948 | Acc: 76.176% (27399/35968)/ 93.013% (33455/35968)
01/15/2023 21:02:54 - INFO - __main__ -   test: [batch: 281/391 ] | Loss: 0.949 | Acc: 76.180% (27498/36096)/ 93.005% (33571/36096)
01/15/2023 21:02:56 - INFO - __main__ -   test: [batch: 282/391 ] | Loss: 0.948 | Acc: 76.217% (27609/36224)/ 93.005% (33690/36224)
01/15/2023 21:02:57 - INFO - __main__ -   test: [batch: 283/391 ] | Loss: 0.948 | Acc: 76.208% (27703/36352)/ 92.999% (33807/36352)
01/15/2023 21:02:58 - INFO - __main__ -   test: [batch: 284/391 ] | Loss: 0.952 | Acc: 76.160% (27783/36480)/ 92.941% (33905/36480)
01/15/2023 21:03:00 - INFO - __main__ -   test: [batch: 285/391 ] | Loss: 0.954 | Acc: 76.125% (27868/36608)/ 92.898% (34008/36608)
01/15/2023 21:03:01 - INFO - __main__ -   test: [batch: 286/391 ] | Loss: 0.956 | Acc: 76.092% (27953/36736)/ 92.890% (34124/36736)
01/15/2023 21:03:03 - INFO - __main__ -   test: [batch: 287/391 ] | Loss: 0.956 | Acc: 76.088% (28049/36864)/ 92.882% (34240/36864)
01/15/2023 21:03:04 - INFO - __main__ -   test: [batch: 288/391 ] | Loss: 0.955 | Acc: 76.103% (28152/36992)/ 92.888% (34361/36992)
01/15/2023 21:03:06 - INFO - __main__ -   test: [batch: 289/391 ] | Loss: 0.957 | Acc: 76.070% (28237/37120)/ 92.850% (34466/37120)
01/15/2023 21:03:07 - INFO - __main__ -   test: [batch: 290/391 ] | Loss: 0.959 | Acc: 75.988% (28304/37248)/ 92.845% (34583/37248)
01/15/2023 21:03:09 - INFO - __main__ -   test: [batch: 291/391 ] | Loss: 0.959 | Acc: 75.985% (28400/37376)/ 92.846% (34702/37376)
01/15/2023 21:03:10 - INFO - __main__ -   test: [batch: 292/391 ] | Loss: 0.961 | Acc: 75.925% (28475/37504)/ 92.825% (34813/37504)
01/15/2023 21:03:12 - INFO - __main__ -   test: [batch: 293/391 ] | Loss: 0.961 | Acc: 75.911% (28567/37632)/ 92.823% (34931/37632)
01/15/2023 21:03:14 - INFO - __main__ -   test: [batch: 294/391 ] | Loss: 0.963 | Acc: 75.885% (28654/37760)/ 92.802% (35042/37760)
01/15/2023 21:03:15 - INFO - __main__ -   test: [batch: 295/391 ] | Loss: 0.962 | Acc: 75.911% (28761/37888)/ 92.802% (35161/37888)
01/15/2023 21:03:17 - INFO - __main__ -   test: [batch: 296/391 ] | Loss: 0.962 | Acc: 75.902% (28855/38016)/ 92.795% (35277/38016)
01/15/2023 21:03:18 - INFO - __main__ -   test: [batch: 297/391 ] | Loss: 0.964 | Acc: 75.873% (28941/38144)/ 92.770% (35386/38144)
01/15/2023 21:03:19 - INFO - __main__ -   test: [batch: 298/391 ] | Loss: 0.966 | Acc: 75.831% (29022/38272)/ 92.747% (35496/38272)
01/15/2023 21:03:20 - INFO - __main__ -   test: [batch: 299/391 ] | Loss: 0.967 | Acc: 75.810% (29111/38400)/ 92.727% (35607/38400)
01/15/2023 21:03:22 - INFO - __main__ -   test: [batch: 300/391 ] | Loss: 0.968 | Acc: 75.823% (29213/38528)/ 92.725% (35725/38528)
01/15/2023 21:03:23 - INFO - __main__ -   test: [batch: 301/391 ] | Loss: 0.968 | Acc: 75.815% (29307/38656)/ 92.710% (35838/38656)
01/15/2023 21:03:25 - INFO - __main__ -   test: [batch: 302/391 ] | Loss: 0.970 | Acc: 75.763% (29384/38784)/ 92.688% (35948/38784)
01/15/2023 21:03:26 - INFO - __main__ -   test: [batch: 303/391 ] | Loss: 0.971 | Acc: 75.730% (29468/38912)/ 92.678% (36063/38912)
01/15/2023 21:03:27 - INFO - __main__ -   test: [batch: 304/391 ] | Loss: 0.971 | Acc: 75.740% (29569/39040)/ 92.679% (36182/39040)
01/15/2023 21:03:28 - INFO - __main__ -   test: [batch: 305/391 ] | Loss: 0.972 | Acc: 75.700% (29650/39168)/ 92.673% (36298/39168)
01/15/2023 21:03:29 - INFO - __main__ -   test: [batch: 306/391 ] | Loss: 0.973 | Acc: 75.697% (29746/39296)/ 92.646% (36406/39296)
01/15/2023 21:03:30 - INFO - __main__ -   test: [batch: 307/391 ] | Loss: 0.974 | Acc: 75.682% (29837/39424)/ 92.626% (36517/39424)
01/15/2023 21:03:31 - INFO - __main__ -   test: [batch: 308/391 ] | Loss: 0.974 | Acc: 75.675% (29931/39552)/ 92.617% (36632/39552)
01/15/2023 21:03:32 - INFO - __main__ -   test: [batch: 309/391 ] | Loss: 0.976 | Acc: 75.665% (30024/39680)/ 92.596% (36742/39680)
01/15/2023 21:03:33 - INFO - __main__ -   test: [batch: 310/391 ] | Loss: 0.976 | Acc: 75.656% (30117/39808)/ 92.584% (36856/39808)
01/15/2023 21:03:34 - INFO - __main__ -   test: [batch: 311/391 ] | Loss: 0.978 | Acc: 75.631% (30204/39936)/ 92.566% (36967/39936)
01/15/2023 21:03:35 - INFO - __main__ -   test: [batch: 312/391 ] | Loss: 0.979 | Acc: 75.617% (30295/40064)/ 92.547% (37078/40064)
01/15/2023 21:03:36 - INFO - __main__ -   test: [batch: 313/391 ] | Loss: 0.977 | Acc: 75.664% (30411/40192)/ 92.571% (37206/40192)
01/15/2023 21:03:37 - INFO - __main__ -   test: [batch: 314/391 ] | Loss: 0.978 | Acc: 75.650% (30502/40320)/ 92.560% (37320/40320)
01/15/2023 21:03:38 - INFO - __main__ -   test: [batch: 315/391 ] | Loss: 0.978 | Acc: 75.635% (30593/40448)/ 92.546% (37433/40448)
01/15/2023 21:03:39 - INFO - __main__ -   test: [batch: 316/391 ] | Loss: 0.981 | Acc: 75.545% (30653/40576)/ 92.520% (37541/40576)
01/15/2023 21:03:40 - INFO - __main__ -   test: [batch: 317/391 ] | Loss: 0.983 | Acc: 75.506% (30734/40704)/ 92.490% (37647/40704)
01/15/2023 21:03:41 - INFO - __main__ -   test: [batch: 318/391 ] | Loss: 0.982 | Acc: 75.536% (30843/40832)/ 92.508% (37773/40832)
01/15/2023 21:03:42 - INFO - __main__ -   test: [batch: 319/391 ] | Loss: 0.984 | Acc: 75.481% (30917/40960)/ 92.480% (37880/40960)
01/15/2023 21:03:43 - INFO - __main__ -   test: [batch: 320/391 ] | Loss: 0.983 | Acc: 75.514% (31027/41088)/ 92.492% (38003/41088)
01/15/2023 21:03:43 - INFO - __main__ -   test: [batch: 321/391 ] | Loss: 0.983 | Acc: 75.529% (31130/41216)/ 92.491% (38121/41216)
01/15/2023 21:03:44 - INFO - __main__ -   test: [batch: 322/391 ] | Loss: 0.984 | Acc: 75.493% (31212/41344)/ 92.483% (38236/41344)
01/15/2023 21:03:45 - INFO - __main__ -   test: [batch: 323/391 ] | Loss: 0.987 | Acc: 75.463% (31296/41472)/ 92.462% (38346/41472)
01/15/2023 21:03:46 - INFO - __main__ -   test: [batch: 324/391 ] | Loss: 0.987 | Acc: 75.459% (31391/41600)/ 92.457% (38462/41600)
01/15/2023 21:03:47 - INFO - __main__ -   test: [batch: 325/391 ] | Loss: 0.986 | Acc: 75.470% (31492/41728)/ 92.458% (38581/41728)
01/15/2023 21:03:48 - INFO - __main__ -   test: [batch: 326/391 ] | Loss: 0.990 | Acc: 75.392% (31556/41856)/ 92.414% (38681/41856)
01/15/2023 21:03:49 - INFO - __main__ -   test: [batch: 327/391 ] | Loss: 0.993 | Acc: 75.305% (31616/41984)/ 92.369% (38780/41984)
01/15/2023 21:03:50 - INFO - __main__ -   test: [batch: 328/391 ] | Loss: 0.995 | Acc: 75.256% (31692/42112)/ 92.344% (38888/42112)
01/15/2023 21:03:51 - INFO - __main__ -   test: [batch: 329/391 ] | Loss: 0.995 | Acc: 75.258% (31789/42240)/ 92.346% (39007/42240)
01/15/2023 21:03:52 - INFO - __main__ -   test: [batch: 330/391 ] | Loss: 0.997 | Acc: 75.215% (31867/42368)/ 92.324% (39116/42368)
01/15/2023 21:03:53 - INFO - __main__ -   test: [batch: 331/391 ] | Loss: 0.997 | Acc: 75.184% (31950/42496)/ 92.340% (39241/42496)
01/15/2023 21:03:54 - INFO - __main__ -   test: [batch: 332/391 ] | Loss: 0.998 | Acc: 75.167% (32039/42624)/ 92.340% (39359/42624)
01/15/2023 21:03:55 - INFO - __main__ -   test: [batch: 333/391 ] | Loss: 0.996 | Acc: 75.196% (32148/42752)/ 92.344% (39479/42752)
01/15/2023 21:03:56 - INFO - __main__ -   test: [batch: 334/391 ] | Loss: 0.998 | Acc: 75.166% (32231/42880)/ 92.332% (39592/42880)
01/15/2023 21:03:57 - INFO - __main__ -   test: [batch: 335/391 ] | Loss: 0.999 | Acc: 75.142% (32317/43008)/ 92.315% (39703/43008)
01/15/2023 21:03:58 - INFO - __main__ -   test: [batch: 336/391 ] | Loss: 1.001 | Acc: 75.100% (32395/43136)/ 92.301% (39815/43136)
01/15/2023 21:03:59 - INFO - __main__ -   test: [batch: 337/391 ] | Loss: 1.001 | Acc: 75.088% (32486/43264)/ 92.294% (39930/43264)
01/15/2023 21:04:00 - INFO - __main__ -   test: [batch: 338/391 ] | Loss: 1.001 | Acc: 75.081% (32579/43392)/ 92.298% (40050/43392)
01/15/2023 21:04:01 - INFO - __main__ -   test: [batch: 339/391 ] | Loss: 1.003 | Acc: 75.041% (32658/43520)/ 92.263% (40153/43520)
01/15/2023 21:04:02 - INFO - __main__ -   test: [batch: 340/391 ] | Loss: 1.003 | Acc: 75.039% (32753/43648)/ 92.272% (40275/43648)
01/15/2023 21:04:03 - INFO - __main__ -   test: [batch: 341/391 ] | Loss: 1.001 | Acc: 75.078% (32866/43776)/ 92.290% (40401/43776)
01/15/2023 21:04:04 - INFO - __main__ -   test: [batch: 342/391 ] | Loss: 1.002 | Acc: 75.025% (32939/43904)/ 92.281% (40515/43904)
01/15/2023 21:04:05 - INFO - __main__ -   test: [batch: 343/391 ] | Loss: 1.002 | Acc: 75.030% (33037/44032)/ 92.285% (40635/44032)
01/15/2023 21:04:06 - INFO - __main__ -   test: [batch: 344/391 ] | Loss: 1.002 | Acc: 75.032% (33134/44160)/ 92.278% (40750/44160)
01/15/2023 21:04:07 - INFO - __main__ -   test: [batch: 345/391 ] | Loss: 1.006 | Acc: 74.959% (33198/44288)/ 92.239% (40851/44288)
01/15/2023 21:04:08 - INFO - __main__ -   test: [batch: 346/391 ] | Loss: 1.007 | Acc: 74.944% (33287/44416)/ 92.228% (40964/44416)
01/15/2023 21:04:09 - INFO - __main__ -   test: [batch: 347/391 ] | Loss: 1.007 | Acc: 74.955% (33388/44544)/ 92.237% (41086/44544)
01/15/2023 21:04:10 - INFO - __main__ -   test: [batch: 348/391 ] | Loss: 1.008 | Acc: 74.937% (33476/44672)/ 92.219% (41196/44672)
01/15/2023 21:04:10 - INFO - __main__ -   test: [batch: 349/391 ] | Loss: 1.008 | Acc: 74.942% (33574/44800)/ 92.228% (41318/44800)
01/15/2023 21:04:11 - INFO - __main__ -   test: [batch: 350/391 ] | Loss: 1.008 | Acc: 74.944% (33671/44928)/ 92.225% (41435/44928)
01/15/2023 21:04:12 - INFO - __main__ -   test: [batch: 351/391 ] | Loss: 1.010 | Acc: 74.887% (33741/45056)/ 92.205% (41544/45056)
01/15/2023 21:04:13 - INFO - __main__ -   test: [batch: 352/391 ] | Loss: 1.010 | Acc: 74.883% (33835/45184)/ 92.201% (41660/45184)
01/15/2023 21:04:14 - INFO - __main__ -   test: [batch: 353/391 ] | Loss: 1.013 | Acc: 74.832% (33908/45312)/ 92.161% (41760/45312)
01/15/2023 21:04:15 - INFO - __main__ -   test: [batch: 354/391 ] | Loss: 1.015 | Acc: 74.773% (33977/45440)/ 92.146% (41871/45440)
01/15/2023 21:04:16 - INFO - __main__ -   test: [batch: 355/391 ] | Loss: 1.018 | Acc: 74.708% (34043/45568)/ 92.130% (41982/45568)
01/15/2023 21:04:17 - INFO - __main__ -   test: [batch: 356/391 ] | Loss: 1.018 | Acc: 74.707% (34138/45696)/ 92.137% (42103/45696)
01/15/2023 21:04:18 - INFO - __main__ -   test: [batch: 357/391 ] | Loss: 1.017 | Acc: 74.738% (34248/45824)/ 92.150% (42227/45824)
01/15/2023 21:04:19 - INFO - __main__ -   test: [batch: 358/391 ] | Loss: 1.016 | Acc: 74.758% (34353/45952)/ 92.153% (42346/45952)
01/15/2023 21:04:20 - INFO - __main__ -   test: [batch: 359/391 ] | Loss: 1.016 | Acc: 74.763% (34451/46080)/ 92.144% (42460/46080)
01/15/2023 21:04:21 - INFO - __main__ -   test: [batch: 360/391 ] | Loss: 1.018 | Acc: 74.716% (34525/46208)/ 92.142% (42577/46208)
01/15/2023 21:04:21 - INFO - __main__ -   test: [batch: 361/391 ] | Loss: 1.018 | Acc: 74.713% (34619/46336)/ 92.149% (42698/46336)
01/15/2023 21:04:22 - INFO - __main__ -   test: [batch: 362/391 ] | Loss: 1.018 | Acc: 74.714% (34715/46464)/ 92.160% (42821/46464)
01/15/2023 21:04:23 - INFO - __main__ -   test: [batch: 363/391 ] | Loss: 1.018 | Acc: 74.704% (34806/46592)/ 92.151% (42935/46592)
01/15/2023 21:04:24 - INFO - __main__ -   test: [batch: 364/391 ] | Loss: 1.017 | Acc: 74.730% (34914/46720)/ 92.164% (43059/46720)
01/15/2023 21:04:25 - INFO - __main__ -   test: [batch: 365/391 ] | Loss: 1.017 | Acc: 74.733% (35011/46848)/ 92.166% (43178/46848)
01/15/2023 21:04:26 - INFO - __main__ -   test: [batch: 366/391 ] | Loss: 1.015 | Acc: 74.772% (35125/46976)/ 92.183% (43304/46976)
01/15/2023 21:04:26 - INFO - __main__ -   test: [batch: 367/391 ] | Loss: 1.015 | Acc: 74.790% (35229/47104)/ 92.194% (43427/47104)
01/15/2023 21:04:27 - INFO - __main__ -   test: [batch: 368/391 ] | Loss: 1.015 | Acc: 74.784% (35322/47232)/ 92.200% (43548/47232)
01/15/2023 21:04:28 - INFO - __main__ -   test: [batch: 369/391 ] | Loss: 1.014 | Acc: 74.806% (35428/47360)/ 92.211% (43671/47360)
01/15/2023 21:04:28 - INFO - __main__ -   test: [batch: 370/391 ] | Loss: 1.014 | Acc: 74.785% (35514/47488)/ 92.213% (43790/47488)
01/15/2023 21:04:29 - INFO - __main__ -   test: [batch: 371/391 ] | Loss: 1.014 | Acc: 74.790% (35612/47616)/ 92.213% (43908/47616)
01/15/2023 21:04:30 - INFO - __main__ -   test: [batch: 372/391 ] | Loss: 1.012 | Acc: 74.837% (35730/47744)/ 92.231% (44035/47744)
01/15/2023 21:04:30 - INFO - __main__ -   test: [batch: 373/391 ] | Loss: 1.011 | Acc: 74.870% (35842/47872)/ 92.242% (44158/47872)
01/15/2023 21:04:31 - INFO - __main__ -   test: [batch: 374/391 ] | Loss: 1.010 | Acc: 74.898% (35951/48000)/ 92.248% (44279/48000)
01/15/2023 21:04:32 - INFO - __main__ -   test: [batch: 375/391 ] | Loss: 1.013 | Acc: 74.823% (36011/48128)/ 92.217% (44382/48128)
01/15/2023 21:04:32 - INFO - __main__ -   test: [batch: 376/391 ] | Loss: 1.014 | Acc: 74.807% (36099/48256)/ 92.198% (44491/48256)
01/15/2023 21:04:33 - INFO - __main__ -   test: [batch: 377/391 ] | Loss: 1.014 | Acc: 74.797% (36190/48384)/ 92.188% (44604/48384)
01/15/2023 21:04:34 - INFO - __main__ -   test: [batch: 378/391 ] | Loss: 1.018 | Acc: 74.724% (36250/48512)/ 92.146% (44702/48512)
01/15/2023 21:04:34 - INFO - __main__ -   test: [batch: 379/391 ] | Loss: 1.018 | Acc: 74.708% (36338/48640)/ 92.157% (44825/48640)
01/15/2023 21:04:35 - INFO - __main__ -   test: [batch: 380/391 ] | Loss: 1.017 | Acc: 74.715% (36437/48768)/ 92.171% (44950/48768)
01/15/2023 21:04:36 - INFO - __main__ -   test: [batch: 381/391 ] | Loss: 1.019 | Acc: 74.673% (36512/48896)/ 92.169% (45067/48896)
01/15/2023 21:04:36 - INFO - __main__ -   test: [batch: 382/391 ] | Loss: 1.020 | Acc: 74.641% (36592/49024)/ 92.159% (45180/49024)
01/15/2023 21:04:37 - INFO - __main__ -   test: [batch: 383/391 ] | Loss: 1.020 | Acc: 74.644% (36689/49152)/ 92.155% (45296/49152)
01/15/2023 21:04:38 - INFO - __main__ -   test: [batch: 384/391 ] | Loss: 1.019 | Acc: 74.679% (36802/49280)/ 92.169% (45421/49280)
01/15/2023 21:04:38 - INFO - __main__ -   test: [batch: 385/391 ] | Loss: 1.017 | Acc: 74.688% (36902/49408)/ 92.179% (45544/49408)
01/15/2023 21:04:39 - INFO - __main__ -   test: [batch: 386/391 ] | Loss: 1.016 | Acc: 74.732% (37019/49536)/ 92.196% (45670/49536)
01/15/2023 21:04:40 - INFO - __main__ -   test: [batch: 387/391 ] | Loss: 1.014 | Acc: 74.774% (37136/49664)/ 92.206% (45793/49664)
01/15/2023 21:04:40 - INFO - __main__ -   test: [batch: 388/391 ] | Loss: 1.012 | Acc: 74.823% (37256/49792)/ 92.220% (45918/49792)
01/15/2023 21:04:41 - INFO - __main__ -   test: [batch: 389/391 ] | Loss: 1.012 | Acc: 74.828% (37354/49920)/ 92.228% (46040/49920)
01/15/2023 21:04:41 - INFO - __main__ -   test: [batch: 390/391 ] | Loss: 1.013 | Acc: 74.784% (37392/50000)/ 92.222% (46111/50000)
01/15/2023 21:04:41 - INFO - __main__ -   Final accuracy: 74.784
