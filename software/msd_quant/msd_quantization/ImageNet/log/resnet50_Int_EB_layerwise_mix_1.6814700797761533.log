/home/jjc/miniconda3/envs/ant_quant/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
Namespace(a_low=75, a_up=150, abit=8, batch_size=128, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='csd_eb3', epoch=3, layer_4bit_l=None, layer_8bit_l='25,47,46,35,29,38,46,5,8,15', layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='resnet50', percent=100, ptq=False, resume=False, search=False, tag='', train=True, w_low=75, w_up=150, wbit=8)
01/13/2023 15:46:04 - INFO - __main__ -   output/resnet50_imagenet/int_W8A8_5651/gpu_0
01/13/2023 15:46:04 - INFO - __main__ -   Namespace(a_low=75, a_up=150, abit=8, batch_size=128, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='csd_eb3', epoch=3, layer_4bit_l=None, layer_8bit_l='25,47,46,35,29,38,46,5,8,15', layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='resnet50', percent=100, ptq=False, resume=False, search=False, tag='', train=True, w_low=75, w_up=150, wbit=8)
01/13/2023 15:46:04 - INFO - __main__ -   ==> Preparing data..
01/13/2023 15:46:08 - INFO - __main__ -   ==> Setting quantizer..
Namespace(a_low=75, a_up=150, abit=8, batch_size=128, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='csd_eb3', epoch=3, layer_4bit_l=None, layer_8bit_l='25,47,46,35,29,38,46,5,8,15', layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='resnet50', percent=100, ptq=False, resume=False, search=False, tag='', train=True, w_low=75, w_up=150, wbit=8)
01/13/2023 15:46:08 - INFO - __main__ -   Namespace(a_low=75, a_up=150, abit=8, batch_size=128, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='csd_eb3', epoch=3, layer_4bit_l=None, layer_8bit_l='25,47,46,35,29,38,46,5,8,15', layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='resnet50', percent=100, ptq=False, resume=False, search=False, tag='', train=True, w_low=75, w_up=150, wbit=8)
01/13/2023 15:46:08 - INFO - __main__ -   ==> Building model..
ResNet(
  (conv1): Conv2dQuantizer(
    (quant_weight): TensorQuantizer()
    (quant_input): TensorQuantizer()
  )
  (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (1): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (1): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): LinearQuantizer(
    (quant_weight): TensorQuantizer()
    (quant_input): TensorQuantizer()
  )
)
01/13/2023 15:46:14 - INFO - __main__ -   {'milestones': Counter({2: 1, 3: 1, 4: 1}), 'gamma': 0.1, 'base_lrs': [0.00025], 'last_epoch': 0, '_step_count': 1, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.00025]}
01/13/2023 15:46:14 - INFO - __main__ -   
Epoch: 0
Layer quant EB csd_eb3
int	8-bit 	 conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.3.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.3.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.3.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.3.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.3.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.3.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.4.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.4.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.4.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.5.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.5.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.5.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 fc.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 fc.quant_input,
set init to 1
------------- 8-bit EB2 Re-SET -------------
10
layer1.1.conv1.quant_weight 5
layer1.1.conv1.quant_input 5
layer1.2.conv1.quant_weight 8
layer1.2.conv1.quant_input 8
layer2.1.conv1.quant_weight 15
layer2.1.conv1.quant_input 15
layer3.0.conv2.quant_weight 25
layer3.0.conv2.quant_input 25
layer3.1.conv2.quant_weight 29
layer3.1.conv2.quant_input 29
layer3.3.conv2.quant_weight 35
layer3.3.conv2.quant_input 35
layer3.4.conv2.quant_weight 38
layer3.4.conv2.quant_input 38
layer4.0.downsample.0.quant_weight 46
layer4.0.downsample.0.quant_input 46
layer4.1.conv1.quant_weight 47
layer4.1.conv1.quant_input 47
------------- 8-bit EB2 Re-SET -------------
Layer quant EB csd_eb3
int	8-bit 	 conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.3.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.3.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.3.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.3.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.3.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.3.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.4.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.4.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.4.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.5.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.5.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.5.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 fc.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 fc.quant_input,
set init to 1
01/13/2023 15:46:59 - INFO - __main__ -   test: [epoch: 0 | batch: 0/10010 ] | Loss: 0.672 | Acc: 82.031% (105/128)
01/13/2023 15:51:20 - INFO - __main__ -   test: [epoch: 0 | batch: 100/10010 ] | Loss: 0.841 | Acc: 79.316% (10254/12928)
01/13/2023 15:55:41 - INFO - __main__ -   test: [epoch: 0 | batch: 200/10010 ] | Loss: 0.825 | Acc: 79.392% (20426/25728)
01/13/2023 16:00:01 - INFO - __main__ -   test: [epoch: 0 | batch: 300/10010 ] | Loss: 0.839 | Acc: 79.078% (30467/38528)
01/13/2023 16:04:22 - INFO - __main__ -   test: [epoch: 0 | batch: 400/10010 ] | Loss: 0.850 | Acc: 78.793% (40443/51328)
01/13/2023 16:08:45 - INFO - __main__ -   test: [epoch: 0 | batch: 500/10010 ] | Loss: 0.851 | Acc: 78.671% (50450/64128)
01/13/2023 16:13:05 - INFO - __main__ -   test: [epoch: 0 | batch: 600/10010 ] | Loss: 0.852 | Acc: 78.638% (60495/76928)
01/13/2023 16:17:25 - INFO - __main__ -   test: [epoch: 0 | batch: 700/10010 ] | Loss: 0.853 | Acc: 78.655% (70576/89728)
01/13/2023 16:21:45 - INFO - __main__ -   test: [epoch: 0 | batch: 800/10010 ] | Loss: 0.854 | Acc: 78.617% (80604/102528)
01/13/2023 16:26:05 - INFO - __main__ -   test: [epoch: 0 | batch: 900/10010 ] | Loss: 0.855 | Acc: 78.600% (90648/115328)
01/13/2023 16:30:29 - INFO - __main__ -   test: [epoch: 0 | batch: 1000/10010 ] | Loss: 0.856 | Acc: 78.584% (100688/128128)
01/13/2023 16:34:49 - INFO - __main__ -   test: [epoch: 0 | batch: 1100/10010 ] | Loss: 0.858 | Acc: 78.588% (110752/140928)
01/13/2023 16:39:12 - INFO - __main__ -   test: [epoch: 0 | batch: 1200/10010 ] | Loss: 0.859 | Acc: 78.563% (120774/153728)
01/13/2023 16:43:32 - INFO - __main__ -   test: [epoch: 0 | batch: 1300/10010 ] | Loss: 0.858 | Acc: 78.616% (130917/166528)
01/13/2023 16:47:52 - INFO - __main__ -   test: [epoch: 0 | batch: 1400/10010 ] | Loss: 0.858 | Acc: 78.631% (141007/179328)
01/13/2023 16:52:13 - INFO - __main__ -   test: [epoch: 0 | batch: 1500/10010 ] | Loss: 0.856 | Acc: 78.685% (151176/192128)
01/13/2023 16:56:34 - INFO - __main__ -   test: [epoch: 0 | batch: 1600/10010 ] | Loss: 0.857 | Acc: 78.673% (161223/204928)
01/13/2023 17:00:56 - INFO - __main__ -   test: [epoch: 0 | batch: 1700/10010 ] | Loss: 0.857 | Acc: 78.666% (171278/217728)
01/13/2023 17:05:18 - INFO - __main__ -   test: [epoch: 0 | batch: 1800/10010 ] | Loss: 0.856 | Acc: 78.711% (181451/230528)
01/13/2023 17:09:40 - INFO - __main__ -   test: [epoch: 0 | batch: 1900/10010 ] | Loss: 0.854 | Acc: 78.717% (191541/243328)
01/13/2023 17:14:00 - INFO - __main__ -   test: [epoch: 0 | batch: 2000/10010 ] | Loss: 0.854 | Acc: 78.731% (201652/256128)
01/13/2023 17:18:22 - INFO - __main__ -   test: [epoch: 0 | batch: 2100/10010 ] | Loss: 0.855 | Acc: 78.715% (211686/268928)
01/13/2023 17:22:43 - INFO - __main__ -   test: [epoch: 0 | batch: 2200/10010 ] | Loss: 0.855 | Acc: 78.703% (221729/281728)
01/13/2023 17:27:05 - INFO - __main__ -   test: [epoch: 0 | batch: 2300/10010 ] | Loss: 0.855 | Acc: 78.694% (231776/294528)
01/13/2023 17:31:26 - INFO - __main__ -   test: [epoch: 0 | batch: 2400/10010 ] | Loss: 0.856 | Acc: 78.700% (241868/307328)
01/13/2023 17:35:46 - INFO - __main__ -   test: [epoch: 0 | batch: 2500/10010 ] | Loss: 0.856 | Acc: 78.702% (251948/320128)
01/13/2023 17:40:08 - INFO - __main__ -   test: [epoch: 0 | batch: 2600/10010 ] | Loss: 0.856 | Acc: 78.699% (262012/332928)
01/13/2023 17:44:29 - INFO - __main__ -   test: [epoch: 0 | batch: 2700/10010 ] | Loss: 0.855 | Acc: 78.702% (272095/345728)
01/13/2023 17:48:51 - INFO - __main__ -   test: [epoch: 0 | batch: 2800/10010 ] | Loss: 0.856 | Acc: 78.690% (282126/358528)
01/13/2023 17:53:11 - INFO - __main__ -   test: [epoch: 0 | batch: 2900/10010 ] | Loss: 0.856 | Acc: 78.687% (292187/371328)
01/13/2023 17:57:32 - INFO - __main__ -   test: [epoch: 0 | batch: 3000/10010 ] | Loss: 0.855 | Acc: 78.683% (302244/384128)
01/13/2023 18:01:54 - INFO - __main__ -   test: [epoch: 0 | batch: 3100/10010 ] | Loss: 0.856 | Acc: 78.682% (312312/396928)
01/13/2023 18:06:16 - INFO - __main__ -   test: [epoch: 0 | batch: 3200/10010 ] | Loss: 0.856 | Acc: 78.678% (322365/409728)
01/13/2023 18:10:36 - INFO - __main__ -   test: [epoch: 0 | batch: 3300/10010 ] | Loss: 0.856 | Acc: 78.668% (332396/422528)
01/13/2023 18:14:54 - INFO - __main__ -   test: [epoch: 0 | batch: 3400/10010 ] | Loss: 0.856 | Acc: 78.679% (342513/435328)
01/13/2023 18:19:14 - INFO - __main__ -   test: [epoch: 0 | batch: 3500/10010 ] | Loss: 0.856 | Acc: 78.686% (352615/448128)
01/13/2023 18:23:36 - INFO - __main__ -   test: [epoch: 0 | batch: 3600/10010 ] | Loss: 0.856 | Acc: 78.681% (362663/460928)
01/13/2023 18:27:55 - INFO - __main__ -   test: [epoch: 0 | batch: 3700/10010 ] | Loss: 0.856 | Acc: 78.670% (372682/473728)
01/13/2023 18:32:17 - INFO - __main__ -   test: [epoch: 0 | batch: 3800/10010 ] | Loss: 0.856 | Acc: 78.671% (382757/486528)
01/13/2023 18:36:38 - INFO - __main__ -   test: [epoch: 0 | batch: 3900/10010 ] | Loss: 0.857 | Acc: 78.655% (392748/499328)
01/13/2023 18:41:01 - INFO - __main__ -   test: [epoch: 0 | batch: 4000/10010 ] | Loss: 0.857 | Acc: 78.651% (402794/512128)
01/13/2023 18:45:22 - INFO - __main__ -   test: [epoch: 0 | batch: 4100/10010 ] | Loss: 0.857 | Acc: 78.656% (412885/524928)
01/13/2023 18:49:42 - INFO - __main__ -   test: [epoch: 0 | batch: 4200/10010 ] | Loss: 0.857 | Acc: 78.652% (422936/537728)
01/13/2023 18:54:02 - INFO - __main__ -   test: [epoch: 0 | batch: 4300/10010 ] | Loss: 0.857 | Acc: 78.653% (433008/550528)
01/13/2023 18:58:25 - INFO - __main__ -   test: [epoch: 0 | batch: 4400/10010 ] | Loss: 0.857 | Acc: 78.659% (443110/563328)
01/13/2023 19:02:48 - INFO - __main__ -   test: [epoch: 0 | batch: 4500/10010 ] | Loss: 0.857 | Acc: 78.652% (453135/576128)
01/13/2023 19:07:08 - INFO - __main__ -   test: [epoch: 0 | batch: 4600/10010 ] | Loss: 0.857 | Acc: 78.661% (463258/588928)
01/13/2023 19:11:30 - INFO - __main__ -   test: [epoch: 0 | batch: 4700/10010 ] | Loss: 0.857 | Acc: 78.661% (473328/601728)
01/13/2023 19:15:49 - INFO - __main__ -   test: [epoch: 0 | batch: 4800/10010 ] | Loss: 0.857 | Acc: 78.661% (483395/614528)
01/13/2023 19:20:09 - INFO - __main__ -   test: [epoch: 0 | batch: 4900/10010 ] | Loss: 0.857 | Acc: 78.664% (493479/627328)
01/13/2023 19:24:30 - INFO - __main__ -   test: [epoch: 0 | batch: 5000/10010 ] | Loss: 0.856 | Acc: 78.676% (503628/640128)
01/13/2023 19:28:51 - INFO - __main__ -   test: [epoch: 0 | batch: 5100/10010 ] | Loss: 0.856 | Acc: 78.680% (513723/652928)
01/13/2023 19:33:13 - INFO - __main__ -   test: [epoch: 0 | batch: 5200/10010 ] | Loss: 0.856 | Acc: 78.678% (523782/665728)
01/13/2023 19:37:34 - INFO - __main__ -   test: [epoch: 0 | batch: 5300/10010 ] | Loss: 0.857 | Acc: 78.682% (533879/678528)
01/13/2023 19:41:55 - INFO - __main__ -   test: [epoch: 0 | batch: 5400/10010 ] | Loss: 0.857 | Acc: 78.671% (543878/691328)
01/13/2023 19:46:15 - INFO - __main__ -   test: [epoch: 0 | batch: 5500/10010 ] | Loss: 0.857 | Acc: 78.661% (553872/704128)
01/13/2023 19:50:37 - INFO - __main__ -   test: [epoch: 0 | batch: 5600/10010 ] | Loss: 0.857 | Acc: 78.668% (563990/716928)
01/13/2023 19:54:58 - INFO - __main__ -   test: [epoch: 0 | batch: 5700/10010 ] | Loss: 0.857 | Acc: 78.670% (574075/729728)
01/13/2023 19:59:18 - INFO - __main__ -   test: [epoch: 0 | batch: 5800/10010 ] | Loss: 0.856 | Acc: 78.669% (584137/742528)
01/13/2023 20:03:42 - INFO - __main__ -   test: [epoch: 0 | batch: 5900/10010 ] | Loss: 0.856 | Acc: 78.674% (594243/755328)
01/13/2023 20:08:02 - INFO - __main__ -   test: [epoch: 0 | batch: 6000/10010 ] | Loss: 0.857 | Acc: 78.670% (604287/768128)
01/13/2023 20:12:21 - INFO - __main__ -   test: [epoch: 0 | batch: 6100/10010 ] | Loss: 0.857 | Acc: 78.670% (614357/780928)
01/13/2023 20:16:43 - INFO - __main__ -   test: [epoch: 0 | batch: 6200/10010 ] | Loss: 0.857 | Acc: 78.662% (624363/793728)
01/13/2023 20:21:04 - INFO - __main__ -   test: [epoch: 0 | batch: 6300/10010 ] | Loss: 0.857 | Acc: 78.661% (634426/806528)
01/13/2023 20:25:25 - INFO - __main__ -   test: [epoch: 0 | batch: 6400/10010 ] | Loss: 0.858 | Acc: 78.659% (644473/819328)
01/13/2023 20:29:49 - INFO - __main__ -   test: [epoch: 0 | batch: 6500/10010 ] | Loss: 0.858 | Acc: 78.658% (654533/832128)
01/13/2023 20:34:10 - INFO - __main__ -   test: [epoch: 0 | batch: 6600/10010 ] | Loss: 0.858 | Acc: 78.656% (664590/844928)
01/13/2023 20:38:30 - INFO - __main__ -   test: [epoch: 0 | batch: 6700/10010 ] | Loss: 0.857 | Acc: 78.660% (674690/857728)
01/13/2023 20:42:50 - INFO - __main__ -   test: [epoch: 0 | batch: 6800/10010 ] | Loss: 0.858 | Acc: 78.655% (684718/870528)
01/13/2023 20:47:14 - INFO - __main__ -   test: [epoch: 0 | batch: 6900/10010 ] | Loss: 0.857 | Acc: 78.660% (694827/883328)
01/13/2023 20:51:36 - INFO - __main__ -   test: [epoch: 0 | batch: 7000/10010 ] | Loss: 0.857 | Acc: 78.657% (704867/896128)
01/13/2023 20:55:57 - INFO - __main__ -   test: [epoch: 0 | batch: 7100/10010 ] | Loss: 0.857 | Acc: 78.666% (715021/908928)
01/13/2023 21:00:19 - INFO - __main__ -   test: [epoch: 0 | batch: 7200/10010 ] | Loss: 0.857 | Acc: 78.675% (725165/921728)
01/13/2023 21:04:40 - INFO - __main__ -   test: [epoch: 0 | batch: 7300/10010 ] | Loss: 0.857 | Acc: 78.665% (735151/934528)
01/13/2023 21:08:59 - INFO - __main__ -   test: [epoch: 0 | batch: 7400/10010 ] | Loss: 0.857 | Acc: 78.661% (745181/947328)
01/13/2023 21:13:22 - INFO - __main__ -   test: [epoch: 0 | batch: 7500/10010 ] | Loss: 0.857 | Acc: 78.664% (755278/960128)
01/13/2023 21:17:44 - INFO - __main__ -   test: [epoch: 0 | batch: 7600/10010 ] | Loss: 0.857 | Acc: 78.666% (765363/972928)
01/13/2023 21:22:06 - INFO - __main__ -   test: [epoch: 0 | batch: 7700/10010 ] | Loss: 0.858 | Acc: 78.655% (775320/985728)
01/13/2023 21:26:28 - INFO - __main__ -   test: [epoch: 0 | batch: 7800/10010 ] | Loss: 0.858 | Acc: 78.655% (785392/998528)
01/13/2023 21:30:53 - INFO - __main__ -   test: [epoch: 0 | batch: 7900/10010 ] | Loss: 0.858 | Acc: 78.658% (795490/1011328)
01/13/2023 21:35:12 - INFO - __main__ -   test: [epoch: 0 | batch: 8000/10010 ] | Loss: 0.858 | Acc: 78.654% (805521/1024128)
01/13/2023 21:39:33 - INFO - __main__ -   test: [epoch: 0 | batch: 8100/10010 ] | Loss: 0.858 | Acc: 78.659% (815634/1036928)
01/13/2023 21:43:55 - INFO - __main__ -   test: [epoch: 0 | batch: 8200/10010 ] | Loss: 0.858 | Acc: 78.653% (825646/1049728)
01/13/2023 21:48:15 - INFO - __main__ -   test: [epoch: 0 | batch: 8300/10010 ] | Loss: 0.858 | Acc: 78.654% (835724/1062528)
01/13/2023 21:52:36 - INFO - __main__ -   test: [epoch: 0 | batch: 8400/10010 ] | Loss: 0.858 | Acc: 78.656% (845810/1075328)
01/13/2023 21:56:57 - INFO - __main__ -   test: [epoch: 0 | batch: 8500/10010 ] | Loss: 0.857 | Acc: 78.660% (855923/1088128)
01/13/2023 22:01:17 - INFO - __main__ -   test: [epoch: 0 | batch: 8600/10010 ] | Loss: 0.858 | Acc: 78.654% (865923/1100928)
01/13/2023 22:05:39 - INFO - __main__ -   test: [epoch: 0 | batch: 8700/10010 ] | Loss: 0.858 | Acc: 78.653% (875980/1113728)
01/13/2023 22:10:01 - INFO - __main__ -   test: [epoch: 0 | batch: 8800/10010 ] | Loss: 0.858 | Acc: 78.653% (886048/1126528)
01/13/2023 22:14:25 - INFO - __main__ -   test: [epoch: 0 | batch: 8900/10010 ] | Loss: 0.858 | Acc: 78.658% (896167/1139328)
01/13/2023 22:18:45 - INFO - __main__ -   test: [epoch: 0 | batch: 9000/10010 ] | Loss: 0.857 | Acc: 78.661% (906272/1152128)
01/13/2023 22:23:07 - INFO - __main__ -   test: [epoch: 0 | batch: 9100/10010 ] | Loss: 0.857 | Acc: 78.659% (916325/1164928)
01/13/2023 22:27:27 - INFO - __main__ -   test: [epoch: 0 | batch: 9200/10010 ] | Loss: 0.857 | Acc: 78.660% (926402/1177728)
01/13/2023 22:31:46 - INFO - __main__ -   test: [epoch: 0 | batch: 9300/10010 ] | Loss: 0.857 | Acc: 78.664% (936514/1190528)
01/13/2023 22:36:08 - INFO - __main__ -   test: [epoch: 0 | batch: 9400/10010 ] | Loss: 0.857 | Acc: 78.663% (946574/1203328)
01/13/2023 22:40:29 - INFO - __main__ -   test: [epoch: 0 | batch: 9500/10010 ] | Loss: 0.857 | Acc: 78.667% (956696/1216128)
01/13/2023 22:44:50 - INFO - __main__ -   test: [epoch: 0 | batch: 9600/10010 ] | Loss: 0.858 | Acc: 78.667% (966756/1228928)
01/13/2023 22:49:12 - INFO - __main__ -   test: [epoch: 0 | batch: 9700/10010 ] | Loss: 0.858 | Acc: 78.664% (976796/1241728)
01/13/2023 22:53:34 - INFO - __main__ -   test: [epoch: 0 | batch: 9800/10010 ] | Loss: 0.858 | Acc: 78.660% (986814/1254528)
01/13/2023 22:57:54 - INFO - __main__ -   test: [epoch: 0 | batch: 9900/10010 ] | Loss: 0.858 | Acc: 78.664% (996931/1267328)
01/13/2023 23:02:18 - INFO - __main__ -   test: [epoch: 0 | batch: 10000/10010 ] | Loss: 0.858 | Acc: 78.661% (1006960/1280128)
01/13/2023 23:02:42 - INFO - __main__ -   Saving Checkpoint
01/13/2023 23:02:44 - INFO - __main__ -   test: [batch: 0/391 ] | Loss: 0.457 | Acc: 86.719% (111/128)/ 97.656% (125/128)
01/13/2023 23:02:47 - INFO - __main__ -   test: [batch: 1/391 ] | Loss: 0.471 | Acc: 86.328% (221/256)/ 98.047% (251/256)
01/13/2023 23:02:50 - INFO - __main__ -   test: [batch: 2/391 ] | Loss: 0.619 | Acc: 82.552% (317/384)/ 95.833% (368/384)
01/13/2023 23:02:52 - INFO - __main__ -   test: [batch: 3/391 ] | Loss: 0.571 | Acc: 84.375% (432/512)/ 96.484% (494/512)
01/13/2023 23:02:55 - INFO - __main__ -   test: [batch: 4/391 ] | Loss: 0.487 | Acc: 86.719% (555/640)/ 97.188% (622/640)
01/13/2023 23:02:58 - INFO - __main__ -   test: [batch: 5/391 ] | Loss: 0.431 | Acc: 87.891% (675/768)/ 97.656% (750/768)
01/13/2023 23:03:00 - INFO - __main__ -   test: [batch: 6/391 ] | Loss: 0.426 | Acc: 88.058% (789/896)/ 97.545% (874/896)
01/13/2023 23:03:03 - INFO - __main__ -   test: [batch: 7/391 ] | Loss: 0.407 | Acc: 89.062% (912/1024)/ 97.656% (1000/1024)
01/13/2023 23:03:06 - INFO - __main__ -   test: [batch: 8/391 ] | Loss: 0.424 | Acc: 89.062% (1026/1152)/ 97.569% (1124/1152)
01/13/2023 23:03:08 - INFO - __main__ -   test: [batch: 9/391 ] | Loss: 0.404 | Acc: 89.609% (1147/1280)/ 97.656% (1250/1280)
01/13/2023 23:03:11 - INFO - __main__ -   test: [batch: 10/391 ] | Loss: 0.454 | Acc: 88.352% (1244/1408)/ 97.514% (1373/1408)
01/13/2023 23:03:13 - INFO - __main__ -   test: [batch: 11/391 ] | Loss: 0.457 | Acc: 88.542% (1360/1536)/ 97.331% (1495/1536)
01/13/2023 23:03:16 - INFO - __main__ -   test: [batch: 12/391 ] | Loss: 0.505 | Acc: 87.500% (1456/1664)/ 96.995% (1614/1664)
01/13/2023 23:03:19 - INFO - __main__ -   test: [batch: 13/391 ] | Loss: 0.554 | Acc: 85.993% (1541/1792)/ 96.484% (1729/1792)
01/13/2023 23:03:22 - INFO - __main__ -   test: [batch: 14/391 ] | Loss: 0.575 | Acc: 85.208% (1636/1920)/ 96.562% (1854/1920)
01/13/2023 23:03:24 - INFO - __main__ -   test: [batch: 15/391 ] | Loss: 0.583 | Acc: 84.863% (1738/2048)/ 96.631% (1979/2048)
01/13/2023 23:03:27 - INFO - __main__ -   test: [batch: 16/391 ] | Loss: 0.592 | Acc: 84.651% (1842/2176)/ 96.461% (2099/2176)
01/13/2023 23:03:30 - INFO - __main__ -   test: [batch: 17/391 ] | Loss: 0.619 | Acc: 84.201% (1940/2304)/ 95.964% (2211/2304)
01/13/2023 23:03:32 - INFO - __main__ -   test: [batch: 18/391 ] | Loss: 0.641 | Acc: 83.635% (2034/2432)/ 95.806% (2330/2432)
01/13/2023 23:03:35 - INFO - __main__ -   test: [batch: 19/391 ] | Loss: 0.648 | Acc: 83.398% (2135/2560)/ 95.742% (2451/2560)
01/13/2023 23:03:37 - INFO - __main__ -   test: [batch: 20/391 ] | Loss: 0.646 | Acc: 83.482% (2244/2688)/ 95.647% (2571/2688)
01/13/2023 23:03:40 - INFO - __main__ -   test: [batch: 21/391 ] | Loss: 0.678 | Acc: 82.848% (2333/2816)/ 95.490% (2689/2816)
01/13/2023 23:03:43 - INFO - __main__ -   test: [batch: 22/391 ] | Loss: 0.677 | Acc: 82.711% (2435/2944)/ 95.516% (2812/2944)
01/13/2023 23:03:45 - INFO - __main__ -   test: [batch: 23/391 ] | Loss: 0.721 | Acc: 81.803% (2513/3072)/ 95.247% (2926/3072)
01/13/2023 23:03:48 - INFO - __main__ -   test: [batch: 24/391 ] | Loss: 0.740 | Acc: 81.375% (2604/3200)/ 95.094% (3043/3200)
01/13/2023 23:03:50 - INFO - __main__ -   test: [batch: 25/391 ] | Loss: 0.764 | Acc: 80.739% (2687/3328)/ 94.832% (3156/3328)
01/13/2023 23:03:53 - INFO - __main__ -   test: [batch: 26/391 ] | Loss: 0.778 | Acc: 80.035% (2766/3456)/ 94.821% (3277/3456)
01/13/2023 23:03:55 - INFO - __main__ -   test: [batch: 27/391 ] | Loss: 0.760 | Acc: 80.552% (2887/3584)/ 94.866% (3400/3584)
01/13/2023 23:03:58 - INFO - __main__ -   test: [batch: 28/391 ] | Loss: 0.768 | Acc: 80.011% (2970/3712)/ 94.935% (3524/3712)
01/13/2023 23:04:01 - INFO - __main__ -   test: [batch: 29/391 ] | Loss: 0.762 | Acc: 80.156% (3078/3840)/ 95.052% (3650/3840)
01/13/2023 23:04:03 - INFO - __main__ -   test: [batch: 30/391 ] | Loss: 0.776 | Acc: 80.066% (3177/3968)/ 94.934% (3767/3968)
01/13/2023 23:04:06 - INFO - __main__ -   test: [batch: 31/391 ] | Loss: 0.772 | Acc: 80.273% (3288/4096)/ 95.020% (3892/4096)
01/13/2023 23:04:09 - INFO - __main__ -   test: [batch: 32/391 ] | Loss: 0.757 | Acc: 80.563% (3403/4224)/ 95.099% (4017/4224)
01/13/2023 23:04:11 - INFO - __main__ -   test: [batch: 33/391 ] | Loss: 0.751 | Acc: 80.767% (3515/4352)/ 95.152% (4141/4352)
01/13/2023 23:04:14 - INFO - __main__ -   test: [batch: 34/391 ] | Loss: 0.737 | Acc: 81.138% (3635/4480)/ 95.223% (4266/4480)
01/13/2023 23:04:16 - INFO - __main__ -   test: [batch: 35/391 ] | Loss: 0.724 | Acc: 81.510% (3756/4608)/ 95.226% (4388/4608)
01/13/2023 23:04:19 - INFO - __main__ -   test: [batch: 36/391 ] | Loss: 0.709 | Acc: 81.947% (3881/4736)/ 95.334% (4515/4736)
01/13/2023 23:04:21 - INFO - __main__ -   test: [batch: 37/391 ] | Loss: 0.700 | Acc: 82.257% (4001/4864)/ 95.395% (4640/4864)
01/13/2023 23:04:24 - INFO - __main__ -   test: [batch: 38/391 ] | Loss: 0.692 | Acc: 82.372% (4112/4992)/ 95.473% (4766/4992)
01/13/2023 23:04:27 - INFO - __main__ -   test: [batch: 39/391 ] | Loss: 0.687 | Acc: 82.422% (4220/5120)/ 95.508% (4890/5120)
01/13/2023 23:04:29 - INFO - __main__ -   test: [batch: 40/391 ] | Loss: 0.686 | Acc: 82.546% (4332/5248)/ 95.408% (5007/5248)
01/13/2023 23:04:32 - INFO - __main__ -   test: [batch: 41/391 ] | Loss: 0.690 | Acc: 82.571% (4439/5376)/ 95.331% (5125/5376)
01/13/2023 23:04:35 - INFO - __main__ -   test: [batch: 42/391 ] | Loss: 0.690 | Acc: 82.522% (4542/5504)/ 95.385% (5250/5504)
01/13/2023 23:04:37 - INFO - __main__ -   test: [batch: 43/391 ] | Loss: 0.687 | Acc: 82.599% (4652/5632)/ 95.366% (5371/5632)
01/13/2023 23:04:40 - INFO - __main__ -   test: [batch: 44/391 ] | Loss: 0.691 | Acc: 82.604% (4758/5760)/ 95.278% (5488/5760)
01/13/2023 23:04:42 - INFO - __main__ -   test: [batch: 45/391 ] | Loss: 0.688 | Acc: 82.745% (4872/5888)/ 95.279% (5610/5888)
01/13/2023 23:04:45 - INFO - __main__ -   test: [batch: 46/391 ] | Loss: 0.691 | Acc: 82.696% (4975/6016)/ 95.296% (5733/6016)
01/13/2023 23:04:47 - INFO - __main__ -   test: [batch: 47/391 ] | Loss: 0.692 | Acc: 82.650% (5078/6144)/ 95.345% (5858/6144)
01/13/2023 23:04:50 - INFO - __main__ -   test: [batch: 48/391 ] | Loss: 0.697 | Acc: 82.494% (5174/6272)/ 95.328% (5979/6272)
01/13/2023 23:04:53 - INFO - __main__ -   test: [batch: 49/391 ] | Loss: 0.701 | Acc: 82.531% (5282/6400)/ 95.250% (6096/6400)
01/13/2023 23:04:55 - INFO - __main__ -   test: [batch: 50/391 ] | Loss: 0.692 | Acc: 82.736% (5401/6528)/ 95.297% (6221/6528)
01/13/2023 23:04:58 - INFO - __main__ -   test: [batch: 51/391 ] | Loss: 0.685 | Acc: 82.933% (5520/6656)/ 95.388% (6349/6656)
01/13/2023 23:05:00 - INFO - __main__ -   test: [batch: 52/391 ] | Loss: 0.681 | Acc: 83.019% (5632/6784)/ 95.416% (6473/6784)
01/13/2023 23:05:03 - INFO - __main__ -   test: [batch: 53/391 ] | Loss: 0.672 | Acc: 83.232% (5753/6912)/ 95.486% (6600/6912)
01/13/2023 23:05:05 - INFO - __main__ -   test: [batch: 54/391 ] | Loss: 0.665 | Acc: 83.366% (5869/7040)/ 95.511% (6724/7040)
01/13/2023 23:05:08 - INFO - __main__ -   test: [batch: 55/391 ] | Loss: 0.659 | Acc: 83.510% (5986/7168)/ 95.550% (6849/7168)
01/13/2023 23:05:11 - INFO - __main__ -   test: [batch: 56/391 ] | Loss: 0.651 | Acc: 83.717% (6108/7296)/ 95.600% (6975/7296)
01/13/2023 23:05:13 - INFO - __main__ -   test: [batch: 57/391 ] | Loss: 0.644 | Acc: 83.904% (6229/7424)/ 95.649% (7101/7424)
01/13/2023 23:05:16 - INFO - __main__ -   test: [batch: 58/391 ] | Loss: 0.642 | Acc: 83.965% (6341/7552)/ 95.657% (7224/7552)
01/13/2023 23:05:19 - INFO - __main__ -   test: [batch: 59/391 ] | Loss: 0.644 | Acc: 83.880% (6442/7680)/ 95.677% (7348/7680)
01/13/2023 23:05:21 - INFO - __main__ -   test: [batch: 60/391 ] | Loss: 0.649 | Acc: 83.773% (6541/7808)/ 95.645% (7468/7808)
01/13/2023 23:05:24 - INFO - __main__ -   test: [batch: 61/391 ] | Loss: 0.650 | Acc: 83.783% (6649/7936)/ 95.665% (7592/7936)
01/13/2023 23:05:27 - INFO - __main__ -   test: [batch: 62/391 ] | Loss: 0.649 | Acc: 83.743% (6753/8064)/ 95.685% (7716/8064)
01/13/2023 23:05:29 - INFO - __main__ -   test: [batch: 63/391 ] | Loss: 0.657 | Acc: 83.606% (6849/8192)/ 95.618% (7833/8192)
01/13/2023 23:05:32 - INFO - __main__ -   test: [batch: 64/391 ] | Loss: 0.665 | Acc: 83.377% (6937/8320)/ 95.565% (7951/8320)
01/13/2023 23:05:35 - INFO - __main__ -   test: [batch: 65/391 ] | Loss: 0.678 | Acc: 82.872% (7001/8448)/ 95.490% (8067/8448)
01/13/2023 23:05:37 - INFO - __main__ -   test: [batch: 66/391 ] | Loss: 0.683 | Acc: 82.859% (7106/8576)/ 95.452% (8186/8576)
01/13/2023 23:05:40 - INFO - __main__ -   test: [batch: 67/391 ] | Loss: 0.685 | Acc: 82.790% (7206/8704)/ 95.473% (8310/8704)
01/13/2023 23:05:42 - INFO - __main__ -   test: [batch: 68/391 ] | Loss: 0.686 | Acc: 82.722% (7306/8832)/ 95.494% (8434/8832)
01/13/2023 23:05:45 - INFO - __main__ -   test: [batch: 69/391 ] | Loss: 0.682 | Acc: 82.801% (7419/8960)/ 95.536% (8560/8960)
01/13/2023 23:05:48 - INFO - __main__ -   test: [batch: 70/391 ] | Loss: 0.683 | Acc: 82.735% (7519/9088)/ 95.544% (8683/9088)
01/13/2023 23:05:50 - INFO - __main__ -   test: [batch: 71/391 ] | Loss: 0.681 | Acc: 82.769% (7628/9216)/ 95.551% (8806/9216)
01/13/2023 23:05:53 - INFO - __main__ -   test: [batch: 72/391 ] | Loss: 0.684 | Acc: 82.663% (7724/9344)/ 95.569% (8930/9344)
01/13/2023 23:05:55 - INFO - __main__ -   test: [batch: 73/391 ] | Loss: 0.687 | Acc: 82.549% (7819/9472)/ 95.576% (9053/9472)
01/13/2023 23:05:58 - INFO - __main__ -   test: [batch: 74/391 ] | Loss: 0.688 | Acc: 82.521% (7922/9600)/ 95.562% (9174/9600)
01/13/2023 23:06:01 - INFO - __main__ -   test: [batch: 75/391 ] | Loss: 0.693 | Acc: 82.350% (8011/9728)/ 95.539% (9294/9728)
01/13/2023 23:06:03 - INFO - __main__ -   test: [batch: 76/391 ] | Loss: 0.694 | Acc: 82.325% (8114/9856)/ 95.536% (9416/9856)
01/13/2023 23:06:06 - INFO - __main__ -   test: [batch: 77/391 ] | Loss: 0.694 | Acc: 82.302% (8217/9984)/ 95.563% (9541/9984)
01/13/2023 23:06:09 - INFO - __main__ -   test: [batch: 78/391 ] | Loss: 0.693 | Acc: 82.199% (8312/10112)/ 95.609% (9668/10112)
01/13/2023 23:06:11 - INFO - __main__ -   test: [batch: 79/391 ] | Loss: 0.692 | Acc: 82.197% (8417/10240)/ 95.635% (9793/10240)
01/13/2023 23:06:14 - INFO - __main__ -   test: [batch: 80/391 ] | Loss: 0.691 | Acc: 82.186% (8521/10368)/ 95.640% (9916/10368)
01/13/2023 23:06:17 - INFO - __main__ -   test: [batch: 81/391 ] | Loss: 0.691 | Acc: 82.203% (8628/10496)/ 95.665% (10041/10496)
01/13/2023 23:06:19 - INFO - __main__ -   test: [batch: 82/391 ] | Loss: 0.690 | Acc: 82.210% (8734/10624)/ 95.661% (10163/10624)
01/13/2023 23:06:22 - INFO - __main__ -   test: [batch: 83/391 ] | Loss: 0.690 | Acc: 82.236% (8842/10752)/ 95.647% (10284/10752)
01/13/2023 23:06:24 - INFO - __main__ -   test: [batch: 84/391 ] | Loss: 0.686 | Acc: 82.353% (8960/10880)/ 95.680% (10410/10880)
01/13/2023 23:06:27 - INFO - __main__ -   test: [batch: 85/391 ] | Loss: 0.684 | Acc: 82.349% (9065/11008)/ 95.721% (10537/11008)
01/13/2023 23:06:30 - INFO - __main__ -   test: [batch: 86/391 ] | Loss: 0.688 | Acc: 82.274% (9162/11136)/ 95.690% (10656/11136)
01/13/2023 23:06:32 - INFO - __main__ -   test: [batch: 87/391 ] | Loss: 0.687 | Acc: 82.324% (9273/11264)/ 95.694% (10779/11264)
01/13/2023 23:06:35 - INFO - __main__ -   test: [batch: 88/391 ] | Loss: 0.694 | Acc: 82.277% (9373/11392)/ 95.629% (10894/11392)
01/13/2023 23:06:37 - INFO - __main__ -   test: [batch: 89/391 ] | Loss: 0.692 | Acc: 82.318% (9483/11520)/ 95.634% (11017/11520)
01/13/2023 23:06:40 - INFO - __main__ -   test: [batch: 90/391 ] | Loss: 0.693 | Acc: 82.212% (9576/11648)/ 95.639% (11140/11648)
01/13/2023 23:06:42 - INFO - __main__ -   test: [batch: 91/391 ] | Loss: 0.691 | Acc: 82.252% (9686/11776)/ 95.652% (11264/11776)
01/13/2023 23:06:45 - INFO - __main__ -   test: [batch: 92/391 ] | Loss: 0.693 | Acc: 82.216% (9787/11904)/ 95.640% (11385/11904)
01/13/2023 23:06:48 - INFO - __main__ -   test: [batch: 93/391 ] | Loss: 0.696 | Acc: 82.015% (9868/12032)/ 95.670% (11511/12032)
01/13/2023 23:06:50 - INFO - __main__ -   test: [batch: 94/391 ] | Loss: 0.698 | Acc: 81.891% (9958/12160)/ 95.683% (11635/12160)
01/13/2023 23:06:53 - INFO - __main__ -   test: [batch: 95/391 ] | Loss: 0.696 | Acc: 81.958% (10071/12288)/ 95.687% (11758/12288)
01/13/2023 23:06:55 - INFO - __main__ -   test: [batch: 96/391 ] | Loss: 0.699 | Acc: 81.870% (10165/12416)/ 95.699% (11882/12416)
01/13/2023 23:06:58 - INFO - __main__ -   test: [batch: 97/391 ] | Loss: 0.701 | Acc: 81.696% (10248/12544)/ 95.719% (12007/12544)
01/13/2023 23:07:01 - INFO - __main__ -   test: [batch: 98/391 ] | Loss: 0.697 | Acc: 81.795% (10365/12672)/ 95.747% (12133/12672)
01/13/2023 23:07:03 - INFO - __main__ -   test: [batch: 99/391 ] | Loss: 0.692 | Acc: 81.938% (10488/12800)/ 95.789% (12261/12800)
01/13/2023 23:07:06 - INFO - __main__ -   test: [batch: 100/391 ] | Loss: 0.691 | Acc: 81.985% (10599/12928)/ 95.808% (12386/12928)
01/13/2023 23:07:09 - INFO - __main__ -   test: [batch: 101/391 ] | Loss: 0.686 | Acc: 82.085% (10717/13056)/ 95.841% (12513/13056)
01/13/2023 23:07:11 - INFO - __main__ -   test: [batch: 102/391 ] | Loss: 0.683 | Acc: 82.206% (10838/13184)/ 95.866% (12639/13184)
01/13/2023 23:07:14 - INFO - __main__ -   test: [batch: 103/391 ] | Loss: 0.685 | Acc: 82.039% (10921/13312)/ 95.876% (12763/13312)
01/13/2023 23:07:16 - INFO - __main__ -   test: [batch: 104/391 ] | Loss: 0.684 | Acc: 81.987% (11019/13440)/ 95.871% (12885/13440)
01/13/2023 23:07:19 - INFO - __main__ -   test: [batch: 105/391 ] | Loss: 0.685 | Acc: 81.972% (11122/13568)/ 95.865% (13007/13568)
01/13/2023 23:07:21 - INFO - __main__ -   test: [batch: 106/391 ] | Loss: 0.693 | Acc: 81.863% (11212/13696)/ 95.794% (13120/13696)
01/13/2023 23:07:24 - INFO - __main__ -   test: [batch: 107/391 ] | Loss: 0.690 | Acc: 81.988% (11334/13824)/ 95.826% (13247/13824)
01/13/2023 23:07:27 - INFO - __main__ -   test: [batch: 108/391 ] | Loss: 0.693 | Acc: 81.845% (11419/13952)/ 95.836% (13371/13952)
01/13/2023 23:07:29 - INFO - __main__ -   test: [batch: 109/391 ] | Loss: 0.694 | Acc: 81.825% (11521/14080)/ 95.831% (13493/14080)
01/13/2023 23:07:32 - INFO - __main__ -   test: [batch: 110/391 ] | Loss: 0.697 | Acc: 81.644% (11600/14208)/ 95.840% (13617/14208)
01/13/2023 23:07:34 - INFO - __main__ -   test: [batch: 111/391 ] | Loss: 0.700 | Acc: 81.599% (11698/14336)/ 95.808% (13735/14336)
01/13/2023 23:07:37 - INFO - __main__ -   test: [batch: 112/391 ] | Loss: 0.700 | Acc: 81.616% (11805/14464)/ 95.824% (13860/14464)
01/13/2023 23:07:40 - INFO - __main__ -   test: [batch: 113/391 ] | Loss: 0.700 | Acc: 81.661% (11916/14592)/ 95.826% (13983/14592)
01/13/2023 23:07:43 - INFO - __main__ -   test: [batch: 114/391 ] | Loss: 0.696 | Acc: 81.753% (12034/14720)/ 95.849% (14109/14720)
01/13/2023 23:07:45 - INFO - __main__ -   test: [batch: 115/391 ] | Loss: 0.693 | Acc: 81.843% (12152/14848)/ 95.871% (14235/14848)
01/13/2023 23:07:48 - INFO - __main__ -   test: [batch: 116/391 ] | Loss: 0.696 | Acc: 81.818% (12253/14976)/ 95.840% (14353/14976)
01/13/2023 23:07:50 - INFO - __main__ -   test: [batch: 117/391 ] | Loss: 0.695 | Acc: 81.833% (12360/15104)/ 95.849% (14477/15104)
01/13/2023 23:07:53 - INFO - __main__ -   test: [batch: 118/391 ] | Loss: 0.698 | Acc: 81.723% (12448/15232)/ 95.864% (14602/15232)
01/13/2023 23:07:56 - INFO - __main__ -   test: [batch: 119/391 ] | Loss: 0.696 | Acc: 81.771% (12560/15360)/ 95.885% (14728/15360)
01/13/2023 23:07:58 - INFO - __main__ -   test: [batch: 120/391 ] | Loss: 0.695 | Acc: 81.818% (12672/15488)/ 95.900% (14853/15488)
01/13/2023 23:08:01 - INFO - __main__ -   test: [batch: 121/391 ] | Loss: 0.700 | Acc: 81.666% (12753/15616)/ 95.863% (14970/15616)
01/13/2023 23:08:04 - INFO - __main__ -   test: [batch: 122/391 ] | Loss: 0.702 | Acc: 81.612% (12849/15744)/ 95.833% (15088/15744)
01/13/2023 23:08:06 - INFO - __main__ -   test: [batch: 123/391 ] | Loss: 0.703 | Acc: 81.628% (12956/15872)/ 95.823% (15209/15872)
01/13/2023 23:08:09 - INFO - __main__ -   test: [batch: 124/391 ] | Loss: 0.702 | Acc: 81.638% (13062/16000)/ 95.838% (15334/16000)
01/13/2023 23:08:12 - INFO - __main__ -   test: [batch: 125/391 ] | Loss: 0.698 | Acc: 81.734% (13182/16128)/ 95.864% (15461/16128)
01/13/2023 23:08:14 - INFO - __main__ -   test: [batch: 126/391 ] | Loss: 0.695 | Acc: 81.822% (13301/16256)/ 95.878% (15586/16256)
01/13/2023 23:08:17 - INFO - __main__ -   test: [batch: 127/391 ] | Loss: 0.692 | Acc: 81.903% (13419/16384)/ 95.886% (15710/16384)
01/13/2023 23:08:19 - INFO - __main__ -   test: [batch: 128/391 ] | Loss: 0.693 | Acc: 81.886% (13521/16512)/ 95.858% (15828/16512)
01/13/2023 23:08:22 - INFO - __main__ -   test: [batch: 129/391 ] | Loss: 0.692 | Acc: 81.899% (13628/16640)/ 95.865% (15952/16640)
01/13/2023 23:08:25 - INFO - __main__ -   test: [batch: 130/391 ] | Loss: 0.689 | Acc: 82.001% (13750/16768)/ 95.885% (16078/16768)
01/13/2023 23:08:27 - INFO - __main__ -   test: [batch: 131/391 ] | Loss: 0.689 | Acc: 82.049% (13863/16896)/ 95.893% (16202/16896)
01/13/2023 23:08:30 - INFO - __main__ -   test: [batch: 132/391 ] | Loss: 0.686 | Acc: 82.119% (13980/17024)/ 95.906% (16327/17024)
01/13/2023 23:08:32 - INFO - __main__ -   test: [batch: 133/391 ] | Loss: 0.688 | Acc: 82.072% (14077/17152)/ 95.890% (16447/17152)
01/13/2023 23:08:35 - INFO - __main__ -   test: [batch: 134/391 ] | Loss: 0.685 | Acc: 82.101% (14187/17280)/ 95.914% (16574/17280)
01/13/2023 23:08:38 - INFO - __main__ -   test: [batch: 135/391 ] | Loss: 0.686 | Acc: 82.094% (14291/17408)/ 95.910% (16696/17408)
01/13/2023 23:08:40 - INFO - __main__ -   test: [batch: 136/391 ] | Loss: 0.684 | Acc: 82.060% (14390/17536)/ 95.934% (16823/17536)
01/13/2023 23:08:43 - INFO - __main__ -   test: [batch: 137/391 ] | Loss: 0.682 | Acc: 82.099% (14502/17664)/ 95.964% (16951/17664)
01/13/2023 23:08:45 - INFO - __main__ -   test: [batch: 138/391 ] | Loss: 0.681 | Acc: 82.116% (14610/17792)/ 95.970% (17075/17792)
01/13/2023 23:08:48 - INFO - __main__ -   test: [batch: 139/391 ] | Loss: 0.687 | Acc: 81.959% (14687/17920)/ 95.960% (17196/17920)
01/13/2023 23:08:51 - INFO - __main__ -   test: [batch: 140/391 ] | Loss: 0.690 | Acc: 81.898% (14781/18048)/ 95.955% (17318/18048)
01/13/2023 23:08:53 - INFO - __main__ -   test: [batch: 141/391 ] | Loss: 0.688 | Acc: 81.938% (14893/18176)/ 95.967% (17443/18176)
01/13/2023 23:08:56 - INFO - __main__ -   test: [batch: 142/391 ] | Loss: 0.686 | Acc: 82.020% (15013/18304)/ 95.979% (17568/18304)
01/13/2023 23:08:59 - INFO - __main__ -   test: [batch: 143/391 ] | Loss: 0.686 | Acc: 82.037% (15121/18432)/ 95.964% (17688/18432)
01/13/2023 23:09:01 - INFO - __main__ -   test: [batch: 144/391 ] | Loss: 0.688 | Acc: 82.010% (15221/18560)/ 95.943% (17807/18560)
01/13/2023 23:09:04 - INFO - __main__ -   test: [batch: 145/391 ] | Loss: 0.691 | Acc: 81.988% (15322/18688)/ 95.917% (17925/18688)
01/13/2023 23:09:06 - INFO - __main__ -   test: [batch: 146/391 ] | Loss: 0.692 | Acc: 81.967% (15423/18816)/ 95.902% (18045/18816)
01/13/2023 23:09:09 - INFO - __main__ -   test: [batch: 147/391 ] | Loss: 0.692 | Acc: 81.968% (15528/18944)/ 95.883% (18164/18944)
01/13/2023 23:09:12 - INFO - __main__ -   test: [batch: 148/391 ] | Loss: 0.694 | Acc: 81.905% (15621/19072)/ 95.884% (18287/19072)
01/13/2023 23:09:15 - INFO - __main__ -   test: [batch: 149/391 ] | Loss: 0.698 | Acc: 81.802% (15706/19200)/ 95.849% (18403/19200)
01/13/2023 23:09:17 - INFO - __main__ -   test: [batch: 150/391 ] | Loss: 0.698 | Acc: 81.736% (15798/19328)/ 95.856% (18527/19328)
01/13/2023 23:09:20 - INFO - __main__ -   test: [batch: 151/391 ] | Loss: 0.698 | Acc: 81.769% (15909/19456)/ 95.857% (18650/19456)
01/13/2023 23:09:22 - INFO - __main__ -   test: [batch: 152/391 ] | Loss: 0.699 | Acc: 81.756% (16011/19584)/ 95.849% (18771/19584)
01/13/2023 23:09:25 - INFO - __main__ -   test: [batch: 153/391 ] | Loss: 0.697 | Acc: 81.818% (16128/19712)/ 95.850% (18894/19712)
01/13/2023 23:09:28 - INFO - __main__ -   test: [batch: 154/391 ] | Loss: 0.697 | Acc: 81.804% (16230/19840)/ 95.822% (19011/19840)
01/13/2023 23:09:30 - INFO - __main__ -   test: [batch: 155/391 ] | Loss: 0.698 | Acc: 81.821% (16338/19968)/ 95.813% (19132/19968)
01/13/2023 23:09:33 - INFO - __main__ -   test: [batch: 156/391 ] | Loss: 0.700 | Acc: 81.738% (16426/20096)/ 95.800% (19252/20096)
01/13/2023 23:09:36 - INFO - __main__ -   test: [batch: 157/391 ] | Loss: 0.701 | Acc: 81.725% (16528/20224)/ 95.777% (19370/20224)
01/13/2023 23:09:38 - INFO - __main__ -   test: [batch: 158/391 ] | Loss: 0.702 | Acc: 81.682% (16624/20352)/ 95.755% (19488/20352)
01/13/2023 23:09:41 - INFO - __main__ -   test: [batch: 159/391 ] | Loss: 0.704 | Acc: 81.655% (16723/20480)/ 95.747% (19609/20480)
01/13/2023 23:09:44 - INFO - __main__ -   test: [batch: 160/391 ] | Loss: 0.704 | Acc: 81.633% (16823/20608)/ 95.735% (19729/20608)
01/13/2023 23:09:46 - INFO - __main__ -   test: [batch: 161/391 ] | Loss: 0.714 | Acc: 81.419% (16883/20736)/ 95.645% (19833/20736)
01/13/2023 23:09:48 - INFO - __main__ -   test: [batch: 162/391 ] | Loss: 0.719 | Acc: 81.322% (16967/20864)/ 95.576% (19941/20864)
01/13/2023 23:09:51 - INFO - __main__ -   test: [batch: 163/391 ] | Loss: 0.722 | Acc: 81.260% (17058/20992)/ 95.560% (20060/20992)
01/13/2023 23:09:54 - INFO - __main__ -   test: [batch: 164/391 ] | Loss: 0.722 | Acc: 81.250% (17160/21120)/ 95.578% (20186/21120)
01/13/2023 23:09:56 - INFO - __main__ -   test: [batch: 165/391 ] | Loss: 0.724 | Acc: 81.175% (17248/21248)/ 95.576% (20308/21248)
01/13/2023 23:09:59 - INFO - __main__ -   test: [batch: 166/391 ] | Loss: 0.723 | Acc: 81.185% (17354/21376)/ 95.560% (20427/21376)
01/13/2023 23:10:01 - INFO - __main__ -   test: [batch: 167/391 ] | Loss: 0.725 | Acc: 81.124% (17445/21504)/ 95.545% (20546/21504)
01/13/2023 23:10:04 - INFO - __main__ -   test: [batch: 168/391 ] | Loss: 0.725 | Acc: 81.130% (17550/21632)/ 95.548% (20669/21632)
01/13/2023 23:10:06 - INFO - __main__ -   test: [batch: 169/391 ] | Loss: 0.727 | Acc: 81.098% (17647/21760)/ 95.515% (20784/21760)
01/13/2023 23:10:09 - INFO - __main__ -   test: [batch: 170/391 ] | Loss: 0.732 | Acc: 80.990% (17727/21888)/ 95.472% (20897/21888)
01/13/2023 23:10:12 - INFO - __main__ -   test: [batch: 171/391 ] | Loss: 0.735 | Acc: 80.937% (17819/22016)/ 95.444% (21013/22016)
01/13/2023 23:10:14 - INFO - __main__ -   test: [batch: 172/391 ] | Loss: 0.736 | Acc: 80.889% (17912/22144)/ 95.443% (21135/22144)
01/13/2023 23:10:17 - INFO - __main__ -   test: [batch: 173/391 ] | Loss: 0.738 | Acc: 80.832% (18003/22272)/ 95.416% (21251/22272)
01/13/2023 23:10:19 - INFO - __main__ -   test: [batch: 174/391 ] | Loss: 0.742 | Acc: 80.741% (18086/22400)/ 95.375% (21364/22400)
01/13/2023 23:10:22 - INFO - __main__ -   test: [batch: 175/391 ] | Loss: 0.741 | Acc: 80.788% (18200/22528)/ 95.379% (21487/22528)
01/13/2023 23:10:24 - INFO - __main__ -   test: [batch: 176/391 ] | Loss: 0.741 | Acc: 80.791% (18304/22656)/ 95.361% (21605/22656)
01/13/2023 23:10:27 - INFO - __main__ -   test: [batch: 177/391 ] | Loss: 0.745 | Acc: 80.719% (18391/22784)/ 95.321% (21718/22784)
01/13/2023 23:10:30 - INFO - __main__ -   test: [batch: 178/391 ] | Loss: 0.746 | Acc: 80.700% (18490/22912)/ 95.299% (21835/22912)
01/13/2023 23:10:32 - INFO - __main__ -   test: [batch: 179/391 ] | Loss: 0.749 | Acc: 80.647% (18581/23040)/ 95.265% (21949/23040)
01/13/2023 23:10:35 - INFO - __main__ -   test: [batch: 180/391 ] | Loss: 0.755 | Acc: 80.521% (18655/23168)/ 95.226% (22062/23168)
01/13/2023 23:10:37 - INFO - __main__ -   test: [batch: 181/391 ] | Loss: 0.762 | Acc: 80.383% (18726/23296)/ 95.171% (22171/23296)
01/13/2023 23:10:40 - INFO - __main__ -   test: [batch: 182/391 ] | Loss: 0.761 | Acc: 80.396% (18832/23424)/ 95.172% (22293/23424)
01/13/2023 23:10:43 - INFO - __main__ -   test: [batch: 183/391 ] | Loss: 0.767 | Acc: 80.265% (18904/23552)/ 95.083% (22394/23552)
01/13/2023 23:10:45 - INFO - __main__ -   test: [batch: 184/391 ] | Loss: 0.767 | Acc: 80.291% (19013/23680)/ 95.072% (22513/23680)
01/13/2023 23:10:48 - INFO - __main__ -   test: [batch: 185/391 ] | Loss: 0.766 | Acc: 80.305% (19119/23808)/ 95.065% (22633/23808)
01/13/2023 23:10:50 - INFO - __main__ -   test: [batch: 186/391 ] | Loss: 0.769 | Acc: 80.268% (19213/23936)/ 95.020% (22744/23936)
01/13/2023 23:10:53 - INFO - __main__ -   test: [batch: 187/391 ] | Loss: 0.774 | Acc: 80.153% (19288/24064)/ 94.993% (22859/24064)
01/13/2023 23:10:55 - INFO - __main__ -   test: [batch: 188/391 ] | Loss: 0.778 | Acc: 80.018% (19358/24192)/ 94.974% (22976/24192)
01/13/2023 23:10:58 - INFO - __main__ -   test: [batch: 189/391 ] | Loss: 0.779 | Acc: 79.951% (19444/24320)/ 94.975% (23098/24320)
01/13/2023 23:11:01 - INFO - __main__ -   test: [batch: 190/391 ] | Loss: 0.784 | Acc: 79.863% (19525/24448)/ 94.953% (23214/24448)
01/13/2023 23:11:03 - INFO - __main__ -   test: [batch: 191/391 ] | Loss: 0.786 | Acc: 79.834% (19620/24576)/ 94.930% (23330/24576)
01/13/2023 23:11:06 - INFO - __main__ -   test: [batch: 192/391 ] | Loss: 0.792 | Acc: 79.716% (19693/24704)/ 94.847% (23431/24704)
01/13/2023 23:11:09 - INFO - __main__ -   test: [batch: 193/391 ] | Loss: 0.792 | Acc: 79.732% (19799/24832)/ 94.845% (23552/24832)
01/13/2023 23:11:11 - INFO - __main__ -   test: [batch: 194/391 ] | Loss: 0.795 | Acc: 79.667% (19885/24960)/ 94.824% (23668/24960)
01/13/2023 23:11:14 - INFO - __main__ -   test: [batch: 195/391 ] | Loss: 0.800 | Acc: 79.596% (19969/25088)/ 94.762% (23774/25088)
01/13/2023 23:11:17 - INFO - __main__ -   test: [batch: 196/391 ] | Loss: 0.804 | Acc: 79.493% (20045/25216)/ 94.714% (23883/25216)
01/13/2023 23:11:19 - INFO - __main__ -   test: [batch: 197/391 ] | Loss: 0.808 | Acc: 79.415% (20127/25344)/ 94.697% (24000/25344)
01/13/2023 23:11:22 - INFO - __main__ -   test: [batch: 198/391 ] | Loss: 0.810 | Acc: 79.373% (20218/25472)/ 94.665% (24113/25472)
01/13/2023 23:11:24 - INFO - __main__ -   test: [batch: 199/391 ] | Loss: 0.810 | Acc: 79.359% (20316/25600)/ 94.672% (24236/25600)
01/13/2023 23:11:27 - INFO - __main__ -   test: [batch: 200/391 ] | Loss: 0.809 | Acc: 79.330% (20410/25728)/ 94.671% (24357/25728)
01/13/2023 23:11:30 - INFO - __main__ -   test: [batch: 201/391 ] | Loss: 0.813 | Acc: 79.247% (20490/25856)/ 94.636% (24469/25856)
01/13/2023 23:11:32 - INFO - __main__ -   test: [batch: 202/391 ] | Loss: 0.814 | Acc: 79.237% (20589/25984)/ 94.624% (24587/25984)
01/13/2023 23:11:35 - INFO - __main__ -   test: [batch: 203/391 ] | Loss: 0.815 | Acc: 79.228% (20688/26112)/ 94.616% (24706/26112)
01/13/2023 23:11:37 - INFO - __main__ -   test: [batch: 204/391 ] | Loss: 0.818 | Acc: 79.143% (20767/26240)/ 94.592% (24821/26240)
01/13/2023 23:11:40 - INFO - __main__ -   test: [batch: 205/391 ] | Loss: 0.821 | Acc: 79.058% (20846/26368)/ 94.565% (24935/26368)
01/13/2023 23:11:43 - INFO - __main__ -   test: [batch: 206/391 ] | Loss: 0.822 | Acc: 79.038% (20942/26496)/ 94.558% (25054/26496)
01/13/2023 23:11:45 - INFO - __main__ -   test: [batch: 207/391 ] | Loss: 0.826 | Acc: 78.955% (21021/26624)/ 94.520% (25165/26624)
01/13/2023 23:11:48 - INFO - __main__ -   test: [batch: 208/391 ] | Loss: 0.828 | Acc: 78.917% (21112/26752)/ 94.494% (25279/26752)
01/13/2023 23:11:51 - INFO - __main__ -   test: [batch: 209/391 ] | Loss: 0.828 | Acc: 78.914% (21212/26880)/ 94.513% (25405/26880)
01/13/2023 23:11:53 - INFO - __main__ -   test: [batch: 210/391 ] | Loss: 0.829 | Acc: 78.895% (21308/27008)/ 94.494% (25521/27008)
01/13/2023 23:11:56 - INFO - __main__ -   test: [batch: 211/391 ] | Loss: 0.831 | Acc: 78.829% (21391/27136)/ 94.472% (25636/27136)
01/13/2023 23:11:59 - INFO - __main__ -   test: [batch: 212/391 ] | Loss: 0.833 | Acc: 78.767% (21475/27264)/ 94.451% (25751/27264)
01/13/2023 23:12:01 - INFO - __main__ -   test: [batch: 213/391 ] | Loss: 0.833 | Acc: 78.775% (21578/27392)/ 94.455% (25873/27392)
01/13/2023 23:12:04 - INFO - __main__ -   test: [batch: 214/391 ] | Loss: 0.833 | Acc: 78.754% (21673/27520)/ 94.459% (25995/27520)
01/13/2023 23:12:06 - INFO - __main__ -   test: [batch: 215/391 ] | Loss: 0.833 | Acc: 78.780% (21781/27648)/ 94.452% (26114/27648)
01/13/2023 23:12:09 - INFO - __main__ -   test: [batch: 216/391 ] | Loss: 0.831 | Acc: 78.820% (21893/27776)/ 94.474% (26241/27776)
01/13/2023 23:12:12 - INFO - __main__ -   test: [batch: 217/391 ] | Loss: 0.836 | Acc: 78.749% (21974/27904)/ 94.413% (26345/27904)
01/13/2023 23:12:14 - INFO - __main__ -   test: [batch: 218/391 ] | Loss: 0.839 | Acc: 78.682% (22056/28032)/ 94.374% (26455/28032)
01/13/2023 23:12:17 - INFO - __main__ -   test: [batch: 219/391 ] | Loss: 0.838 | Acc: 78.718% (22167/28160)/ 94.379% (26577/28160)
01/13/2023 23:12:19 - INFO - __main__ -   test: [batch: 220/391 ] | Loss: 0.836 | Acc: 78.747% (22276/28288)/ 94.393% (26702/28288)
01/13/2023 23:12:22 - INFO - __main__ -   test: [batch: 221/391 ] | Loss: 0.838 | Acc: 78.706% (22365/28416)/ 94.376% (26818/28416)
01/13/2023 23:12:25 - INFO - __main__ -   test: [batch: 222/391 ] | Loss: 0.836 | Acc: 78.763% (22482/28544)/ 94.391% (26943/28544)
01/13/2023 23:12:27 - INFO - __main__ -   test: [batch: 223/391 ] | Loss: 0.836 | Acc: 78.763% (22583/28672)/ 94.381% (27061/28672)
01/13/2023 23:12:30 - INFO - __main__ -   test: [batch: 224/391 ] | Loss: 0.836 | Acc: 78.785% (22690/28800)/ 94.375% (27180/28800)
01/13/2023 23:12:33 - INFO - __main__ -   test: [batch: 225/391 ] | Loss: 0.835 | Acc: 78.785% (22791/28928)/ 94.379% (27302/28928)
01/13/2023 23:12:35 - INFO - __main__ -   test: [batch: 226/391 ] | Loss: 0.835 | Acc: 78.755% (22883/29056)/ 94.383% (27424/29056)
01/13/2023 23:12:38 - INFO - __main__ -   test: [batch: 227/391 ] | Loss: 0.837 | Acc: 78.742% (22980/29184)/ 94.367% (27540/29184)
01/13/2023 23:12:41 - INFO - __main__ -   test: [batch: 228/391 ] | Loss: 0.841 | Acc: 78.674% (23061/29312)/ 94.309% (27644/29312)
01/13/2023 23:12:43 - INFO - __main__ -   test: [batch: 229/391 ] | Loss: 0.845 | Acc: 78.618% (23145/29440)/ 94.263% (27751/29440)
01/13/2023 23:12:46 - INFO - __main__ -   test: [batch: 230/391 ] | Loss: 0.847 | Acc: 78.565% (23230/29568)/ 94.227% (27861/29568)
01/13/2023 23:12:49 - INFO - __main__ -   test: [batch: 231/391 ] | Loss: 0.848 | Acc: 78.553% (23327/29696)/ 94.211% (27977/29696)
01/13/2023 23:12:51 - INFO - __main__ -   test: [batch: 232/391 ] | Loss: 0.846 | Acc: 78.578% (23435/29824)/ 94.226% (28102/29824)
01/13/2023 23:12:54 - INFO - __main__ -   test: [batch: 233/391 ] | Loss: 0.849 | Acc: 78.526% (23520/29952)/ 94.204% (28216/29952)
01/13/2023 23:12:57 - INFO - __main__ -   test: [batch: 234/391 ] | Loss: 0.855 | Acc: 78.414% (23587/30080)/ 94.132% (28315/30080)
01/13/2023 23:12:59 - INFO - __main__ -   test: [batch: 235/391 ] | Loss: 0.855 | Acc: 78.400% (23683/30208)/ 94.134% (28436/30208)
01/13/2023 23:13:02 - INFO - __main__ -   test: [batch: 236/391 ] | Loss: 0.854 | Acc: 78.441% (23796/30336)/ 94.142% (28559/30336)
01/13/2023 23:13:04 - INFO - __main__ -   test: [batch: 237/391 ] | Loss: 0.854 | Acc: 78.447% (23898/30464)/ 94.114% (28671/30464)
01/13/2023 23:13:07 - INFO - __main__ -   test: [batch: 238/391 ] | Loss: 0.853 | Acc: 78.488% (24011/30592)/ 94.123% (28794/30592)
01/13/2023 23:13:10 - INFO - __main__ -   test: [batch: 239/391 ] | Loss: 0.851 | Acc: 78.525% (24123/30720)/ 94.124% (28915/30720)
01/13/2023 23:13:12 - INFO - __main__ -   test: [batch: 240/391 ] | Loss: 0.852 | Acc: 78.540% (24228/30848)/ 94.110% (29031/30848)
01/13/2023 23:13:15 - INFO - __main__ -   test: [batch: 241/391 ] | Loss: 0.855 | Acc: 78.448% (24300/30976)/ 94.066% (29138/30976)
01/13/2023 23:13:17 - INFO - __main__ -   test: [batch: 242/391 ] | Loss: 0.857 | Acc: 78.347% (24369/31104)/ 94.046% (29252/31104)
01/13/2023 23:13:20 - INFO - __main__ -   test: [batch: 243/391 ] | Loss: 0.863 | Acc: 78.221% (24430/31232)/ 93.984% (29353/31232)
01/13/2023 23:13:23 - INFO - __main__ -   test: [batch: 244/391 ] | Loss: 0.863 | Acc: 78.230% (24533/31360)/ 93.980% (29472/31360)
01/13/2023 23:13:25 - INFO - __main__ -   test: [batch: 245/391 ] | Loss: 0.863 | Acc: 78.243% (24637/31488)/ 93.969% (29589/31488)
01/13/2023 23:13:28 - INFO - __main__ -   test: [batch: 246/391 ] | Loss: 0.863 | Acc: 78.226% (24732/31616)/ 93.959% (29706/31616)
01/13/2023 23:13:31 - INFO - __main__ -   test: [batch: 247/391 ] | Loss: 0.869 | Acc: 78.134% (24803/31744)/ 93.885% (29803/31744)
01/13/2023 23:13:33 - INFO - __main__ -   test: [batch: 248/391 ] | Loss: 0.870 | Acc: 78.100% (24892/31872)/ 93.879% (29921/31872)
01/13/2023 23:13:36 - INFO - __main__ -   test: [batch: 249/391 ] | Loss: 0.872 | Acc: 77.978% (24953/32000)/ 93.869% (30038/32000)
01/13/2023 23:13:38 - INFO - __main__ -   test: [batch: 250/391 ] | Loss: 0.870 | Acc: 78.007% (25062/32128)/ 93.887% (30164/32128)
01/13/2023 23:13:41 - INFO - __main__ -   test: [batch: 251/391 ] | Loss: 0.873 | Acc: 77.958% (25146/32256)/ 93.859% (30275/32256)
01/13/2023 23:13:43 - INFO - __main__ -   test: [batch: 252/391 ] | Loss: 0.872 | Acc: 77.989% (25256/32384)/ 93.852% (30393/32384)
01/13/2023 23:13:46 - INFO - __main__ -   test: [batch: 253/391 ] | Loss: 0.873 | Acc: 77.962% (25347/32512)/ 93.836% (30508/32512)
01/13/2023 23:13:49 - INFO - __main__ -   test: [batch: 254/391 ] | Loss: 0.877 | Acc: 77.898% (25426/32640)/ 93.793% (30614/32640)
01/13/2023 23:13:51 - INFO - __main__ -   test: [batch: 255/391 ] | Loss: 0.879 | Acc: 77.866% (25515/32768)/ 93.784% (30731/32768)
01/13/2023 23:13:54 - INFO - __main__ -   test: [batch: 256/391 ] | Loss: 0.884 | Acc: 77.745% (25575/32896)/ 93.750% (30840/32896)
01/13/2023 23:13:56 - INFO - __main__ -   test: [batch: 257/391 ] | Loss: 0.885 | Acc: 77.731% (25670/33024)/ 93.738% (30956/33024)
01/13/2023 23:13:59 - INFO - __main__ -   test: [batch: 258/391 ] | Loss: 0.886 | Acc: 77.727% (25768/33152)/ 93.723% (31071/33152)
01/13/2023 23:14:02 - INFO - __main__ -   test: [batch: 259/391 ] | Loss: 0.890 | Acc: 77.608% (25828/33280)/ 93.708% (31186/33280)
01/13/2023 23:14:04 - INFO - __main__ -   test: [batch: 260/391 ] | Loss: 0.891 | Acc: 77.577% (25917/33408)/ 93.702% (31304/33408)
01/13/2023 23:14:07 - INFO - __main__ -   test: [batch: 261/391 ] | Loss: 0.888 | Acc: 77.624% (26032/33536)/ 93.723% (31431/33536)
01/13/2023 23:14:10 - INFO - __main__ -   test: [batch: 262/391 ] | Loss: 0.887 | Acc: 77.647% (26139/33664)/ 93.732% (31554/33664)
01/13/2023 23:14:12 - INFO - __main__ -   test: [batch: 263/391 ] | Loss: 0.890 | Acc: 77.578% (26215/33792)/ 93.706% (31665/33792)
01/13/2023 23:14:15 - INFO - __main__ -   test: [batch: 264/391 ] | Loss: 0.895 | Acc: 77.518% (26294/33920)/ 93.644% (31764/33920)
01/13/2023 23:14:17 - INFO - __main__ -   test: [batch: 265/391 ] | Loss: 0.895 | Acc: 77.546% (26403/34048)/ 93.647% (31885/34048)
01/13/2023 23:14:20 - INFO - __main__ -   test: [batch: 266/391 ] | Loss: 0.897 | Acc: 77.475% (26478/34176)/ 93.630% (31999/34176)
01/13/2023 23:14:23 - INFO - __main__ -   test: [batch: 267/391 ] | Loss: 0.896 | Acc: 77.507% (26588/34304)/ 93.630% (32119/34304)
01/13/2023 23:14:25 - INFO - __main__ -   test: [batch: 268/391 ] | Loss: 0.896 | Acc: 77.527% (26694/34432)/ 93.628% (32238/34432)
01/13/2023 23:14:28 - INFO - __main__ -   test: [batch: 269/391 ] | Loss: 0.897 | Acc: 77.471% (26774/34560)/ 93.611% (32352/34560)
01/13/2023 23:14:31 - INFO - __main__ -   test: [batch: 270/391 ] | Loss: 0.901 | Acc: 77.410% (26852/34688)/ 93.580% (32461/34688)
01/13/2023 23:14:34 - INFO - __main__ -   test: [batch: 271/391 ] | Loss: 0.901 | Acc: 77.421% (26955/34816)/ 93.575% (32579/34816)
01/13/2023 23:14:36 - INFO - __main__ -   test: [batch: 272/391 ] | Loss: 0.902 | Acc: 77.387% (27042/34944)/ 93.573% (32698/34944)
01/13/2023 23:14:39 - INFO - __main__ -   test: [batch: 273/391 ] | Loss: 0.902 | Acc: 77.395% (27144/35072)/ 93.559% (32813/35072)
01/13/2023 23:14:41 - INFO - __main__ -   test: [batch: 274/391 ] | Loss: 0.902 | Acc: 77.395% (27243/35200)/ 93.562% (32934/35200)
01/13/2023 23:14:44 - INFO - __main__ -   test: [batch: 275/391 ] | Loss: 0.904 | Acc: 77.366% (27332/35328)/ 93.555% (33051/35328)
01/13/2023 23:14:46 - INFO - __main__ -   test: [batch: 276/391 ] | Loss: 0.905 | Acc: 77.332% (27419/35456)/ 93.553% (33170/35456)
01/13/2023 23:14:49 - INFO - __main__ -   test: [batch: 277/391 ] | Loss: 0.905 | Acc: 77.304% (27508/35584)/ 93.545% (33287/35584)
01/13/2023 23:14:52 - INFO - __main__ -   test: [batch: 278/391 ] | Loss: 0.905 | Acc: 77.319% (27612/35712)/ 93.543% (33406/35712)
01/13/2023 23:14:54 - INFO - __main__ -   test: [batch: 279/391 ] | Loss: 0.905 | Acc: 77.330% (27715/35840)/ 93.538% (33524/35840)
01/13/2023 23:14:57 - INFO - __main__ -   test: [batch: 280/391 ] | Loss: 0.906 | Acc: 77.313% (27808/35968)/ 93.533% (33642/35968)
01/13/2023 23:14:59 - INFO - __main__ -   test: [batch: 281/391 ] | Loss: 0.906 | Acc: 77.322% (27910/36096)/ 93.528% (33760/36096)
01/13/2023 23:15:02 - INFO - __main__ -   test: [batch: 282/391 ] | Loss: 0.905 | Acc: 77.355% (28021/36224)/ 93.532% (33881/36224)
01/13/2023 23:15:05 - INFO - __main__ -   test: [batch: 283/391 ] | Loss: 0.906 | Acc: 77.324% (28109/36352)/ 93.527% (33999/36352)
01/13/2023 23:15:07 - INFO - __main__ -   test: [batch: 284/391 ] | Loss: 0.909 | Acc: 77.278% (28191/36480)/ 93.495% (34107/36480)
01/13/2023 23:15:10 - INFO - __main__ -   test: [batch: 285/391 ] | Loss: 0.912 | Acc: 77.234% (28274/36608)/ 93.455% (34212/36608)
01/13/2023 23:15:12 - INFO - __main__ -   test: [batch: 286/391 ] | Loss: 0.913 | Acc: 77.202% (28361/36736)/ 93.442% (34327/36736)
01/13/2023 23:15:15 - INFO - __main__ -   test: [batch: 287/391 ] | Loss: 0.913 | Acc: 77.203% (28460/36864)/ 93.435% (34444/36864)
01/13/2023 23:15:18 - INFO - __main__ -   test: [batch: 288/391 ] | Loss: 0.912 | Acc: 77.219% (28565/36992)/ 93.439% (34565/36992)
01/13/2023 23:15:20 - INFO - __main__ -   test: [batch: 289/391 ] | Loss: 0.914 | Acc: 77.179% (28649/37120)/ 93.405% (34672/37120)
01/13/2023 23:15:23 - INFO - __main__ -   test: [batch: 290/391 ] | Loss: 0.915 | Acc: 77.099% (28718/37248)/ 93.409% (34793/37248)
01/13/2023 23:15:25 - INFO - __main__ -   test: [batch: 291/391 ] | Loss: 0.915 | Acc: 77.116% (28823/37376)/ 93.402% (34910/37376)
01/13/2023 23:15:28 - INFO - __main__ -   test: [batch: 292/391 ] | Loss: 0.917 | Acc: 77.064% (28902/37504)/ 93.385% (35023/37504)
01/13/2023 23:15:30 - INFO - __main__ -   test: [batch: 293/391 ] | Loss: 0.917 | Acc: 77.065% (29001/37632)/ 93.378% (35140/37632)
01/13/2023 23:15:33 - INFO - __main__ -   test: [batch: 294/391 ] | Loss: 0.918 | Acc: 77.042% (29091/37760)/ 93.374% (35258/37760)
01/13/2023 23:15:35 - INFO - __main__ -   test: [batch: 295/391 ] | Loss: 0.917 | Acc: 77.077% (29203/37888)/ 93.375% (35378/37888)
01/13/2023 23:15:38 - INFO - __main__ -   test: [batch: 296/391 ] | Loss: 0.918 | Acc: 77.078% (29302/38016)/ 93.366% (35494/38016)
01/13/2023 23:15:41 - INFO - __main__ -   test: [batch: 297/391 ] | Loss: 0.920 | Acc: 77.061% (29394/38144)/ 93.344% (35605/38144)
01/13/2023 23:15:43 - INFO - __main__ -   test: [batch: 298/391 ] | Loss: 0.922 | Acc: 77.028% (29480/38272)/ 93.316% (35714/38272)
01/13/2023 23:15:46 - INFO - __main__ -   test: [batch: 299/391 ] | Loss: 0.923 | Acc: 77.021% (29576/38400)/ 93.297% (35826/38400)
01/13/2023 23:15:48 - INFO - __main__ -   test: [batch: 300/391 ] | Loss: 0.923 | Acc: 77.025% (29676/38528)/ 93.293% (35944/38528)
01/13/2023 23:15:51 - INFO - __main__ -   test: [batch: 301/391 ] | Loss: 0.924 | Acc: 77.013% (29770/38656)/ 93.279% (36058/38656)
01/13/2023 23:15:54 - INFO - __main__ -   test: [batch: 302/391 ] | Loss: 0.925 | Acc: 76.980% (29856/38784)/ 93.263% (36171/38784)
01/13/2023 23:15:56 - INFO - __main__ -   test: [batch: 303/391 ] | Loss: 0.926 | Acc: 76.958% (29946/38912)/ 93.249% (36285/38912)
01/13/2023 23:15:59 - INFO - __main__ -   test: [batch: 304/391 ] | Loss: 0.926 | Acc: 76.975% (30051/39040)/ 93.253% (36406/39040)
01/13/2023 23:16:01 - INFO - __main__ -   test: [batch: 305/391 ] | Loss: 0.927 | Acc: 76.930% (30132/39168)/ 93.250% (36524/39168)
01/13/2023 23:16:04 - INFO - __main__ -   test: [batch: 306/391 ] | Loss: 0.928 | Acc: 76.911% (30223/39296)/ 93.223% (36633/39296)
01/13/2023 23:16:07 - INFO - __main__ -   test: [batch: 307/391 ] | Loss: 0.929 | Acc: 76.910% (30321/39424)/ 93.207% (36746/39424)
01/13/2023 23:16:09 - INFO - __main__ -   test: [batch: 308/391 ] | Loss: 0.929 | Acc: 76.889% (30411/39552)/ 93.206% (36865/39552)
01/13/2023 23:16:12 - INFO - __main__ -   test: [batch: 309/391 ] | Loss: 0.930 | Acc: 76.885% (30508/39680)/ 93.188% (36977/39680)
01/13/2023 23:16:15 - INFO - __main__ -   test: [batch: 310/391 ] | Loss: 0.930 | Acc: 76.882% (30605/39808)/ 93.175% (37091/39808)
01/13/2023 23:16:17 - INFO - __main__ -   test: [batch: 311/391 ] | Loss: 0.932 | Acc: 76.853% (30692/39936)/ 93.167% (37207/39936)
01/13/2023 23:16:20 - INFO - __main__ -   test: [batch: 312/391 ] | Loss: 0.933 | Acc: 76.837% (30784/40064)/ 93.143% (37317/40064)
01/13/2023 23:16:22 - INFO - __main__ -   test: [batch: 313/391 ] | Loss: 0.932 | Acc: 76.878% (30899/40192)/ 93.163% (37444/40192)
01/13/2023 23:16:25 - INFO - __main__ -   test: [batch: 314/391 ] | Loss: 0.932 | Acc: 76.868% (30993/40320)/ 93.160% (37562/40320)
01/13/2023 23:16:27 - INFO - __main__ -   test: [batch: 315/391 ] | Loss: 0.933 | Acc: 76.847% (31083/40448)/ 93.144% (37675/40448)
01/13/2023 23:16:30 - INFO - __main__ -   test: [batch: 316/391 ] | Loss: 0.936 | Acc: 76.752% (31143/40576)/ 93.119% (37784/40576)
01/13/2023 23:16:33 - INFO - __main__ -   test: [batch: 317/391 ] | Loss: 0.937 | Acc: 76.710% (31224/40704)/ 93.092% (37892/40704)
01/13/2023 23:16:35 - INFO - __main__ -   test: [batch: 318/391 ] | Loss: 0.936 | Acc: 76.741% (31335/40832)/ 93.108% (38018/40832)
01/13/2023 23:16:38 - INFO - __main__ -   test: [batch: 319/391 ] | Loss: 0.939 | Acc: 76.687% (31411/40960)/ 93.076% (38124/40960)
01/13/2023 23:16:41 - INFO - __main__ -   test: [batch: 320/391 ] | Loss: 0.938 | Acc: 76.723% (31524/41088)/ 93.086% (38247/41088)
01/13/2023 23:16:43 - INFO - __main__ -   test: [batch: 321/391 ] | Loss: 0.938 | Acc: 76.742% (31630/41216)/ 93.080% (38364/41216)
01/13/2023 23:16:46 - INFO - __main__ -   test: [batch: 322/391 ] | Loss: 0.939 | Acc: 76.720% (31719/41344)/ 93.075% (38481/41344)
01/13/2023 23:16:48 - INFO - __main__ -   test: [batch: 323/391 ] | Loss: 0.941 | Acc: 76.681% (31801/41472)/ 93.051% (38590/41472)
01/13/2023 23:16:51 - INFO - __main__ -   test: [batch: 324/391 ] | Loss: 0.941 | Acc: 76.675% (31897/41600)/ 93.046% (38707/41600)
01/13/2023 23:16:53 - INFO - __main__ -   test: [batch: 325/391 ] | Loss: 0.941 | Acc: 76.685% (31999/41728)/ 93.041% (38824/41728)
01/13/2023 23:16:56 - INFO - __main__ -   test: [batch: 326/391 ] | Loss: 0.944 | Acc: 76.603% (32063/41856)/ 93.012% (38931/41856)
01/13/2023 23:16:59 - INFO - __main__ -   test: [batch: 327/391 ] | Loss: 0.948 | Acc: 76.524% (32128/41984)/ 92.981% (39037/41984)
01/13/2023 23:17:01 - INFO - __main__ -   test: [batch: 328/391 ] | Loss: 0.949 | Acc: 76.472% (32204/42112)/ 92.957% (39146/42112)
01/13/2023 23:17:04 - INFO - __main__ -   test: [batch: 329/391 ] | Loss: 0.950 | Acc: 76.463% (32298/42240)/ 92.957% (39265/42240)
01/13/2023 23:17:06 - INFO - __main__ -   test: [batch: 330/391 ] | Loss: 0.951 | Acc: 76.423% (32379/42368)/ 92.931% (39373/42368)
01/13/2023 23:17:09 - INFO - __main__ -   test: [batch: 331/391 ] | Loss: 0.952 | Acc: 76.388% (32462/42496)/ 92.943% (39497/42496)
01/13/2023 23:17:11 - INFO - __main__ -   test: [batch: 332/391 ] | Loss: 0.952 | Acc: 76.377% (32555/42624)/ 92.941% (39615/42624)
01/13/2023 23:17:14 - INFO - __main__ -   test: [batch: 333/391 ] | Loss: 0.951 | Acc: 76.406% (32665/42752)/ 92.950% (39738/42752)
01/13/2023 23:17:17 - INFO - __main__ -   test: [batch: 334/391 ] | Loss: 0.952 | Acc: 76.376% (32750/42880)/ 92.934% (39850/42880)
01/13/2023 23:17:19 - INFO - __main__ -   test: [batch: 335/391 ] | Loss: 0.953 | Acc: 76.351% (32837/43008)/ 92.920% (39963/43008)
01/13/2023 23:17:22 - INFO - __main__ -   test: [batch: 336/391 ] | Loss: 0.955 | Acc: 76.305% (32915/43136)/ 92.906% (40076/43136)
01/13/2023 23:17:25 - INFO - __main__ -   test: [batch: 337/391 ] | Loss: 0.955 | Acc: 76.299% (33010/43264)/ 92.904% (40194/43264)
01/13/2023 23:17:27 - INFO - __main__ -   test: [batch: 338/391 ] | Loss: 0.955 | Acc: 76.288% (33103/43392)/ 92.913% (40317/43392)
01/13/2023 23:17:30 - INFO - __main__ -   test: [batch: 339/391 ] | Loss: 0.957 | Acc: 76.257% (33187/43520)/ 92.888% (40425/43520)
01/13/2023 23:17:32 - INFO - __main__ -   test: [batch: 340/391 ] | Loss: 0.957 | Acc: 76.251% (33282/43648)/ 92.898% (40548/43648)
01/13/2023 23:17:35 - INFO - __main__ -   test: [batch: 341/391 ] | Loss: 0.956 | Acc: 76.288% (33396/43776)/ 92.916% (40675/43776)
01/13/2023 23:17:38 - INFO - __main__ -   test: [batch: 342/391 ] | Loss: 0.956 | Acc: 76.235% (33470/43904)/ 92.910% (40791/43904)
01/13/2023 23:17:40 - INFO - __main__ -   test: [batch: 343/391 ] | Loss: 0.956 | Acc: 76.238% (33569/44032)/ 92.912% (40911/44032)
01/13/2023 23:17:43 - INFO - __main__ -   test: [batch: 344/391 ] | Loss: 0.956 | Acc: 76.236% (33666/44160)/ 92.903% (41026/44160)
01/13/2023 23:17:46 - INFO - __main__ -   test: [batch: 345/391 ] | Loss: 0.960 | Acc: 76.174% (33736/44288)/ 92.867% (41129/44288)
01/13/2023 23:17:48 - INFO - __main__ -   test: [batch: 346/391 ] | Loss: 0.961 | Acc: 76.159% (33827/44416)/ 92.858% (41244/44416)
01/13/2023 23:17:51 - INFO - __main__ -   test: [batch: 347/391 ] | Loss: 0.961 | Acc: 76.172% (33930/44544)/ 92.865% (41366/44544)
01/13/2023 23:17:54 - INFO - __main__ -   test: [batch: 348/391 ] | Loss: 0.962 | Acc: 76.144% (34015/44672)/ 92.841% (41474/44672)
01/13/2023 23:17:56 - INFO - __main__ -   test: [batch: 349/391 ] | Loss: 0.962 | Acc: 76.141% (34111/44800)/ 92.848% (41596/44800)
01/13/2023 23:17:59 - INFO - __main__ -   test: [batch: 350/391 ] | Loss: 0.962 | Acc: 76.151% (34213/44928)/ 92.846% (41714/44928)
01/13/2023 23:18:02 - INFO - __main__ -   test: [batch: 351/391 ] | Loss: 0.964 | Acc: 76.096% (34286/45056)/ 92.833% (41827/45056)
01/13/2023 23:18:05 - INFO - __main__ -   test: [batch: 352/391 ] | Loss: 0.964 | Acc: 76.096% (34383/45184)/ 92.832% (41945/45184)
01/13/2023 23:18:07 - INFO - __main__ -   test: [batch: 353/391 ] | Loss: 0.967 | Acc: 76.050% (34460/45312)/ 92.792% (42046/45312)
01/13/2023 23:18:10 - INFO - __main__ -   test: [batch: 354/391 ] | Loss: 0.969 | Acc: 75.990% (34530/45440)/ 92.768% (42154/45440)
01/13/2023 23:18:13 - INFO - __main__ -   test: [batch: 355/391 ] | Loss: 0.972 | Acc: 75.924% (34597/45568)/ 92.760% (42269/45568)
01/13/2023 23:18:15 - INFO - __main__ -   test: [batch: 356/391 ] | Loss: 0.972 | Acc: 75.919% (34692/45696)/ 92.761% (42388/45696)
01/13/2023 23:18:18 - INFO - __main__ -   test: [batch: 357/391 ] | Loss: 0.970 | Acc: 75.954% (34805/45824)/ 92.775% (42513/45824)
01/13/2023 23:18:20 - INFO - __main__ -   test: [batch: 358/391 ] | Loss: 0.970 | Acc: 75.977% (34913/45952)/ 92.777% (42633/45952)
01/13/2023 23:18:23 - INFO - __main__ -   test: [batch: 359/391 ] | Loss: 0.970 | Acc: 75.985% (35014/46080)/ 92.773% (42750/46080)
01/13/2023 23:18:26 - INFO - __main__ -   test: [batch: 360/391 ] | Loss: 0.972 | Acc: 75.957% (35098/46208)/ 92.765% (42865/46208)
01/13/2023 23:18:28 - INFO - __main__ -   test: [batch: 361/391 ] | Loss: 0.972 | Acc: 75.958% (35196/46336)/ 92.772% (42987/46336)
01/13/2023 23:18:31 - INFO - __main__ -   test: [batch: 362/391 ] | Loss: 0.971 | Acc: 75.953% (35291/46464)/ 92.784% (43111/46464)
01/13/2023 23:18:33 - INFO - __main__ -   test: [batch: 363/391 ] | Loss: 0.972 | Acc: 75.944% (35384/46592)/ 92.776% (43226/46592)
01/13/2023 23:18:36 - INFO - __main__ -   test: [batch: 364/391 ] | Loss: 0.970 | Acc: 75.972% (35494/46720)/ 92.785% (43349/46720)
01/13/2023 23:18:39 - INFO - __main__ -   test: [batch: 365/391 ] | Loss: 0.970 | Acc: 75.986% (35598/46848)/ 92.789% (43470/46848)
01/13/2023 23:18:41 - INFO - __main__ -   test: [batch: 366/391 ] | Loss: 0.968 | Acc: 76.022% (35712/46976)/ 92.807% (43597/46976)
01/13/2023 23:18:44 - INFO - __main__ -   test: [batch: 367/391 ] | Loss: 0.967 | Acc: 76.044% (35820/47104)/ 92.820% (43722/47104)
01/13/2023 23:18:46 - INFO - __main__ -   test: [batch: 368/391 ] | Loss: 0.967 | Acc: 76.048% (35919/47232)/ 92.829% (43845/47232)
01/13/2023 23:18:49 - INFO - __main__ -   test: [batch: 369/391 ] | Loss: 0.966 | Acc: 76.066% (36025/47360)/ 92.840% (43969/47360)
01/13/2023 23:18:52 - INFO - __main__ -   test: [batch: 370/391 ] | Loss: 0.966 | Acc: 76.053% (36116/47488)/ 92.840% (44088/47488)
01/13/2023 23:18:54 - INFO - __main__ -   test: [batch: 371/391 ] | Loss: 0.966 | Acc: 76.054% (36214/47616)/ 92.843% (44208/47616)
01/13/2023 23:18:57 - INFO - __main__ -   test: [batch: 372/391 ] | Loss: 0.964 | Acc: 76.100% (36333/47744)/ 92.856% (44333/47744)
01/13/2023 23:18:59 - INFO - __main__ -   test: [batch: 373/391 ] | Loss: 0.963 | Acc: 76.134% (36447/47872)/ 92.862% (44455/47872)
01/13/2023 23:19:02 - INFO - __main__ -   test: [batch: 374/391 ] | Loss: 0.962 | Acc: 76.165% (36559/48000)/ 92.869% (44577/48000)
01/13/2023 23:19:04 - INFO - __main__ -   test: [batch: 375/391 ] | Loss: 0.965 | Acc: 76.097% (36624/48128)/ 92.834% (44679/48128)
01/13/2023 23:19:07 - INFO - __main__ -   test: [batch: 376/391 ] | Loss: 0.965 | Acc: 76.094% (36720/48256)/ 92.820% (44791/48256)
01/13/2023 23:19:10 - INFO - __main__ -   test: [batch: 377/391 ] | Loss: 0.966 | Acc: 76.081% (36811/48384)/ 92.814% (44907/48384)
01/13/2023 23:19:12 - INFO - __main__ -   test: [batch: 378/391 ] | Loss: 0.969 | Acc: 76.006% (36872/48512)/ 92.777% (45008/48512)
01/13/2023 23:19:15 - INFO - __main__ -   test: [batch: 379/391 ] | Loss: 0.969 | Acc: 75.989% (36961/48640)/ 92.784% (45130/48640)
01/13/2023 23:19:17 - INFO - __main__ -   test: [batch: 380/391 ] | Loss: 0.969 | Acc: 75.988% (37058/48768)/ 92.792% (45253/48768)
01/13/2023 23:19:20 - INFO - __main__ -   test: [batch: 381/391 ] | Loss: 0.971 | Acc: 75.947% (37135/48896)/ 92.795% (45373/48896)
01/13/2023 23:19:23 - INFO - __main__ -   test: [batch: 382/391 ] | Loss: 0.972 | Acc: 75.912% (37215/49024)/ 92.783% (45486/49024)
01/13/2023 23:19:25 - INFO - __main__ -   test: [batch: 383/391 ] | Loss: 0.972 | Acc: 75.920% (37316/49152)/ 92.784% (45605/49152)
01/13/2023 23:19:28 - INFO - __main__ -   test: [batch: 384/391 ] | Loss: 0.970 | Acc: 75.962% (37434/49280)/ 92.796% (45730/49280)
01/13/2023 23:19:30 - INFO - __main__ -   test: [batch: 385/391 ] | Loss: 0.969 | Acc: 75.972% (37536/49408)/ 92.805% (45853/49408)
01/13/2023 23:19:33 - INFO - __main__ -   test: [batch: 386/391 ] | Loss: 0.967 | Acc: 76.017% (37656/49536)/ 92.821% (45980/49536)
01/13/2023 23:19:36 - INFO - __main__ -   test: [batch: 387/391 ] | Loss: 0.966 | Acc: 76.057% (37773/49664)/ 92.832% (46104/49664)
01/13/2023 23:19:38 - INFO - __main__ -   test: [batch: 388/391 ] | Loss: 0.964 | Acc: 76.103% (37893/49792)/ 92.844% (46229/49792)
01/13/2023 23:19:41 - INFO - __main__ -   test: [batch: 389/391 ] | Loss: 0.963 | Acc: 76.106% (37992/49920)/ 92.847% (46349/49920)
01/13/2023 23:19:44 - INFO - __main__ -   test: [batch: 390/391 ] | Loss: 0.965 | Acc: 76.062% (38031/50000)/ 92.836% (46418/50000)
01/13/2023 23:19:44 - INFO - __main__ -   Final accuracy: 76.062
01/13/2023 23:19:44 - INFO - __main__ -   {'milestones': Counter({2: 1, 3: 1, 4: 1}), 'gamma': 0.1, 'base_lrs': [0.00025], 'last_epoch': 1, '_step_count': 2, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.00025]}
01/13/2023 23:19:44 - INFO - __main__ -   
Epoch: 1
01/13/2023 23:19:47 - INFO - __main__ -   test: [epoch: 1 | batch: 0/10010 ] | Loss: 0.630 | Acc: 86.719% (111/128)
01/13/2023 23:24:08 - INFO - __main__ -   test: [epoch: 1 | batch: 100/10010 ] | Loss: 0.826 | Acc: 79.672% (10300/12928)
01/13/2023 23:28:30 - INFO - __main__ -   test: [epoch: 1 | batch: 200/10010 ] | Loss: 0.825 | Acc: 79.489% (20451/25728)
01/13/2023 23:32:52 - INFO - __main__ -   test: [epoch: 1 | batch: 300/10010 ] | Loss: 0.839 | Acc: 79.163% (30500/38528)
01/13/2023 23:37:13 - INFO - __main__ -   test: [epoch: 1 | batch: 400/10010 ] | Loss: 0.852 | Acc: 78.924% (40510/51328)
01/13/2023 23:41:34 - INFO - __main__ -   test: [epoch: 1 | batch: 500/10010 ] | Loss: 0.852 | Acc: 78.883% (50586/64128)
01/13/2023 23:45:56 - INFO - __main__ -   test: [epoch: 1 | batch: 600/10010 ] | Loss: 0.855 | Acc: 78.810% (60627/76928)
01/13/2023 23:50:17 - INFO - __main__ -   test: [epoch: 1 | batch: 700/10010 ] | Loss: 0.855 | Acc: 78.776% (70684/89728)
01/13/2023 23:54:38 - INFO - __main__ -   test: [epoch: 1 | batch: 800/10010 ] | Loss: 0.854 | Acc: 78.762% (80753/102528)
01/13/2023 23:58:59 - INFO - __main__ -   test: [epoch: 1 | batch: 900/10010 ] | Loss: 0.855 | Acc: 78.728% (90795/115328)
01/14/2023 00:03:21 - INFO - __main__ -   test: [epoch: 1 | batch: 1000/10010 ] | Loss: 0.855 | Acc: 78.692% (100826/128128)
01/14/2023 00:07:42 - INFO - __main__ -   test: [epoch: 1 | batch: 1100/10010 ] | Loss: 0.857 | Acc: 78.638% (110823/140928)
01/14/2023 00:12:03 - INFO - __main__ -   test: [epoch: 1 | batch: 1200/10010 ] | Loss: 0.857 | Acc: 78.653% (120912/153728)
01/14/2023 00:16:24 - INFO - __main__ -   test: [epoch: 1 | batch: 1300/10010 ] | Loss: 0.854 | Acc: 78.725% (131100/166528)
01/14/2023 00:20:45 - INFO - __main__ -   test: [epoch: 1 | batch: 1400/10010 ] | Loss: 0.854 | Acc: 78.741% (141205/179328)
01/14/2023 00:25:05 - INFO - __main__ -   test: [epoch: 1 | batch: 1500/10010 ] | Loss: 0.854 | Acc: 78.731% (151265/192128)
01/14/2023 00:29:26 - INFO - __main__ -   test: [epoch: 1 | batch: 1600/10010 ] | Loss: 0.856 | Acc: 78.688% (161253/204928)
01/14/2023 00:33:49 - INFO - __main__ -   test: [epoch: 1 | batch: 1700/10010 ] | Loss: 0.856 | Acc: 78.696% (171344/217728)
01/14/2023 00:38:10 - INFO - __main__ -   test: [epoch: 1 | batch: 1800/10010 ] | Loss: 0.856 | Acc: 78.705% (181438/230528)
01/14/2023 00:42:29 - INFO - __main__ -   test: [epoch: 1 | batch: 1900/10010 ] | Loss: 0.855 | Acc: 78.740% (191597/243328)
01/14/2023 00:46:52 - INFO - __main__ -   test: [epoch: 1 | batch: 2000/10010 ] | Loss: 0.854 | Acc: 78.743% (201684/256128)
01/14/2023 00:51:16 - INFO - __main__ -   test: [epoch: 1 | batch: 2100/10010 ] | Loss: 0.854 | Acc: 78.733% (211736/268928)
01/14/2023 00:55:34 - INFO - __main__ -   test: [epoch: 1 | batch: 2200/10010 ] | Loss: 0.855 | Acc: 78.698% (221715/281728)
01/14/2023 00:59:54 - INFO - __main__ -   test: [epoch: 1 | batch: 2300/10010 ] | Loss: 0.856 | Acc: 78.694% (231775/294528)
01/14/2023 01:04:16 - INFO - __main__ -   test: [epoch: 1 | batch: 2400/10010 ] | Loss: 0.855 | Acc: 78.727% (241950/307328)
01/14/2023 01:08:39 - INFO - __main__ -   test: [epoch: 1 | batch: 2500/10010 ] | Loss: 0.856 | Acc: 78.697% (251931/320128)
01/14/2023 01:13:01 - INFO - __main__ -   test: [epoch: 1 | batch: 2600/10010 ] | Loss: 0.856 | Acc: 78.700% (262016/332928)
01/14/2023 01:17:22 - INFO - __main__ -   test: [epoch: 1 | batch: 2700/10010 ] | Loss: 0.856 | Acc: 78.698% (272080/345728)
01/14/2023 01:21:43 - INFO - __main__ -   test: [epoch: 1 | batch: 2800/10010 ] | Loss: 0.856 | Acc: 78.688% (282119/358528)
01/14/2023 01:26:04 - INFO - __main__ -   test: [epoch: 1 | batch: 2900/10010 ] | Loss: 0.857 | Acc: 78.689% (292195/371328)
01/14/2023 01:30:24 - INFO - __main__ -   test: [epoch: 1 | batch: 3000/10010 ] | Loss: 0.856 | Acc: 78.704% (302324/384128)
01/14/2023 01:34:46 - INFO - __main__ -   test: [epoch: 1 | batch: 3100/10010 ] | Loss: 0.856 | Acc: 78.701% (312386/396928)
01/14/2023 01:39:09 - INFO - __main__ -   test: [epoch: 1 | batch: 3200/10010 ] | Loss: 0.857 | Acc: 78.686% (322399/409728)
01/14/2023 01:43:29 - INFO - __main__ -   test: [epoch: 1 | batch: 3300/10010 ] | Loss: 0.857 | Acc: 78.670% (332403/422528)
01/14/2023 01:47:50 - INFO - __main__ -   test: [epoch: 1 | batch: 3400/10010 ] | Loss: 0.857 | Acc: 78.683% (342528/435328)
01/14/2023 01:52:12 - INFO - __main__ -   test: [epoch: 1 | batch: 3500/10010 ] | Loss: 0.857 | Acc: 78.678% (352580/448128)
01/14/2023 01:56:33 - INFO - __main__ -   test: [epoch: 1 | batch: 3600/10010 ] | Loss: 0.857 | Acc: 78.680% (362660/460928)
01/14/2023 02:00:55 - INFO - __main__ -   test: [epoch: 1 | batch: 3700/10010 ] | Loss: 0.858 | Acc: 78.667% (372669/473728)
01/14/2023 02:05:16 - INFO - __main__ -   test: [epoch: 1 | batch: 3800/10010 ] | Loss: 0.858 | Acc: 78.666% (382733/486528)
01/14/2023 02:09:37 - INFO - __main__ -   test: [epoch: 1 | batch: 3900/10010 ] | Loss: 0.857 | Acc: 78.677% (392858/499328)
01/14/2023 02:13:57 - INFO - __main__ -   test: [epoch: 1 | batch: 4000/10010 ] | Loss: 0.858 | Acc: 78.672% (402901/512128)
01/14/2023 02:18:18 - INFO - __main__ -   test: [epoch: 1 | batch: 4100/10010 ] | Loss: 0.857 | Acc: 78.684% (413032/524928)
01/14/2023 02:22:39 - INFO - __main__ -   test: [epoch: 1 | batch: 4200/10010 ] | Loss: 0.857 | Acc: 78.675% (423060/537728)
01/14/2023 02:27:00 - INFO - __main__ -   test: [epoch: 1 | batch: 4300/10010 ] | Loss: 0.857 | Acc: 78.682% (433165/550528)
01/14/2023 02:31:21 - INFO - __main__ -   test: [epoch: 1 | batch: 4400/10010 ] | Loss: 0.857 | Acc: 78.686% (443263/563328)
01/14/2023 02:35:42 - INFO - __main__ -   test: [epoch: 1 | batch: 4500/10010 ] | Loss: 0.857 | Acc: 78.680% (453300/576128)
01/14/2023 02:40:05 - INFO - __main__ -   test: [epoch: 1 | batch: 4600/10010 ] | Loss: 0.857 | Acc: 78.678% (463357/588928)
01/14/2023 02:44:27 - INFO - __main__ -   test: [epoch: 1 | batch: 4700/10010 ] | Loss: 0.857 | Acc: 78.691% (473507/601728)
01/14/2023 02:48:49 - INFO - __main__ -   test: [epoch: 1 | batch: 4800/10010 ] | Loss: 0.856 | Acc: 78.698% (483624/614528)
01/14/2023 02:53:10 - INFO - __main__ -   test: [epoch: 1 | batch: 4900/10010 ] | Loss: 0.856 | Acc: 78.694% (493670/627328)
01/14/2023 02:57:33 - INFO - __main__ -   test: [epoch: 1 | batch: 5000/10010 ] | Loss: 0.856 | Acc: 78.707% (503825/640128)
01/14/2023 03:01:54 - INFO - __main__ -   test: [epoch: 1 | batch: 5100/10010 ] | Loss: 0.856 | Acc: 78.707% (513899/652928)
01/14/2023 03:06:15 - INFO - __main__ -   test: [epoch: 1 | batch: 5200/10010 ] | Loss: 0.856 | Acc: 78.697% (523909/665728)
01/14/2023 03:10:37 - INFO - __main__ -   test: [epoch: 1 | batch: 5300/10010 ] | Loss: 0.857 | Acc: 78.693% (533952/678528)
01/14/2023 03:14:56 - INFO - __main__ -   test: [epoch: 1 | batch: 5400/10010 ] | Loss: 0.857 | Acc: 78.692% (544017/691328)
01/14/2023 03:19:18 - INFO - __main__ -   test: [epoch: 1 | batch: 5500/10010 ] | Loss: 0.857 | Acc: 78.683% (554031/704128)
01/14/2023 03:23:37 - INFO - __main__ -   test: [epoch: 1 | batch: 5600/10010 ] | Loss: 0.856 | Acc: 78.697% (564198/716928)
01/14/2023 03:28:00 - INFO - __main__ -   test: [epoch: 1 | batch: 5700/10010 ] | Loss: 0.856 | Acc: 78.702% (574310/729728)
01/14/2023 03:32:20 - INFO - __main__ -   test: [epoch: 1 | batch: 5800/10010 ] | Loss: 0.856 | Acc: 78.710% (584445/742528)
01/14/2023 03:36:41 - INFO - __main__ -   test: [epoch: 1 | batch: 5900/10010 ] | Loss: 0.856 | Acc: 78.709% (594513/755328)
01/14/2023 03:41:00 - INFO - __main__ -   test: [epoch: 1 | batch: 6000/10010 ] | Loss: 0.856 | Acc: 78.712% (604611/768128)
01/14/2023 03:45:21 - INFO - __main__ -   test: [epoch: 1 | batch: 6100/10010 ] | Loss: 0.856 | Acc: 78.699% (614584/780928)
01/14/2023 03:49:43 - INFO - __main__ -   test: [epoch: 1 | batch: 6200/10010 ] | Loss: 0.856 | Acc: 78.699% (624657/793728)
01/14/2023 03:54:05 - INFO - __main__ -   test: [epoch: 1 | batch: 6300/10010 ] | Loss: 0.856 | Acc: 78.696% (634709/806528)
01/14/2023 03:58:25 - INFO - __main__ -   test: [epoch: 1 | batch: 6400/10010 ] | Loss: 0.856 | Acc: 78.683% (644675/819328)
01/14/2023 04:02:47 - INFO - __main__ -   test: [epoch: 1 | batch: 6500/10010 ] | Loss: 0.856 | Acc: 78.685% (654761/832128)
01/14/2023 04:07:09 - INFO - __main__ -   test: [epoch: 1 | batch: 6600/10010 ] | Loss: 0.856 | Acc: 78.690% (664871/844928)
01/14/2023 04:11:30 - INFO - __main__ -   test: [epoch: 1 | batch: 6700/10010 ] | Loss: 0.856 | Acc: 78.682% (674881/857728)
01/14/2023 04:15:52 - INFO - __main__ -   test: [epoch: 1 | batch: 6800/10010 ] | Loss: 0.856 | Acc: 78.674% (684882/870528)
01/14/2023 04:20:14 - INFO - __main__ -   test: [epoch: 1 | batch: 6900/10010 ] | Loss: 0.856 | Acc: 78.678% (694988/883328)
01/14/2023 04:24:35 - INFO - __main__ -   test: [epoch: 1 | batch: 7000/10010 ] | Loss: 0.856 | Acc: 78.677% (705046/896128)
01/14/2023 04:28:56 - INFO - __main__ -   test: [epoch: 1 | batch: 7100/10010 ] | Loss: 0.856 | Acc: 78.683% (715173/908928)
01/14/2023 04:33:17 - INFO - __main__ -   test: [epoch: 1 | batch: 7200/10010 ] | Loss: 0.856 | Acc: 78.683% (725240/921728)
01/14/2023 04:37:38 - INFO - __main__ -   test: [epoch: 1 | batch: 7300/10010 ] | Loss: 0.856 | Acc: 78.680% (735287/934528)
01/14/2023 04:42:02 - INFO - __main__ -   test: [epoch: 1 | batch: 7400/10010 ] | Loss: 0.856 | Acc: 78.673% (745293/947328)
01/14/2023 04:46:24 - INFO - __main__ -   test: [epoch: 1 | batch: 7500/10010 ] | Loss: 0.856 | Acc: 78.673% (755363/960128)
01/14/2023 04:50:44 - INFO - __main__ -   test: [epoch: 1 | batch: 7600/10010 ] | Loss: 0.857 | Acc: 78.671% (765412/972928)
01/14/2023 04:55:05 - INFO - __main__ -   test: [epoch: 1 | batch: 7700/10010 ] | Loss: 0.857 | Acc: 78.669% (775464/985728)
01/14/2023 04:59:26 - INFO - __main__ -   test: [epoch: 1 | batch: 7800/10010 ] | Loss: 0.857 | Acc: 78.658% (785421/998528)
01/14/2023 05:03:48 - INFO - __main__ -   test: [epoch: 1 | batch: 7900/10010 ] | Loss: 0.857 | Acc: 78.661% (795517/1011328)
01/14/2023 05:08:08 - INFO - __main__ -   test: [epoch: 1 | batch: 8000/10010 ] | Loss: 0.857 | Acc: 78.664% (805618/1024128)
01/14/2023 05:12:31 - INFO - __main__ -   test: [epoch: 1 | batch: 8100/10010 ] | Loss: 0.857 | Acc: 78.675% (815801/1036928)
01/14/2023 05:16:52 - INFO - __main__ -   test: [epoch: 1 | batch: 8200/10010 ] | Loss: 0.857 | Acc: 78.671% (825828/1049728)
01/14/2023 05:21:15 - INFO - __main__ -   test: [epoch: 1 | batch: 8300/10010 ] | Loss: 0.857 | Acc: 78.667% (835860/1062528)
01/14/2023 05:25:35 - INFO - __main__ -   test: [epoch: 1 | batch: 8400/10010 ] | Loss: 0.857 | Acc: 78.667% (845925/1075328)
01/14/2023 05:29:56 - INFO - __main__ -   test: [epoch: 1 | batch: 8500/10010 ] | Loss: 0.857 | Acc: 78.671% (856041/1088128)
01/14/2023 05:34:19 - INFO - __main__ -   test: [epoch: 1 | batch: 8600/10010 ] | Loss: 0.857 | Acc: 78.668% (866080/1100928)
01/14/2023 05:38:41 - INFO - __main__ -   test: [epoch: 1 | batch: 8700/10010 ] | Loss: 0.857 | Acc: 78.676% (876234/1113728)
01/14/2023 05:43:02 - INFO - __main__ -   test: [epoch: 1 | batch: 8800/10010 ] | Loss: 0.857 | Acc: 78.683% (886383/1126528)
01/14/2023 05:47:23 - INFO - __main__ -   test: [epoch: 1 | batch: 8900/10010 ] | Loss: 0.856 | Acc: 78.686% (896489/1139328)
01/14/2023 05:51:43 - INFO - __main__ -   test: [epoch: 1 | batch: 9000/10010 ] | Loss: 0.856 | Acc: 78.688% (906582/1152128)
01/14/2023 05:56:04 - INFO - __main__ -   test: [epoch: 1 | batch: 9100/10010 ] | Loss: 0.856 | Acc: 78.687% (916651/1164928)
01/14/2023 06:00:25 - INFO - __main__ -   test: [epoch: 1 | batch: 9200/10010 ] | Loss: 0.856 | Acc: 78.688% (926736/1177728)
01/14/2023 06:04:48 - INFO - __main__ -   test: [epoch: 1 | batch: 9300/10010 ] | Loss: 0.856 | Acc: 78.695% (936882/1190528)
01/14/2023 06:09:12 - INFO - __main__ -   test: [epoch: 1 | batch: 9400/10010 ] | Loss: 0.856 | Acc: 78.691% (946912/1203328)
01/14/2023 06:13:43 - INFO - __main__ -   test: [epoch: 1 | batch: 9500/10010 ] | Loss: 0.856 | Acc: 78.693% (957011/1216128)
01/14/2023 06:18:05 - INFO - __main__ -   test: [epoch: 1 | batch: 9600/10010 ] | Loss: 0.856 | Acc: 78.691% (967061/1228928)
01/14/2023 06:22:26 - INFO - __main__ -   test: [epoch: 1 | batch: 9700/10010 ] | Loss: 0.856 | Acc: 78.693% (977155/1241728)
01/14/2023 06:26:47 - INFO - __main__ -   test: [epoch: 1 | batch: 9800/10010 ] | Loss: 0.857 | Acc: 78.692% (987210/1254528)
01/14/2023 06:31:10 - INFO - __main__ -   test: [epoch: 1 | batch: 9900/10010 ] | Loss: 0.857 | Acc: 78.693% (997297/1267328)
01/14/2023 06:35:32 - INFO - __main__ -   test: [epoch: 1 | batch: 10000/10010 ] | Loss: 0.857 | Acc: 78.690% (1007336/1280128)
01/14/2023 06:35:56 - INFO - __main__ -   Saving Checkpoint
01/14/2023 06:35:59 - INFO - __main__ -   test: [batch: 0/391 ] | Loss: 0.447 | Acc: 86.719% (111/128)/ 97.656% (125/128)
01/14/2023 06:36:01 - INFO - __main__ -   test: [batch: 1/391 ] | Loss: 0.458 | Acc: 85.938% (220/256)/ 98.047% (251/256)
01/14/2023 06:36:04 - INFO - __main__ -   test: [batch: 2/391 ] | Loss: 0.613 | Acc: 82.552% (317/384)/ 96.094% (369/384)
01/14/2023 06:36:07 - INFO - __main__ -   test: [batch: 3/391 ] | Loss: 0.567 | Acc: 84.375% (432/512)/ 96.680% (495/512)
01/14/2023 06:36:09 - INFO - __main__ -   test: [batch: 4/391 ] | Loss: 0.486 | Acc: 86.719% (555/640)/ 97.188% (622/640)
01/14/2023 06:36:12 - INFO - __main__ -   test: [batch: 5/391 ] | Loss: 0.430 | Acc: 87.891% (675/768)/ 97.656% (750/768)
01/14/2023 06:36:15 - INFO - __main__ -   test: [batch: 6/391 ] | Loss: 0.424 | Acc: 88.058% (789/896)/ 97.545% (874/896)
01/14/2023 06:36:17 - INFO - __main__ -   test: [batch: 7/391 ] | Loss: 0.406 | Acc: 89.062% (912/1024)/ 97.656% (1000/1024)
01/14/2023 06:36:20 - INFO - __main__ -   test: [batch: 8/391 ] | Loss: 0.424 | Acc: 89.149% (1027/1152)/ 97.656% (1125/1152)
01/14/2023 06:36:22 - INFO - __main__ -   test: [batch: 9/391 ] | Loss: 0.402 | Acc: 89.688% (1148/1280)/ 97.734% (1251/1280)
01/14/2023 06:36:25 - INFO - __main__ -   test: [batch: 10/391 ] | Loss: 0.451 | Acc: 88.423% (1245/1408)/ 97.727% (1376/1408)
01/14/2023 06:36:28 - INFO - __main__ -   test: [batch: 11/391 ] | Loss: 0.455 | Acc: 88.607% (1361/1536)/ 97.526% (1498/1536)
01/14/2023 06:36:30 - INFO - __main__ -   test: [batch: 12/391 ] | Loss: 0.505 | Acc: 87.440% (1455/1664)/ 97.175% (1617/1664)
01/14/2023 06:36:33 - INFO - __main__ -   test: [batch: 13/391 ] | Loss: 0.556 | Acc: 85.882% (1539/1792)/ 96.596% (1731/1792)
01/14/2023 06:36:36 - INFO - __main__ -   test: [batch: 14/391 ] | Loss: 0.576 | Acc: 85.052% (1633/1920)/ 96.667% (1856/1920)
01/14/2023 06:36:38 - INFO - __main__ -   test: [batch: 15/391 ] | Loss: 0.584 | Acc: 84.619% (1733/2048)/ 96.777% (1982/2048)
01/14/2023 06:36:41 - INFO - __main__ -   test: [batch: 16/391 ] | Loss: 0.594 | Acc: 84.513% (1839/2176)/ 96.599% (2102/2176)
01/14/2023 06:36:44 - INFO - __main__ -   test: [batch: 17/391 ] | Loss: 0.621 | Acc: 84.115% (1938/2304)/ 96.137% (2215/2304)
01/14/2023 06:36:46 - INFO - __main__ -   test: [batch: 18/391 ] | Loss: 0.643 | Acc: 83.594% (2033/2432)/ 95.970% (2334/2432)
01/14/2023 06:36:49 - INFO - __main__ -   test: [batch: 19/391 ] | Loss: 0.652 | Acc: 83.320% (2133/2560)/ 95.898% (2455/2560)
01/14/2023 06:36:51 - INFO - __main__ -   test: [batch: 20/391 ] | Loss: 0.649 | Acc: 83.408% (2242/2688)/ 95.833% (2576/2688)
01/14/2023 06:36:54 - INFO - __main__ -   test: [batch: 21/391 ] | Loss: 0.681 | Acc: 82.706% (2329/2816)/ 95.668% (2694/2816)
01/14/2023 06:36:57 - INFO - __main__ -   test: [batch: 22/391 ] | Loss: 0.679 | Acc: 82.609% (2432/2944)/ 95.686% (2817/2944)
01/14/2023 06:36:59 - INFO - __main__ -   test: [batch: 23/391 ] | Loss: 0.724 | Acc: 81.738% (2511/3072)/ 95.410% (2931/3072)
01/14/2023 06:37:02 - INFO - __main__ -   test: [batch: 24/391 ] | Loss: 0.744 | Acc: 81.281% (2601/3200)/ 95.219% (3047/3200)
01/14/2023 06:37:05 - INFO - __main__ -   test: [batch: 25/391 ] | Loss: 0.768 | Acc: 80.589% (2682/3328)/ 95.012% (3162/3328)
01/14/2023 06:37:07 - INFO - __main__ -   test: [batch: 26/391 ] | Loss: 0.783 | Acc: 79.861% (2760/3456)/ 94.965% (3282/3456)
01/14/2023 06:37:10 - INFO - __main__ -   test: [batch: 27/391 ] | Loss: 0.765 | Acc: 80.385% (2881/3584)/ 95.006% (3405/3584)
01/14/2023 06:37:13 - INFO - __main__ -   test: [batch: 28/391 ] | Loss: 0.774 | Acc: 79.822% (2963/3712)/ 95.070% (3529/3712)
01/14/2023 06:37:15 - INFO - __main__ -   test: [batch: 29/391 ] | Loss: 0.767 | Acc: 79.974% (3071/3840)/ 95.182% (3655/3840)
01/14/2023 06:37:18 - INFO - __main__ -   test: [batch: 30/391 ] | Loss: 0.781 | Acc: 79.940% (3172/3968)/ 95.060% (3772/3968)
01/14/2023 06:37:21 - INFO - __main__ -   test: [batch: 31/391 ] | Loss: 0.777 | Acc: 80.127% (3282/4096)/ 95.142% (3897/4096)
01/14/2023 06:37:23 - INFO - __main__ -   test: [batch: 32/391 ] | Loss: 0.762 | Acc: 80.421% (3397/4224)/ 95.241% (4023/4224)
01/14/2023 06:37:26 - INFO - __main__ -   test: [batch: 33/391 ] | Loss: 0.756 | Acc: 80.561% (3506/4352)/ 95.290% (4147/4352)
01/14/2023 06:37:29 - INFO - __main__ -   test: [batch: 34/391 ] | Loss: 0.742 | Acc: 80.938% (3626/4480)/ 95.335% (4271/4480)
01/14/2023 06:37:31 - INFO - __main__ -   test: [batch: 35/391 ] | Loss: 0.729 | Acc: 81.315% (3747/4608)/ 95.334% (4393/4608)
01/14/2023 06:37:34 - INFO - __main__ -   test: [batch: 36/391 ] | Loss: 0.714 | Acc: 81.757% (3872/4736)/ 95.439% (4520/4736)
01/14/2023 06:37:36 - INFO - __main__ -   test: [batch: 37/391 ] | Loss: 0.704 | Acc: 82.072% (3992/4864)/ 95.498% (4645/4864)
01/14/2023 06:37:39 - INFO - __main__ -   test: [batch: 38/391 ] | Loss: 0.697 | Acc: 82.252% (4106/4992)/ 95.573% (4771/4992)
01/14/2023 06:37:41 - INFO - __main__ -   test: [batch: 39/391 ] | Loss: 0.692 | Acc: 82.305% (4214/5120)/ 95.605% (4895/5120)
01/14/2023 06:37:44 - INFO - __main__ -   test: [batch: 40/391 ] | Loss: 0.690 | Acc: 82.412% (4325/5248)/ 95.522% (5013/5248)
01/14/2023 06:37:46 - INFO - __main__ -   test: [batch: 41/391 ] | Loss: 0.694 | Acc: 82.459% (4433/5376)/ 95.461% (5132/5376)
01/14/2023 06:37:49 - INFO - __main__ -   test: [batch: 42/391 ] | Loss: 0.693 | Acc: 82.431% (4537/5504)/ 95.494% (5256/5504)
01/14/2023 06:37:52 - INFO - __main__ -   test: [batch: 43/391 ] | Loss: 0.690 | Acc: 82.511% (4647/5632)/ 95.455% (5376/5632)
01/14/2023 06:37:54 - INFO - __main__ -   test: [batch: 44/391 ] | Loss: 0.693 | Acc: 82.535% (4754/5760)/ 95.382% (5494/5760)
01/14/2023 06:37:57 - INFO - __main__ -   test: [batch: 45/391 ] | Loss: 0.690 | Acc: 82.711% (4870/5888)/ 95.380% (5616/5888)
01/14/2023 06:37:59 - INFO - __main__ -   test: [batch: 46/391 ] | Loss: 0.693 | Acc: 82.696% (4975/6016)/ 95.396% (5739/6016)
01/14/2023 06:38:02 - INFO - __main__ -   test: [batch: 47/391 ] | Loss: 0.694 | Acc: 82.617% (5076/6144)/ 95.459% (5865/6144)
01/14/2023 06:38:04 - INFO - __main__ -   test: [batch: 48/391 ] | Loss: 0.700 | Acc: 82.478% (5173/6272)/ 95.424% (5985/6272)
01/14/2023 06:38:07 - INFO - __main__ -   test: [batch: 49/391 ] | Loss: 0.703 | Acc: 82.531% (5282/6400)/ 95.344% (6102/6400)
01/14/2023 06:38:09 - INFO - __main__ -   test: [batch: 50/391 ] | Loss: 0.694 | Acc: 82.721% (5400/6528)/ 95.389% (6227/6528)
01/14/2023 06:38:12 - INFO - __main__ -   test: [batch: 51/391 ] | Loss: 0.687 | Acc: 82.888% (5517/6656)/ 95.478% (6355/6656)
01/14/2023 06:38:15 - INFO - __main__ -   test: [batch: 52/391 ] | Loss: 0.683 | Acc: 82.975% (5629/6784)/ 95.504% (6479/6784)
01/14/2023 06:38:17 - INFO - __main__ -   test: [batch: 53/391 ] | Loss: 0.674 | Acc: 83.189% (5750/6912)/ 95.573% (6606/6912)
01/14/2023 06:38:20 - INFO - __main__ -   test: [batch: 54/391 ] | Loss: 0.667 | Acc: 83.324% (5866/7040)/ 95.625% (6732/7040)
01/14/2023 06:38:23 - INFO - __main__ -   test: [batch: 55/391 ] | Loss: 0.661 | Acc: 83.468% (5983/7168)/ 95.661% (6857/7168)
01/14/2023 06:38:25 - INFO - __main__ -   test: [batch: 56/391 ] | Loss: 0.652 | Acc: 83.690% (6106/7296)/ 95.710% (6983/7296)
01/14/2023 06:38:28 - INFO - __main__ -   test: [batch: 57/391 ] | Loss: 0.645 | Acc: 83.890% (6228/7424)/ 95.757% (7109/7424)
01/14/2023 06:38:30 - INFO - __main__ -   test: [batch: 58/391 ] | Loss: 0.643 | Acc: 83.951% (6340/7552)/ 95.736% (7230/7552)
01/14/2023 06:38:33 - INFO - __main__ -   test: [batch: 59/391 ] | Loss: 0.646 | Acc: 83.880% (6442/7680)/ 95.755% (7354/7680)
01/14/2023 06:38:36 - INFO - __main__ -   test: [batch: 60/391 ] | Loss: 0.650 | Acc: 83.799% (6543/7808)/ 95.735% (7475/7808)
01/14/2023 06:38:38 - INFO - __main__ -   test: [batch: 61/391 ] | Loss: 0.651 | Acc: 83.795% (6650/7936)/ 95.728% (7597/7936)
01/14/2023 06:38:41 - INFO - __main__ -   test: [batch: 62/391 ] | Loss: 0.650 | Acc: 83.767% (6755/8064)/ 95.747% (7721/8064)
01/14/2023 06:38:43 - INFO - __main__ -   test: [batch: 63/391 ] | Loss: 0.658 | Acc: 83.630% (6851/8192)/ 95.691% (7839/8192)
01/14/2023 06:38:46 - INFO - __main__ -   test: [batch: 64/391 ] | Loss: 0.666 | Acc: 83.450% (6943/8320)/ 95.649% (7958/8320)
01/14/2023 06:38:49 - INFO - __main__ -   test: [batch: 65/391 ] | Loss: 0.680 | Acc: 82.955% (7008/8448)/ 95.573% (8074/8448)
01/14/2023 06:38:52 - INFO - __main__ -   test: [batch: 66/391 ] | Loss: 0.684 | Acc: 82.952% (7114/8576)/ 95.534% (8193/8576)
01/14/2023 06:38:54 - INFO - __main__ -   test: [batch: 67/391 ] | Loss: 0.687 | Acc: 82.881% (7214/8704)/ 95.542% (8316/8704)
01/14/2023 06:38:57 - INFO - __main__ -   test: [batch: 68/391 ] | Loss: 0.688 | Acc: 82.835% (7316/8832)/ 95.562% (8440/8832)
01/14/2023 06:38:59 - INFO - __main__ -   test: [batch: 69/391 ] | Loss: 0.683 | Acc: 82.935% (7431/8960)/ 95.603% (8566/8960)
01/14/2023 06:39:02 - INFO - __main__ -   test: [batch: 70/391 ] | Loss: 0.685 | Acc: 82.857% (7530/9088)/ 95.599% (8688/9088)
01/14/2023 06:39:05 - INFO - __main__ -   test: [batch: 71/391 ] | Loss: 0.683 | Acc: 82.888% (7639/9216)/ 95.605% (8811/9216)
01/14/2023 06:39:07 - INFO - __main__ -   test: [batch: 72/391 ] | Loss: 0.685 | Acc: 82.770% (7734/9344)/ 95.623% (8935/9344)
01/14/2023 06:39:10 - INFO - __main__ -   test: [batch: 73/391 ] | Loss: 0.689 | Acc: 82.644% (7828/9472)/ 95.629% (9058/9472)
01/14/2023 06:39:13 - INFO - __main__ -   test: [batch: 74/391 ] | Loss: 0.690 | Acc: 82.615% (7931/9600)/ 95.615% (9179/9600)
01/14/2023 06:39:15 - INFO - __main__ -   test: [batch: 75/391 ] | Loss: 0.696 | Acc: 82.453% (8021/9728)/ 95.590% (9299/9728)
01/14/2023 06:39:18 - INFO - __main__ -   test: [batch: 76/391 ] | Loss: 0.695 | Acc: 82.427% (8124/9856)/ 95.597% (9422/9856)
01/14/2023 06:39:21 - INFO - __main__ -   test: [batch: 77/391 ] | Loss: 0.696 | Acc: 82.382% (8225/9984)/ 95.623% (9547/9984)
01/14/2023 06:39:23 - INFO - __main__ -   test: [batch: 78/391 ] | Loss: 0.695 | Acc: 82.269% (8319/10112)/ 95.669% (9674/10112)
01/14/2023 06:39:26 - INFO - __main__ -   test: [batch: 79/391 ] | Loss: 0.694 | Acc: 82.256% (8423/10240)/ 95.693% (9799/10240)
01/14/2023 06:39:28 - INFO - __main__ -   test: [batch: 80/391 ] | Loss: 0.693 | Acc: 82.263% (8529/10368)/ 95.698% (9922/10368)
01/14/2023 06:39:31 - INFO - __main__ -   test: [batch: 81/391 ] | Loss: 0.692 | Acc: 82.260% (8634/10496)/ 95.722% (10047/10496)
01/14/2023 06:39:34 - INFO - __main__ -   test: [batch: 82/391 ] | Loss: 0.692 | Acc: 82.276% (8741/10624)/ 95.717% (10169/10624)
01/14/2023 06:39:36 - INFO - __main__ -   test: [batch: 83/391 ] | Loss: 0.691 | Acc: 82.301% (8849/10752)/ 95.703% (10290/10752)
01/14/2023 06:39:39 - INFO - __main__ -   test: [batch: 84/391 ] | Loss: 0.688 | Acc: 82.426% (8968/10880)/ 95.735% (10416/10880)
01/14/2023 06:39:42 - INFO - __main__ -   test: [batch: 85/391 ] | Loss: 0.686 | Acc: 82.422% (9073/11008)/ 95.776% (10543/11008)
01/14/2023 06:39:44 - INFO - __main__ -   test: [batch: 86/391 ] | Loss: 0.690 | Acc: 82.337% (9169/11136)/ 95.753% (10663/11136)
01/14/2023 06:39:47 - INFO - __main__ -   test: [batch: 87/391 ] | Loss: 0.688 | Acc: 82.351% (9276/11264)/ 95.756% (10786/11264)
01/14/2023 06:39:50 - INFO - __main__ -   test: [batch: 88/391 ] | Loss: 0.696 | Acc: 82.277% (9373/11392)/ 95.690% (10901/11392)
01/14/2023 06:39:52 - INFO - __main__ -   test: [batch: 89/391 ] | Loss: 0.693 | Acc: 82.335% (9485/11520)/ 95.703% (11025/11520)
01/14/2023 06:39:55 - INFO - __main__ -   test: [batch: 90/391 ] | Loss: 0.694 | Acc: 82.254% (9581/11648)/ 95.699% (11147/11648)
01/14/2023 06:39:57 - INFO - __main__ -   test: [batch: 91/391 ] | Loss: 0.693 | Acc: 82.294% (9691/11776)/ 95.712% (11271/11776)
01/14/2023 06:40:00 - INFO - __main__ -   test: [batch: 92/391 ] | Loss: 0.695 | Acc: 82.275% (9794/11904)/ 95.682% (11390/11904)
01/14/2023 06:40:03 - INFO - __main__ -   test: [batch: 93/391 ] | Loss: 0.698 | Acc: 82.081% (9876/12032)/ 95.711% (11516/12032)
01/14/2023 06:40:05 - INFO - __main__ -   test: [batch: 94/391 ] | Loss: 0.700 | Acc: 81.957% (9966/12160)/ 95.724% (11640/12160)
01/14/2023 06:40:08 - INFO - __main__ -   test: [batch: 95/391 ] | Loss: 0.698 | Acc: 82.015% (10078/12288)/ 95.728% (11763/12288)
01/14/2023 06:40:11 - INFO - __main__ -   test: [batch: 96/391 ] | Loss: 0.701 | Acc: 81.927% (10172/12416)/ 95.739% (11887/12416)
01/14/2023 06:40:13 - INFO - __main__ -   test: [batch: 97/391 ] | Loss: 0.703 | Acc: 81.752% (10255/12544)/ 95.759% (12012/12544)
01/14/2023 06:40:16 - INFO - __main__ -   test: [batch: 98/391 ] | Loss: 0.700 | Acc: 81.850% (10372/12672)/ 95.786% (12138/12672)
01/14/2023 06:40:18 - INFO - __main__ -   test: [batch: 99/391 ] | Loss: 0.694 | Acc: 81.992% (10495/12800)/ 95.828% (12266/12800)
01/14/2023 06:40:21 - INFO - __main__ -   test: [batch: 100/391 ] | Loss: 0.693 | Acc: 82.047% (10607/12928)/ 95.846% (12391/12928)
01/14/2023 06:40:24 - INFO - __main__ -   test: [batch: 101/391 ] | Loss: 0.689 | Acc: 82.146% (10725/13056)/ 95.879% (12518/13056)
01/14/2023 06:40:26 - INFO - __main__ -   test: [batch: 102/391 ] | Loss: 0.685 | Acc: 82.282% (10848/13184)/ 95.904% (12644/13184)
01/14/2023 06:40:29 - INFO - __main__ -   test: [batch: 103/391 ] | Loss: 0.687 | Acc: 82.114% (10931/13312)/ 95.921% (12769/13312)
01/14/2023 06:40:32 - INFO - __main__ -   test: [batch: 104/391 ] | Loss: 0.687 | Acc: 82.046% (11027/13440)/ 95.923% (12892/13440)
01/14/2023 06:40:34 - INFO - __main__ -   test: [batch: 105/391 ] | Loss: 0.687 | Acc: 82.031% (11130/13568)/ 95.917% (13014/13568)
01/14/2023 06:40:37 - INFO - __main__ -   test: [batch: 106/391 ] | Loss: 0.696 | Acc: 81.907% (11218/13696)/ 95.846% (13127/13696)
01/14/2023 06:40:40 - INFO - __main__ -   test: [batch: 107/391 ] | Loss: 0.692 | Acc: 82.031% (11340/13824)/ 95.877% (13254/13824)
01/14/2023 06:40:42 - INFO - __main__ -   test: [batch: 108/391 ] | Loss: 0.695 | Acc: 81.902% (11427/13952)/ 95.886% (13378/13952)
01/14/2023 06:40:45 - INFO - __main__ -   test: [batch: 109/391 ] | Loss: 0.696 | Acc: 81.882% (11529/14080)/ 95.888% (13501/14080)
01/14/2023 06:40:48 - INFO - __main__ -   test: [batch: 110/391 ] | Loss: 0.699 | Acc: 81.700% (11608/14208)/ 95.897% (13625/14208)
01/14/2023 06:40:50 - INFO - __main__ -   test: [batch: 111/391 ] | Loss: 0.702 | Acc: 81.662% (11707/14336)/ 95.864% (13743/14336)
01/14/2023 06:40:53 - INFO - __main__ -   test: [batch: 112/391 ] | Loss: 0.703 | Acc: 81.692% (11816/14464)/ 95.879% (13868/14464)
01/14/2023 06:40:56 - INFO - __main__ -   test: [batch: 113/391 ] | Loss: 0.702 | Acc: 81.730% (11926/14592)/ 95.888% (13992/14592)
01/14/2023 06:40:59 - INFO - __main__ -   test: [batch: 114/391 ] | Loss: 0.698 | Acc: 81.821% (12044/14720)/ 95.910% (14118/14720)
01/14/2023 06:41:01 - INFO - __main__ -   test: [batch: 115/391 ] | Loss: 0.695 | Acc: 81.903% (12161/14848)/ 95.932% (14244/14848)
01/14/2023 06:41:04 - INFO - __main__ -   test: [batch: 116/391 ] | Loss: 0.699 | Acc: 81.851% (12258/14976)/ 95.893% (14361/14976)
01/14/2023 06:41:06 - INFO - __main__ -   test: [batch: 117/391 ] | Loss: 0.697 | Acc: 81.872% (12366/15104)/ 95.902% (14485/15104)
01/14/2023 06:41:09 - INFO - __main__ -   test: [batch: 118/391 ] | Loss: 0.701 | Acc: 81.762% (12454/15232)/ 95.910% (14609/15232)
01/14/2023 06:41:12 - INFO - __main__ -   test: [batch: 119/391 ] | Loss: 0.699 | Acc: 81.810% (12566/15360)/ 95.931% (14735/15360)
01/14/2023 06:41:14 - INFO - __main__ -   test: [batch: 120/391 ] | Loss: 0.697 | Acc: 81.850% (12677/15488)/ 95.939% (14859/15488)
01/14/2023 06:41:17 - INFO - __main__ -   test: [batch: 121/391 ] | Loss: 0.702 | Acc: 81.705% (12759/15616)/ 95.908% (14977/15616)
01/14/2023 06:41:19 - INFO - __main__ -   test: [batch: 122/391 ] | Loss: 0.704 | Acc: 81.650% (12855/15744)/ 95.884% (15096/15744)
01/14/2023 06:41:22 - INFO - __main__ -   test: [batch: 123/391 ] | Loss: 0.705 | Acc: 81.666% (12962/15872)/ 95.880% (15218/15872)
01/14/2023 06:41:25 - INFO - __main__ -   test: [batch: 124/391 ] | Loss: 0.704 | Acc: 81.675% (13068/16000)/ 95.894% (15343/16000)
01/14/2023 06:41:27 - INFO - __main__ -   test: [batch: 125/391 ] | Loss: 0.699 | Acc: 81.777% (13189/16128)/ 95.920% (15470/16128)
01/14/2023 06:41:30 - INFO - __main__ -   test: [batch: 126/391 ] | Loss: 0.696 | Acc: 81.871% (13309/16256)/ 95.934% (15595/16256)
01/14/2023 06:41:32 - INFO - __main__ -   test: [batch: 127/391 ] | Loss: 0.694 | Acc: 81.958% (13428/16384)/ 95.941% (15719/16384)
01/14/2023 06:41:35 - INFO - __main__ -   test: [batch: 128/391 ] | Loss: 0.695 | Acc: 81.940% (13530/16512)/ 95.912% (15837/16512)
01/14/2023 06:41:38 - INFO - __main__ -   test: [batch: 129/391 ] | Loss: 0.693 | Acc: 81.953% (13637/16640)/ 95.919% (15961/16640)
01/14/2023 06:41:40 - INFO - __main__ -   test: [batch: 130/391 ] | Loss: 0.690 | Acc: 82.049% (13758/16768)/ 95.939% (16087/16768)
01/14/2023 06:41:43 - INFO - __main__ -   test: [batch: 131/391 ] | Loss: 0.690 | Acc: 82.096% (13871/16896)/ 95.946% (16211/16896)
01/14/2023 06:41:46 - INFO - __main__ -   test: [batch: 132/391 ] | Loss: 0.687 | Acc: 82.172% (13989/17024)/ 95.959% (16336/17024)
01/14/2023 06:41:48 - INFO - __main__ -   test: [batch: 133/391 ] | Loss: 0.689 | Acc: 82.125% (14086/17152)/ 95.942% (16456/17152)
01/14/2023 06:41:51 - INFO - __main__ -   test: [batch: 134/391 ] | Loss: 0.687 | Acc: 82.153% (14196/17280)/ 95.961% (16582/17280)
01/14/2023 06:41:53 - INFO - __main__ -   test: [batch: 135/391 ] | Loss: 0.687 | Acc: 82.146% (14300/17408)/ 95.956% (16704/17408)
01/14/2023 06:41:56 - INFO - __main__ -   test: [batch: 136/391 ] | Loss: 0.686 | Acc: 82.094% (14396/17536)/ 95.980% (16831/17536)
01/14/2023 06:41:58 - INFO - __main__ -   test: [batch: 137/391 ] | Loss: 0.684 | Acc: 82.133% (14508/17664)/ 96.009% (16959/17664)
01/14/2023 06:42:01 - INFO - __main__ -   test: [batch: 138/391 ] | Loss: 0.683 | Acc: 82.138% (14614/17792)/ 96.015% (17083/17792)
01/14/2023 06:42:04 - INFO - __main__ -   test: [batch: 139/391 ] | Loss: 0.688 | Acc: 81.987% (14692/17920)/ 96.010% (17205/17920)
01/14/2023 06:42:07 - INFO - __main__ -   test: [batch: 140/391 ] | Loss: 0.691 | Acc: 81.915% (14784/18048)/ 96.005% (17327/18048)
01/14/2023 06:42:09 - INFO - __main__ -   test: [batch: 141/391 ] | Loss: 0.689 | Acc: 81.954% (14896/18176)/ 96.017% (17452/18176)
01/14/2023 06:42:12 - INFO - __main__ -   test: [batch: 142/391 ] | Loss: 0.686 | Acc: 82.031% (15015/18304)/ 96.034% (17578/18304)
01/14/2023 06:42:14 - INFO - __main__ -   test: [batch: 143/391 ] | Loss: 0.687 | Acc: 82.042% (15122/18432)/ 96.012% (17697/18432)
01/14/2023 06:42:17 - INFO - __main__ -   test: [batch: 144/391 ] | Loss: 0.689 | Acc: 82.015% (15222/18560)/ 95.981% (17814/18560)
01/14/2023 06:42:20 - INFO - __main__ -   test: [batch: 145/391 ] | Loss: 0.692 | Acc: 81.999% (15324/18688)/ 95.955% (17932/18688)
01/14/2023 06:42:23 - INFO - __main__ -   test: [batch: 146/391 ] | Loss: 0.693 | Acc: 81.978% (15425/18816)/ 95.945% (18053/18816)
01/14/2023 06:42:25 - INFO - __main__ -   test: [batch: 147/391 ] | Loss: 0.694 | Acc: 81.994% (15533/18944)/ 95.925% (18172/18944)
01/14/2023 06:42:28 - INFO - __main__ -   test: [batch: 148/391 ] | Loss: 0.695 | Acc: 81.932% (15626/19072)/ 95.921% (18294/19072)
01/14/2023 06:42:30 - INFO - __main__ -   test: [batch: 149/391 ] | Loss: 0.699 | Acc: 81.849% (15715/19200)/ 95.885% (18410/19200)
01/14/2023 06:42:33 - INFO - __main__ -   test: [batch: 150/391 ] | Loss: 0.699 | Acc: 81.798% (15810/19328)/ 95.892% (18534/19328)
01/14/2023 06:42:35 - INFO - __main__ -   test: [batch: 151/391 ] | Loss: 0.699 | Acc: 81.826% (15920/19456)/ 95.893% (18657/19456)
01/14/2023 06:42:38 - INFO - __main__ -   test: [batch: 152/391 ] | Loss: 0.700 | Acc: 81.807% (16021/19584)/ 95.884% (18778/19584)
01/14/2023 06:42:41 - INFO - __main__ -   test: [batch: 153/391 ] | Loss: 0.698 | Acc: 81.859% (16136/19712)/ 95.886% (18901/19712)
01/14/2023 06:42:43 - INFO - __main__ -   test: [batch: 154/391 ] | Loss: 0.698 | Acc: 81.855% (16240/19840)/ 95.872% (19021/19840)
01/14/2023 06:42:46 - INFO - __main__ -   test: [batch: 155/391 ] | Loss: 0.699 | Acc: 81.871% (16348/19968)/ 95.868% (19143/19968)
01/14/2023 06:42:49 - INFO - __main__ -   test: [batch: 156/391 ] | Loss: 0.701 | Acc: 81.777% (16434/20096)/ 95.845% (19261/20096)
01/14/2023 06:42:51 - INFO - __main__ -   test: [batch: 157/391 ] | Loss: 0.702 | Acc: 81.759% (16535/20224)/ 95.827% (19380/20224)
01/14/2023 06:42:54 - INFO - __main__ -   test: [batch: 158/391 ] | Loss: 0.703 | Acc: 81.722% (16632/20352)/ 95.804% (19498/20352)
01/14/2023 06:42:56 - INFO - __main__ -   test: [batch: 159/391 ] | Loss: 0.705 | Acc: 81.699% (16732/20480)/ 95.796% (19619/20480)
01/14/2023 06:42:59 - INFO - __main__ -   test: [batch: 160/391 ] | Loss: 0.705 | Acc: 81.682% (16833/20608)/ 95.783% (19739/20608)
01/14/2023 06:43:01 - INFO - __main__ -   test: [batch: 161/391 ] | Loss: 0.714 | Acc: 81.448% (16889/20736)/ 95.693% (19843/20736)
01/14/2023 06:43:04 - INFO - __main__ -   test: [batch: 162/391 ] | Loss: 0.720 | Acc: 81.355% (16974/20864)/ 95.614% (19949/20864)
01/14/2023 06:43:07 - INFO - __main__ -   test: [batch: 163/391 ] | Loss: 0.722 | Acc: 81.307% (17068/20992)/ 95.603% (20069/20992)
01/14/2023 06:43:09 - INFO - __main__ -   test: [batch: 164/391 ] | Loss: 0.722 | Acc: 81.297% (17170/21120)/ 95.620% (20195/21120)
01/14/2023 06:43:12 - INFO - __main__ -   test: [batch: 165/391 ] | Loss: 0.724 | Acc: 81.208% (17255/21248)/ 95.614% (20316/21248)
01/14/2023 06:43:15 - INFO - __main__ -   test: [batch: 166/391 ] | Loss: 0.724 | Acc: 81.217% (17361/21376)/ 95.598% (20435/21376)
01/14/2023 06:43:17 - INFO - __main__ -   test: [batch: 167/391 ] | Loss: 0.726 | Acc: 81.152% (17451/21504)/ 95.578% (20553/21504)
01/14/2023 06:43:20 - INFO - __main__ -   test: [batch: 168/391 ] | Loss: 0.725 | Acc: 81.162% (17557/21632)/ 95.571% (20674/21632)
01/14/2023 06:43:23 - INFO - __main__ -   test: [batch: 169/391 ] | Loss: 0.727 | Acc: 81.135% (17655/21760)/ 95.533% (20788/21760)
01/14/2023 06:43:25 - INFO - __main__ -   test: [batch: 170/391 ] | Loss: 0.732 | Acc: 81.022% (17734/21888)/ 95.491% (20901/21888)
01/14/2023 06:43:28 - INFO - __main__ -   test: [batch: 171/391 ] | Loss: 0.735 | Acc: 80.959% (17824/22016)/ 95.458% (21016/22016)
01/14/2023 06:43:31 - INFO - __main__ -   test: [batch: 172/391 ] | Loss: 0.736 | Acc: 80.911% (17917/22144)/ 95.457% (21138/22144)
01/14/2023 06:43:33 - INFO - __main__ -   test: [batch: 173/391 ] | Loss: 0.738 | Acc: 80.855% (18008/22272)/ 95.429% (21254/22272)
01/14/2023 06:43:36 - INFO - __main__ -   test: [batch: 174/391 ] | Loss: 0.743 | Acc: 80.768% (18092/22400)/ 95.379% (21365/22400)
01/14/2023 06:43:38 - INFO - __main__ -   test: [batch: 175/391 ] | Loss: 0.741 | Acc: 80.811% (18205/22528)/ 95.384% (21488/22528)
01/14/2023 06:43:41 - INFO - __main__ -   test: [batch: 176/391 ] | Loss: 0.742 | Acc: 80.809% (18308/22656)/ 95.361% (21605/22656)
01/14/2023 06:43:43 - INFO - __main__ -   test: [batch: 177/391 ] | Loss: 0.745 | Acc: 80.728% (18393/22784)/ 95.317% (21717/22784)
01/14/2023 06:43:46 - INFO - __main__ -   test: [batch: 178/391 ] | Loss: 0.747 | Acc: 80.709% (18492/22912)/ 95.295% (21834/22912)
01/14/2023 06:43:49 - INFO - __main__ -   test: [batch: 179/391 ] | Loss: 0.750 | Acc: 80.647% (18581/23040)/ 95.252% (21946/23040)
01/14/2023 06:43:51 - INFO - __main__ -   test: [batch: 180/391 ] | Loss: 0.757 | Acc: 80.521% (18655/23168)/ 95.218% (22060/23168)
01/14/2023 06:43:54 - INFO - __main__ -   test: [batch: 181/391 ] | Loss: 0.763 | Acc: 80.391% (18728/23296)/ 95.162% (22169/23296)
01/14/2023 06:43:56 - INFO - __main__ -   test: [batch: 182/391 ] | Loss: 0.762 | Acc: 80.400% (18833/23424)/ 95.163% (22291/23424)
01/14/2023 06:43:59 - INFO - __main__ -   test: [batch: 183/391 ] | Loss: 0.768 | Acc: 80.282% (18908/23552)/ 95.070% (22391/23552)
01/14/2023 06:44:02 - INFO - __main__ -   test: [batch: 184/391 ] | Loss: 0.768 | Acc: 80.308% (19017/23680)/ 95.055% (22509/23680)
01/14/2023 06:44:04 - INFO - __main__ -   test: [batch: 185/391 ] | Loss: 0.768 | Acc: 80.318% (19122/23808)/ 95.048% (22629/23808)
01/14/2023 06:44:07 - INFO - __main__ -   test: [batch: 186/391 ] | Loss: 0.771 | Acc: 80.285% (19217/23936)/ 95.012% (22742/23936)
01/14/2023 06:44:10 - INFO - __main__ -   test: [batch: 187/391 ] | Loss: 0.776 | Acc: 80.170% (19292/24064)/ 94.984% (22857/24064)
01/14/2023 06:44:12 - INFO - __main__ -   test: [batch: 188/391 ] | Loss: 0.779 | Acc: 80.039% (19363/24192)/ 94.961% (22973/24192)
01/14/2023 06:44:15 - INFO - __main__ -   test: [batch: 189/391 ] | Loss: 0.781 | Acc: 79.975% (19450/24320)/ 94.963% (23095/24320)
01/14/2023 06:44:17 - INFO - __main__ -   test: [batch: 190/391 ] | Loss: 0.786 | Acc: 79.888% (19531/24448)/ 94.932% (23209/24448)
01/14/2023 06:44:20 - INFO - __main__ -   test: [batch: 191/391 ] | Loss: 0.788 | Acc: 79.850% (19624/24576)/ 94.906% (23324/24576)
01/14/2023 06:44:23 - INFO - __main__ -   test: [batch: 192/391 ] | Loss: 0.794 | Acc: 79.740% (19699/24704)/ 94.827% (23426/24704)
01/14/2023 06:44:25 - INFO - __main__ -   test: [batch: 193/391 ] | Loss: 0.794 | Acc: 79.756% (19805/24832)/ 94.825% (23547/24832)
01/14/2023 06:44:28 - INFO - __main__ -   test: [batch: 194/391 ] | Loss: 0.797 | Acc: 79.688% (19890/24960)/ 94.816% (23666/24960)
01/14/2023 06:44:31 - INFO - __main__ -   test: [batch: 195/391 ] | Loss: 0.802 | Acc: 79.616% (19974/25088)/ 94.754% (23772/25088)
01/14/2023 06:44:33 - INFO - __main__ -   test: [batch: 196/391 ] | Loss: 0.805 | Acc: 79.517% (20051/25216)/ 94.706% (23881/25216)
01/14/2023 06:44:36 - INFO - __main__ -   test: [batch: 197/391 ] | Loss: 0.810 | Acc: 79.443% (20134/25344)/ 94.681% (23996/25344)
01/14/2023 06:44:39 - INFO - __main__ -   test: [batch: 198/391 ] | Loss: 0.812 | Acc: 79.401% (20225/25472)/ 94.649% (24109/25472)
01/14/2023 06:44:41 - INFO - __main__ -   test: [batch: 199/391 ] | Loss: 0.811 | Acc: 79.395% (20325/25600)/ 94.652% (24231/25600)
01/14/2023 06:44:43 - INFO - __main__ -   test: [batch: 200/391 ] | Loss: 0.811 | Acc: 79.349% (20415/25728)/ 94.648% (24351/25728)
01/14/2023 06:44:46 - INFO - __main__ -   test: [batch: 201/391 ] | Loss: 0.815 | Acc: 79.266% (20495/25856)/ 94.612% (24463/25856)
01/14/2023 06:44:49 - INFO - __main__ -   test: [batch: 202/391 ] | Loss: 0.816 | Acc: 79.256% (20594/25984)/ 94.604% (24582/25984)
01/14/2023 06:44:51 - INFO - __main__ -   test: [batch: 203/391 ] | Loss: 0.816 | Acc: 79.247% (20693/26112)/ 94.600% (24702/26112)
01/14/2023 06:44:54 - INFO - __main__ -   test: [batch: 204/391 ] | Loss: 0.819 | Acc: 79.154% (20770/26240)/ 94.577% (24817/26240)
01/14/2023 06:44:56 - INFO - __main__ -   test: [batch: 205/391 ] | Loss: 0.822 | Acc: 79.069% (20849/26368)/ 94.543% (24929/26368)
01/14/2023 06:44:59 - INFO - __main__ -   test: [batch: 206/391 ] | Loss: 0.824 | Acc: 79.046% (20944/26496)/ 94.539% (25049/26496)
01/14/2023 06:45:02 - INFO - __main__ -   test: [batch: 207/391 ] | Loss: 0.827 | Acc: 78.966% (21024/26624)/ 94.497% (25159/26624)
01/14/2023 06:45:04 - INFO - __main__ -   test: [batch: 208/391 ] | Loss: 0.829 | Acc: 78.932% (21116/26752)/ 94.468% (25272/26752)
01/14/2023 06:45:07 - INFO - __main__ -   test: [batch: 209/391 ] | Loss: 0.829 | Acc: 78.932% (21217/26880)/ 94.487% (25398/26880)
01/14/2023 06:45:09 - INFO - __main__ -   test: [batch: 210/391 ] | Loss: 0.830 | Acc: 78.914% (21313/27008)/ 94.468% (25514/27008)
01/14/2023 06:45:12 - INFO - __main__ -   test: [batch: 211/391 ] | Loss: 0.832 | Acc: 78.847% (21396/27136)/ 94.458% (25632/27136)
01/14/2023 06:45:15 - INFO - __main__ -   test: [batch: 212/391 ] | Loss: 0.834 | Acc: 78.782% (21479/27264)/ 94.436% (25747/27264)
01/14/2023 06:45:17 - INFO - __main__ -   test: [batch: 213/391 ] | Loss: 0.834 | Acc: 78.786% (21581/27392)/ 94.440% (25869/27392)
01/14/2023 06:45:20 - INFO - __main__ -   test: [batch: 214/391 ] | Loss: 0.834 | Acc: 78.765% (21676/27520)/ 94.448% (25992/27520)
01/14/2023 06:45:22 - INFO - __main__ -   test: [batch: 215/391 ] | Loss: 0.834 | Acc: 78.783% (21782/27648)/ 94.441% (26111/27648)
01/14/2023 06:45:25 - INFO - __main__ -   test: [batch: 216/391 ] | Loss: 0.832 | Acc: 78.823% (21894/27776)/ 94.463% (26238/27776)
01/14/2023 06:45:28 - INFO - __main__ -   test: [batch: 217/391 ] | Loss: 0.837 | Acc: 78.759% (21977/27904)/ 94.399% (26341/27904)
01/14/2023 06:45:30 - INFO - __main__ -   test: [batch: 218/391 ] | Loss: 0.840 | Acc: 78.699% (22061/28032)/ 94.364% (26452/28032)
01/14/2023 06:45:33 - INFO - __main__ -   test: [batch: 219/391 ] | Loss: 0.839 | Acc: 78.729% (22170/28160)/ 94.368% (26574/28160)
01/14/2023 06:45:35 - INFO - __main__ -   test: [batch: 220/391 ] | Loss: 0.837 | Acc: 78.765% (22281/28288)/ 94.386% (26700/28288)
01/14/2023 06:45:38 - INFO - __main__ -   test: [batch: 221/391 ] | Loss: 0.839 | Acc: 78.723% (22370/28416)/ 94.369% (26816/28416)
01/14/2023 06:45:41 - INFO - __main__ -   test: [batch: 222/391 ] | Loss: 0.837 | Acc: 78.777% (22486/28544)/ 94.384% (26941/28544)
01/14/2023 06:45:43 - INFO - __main__ -   test: [batch: 223/391 ] | Loss: 0.837 | Acc: 78.777% (22587/28672)/ 94.381% (27061/28672)
01/14/2023 06:45:46 - INFO - __main__ -   test: [batch: 224/391 ] | Loss: 0.837 | Acc: 78.795% (22693/28800)/ 94.375% (27180/28800)
01/14/2023 06:45:48 - INFO - __main__ -   test: [batch: 225/391 ] | Loss: 0.836 | Acc: 78.796% (22794/28928)/ 94.386% (27304/28928)
01/14/2023 06:45:51 - INFO - __main__ -   test: [batch: 226/391 ] | Loss: 0.836 | Acc: 78.779% (22890/29056)/ 94.390% (27426/29056)
01/14/2023 06:45:54 - INFO - __main__ -   test: [batch: 227/391 ] | Loss: 0.839 | Acc: 78.769% (22988/29184)/ 94.370% (27541/29184)
01/14/2023 06:45:56 - INFO - __main__ -   test: [batch: 228/391 ] | Loss: 0.843 | Acc: 78.691% (23066/29312)/ 94.313% (27645/29312)
01/14/2023 06:45:59 - INFO - __main__ -   test: [batch: 229/391 ] | Loss: 0.846 | Acc: 78.635% (23150/29440)/ 94.270% (27753/29440)
01/14/2023 06:46:01 - INFO - __main__ -   test: [batch: 230/391 ] | Loss: 0.849 | Acc: 78.578% (23234/29568)/ 94.234% (27863/29568)
01/14/2023 06:46:04 - INFO - __main__ -   test: [batch: 231/391 ] | Loss: 0.849 | Acc: 78.573% (23333/29696)/ 94.218% (27979/29696)
01/14/2023 06:46:07 - INFO - __main__ -   test: [batch: 232/391 ] | Loss: 0.848 | Acc: 78.601% (23442/29824)/ 94.233% (28104/29824)
01/14/2023 06:46:09 - INFO - __main__ -   test: [batch: 233/391 ] | Loss: 0.850 | Acc: 78.549% (23527/29952)/ 94.214% (28219/29952)
01/14/2023 06:46:12 - INFO - __main__ -   test: [batch: 234/391 ] | Loss: 0.856 | Acc: 78.434% (23593/30080)/ 94.142% (28318/30080)
01/14/2023 06:46:14 - INFO - __main__ -   test: [batch: 235/391 ] | Loss: 0.856 | Acc: 78.423% (23690/30208)/ 94.134% (28436/30208)
01/14/2023 06:46:17 - INFO - __main__ -   test: [batch: 236/391 ] | Loss: 0.855 | Acc: 78.458% (23801/30336)/ 94.139% (28558/30336)
01/14/2023 06:46:19 - INFO - __main__ -   test: [batch: 237/391 ] | Loss: 0.856 | Acc: 78.453% (23900/30464)/ 94.108% (28669/30464)
01/14/2023 06:46:22 - INFO - __main__ -   test: [batch: 238/391 ] | Loss: 0.854 | Acc: 78.491% (24012/30592)/ 94.116% (28792/30592)
01/14/2023 06:46:24 - INFO - __main__ -   test: [batch: 239/391 ] | Loss: 0.853 | Acc: 78.529% (24124/30720)/ 94.118% (28913/30720)
01/14/2023 06:46:27 - INFO - __main__ -   test: [batch: 240/391 ] | Loss: 0.853 | Acc: 78.543% (24229/30848)/ 94.107% (29030/30848)
01/14/2023 06:46:30 - INFO - __main__ -   test: [batch: 241/391 ] | Loss: 0.856 | Acc: 78.454% (24302/30976)/ 94.070% (29139/30976)
01/14/2023 06:46:32 - INFO - __main__ -   test: [batch: 242/391 ] | Loss: 0.859 | Acc: 78.353% (24371/31104)/ 94.052% (29254/31104)
01/14/2023 06:46:35 - INFO - __main__ -   test: [batch: 243/391 ] | Loss: 0.865 | Acc: 78.215% (24428/31232)/ 93.984% (29353/31232)
01/14/2023 06:46:37 - INFO - __main__ -   test: [batch: 244/391 ] | Loss: 0.865 | Acc: 78.217% (24529/31360)/ 93.980% (29472/31360)
01/14/2023 06:46:40 - INFO - __main__ -   test: [batch: 245/391 ] | Loss: 0.864 | Acc: 78.223% (24631/31488)/ 93.972% (29590/31488)
01/14/2023 06:46:43 - INFO - __main__ -   test: [batch: 246/391 ] | Loss: 0.865 | Acc: 78.204% (24725/31616)/ 93.962% (29707/31616)
01/14/2023 06:46:45 - INFO - __main__ -   test: [batch: 247/391 ] | Loss: 0.870 | Acc: 78.109% (24795/31744)/ 93.895% (29806/31744)
01/14/2023 06:46:48 - INFO - __main__ -   test: [batch: 248/391 ] | Loss: 0.871 | Acc: 78.062% (24880/31872)/ 93.888% (29924/31872)
01/14/2023 06:46:51 - INFO - __main__ -   test: [batch: 249/391 ] | Loss: 0.873 | Acc: 77.938% (24940/32000)/ 93.878% (30041/32000)
01/14/2023 06:46:54 - INFO - __main__ -   test: [batch: 250/391 ] | Loss: 0.872 | Acc: 77.972% (25051/32128)/ 93.896% (30167/32128)
01/14/2023 06:46:56 - INFO - __main__ -   test: [batch: 251/391 ] | Loss: 0.874 | Acc: 77.936% (25139/32256)/ 93.865% (30277/32256)
01/14/2023 06:46:59 - INFO - __main__ -   test: [batch: 252/391 ] | Loss: 0.873 | Acc: 77.964% (25248/32384)/ 93.858% (30395/32384)
01/14/2023 06:47:01 - INFO - __main__ -   test: [batch: 253/391 ] | Loss: 0.875 | Acc: 77.934% (25338/32512)/ 93.842% (30510/32512)
01/14/2023 06:47:04 - INFO - __main__ -   test: [batch: 254/391 ] | Loss: 0.879 | Acc: 77.874% (25418/32640)/ 93.802% (30617/32640)
01/14/2023 06:47:07 - INFO - __main__ -   test: [batch: 255/391 ] | Loss: 0.881 | Acc: 77.838% (25506/32768)/ 93.793% (30734/32768)
01/14/2023 06:47:10 - INFO - __main__ -   test: [batch: 256/391 ] | Loss: 0.886 | Acc: 77.721% (25567/32896)/ 93.759% (30843/32896)
01/14/2023 06:47:12 - INFO - __main__ -   test: [batch: 257/391 ] | Loss: 0.887 | Acc: 77.710% (25663/33024)/ 93.747% (30959/33024)
01/14/2023 06:47:15 - INFO - __main__ -   test: [batch: 258/391 ] | Loss: 0.888 | Acc: 77.700% (25759/33152)/ 93.735% (31075/33152)
01/14/2023 06:47:17 - INFO - __main__ -   test: [batch: 259/391 ] | Loss: 0.892 | Acc: 77.581% (25819/33280)/ 93.723% (31191/33280)
01/14/2023 06:47:20 - INFO - __main__ -   test: [batch: 260/391 ] | Loss: 0.892 | Acc: 77.553% (25909/33408)/ 93.720% (31310/33408)
01/14/2023 06:47:23 - INFO - __main__ -   test: [batch: 261/391 ] | Loss: 0.890 | Acc: 77.597% (26023/33536)/ 93.741% (31437/33536)
01/14/2023 06:47:26 - INFO - __main__ -   test: [batch: 262/391 ] | Loss: 0.889 | Acc: 77.617% (26129/33664)/ 93.747% (31559/33664)
01/14/2023 06:47:28 - INFO - __main__ -   test: [batch: 263/391 ] | Loss: 0.892 | Acc: 77.551% (26206/33792)/ 93.720% (31670/33792)
01/14/2023 06:47:31 - INFO - __main__ -   test: [batch: 264/391 ] | Loss: 0.897 | Acc: 77.488% (26284/33920)/ 93.659% (31769/33920)
01/14/2023 06:47:34 - INFO - __main__ -   test: [batch: 265/391 ] | Loss: 0.896 | Acc: 77.517% (26393/34048)/ 93.656% (31888/34048)
01/14/2023 06:47:36 - INFO - __main__ -   test: [batch: 266/391 ] | Loss: 0.899 | Acc: 77.446% (26468/34176)/ 93.639% (32002/34176)
01/14/2023 06:47:39 - INFO - __main__ -   test: [batch: 267/391 ] | Loss: 0.898 | Acc: 77.475% (26577/34304)/ 93.639% (32122/34304)
01/14/2023 06:47:42 - INFO - __main__ -   test: [batch: 268/391 ] | Loss: 0.897 | Acc: 77.495% (26683/34432)/ 93.634% (32240/34432)
01/14/2023 06:47:44 - INFO - __main__ -   test: [batch: 269/391 ] | Loss: 0.899 | Acc: 77.433% (26761/34560)/ 93.614% (32353/34560)
01/14/2023 06:47:46 - INFO - __main__ -   test: [batch: 270/391 ] | Loss: 0.903 | Acc: 77.370% (26838/34688)/ 93.583% (32462/34688)
01/14/2023 06:47:49 - INFO - __main__ -   test: [batch: 271/391 ] | Loss: 0.903 | Acc: 77.381% (26941/34816)/ 93.578% (32580/34816)
01/14/2023 06:47:52 - INFO - __main__ -   test: [batch: 272/391 ] | Loss: 0.904 | Acc: 77.338% (27025/34944)/ 93.573% (32698/34944)
01/14/2023 06:47:54 - INFO - __main__ -   test: [batch: 273/391 ] | Loss: 0.904 | Acc: 77.347% (27127/35072)/ 93.568% (32816/35072)
01/14/2023 06:47:57 - INFO - __main__ -   test: [batch: 274/391 ] | Loss: 0.904 | Acc: 77.341% (27224/35200)/ 93.574% (32938/35200)
01/14/2023 06:47:59 - INFO - __main__ -   test: [batch: 275/391 ] | Loss: 0.906 | Acc: 77.313% (27313/35328)/ 93.569% (33056/35328)
01/14/2023 06:48:02 - INFO - __main__ -   test: [batch: 276/391 ] | Loss: 0.907 | Acc: 77.276% (27399/35456)/ 93.564% (33174/35456)
01/14/2023 06:48:05 - INFO - __main__ -   test: [batch: 277/391 ] | Loss: 0.907 | Acc: 77.248% (27488/35584)/ 93.550% (33289/35584)
01/14/2023 06:48:07 - INFO - __main__ -   test: [batch: 278/391 ] | Loss: 0.907 | Acc: 77.265% (27593/35712)/ 93.548% (33408/35712)
01/14/2023 06:48:10 - INFO - __main__ -   test: [batch: 279/391 ] | Loss: 0.907 | Acc: 77.274% (27695/35840)/ 93.541% (33525/35840)
01/14/2023 06:48:13 - INFO - __main__ -   test: [batch: 280/391 ] | Loss: 0.908 | Acc: 77.252% (27786/35968)/ 93.536% (33643/35968)
01/14/2023 06:48:15 - INFO - __main__ -   test: [batch: 281/391 ] | Loss: 0.908 | Acc: 77.258% (27887/36096)/ 93.531% (33761/36096)
01/14/2023 06:48:18 - INFO - __main__ -   test: [batch: 282/391 ] | Loss: 0.907 | Acc: 77.291% (27998/36224)/ 93.535% (33882/36224)
01/14/2023 06:48:21 - INFO - __main__ -   test: [batch: 283/391 ] | Loss: 0.908 | Acc: 77.267% (28088/36352)/ 93.530% (34000/36352)
01/14/2023 06:48:23 - INFO - __main__ -   test: [batch: 284/391 ] | Loss: 0.911 | Acc: 77.223% (28171/36480)/ 93.506% (34111/36480)
01/14/2023 06:48:26 - INFO - __main__ -   test: [batch: 285/391 ] | Loss: 0.913 | Acc: 77.177% (28253/36608)/ 93.469% (34217/36608)
01/14/2023 06:48:28 - INFO - __main__ -   test: [batch: 286/391 ] | Loss: 0.915 | Acc: 77.145% (28340/36736)/ 93.459% (34333/36736)
01/14/2023 06:48:31 - INFO - __main__ -   test: [batch: 287/391 ] | Loss: 0.915 | Acc: 77.146% (28439/36864)/ 93.452% (34450/36864)
01/14/2023 06:48:33 - INFO - __main__ -   test: [batch: 288/391 ] | Loss: 0.914 | Acc: 77.168% (28546/36992)/ 93.455% (34571/36992)
01/14/2023 06:48:36 - INFO - __main__ -   test: [batch: 289/391 ] | Loss: 0.916 | Acc: 77.131% (28631/37120)/ 93.424% (34679/37120)
01/14/2023 06:48:39 - INFO - __main__ -   test: [batch: 290/391 ] | Loss: 0.917 | Acc: 77.051% (28700/37248)/ 93.425% (34799/37248)
01/14/2023 06:48:41 - INFO - __main__ -   test: [batch: 291/391 ] | Loss: 0.917 | Acc: 77.063% (28803/37376)/ 93.416% (34915/37376)
01/14/2023 06:48:44 - INFO - __main__ -   test: [batch: 292/391 ] | Loss: 0.919 | Acc: 77.016% (28884/37504)/ 93.398% (35028/37504)
01/14/2023 06:48:46 - INFO - __main__ -   test: [batch: 293/391 ] | Loss: 0.919 | Acc: 77.014% (28982/37632)/ 93.394% (35146/37632)
01/14/2023 06:48:49 - INFO - __main__ -   test: [batch: 294/391 ] | Loss: 0.920 | Acc: 76.994% (29073/37760)/ 93.387% (35263/37760)
01/14/2023 06:48:51 - INFO - __main__ -   test: [batch: 295/391 ] | Loss: 0.919 | Acc: 77.024% (29183/37888)/ 93.396% (35386/37888)
01/14/2023 06:48:54 - INFO - __main__ -   test: [batch: 296/391 ] | Loss: 0.919 | Acc: 77.023% (29281/38016)/ 93.387% (35502/38016)
01/14/2023 06:48:57 - INFO - __main__ -   test: [batch: 297/391 ] | Loss: 0.921 | Acc: 77.000% (29371/38144)/ 93.362% (35612/38144)
01/14/2023 06:49:00 - INFO - __main__ -   test: [batch: 298/391 ] | Loss: 0.923 | Acc: 76.967% (29457/38272)/ 93.337% (35722/38272)
01/14/2023 06:49:02 - INFO - __main__ -   test: [batch: 299/391 ] | Loss: 0.924 | Acc: 76.956% (29551/38400)/ 93.318% (35834/38400)
01/14/2023 06:49:05 - INFO - __main__ -   test: [batch: 300/391 ] | Loss: 0.924 | Acc: 76.960% (29651/38528)/ 93.317% (35953/38528)
01/14/2023 06:49:07 - INFO - __main__ -   test: [batch: 301/391 ] | Loss: 0.925 | Acc: 76.945% (29744/38656)/ 93.302% (36067/38656)
01/14/2023 06:49:10 - INFO - __main__ -   test: [batch: 302/391 ] | Loss: 0.927 | Acc: 76.908% (29828/38784)/ 93.286% (36180/38784)
01/14/2023 06:49:12 - INFO - __main__ -   test: [batch: 303/391 ] | Loss: 0.928 | Acc: 76.889% (29919/38912)/ 93.269% (36293/38912)
01/14/2023 06:49:15 - INFO - __main__ -   test: [batch: 304/391 ] | Loss: 0.927 | Acc: 76.903% (30023/39040)/ 93.274% (36414/39040)
01/14/2023 06:49:18 - INFO - __main__ -   test: [batch: 305/391 ] | Loss: 0.928 | Acc: 76.859% (30104/39168)/ 93.270% (36532/39168)
01/14/2023 06:49:20 - INFO - __main__ -   test: [batch: 306/391 ] | Loss: 0.929 | Acc: 76.842% (30196/39296)/ 93.244% (36641/39296)
01/14/2023 06:49:23 - INFO - __main__ -   test: [batch: 307/391 ] | Loss: 0.930 | Acc: 76.842% (30294/39424)/ 93.230% (36755/39424)
01/14/2023 06:49:25 - INFO - __main__ -   test: [batch: 308/391 ] | Loss: 0.930 | Acc: 76.823% (30385/39552)/ 93.227% (36873/39552)
01/14/2023 06:49:28 - INFO - __main__ -   test: [batch: 309/391 ] | Loss: 0.931 | Acc: 76.820% (30482/39680)/ 93.208% (36985/39680)
01/14/2023 06:49:31 - INFO - __main__ -   test: [batch: 310/391 ] | Loss: 0.932 | Acc: 76.811% (30577/39808)/ 93.195% (37099/39808)
01/14/2023 06:49:34 - INFO - __main__ -   test: [batch: 311/391 ] | Loss: 0.933 | Acc: 76.793% (30668/39936)/ 93.184% (37214/39936)
01/14/2023 06:49:36 - INFO - __main__ -   test: [batch: 312/391 ] | Loss: 0.935 | Acc: 76.772% (30758/40064)/ 93.158% (37323/40064)
01/14/2023 06:49:39 - INFO - __main__ -   test: [batch: 313/391 ] | Loss: 0.933 | Acc: 76.811% (30872/40192)/ 93.175% (37449/40192)
01/14/2023 06:49:41 - INFO - __main__ -   test: [batch: 314/391 ] | Loss: 0.933 | Acc: 76.796% (30964/40320)/ 93.172% (37567/40320)
01/14/2023 06:49:44 - INFO - __main__ -   test: [batch: 315/391 ] | Loss: 0.934 | Acc: 76.778% (31055/40448)/ 93.157% (37680/40448)
01/14/2023 06:49:46 - INFO - __main__ -   test: [batch: 316/391 ] | Loss: 0.937 | Acc: 76.691% (31118/40576)/ 93.131% (37789/40576)
01/14/2023 06:49:49 - INFO - __main__ -   test: [batch: 317/391 ] | Loss: 0.938 | Acc: 76.648% (31199/40704)/ 93.109% (37899/40704)
01/14/2023 06:49:52 - INFO - __main__ -   test: [batch: 318/391 ] | Loss: 0.937 | Acc: 76.683% (31311/40832)/ 93.125% (38025/40832)
01/14/2023 06:49:54 - INFO - __main__ -   test: [batch: 319/391 ] | Loss: 0.940 | Acc: 76.628% (31387/40960)/ 93.093% (38131/40960)
01/14/2023 06:49:57 - INFO - __main__ -   test: [batch: 320/391 ] | Loss: 0.939 | Acc: 76.665% (31500/41088)/ 93.103% (38254/41088)
01/14/2023 06:50:00 - INFO - __main__ -   test: [batch: 321/391 ] | Loss: 0.938 | Acc: 76.684% (31606/41216)/ 93.100% (38372/41216)
01/14/2023 06:50:02 - INFO - __main__ -   test: [batch: 322/391 ] | Loss: 0.940 | Acc: 76.662% (31695/41344)/ 93.090% (38487/41344)
01/14/2023 06:50:05 - INFO - __main__ -   test: [batch: 323/391 ] | Loss: 0.942 | Acc: 76.625% (31778/41472)/ 93.068% (38597/41472)
01/14/2023 06:50:08 - INFO - __main__ -   test: [batch: 324/391 ] | Loss: 0.942 | Acc: 76.620% (31874/41600)/ 93.058% (38712/41600)
01/14/2023 06:50:10 - INFO - __main__ -   test: [batch: 325/391 ] | Loss: 0.942 | Acc: 76.627% (31975/41728)/ 93.055% (38830/41728)
01/14/2023 06:50:13 - INFO - __main__ -   test: [batch: 326/391 ] | Loss: 0.945 | Acc: 76.546% (32039/41856)/ 93.021% (38935/41856)
01/14/2023 06:50:15 - INFO - __main__ -   test: [batch: 327/391 ] | Loss: 0.949 | Acc: 76.462% (32102/41984)/ 92.990% (39041/41984)
01/14/2023 06:50:18 - INFO - __main__ -   test: [batch: 328/391 ] | Loss: 0.951 | Acc: 76.411% (32178/42112)/ 92.964% (39149/42112)
01/14/2023 06:50:21 - INFO - __main__ -   test: [batch: 329/391 ] | Loss: 0.951 | Acc: 76.406% (32274/42240)/ 92.964% (39268/42240)
01/14/2023 06:50:23 - INFO - __main__ -   test: [batch: 330/391 ] | Loss: 0.953 | Acc: 76.364% (32354/42368)/ 92.936% (39375/42368)
01/14/2023 06:50:26 - INFO - __main__ -   test: [batch: 331/391 ] | Loss: 0.953 | Acc: 76.332% (32438/42496)/ 92.950% (39500/42496)
01/14/2023 06:50:28 - INFO - __main__ -   test: [batch: 332/391 ] | Loss: 0.953 | Acc: 76.328% (32534/42624)/ 92.948% (39618/42624)
01/14/2023 06:50:31 - INFO - __main__ -   test: [batch: 333/391 ] | Loss: 0.952 | Acc: 76.357% (32644/42752)/ 92.957% (39741/42752)
01/14/2023 06:50:34 - INFO - __main__ -   test: [batch: 334/391 ] | Loss: 0.953 | Acc: 76.327% (32729/42880)/ 92.936% (39851/42880)
01/14/2023 06:50:36 - INFO - __main__ -   test: [batch: 335/391 ] | Loss: 0.954 | Acc: 76.304% (32817/43008)/ 92.922% (39964/43008)
01/14/2023 06:50:39 - INFO - __main__ -   test: [batch: 336/391 ] | Loss: 0.956 | Acc: 76.259% (32895/43136)/ 92.908% (40077/43136)
01/14/2023 06:50:41 - INFO - __main__ -   test: [batch: 337/391 ] | Loss: 0.956 | Acc: 76.255% (32991/43264)/ 92.902% (40193/43264)
01/14/2023 06:50:44 - INFO - __main__ -   test: [batch: 338/391 ] | Loss: 0.956 | Acc: 76.242% (33083/43392)/ 92.913% (40317/43392)
01/14/2023 06:50:46 - INFO - __main__ -   test: [batch: 339/391 ] | Loss: 0.959 | Acc: 76.209% (33166/43520)/ 92.891% (40426/43520)
01/14/2023 06:50:49 - INFO - __main__ -   test: [batch: 340/391 ] | Loss: 0.959 | Acc: 76.203% (33261/43648)/ 92.900% (40549/43648)
01/14/2023 06:50:52 - INFO - __main__ -   test: [batch: 341/391 ] | Loss: 0.957 | Acc: 76.243% (33376/43776)/ 92.918% (40676/43776)
01/14/2023 06:50:54 - INFO - __main__ -   test: [batch: 342/391 ] | Loss: 0.957 | Acc: 76.194% (33452/43904)/ 92.914% (40793/43904)
01/14/2023 06:50:57 - INFO - __main__ -   test: [batch: 343/391 ] | Loss: 0.957 | Acc: 76.192% (33549/44032)/ 92.919% (40914/44032)
01/14/2023 06:50:59 - INFO - __main__ -   test: [batch: 344/391 ] | Loss: 0.958 | Acc: 76.191% (33646/44160)/ 92.910% (41029/44160)
01/14/2023 06:51:02 - INFO - __main__ -   test: [batch: 345/391 ] | Loss: 0.961 | Acc: 76.124% (33714/44288)/ 92.869% (41130/44288)
01/14/2023 06:51:04 - INFO - __main__ -   test: [batch: 346/391 ] | Loss: 0.963 | Acc: 76.110% (33805/44416)/ 92.854% (41242/44416)
01/14/2023 06:51:07 - INFO - __main__ -   test: [batch: 347/391 ] | Loss: 0.962 | Acc: 76.122% (33908/44544)/ 92.863% (41365/44544)
01/14/2023 06:51:09 - INFO - __main__ -   test: [batch: 348/391 ] | Loss: 0.964 | Acc: 76.101% (33996/44672)/ 92.841% (41474/44672)
01/14/2023 06:51:12 - INFO - __main__ -   test: [batch: 349/391 ] | Loss: 0.963 | Acc: 76.100% (34093/44800)/ 92.850% (41597/44800)
01/14/2023 06:51:15 - INFO - __main__ -   test: [batch: 350/391 ] | Loss: 0.963 | Acc: 76.111% (34195/44928)/ 92.849% (41715/44928)
01/14/2023 06:51:17 - INFO - __main__ -   test: [batch: 351/391 ] | Loss: 0.965 | Acc: 76.052% (34266/45056)/ 92.829% (41825/45056)
01/14/2023 06:51:20 - INFO - __main__ -   test: [batch: 352/391 ] | Loss: 0.966 | Acc: 76.049% (34362/45184)/ 92.825% (41942/45184)
01/14/2023 06:51:23 - INFO - __main__ -   test: [batch: 353/391 ] | Loss: 0.968 | Acc: 76.006% (34440/45312)/ 92.783% (42042/45312)
01/14/2023 06:51:25 - INFO - __main__ -   test: [batch: 354/391 ] | Loss: 0.970 | Acc: 75.951% (34512/45440)/ 92.766% (42153/45440)
01/14/2023 06:51:28 - INFO - __main__ -   test: [batch: 355/391 ] | Loss: 0.973 | Acc: 75.887% (34580/45568)/ 92.760% (42269/45568)
01/14/2023 06:51:30 - INFO - __main__ -   test: [batch: 356/391 ] | Loss: 0.973 | Acc: 75.882% (34675/45696)/ 92.756% (42386/45696)
01/14/2023 06:51:33 - INFO - __main__ -   test: [batch: 357/391 ] | Loss: 0.972 | Acc: 75.917% (34788/45824)/ 92.770% (42511/45824)
01/14/2023 06:51:35 - INFO - __main__ -   test: [batch: 358/391 ] | Loss: 0.971 | Acc: 75.944% (34898/45952)/ 92.773% (42631/45952)
01/14/2023 06:51:38 - INFO - __main__ -   test: [batch: 359/391 ] | Loss: 0.971 | Acc: 75.955% (35000/46080)/ 92.769% (42748/46080)
01/14/2023 06:51:41 - INFO - __main__ -   test: [batch: 360/391 ] | Loss: 0.973 | Acc: 75.926% (35084/46208)/ 92.761% (42863/46208)
01/14/2023 06:51:43 - INFO - __main__ -   test: [batch: 361/391 ] | Loss: 0.973 | Acc: 75.928% (35182/46336)/ 92.768% (42985/46336)
01/14/2023 06:51:46 - INFO - __main__ -   test: [batch: 362/391 ] | Loss: 0.973 | Acc: 75.928% (35279/46464)/ 92.782% (43110/46464)
01/14/2023 06:51:48 - INFO - __main__ -   test: [batch: 363/391 ] | Loss: 0.973 | Acc: 75.916% (35371/46592)/ 92.771% (43224/46592)
01/14/2023 06:51:51 - INFO - __main__ -   test: [batch: 364/391 ] | Loss: 0.972 | Acc: 75.944% (35481/46720)/ 92.780% (43347/46720)
01/14/2023 06:51:54 - INFO - __main__ -   test: [batch: 365/391 ] | Loss: 0.971 | Acc: 75.956% (35584/46848)/ 92.789% (43470/46848)
01/14/2023 06:51:56 - INFO - __main__ -   test: [batch: 366/391 ] | Loss: 0.970 | Acc: 75.994% (35699/46976)/ 92.807% (43597/46976)
01/14/2023 06:51:59 - INFO - __main__ -   test: [batch: 367/391 ] | Loss: 0.969 | Acc: 76.019% (35808/47104)/ 92.820% (43722/47104)
01/14/2023 06:52:02 - INFO - __main__ -   test: [batch: 368/391 ] | Loss: 0.968 | Acc: 76.025% (35908/47232)/ 92.829% (43845/47232)
01/14/2023 06:52:04 - INFO - __main__ -   test: [batch: 369/391 ] | Loss: 0.967 | Acc: 76.043% (36014/47360)/ 92.838% (43968/47360)
01/14/2023 06:52:07 - INFO - __main__ -   test: [batch: 370/391 ] | Loss: 0.967 | Acc: 76.030% (36105/47488)/ 92.840% (44088/47488)
01/14/2023 06:52:09 - INFO - __main__ -   test: [batch: 371/391 ] | Loss: 0.967 | Acc: 76.037% (36206/47616)/ 92.845% (44209/47616)
01/14/2023 06:52:12 - INFO - __main__ -   test: [batch: 372/391 ] | Loss: 0.966 | Acc: 76.085% (36326/47744)/ 92.860% (44335/47744)
01/14/2023 06:52:15 - INFO - __main__ -   test: [batch: 373/391 ] | Loss: 0.964 | Acc: 76.124% (36442/47872)/ 92.868% (44458/47872)
01/14/2023 06:52:17 - INFO - __main__ -   test: [batch: 374/391 ] | Loss: 0.963 | Acc: 76.156% (36555/48000)/ 92.875% (44580/48000)
01/14/2023 06:52:20 - INFO - __main__ -   test: [batch: 375/391 ] | Loss: 0.966 | Acc: 76.093% (36622/48128)/ 92.836% (44680/48128)
01/14/2023 06:52:22 - INFO - __main__ -   test: [batch: 376/391 ] | Loss: 0.966 | Acc: 76.088% (36717/48256)/ 92.822% (44792/48256)
01/14/2023 06:52:25 - INFO - __main__ -   test: [batch: 377/391 ] | Loss: 0.967 | Acc: 76.075% (36808/48384)/ 92.816% (44908/48384)
01/14/2023 06:52:27 - INFO - __main__ -   test: [batch: 378/391 ] | Loss: 0.970 | Acc: 76.000% (36869/48512)/ 92.771% (45005/48512)
01/14/2023 06:52:30 - INFO - __main__ -   test: [batch: 379/391 ] | Loss: 0.970 | Acc: 75.989% (36961/48640)/ 92.775% (45126/48640)
01/14/2023 06:52:32 - INFO - __main__ -   test: [batch: 380/391 ] | Loss: 0.970 | Acc: 75.982% (37055/48768)/ 92.786% (45250/48768)
01/14/2023 06:52:35 - INFO - __main__ -   test: [batch: 381/391 ] | Loss: 0.972 | Acc: 75.935% (37129/48896)/ 92.783% (45367/48896)
01/14/2023 06:52:38 - INFO - __main__ -   test: [batch: 382/391 ] | Loss: 0.974 | Acc: 75.895% (37207/49024)/ 92.767% (45478/49024)
01/14/2023 06:52:40 - INFO - __main__ -   test: [batch: 383/391 ] | Loss: 0.974 | Acc: 75.901% (37307/49152)/ 92.763% (45595/49152)
01/14/2023 06:52:43 - INFO - __main__ -   test: [batch: 384/391 ] | Loss: 0.972 | Acc: 75.944% (37425/49280)/ 92.776% (45720/49280)
01/14/2023 06:52:45 - INFO - __main__ -   test: [batch: 385/391 ] | Loss: 0.971 | Acc: 75.955% (37528/49408)/ 92.785% (45843/49408)
01/14/2023 06:52:48 - INFO - __main__ -   test: [batch: 386/391 ] | Loss: 0.969 | Acc: 76.001% (37648/49536)/ 92.801% (45970/49536)
01/14/2023 06:52:51 - INFO - __main__ -   test: [batch: 387/391 ] | Loss: 0.967 | Acc: 76.041% (37765/49664)/ 92.812% (46094/49664)
01/14/2023 06:52:53 - INFO - __main__ -   test: [batch: 388/391 ] | Loss: 0.965 | Acc: 76.087% (37885/49792)/ 92.824% (46219/49792)
01/14/2023 06:52:56 - INFO - __main__ -   test: [batch: 389/391 ] | Loss: 0.965 | Acc: 76.088% (37983/49920)/ 92.829% (46340/49920)
01/14/2023 06:52:59 - INFO - __main__ -   test: [batch: 390/391 ] | Loss: 0.967 | Acc: 76.040% (38020/50000)/ 92.820% (46410/50000)
01/14/2023 06:52:59 - INFO - __main__ -   Final accuracy: 76.040
01/14/2023 06:52:59 - INFO - __main__ -   {'milestones': Counter({2: 1, 3: 1, 4: 1}), 'gamma': 0.1, 'base_lrs': [0.00025], 'last_epoch': 2, '_step_count': 3, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [2.5e-05]}
01/14/2023 06:52:59 - INFO - __main__ -   
Epoch: 2
01/14/2023 06:53:01 - INFO - __main__ -   test: [epoch: 2 | batch: 0/10010 ] | Loss: 0.967 | Acc: 78.125% (100/128)
01/14/2023 06:57:21 - INFO - __main__ -   test: [epoch: 2 | batch: 100/10010 ] | Loss: 0.829 | Acc: 79.695% (10303/12928)
01/14/2023 07:01:42 - INFO - __main__ -   test: [epoch: 2 | batch: 200/10010 ] | Loss: 0.837 | Acc: 79.081% (20346/25728)
01/14/2023 07:06:02 - INFO - __main__ -   test: [epoch: 2 | batch: 300/10010 ] | Loss: 0.842 | Acc: 79.044% (30454/38528)
01/14/2023 07:10:21 - INFO - __main__ -   test: [epoch: 2 | batch: 400/10010 ] | Loss: 0.848 | Acc: 78.856% (40475/51328)
01/14/2023 07:14:43 - INFO - __main__ -   test: [epoch: 2 | batch: 500/10010 ] | Loss: 0.851 | Acc: 78.789% (50526/64128)
01/14/2023 07:19:07 - INFO - __main__ -   test: [epoch: 2 | batch: 600/10010 ] | Loss: 0.855 | Acc: 78.729% (60565/76928)
01/14/2023 07:23:28 - INFO - __main__ -   test: [epoch: 2 | batch: 700/10010 ] | Loss: 0.856 | Acc: 78.681% (70599/89728)
01/14/2023 07:27:48 - INFO - __main__ -   test: [epoch: 2 | batch: 800/10010 ] | Loss: 0.857 | Acc: 78.645% (80633/102528)
01/14/2023 07:32:10 - INFO - __main__ -   test: [epoch: 2 | batch: 900/10010 ] | Loss: 0.855 | Acc: 78.680% (90740/115328)
01/14/2023 07:36:30 - INFO - __main__ -   test: [epoch: 2 | batch: 1000/10010 ] | Loss: 0.857 | Acc: 78.621% (100735/128128)
01/14/2023 07:40:51 - INFO - __main__ -   test: [epoch: 2 | batch: 1100/10010 ] | Loss: 0.858 | Acc: 78.617% (110793/140928)
01/14/2023 07:45:11 - INFO - __main__ -   test: [epoch: 2 | batch: 1200/10010 ] | Loss: 0.860 | Acc: 78.575% (120792/153728)
01/14/2023 07:49:33 - INFO - __main__ -   test: [epoch: 2 | batch: 1300/10010 ] | Loss: 0.858 | Acc: 78.600% (130891/166528)
01/14/2023 07:53:56 - INFO - __main__ -   test: [epoch: 2 | batch: 1400/10010 ] | Loss: 0.858 | Acc: 78.626% (140998/179328)
01/14/2023 07:58:18 - INFO - __main__ -   test: [epoch: 2 | batch: 1500/10010 ] | Loss: 0.858 | Acc: 78.592% (150998/192128)
01/14/2023 08:02:38 - INFO - __main__ -   test: [epoch: 2 | batch: 1600/10010 ] | Loss: 0.859 | Acc: 78.573% (161019/204928)
01/14/2023 08:06:58 - INFO - __main__ -   test: [epoch: 2 | batch: 1700/10010 ] | Loss: 0.859 | Acc: 78.572% (171073/217728)
01/14/2023 08:11:17 - INFO - __main__ -   test: [epoch: 2 | batch: 1800/10010 ] | Loss: 0.858 | Acc: 78.580% (181149/230528)
01/14/2023 08:15:40 - INFO - __main__ -   test: [epoch: 2 | batch: 1900/10010 ] | Loss: 0.857 | Acc: 78.600% (191256/243328)
01/14/2023 08:20:01 - INFO - __main__ -   test: [epoch: 2 | batch: 2000/10010 ] | Loss: 0.856 | Acc: 78.619% (201366/256128)
01/14/2023 08:24:21 - INFO - __main__ -   test: [epoch: 2 | batch: 2100/10010 ] | Loss: 0.856 | Acc: 78.612% (211409/268928)
01/14/2023 08:28:43 - INFO - __main__ -   test: [epoch: 2 | batch: 2200/10010 ] | Loss: 0.857 | Acc: 78.599% (221436/281728)
01/14/2023 08:33:04 - INFO - __main__ -   test: [epoch: 2 | batch: 2300/10010 ] | Loss: 0.858 | Acc: 78.580% (231441/294528)
01/14/2023 08:37:28 - INFO - __main__ -   test: [epoch: 2 | batch: 2400/10010 ] | Loss: 0.858 | Acc: 78.584% (241510/307328)
01/14/2023 08:41:50 - INFO - __main__ -   test: [epoch: 2 | batch: 2500/10010 ] | Loss: 0.858 | Acc: 78.585% (251571/320128)
01/14/2023 08:46:10 - INFO - __main__ -   test: [epoch: 2 | batch: 2600/10010 ] | Loss: 0.858 | Acc: 78.599% (261679/332928)
01/14/2023 08:50:32 - INFO - __main__ -   test: [epoch: 2 | batch: 2700/10010 ] | Loss: 0.857 | Acc: 78.636% (271865/345728)
01/14/2023 08:54:52 - INFO - __main__ -   test: [epoch: 2 | batch: 2800/10010 ] | Loss: 0.857 | Acc: 78.636% (281931/358528)
01/14/2023 08:59:13 - INFO - __main__ -   test: [epoch: 2 | batch: 2900/10010 ] | Loss: 0.857 | Acc: 78.636% (291997/371328)
01/14/2023 09:03:33 - INFO - __main__ -   test: [epoch: 2 | batch: 3000/10010 ] | Loss: 0.857 | Acc: 78.641% (302084/384128)
01/14/2023 09:07:54 - INFO - __main__ -   test: [epoch: 2 | batch: 3100/10010 ] | Loss: 0.857 | Acc: 78.651% (312188/396928)
01/14/2023 09:12:17 - INFO - __main__ -   test: [epoch: 2 | batch: 3200/10010 ] | Loss: 0.856 | Acc: 78.660% (322291/409728)
01/14/2023 09:16:39 - INFO - __main__ -   test: [epoch: 2 | batch: 3300/10010 ] | Loss: 0.857 | Acc: 78.646% (332300/422528)
01/14/2023 09:21:01 - INFO - __main__ -   test: [epoch: 2 | batch: 3400/10010 ] | Loss: 0.857 | Acc: 78.657% (342417/435328)
01/14/2023 09:25:23 - INFO - __main__ -   test: [epoch: 2 | batch: 3500/10010 ] | Loss: 0.857 | Acc: 78.659% (352495/448128)
01/14/2023 09:29:43 - INFO - __main__ -   test: [epoch: 2 | batch: 3600/10010 ] | Loss: 0.856 | Acc: 78.679% (362652/460928)
01/14/2023 09:34:03 - INFO - __main__ -   test: [epoch: 2 | batch: 3700/10010 ] | Loss: 0.857 | Acc: 78.662% (372643/473728)
01/14/2023 09:38:23 - INFO - __main__ -   test: [epoch: 2 | batch: 3800/10010 ] | Loss: 0.856 | Acc: 78.683% (382815/486528)
01/14/2023 09:42:45 - INFO - __main__ -   test: [epoch: 2 | batch: 3900/10010 ] | Loss: 0.856 | Acc: 78.675% (392845/499328)
01/14/2023 09:47:04 - INFO - __main__ -   test: [epoch: 2 | batch: 4000/10010 ] | Loss: 0.857 | Acc: 78.659% (402837/512128)
01/14/2023 09:51:25 - INFO - __main__ -   test: [epoch: 2 | batch: 4100/10010 ] | Loss: 0.857 | Acc: 78.662% (412919/524928)
01/14/2023 09:55:46 - INFO - __main__ -   test: [epoch: 2 | batch: 4200/10010 ] | Loss: 0.856 | Acc: 78.654% (422947/537728)
01/14/2023 10:00:05 - INFO - __main__ -   test: [epoch: 2 | batch: 4300/10010 ] | Loss: 0.857 | Acc: 78.653% (433005/550528)
01/14/2023 10:04:25 - INFO - __main__ -   test: [epoch: 2 | batch: 4400/10010 ] | Loss: 0.856 | Acc: 78.666% (443148/563328)
01/14/2023 10:08:48 - INFO - __main__ -   test: [epoch: 2 | batch: 4500/10010 ] | Loss: 0.857 | Acc: 78.659% (453178/576128)
01/14/2023 10:13:09 - INFO - __main__ -   test: [epoch: 2 | batch: 4600/10010 ] | Loss: 0.857 | Acc: 78.644% (463158/588928)
01/14/2023 10:17:29 - INFO - __main__ -   test: [epoch: 2 | batch: 4700/10010 ] | Loss: 0.857 | Acc: 78.643% (473215/601728)
01/14/2023 10:21:53 - INFO - __main__ -   test: [epoch: 2 | batch: 4800/10010 ] | Loss: 0.857 | Acc: 78.650% (483326/614528)
01/14/2023 10:26:14 - INFO - __main__ -   test: [epoch: 2 | batch: 4900/10010 ] | Loss: 0.857 | Acc: 78.646% (493367/627328)
01/14/2023 10:30:36 - INFO - __main__ -   test: [epoch: 2 | batch: 5000/10010 ] | Loss: 0.857 | Acc: 78.662% (503537/640128)
01/14/2023 10:34:58 - INFO - __main__ -   test: [epoch: 2 | batch: 5100/10010 ] | Loss: 0.857 | Acc: 78.653% (513545/652928)
01/14/2023 10:39:20 - INFO - __main__ -   test: [epoch: 2 | batch: 5200/10010 ] | Loss: 0.857 | Acc: 78.648% (523579/665728)
01/14/2023 10:43:42 - INFO - __main__ -   test: [epoch: 2 | batch: 5300/10010 ] | Loss: 0.857 | Acc: 78.645% (533631/678528)
01/14/2023 10:48:02 - INFO - __main__ -   test: [epoch: 2 | batch: 5400/10010 ] | Loss: 0.858 | Acc: 78.639% (543652/691328)
01/14/2023 10:52:24 - INFO - __main__ -   test: [epoch: 2 | batch: 5500/10010 ] | Loss: 0.858 | Acc: 78.638% (553710/704128)
01/14/2023 10:56:46 - INFO - __main__ -   test: [epoch: 2 | batch: 5600/10010 ] | Loss: 0.858 | Acc: 78.643% (563814/716928)
01/14/2023 11:01:10 - INFO - __main__ -   test: [epoch: 2 | batch: 5700/10010 ] | Loss: 0.857 | Acc: 78.645% (573894/729728)
01/14/2023 11:05:32 - INFO - __main__ -   test: [epoch: 2 | batch: 5800/10010 ] | Loss: 0.857 | Acc: 78.649% (583991/742528)
01/14/2023 11:09:53 - INFO - __main__ -   test: [epoch: 2 | batch: 5900/10010 ] | Loss: 0.857 | Acc: 78.659% (594133/755328)
01/14/2023 11:14:14 - INFO - __main__ -   test: [epoch: 2 | batch: 6000/10010 ] | Loss: 0.857 | Acc: 78.657% (604186/768128)
01/14/2023 11:18:37 - INFO - __main__ -   test: [epoch: 2 | batch: 6100/10010 ] | Loss: 0.857 | Acc: 78.644% (614153/780928)
01/14/2023 11:22:58 - INFO - __main__ -   test: [epoch: 2 | batch: 6200/10010 ] | Loss: 0.857 | Acc: 78.641% (624192/793728)
01/14/2023 11:27:19 - INFO - __main__ -   test: [epoch: 2 | batch: 6300/10010 ] | Loss: 0.857 | Acc: 78.641% (634261/806528)
01/14/2023 11:31:41 - INFO - __main__ -   test: [epoch: 2 | batch: 6400/10010 ] | Loss: 0.858 | Acc: 78.634% (644267/819328)
01/14/2023 11:36:02 - INFO - __main__ -   test: [epoch: 2 | batch: 6500/10010 ] | Loss: 0.857 | Acc: 78.642% (654405/832128)
01/14/2023 11:40:25 - INFO - __main__ -   test: [epoch: 2 | batch: 6600/10010 ] | Loss: 0.857 | Acc: 78.640% (664455/844928)
01/14/2023 11:44:46 - INFO - __main__ -   test: [epoch: 2 | batch: 6700/10010 ] | Loss: 0.858 | Acc: 78.636% (674483/857728)
01/14/2023 11:49:08 - INFO - __main__ -   test: [epoch: 2 | batch: 6800/10010 ] | Loss: 0.858 | Acc: 78.637% (684557/870528)
01/14/2023 11:53:29 - INFO - __main__ -   test: [epoch: 2 | batch: 6900/10010 ] | Loss: 0.857 | Acc: 78.640% (694649/883328)
01/14/2023 11:57:52 - INFO - __main__ -   test: [epoch: 2 | batch: 7000/10010 ] | Loss: 0.857 | Acc: 78.633% (704649/896128)
01/14/2023 12:02:14 - INFO - __main__ -   test: [epoch: 2 | batch: 7100/10010 ] | Loss: 0.857 | Acc: 78.638% (714759/908928)
01/14/2023 12:06:35 - INFO - __main__ -   test: [epoch: 2 | batch: 7200/10010 ] | Loss: 0.857 | Acc: 78.641% (724856/921728)
01/14/2023 12:10:56 - INFO - __main__ -   test: [epoch: 2 | batch: 7300/10010 ] | Loss: 0.857 | Acc: 78.639% (734906/934528)
01/14/2023 12:15:18 - INFO - __main__ -   test: [epoch: 2 | batch: 7400/10010 ] | Loss: 0.857 | Acc: 78.645% (745027/947328)
01/14/2023 12:19:40 - INFO - __main__ -   test: [epoch: 2 | batch: 7500/10010 ] | Loss: 0.857 | Acc: 78.652% (755158/960128)
01/14/2023 12:24:01 - INFO - __main__ -   test: [epoch: 2 | batch: 7600/10010 ] | Loss: 0.857 | Acc: 78.658% (765289/972928)
01/14/2023 12:28:23 - INFO - __main__ -   test: [epoch: 2 | batch: 7700/10010 ] | Loss: 0.857 | Acc: 78.653% (775300/985728)
01/14/2023 12:32:44 - INFO - __main__ -   test: [epoch: 2 | batch: 7800/10010 ] | Loss: 0.857 | Acc: 78.649% (785331/998528)
01/14/2023 12:37:07 - INFO - __main__ -   test: [epoch: 2 | batch: 7900/10010 ] | Loss: 0.857 | Acc: 78.653% (795438/1011328)
01/14/2023 12:41:28 - INFO - __main__ -   test: [epoch: 2 | batch: 8000/10010 ] | Loss: 0.857 | Acc: 78.651% (805489/1024128)
01/14/2023 12:45:49 - INFO - __main__ -   test: [epoch: 2 | batch: 8100/10010 ] | Loss: 0.857 | Acc: 78.658% (815622/1036928)
01/14/2023 12:50:10 - INFO - __main__ -   test: [epoch: 2 | batch: 8200/10010 ] | Loss: 0.857 | Acc: 78.655% (825664/1049728)
01/14/2023 12:54:31 - INFO - __main__ -   test: [epoch: 2 | batch: 8300/10010 ] | Loss: 0.857 | Acc: 78.652% (835695/1062528)
01/14/2023 12:58:51 - INFO - __main__ -   test: [epoch: 2 | batch: 8400/10010 ] | Loss: 0.857 | Acc: 78.653% (845778/1075328)
01/14/2023 13:03:13 - INFO - __main__ -   test: [epoch: 2 | batch: 8500/10010 ] | Loss: 0.857 | Acc: 78.657% (855889/1088128)
01/14/2023 13:07:35 - INFO - __main__ -   test: [epoch: 2 | batch: 8600/10010 ] | Loss: 0.857 | Acc: 78.663% (866023/1100928)
01/14/2023 13:11:57 - INFO - __main__ -   test: [epoch: 2 | batch: 8700/10010 ] | Loss: 0.857 | Acc: 78.664% (876105/1113728)
01/14/2023 13:16:18 - INFO - __main__ -   test: [epoch: 2 | batch: 8800/10010 ] | Loss: 0.857 | Acc: 78.670% (886245/1126528)
01/14/2023 13:20:38 - INFO - __main__ -   test: [epoch: 2 | batch: 8900/10010 ] | Loss: 0.857 | Acc: 78.670% (896315/1139328)
01/14/2023 13:24:59 - INFO - __main__ -   test: [epoch: 2 | batch: 9000/10010 ] | Loss: 0.857 | Acc: 78.664% (906308/1152128)
01/14/2023 13:29:20 - INFO - __main__ -   test: [epoch: 2 | batch: 9100/10010 ] | Loss: 0.857 | Acc: 78.659% (916316/1164928)
01/14/2023 13:33:42 - INFO - __main__ -   test: [epoch: 2 | batch: 9200/10010 ] | Loss: 0.857 | Acc: 78.663% (926432/1177728)
01/14/2023 13:38:03 - INFO - __main__ -   test: [epoch: 2 | batch: 9300/10010 ] | Loss: 0.857 | Acc: 78.661% (936476/1190528)
01/14/2023 13:42:25 - INFO - __main__ -   test: [epoch: 2 | batch: 9400/10010 ] | Loss: 0.857 | Acc: 78.663% (946570/1203328)
01/14/2023 13:46:47 - INFO - __main__ -   test: [epoch: 2 | batch: 9500/10010 ] | Loss: 0.857 | Acc: 78.659% (956591/1216128)
01/14/2023 13:51:09 - INFO - __main__ -   test: [epoch: 2 | batch: 9600/10010 ] | Loss: 0.857 | Acc: 78.657% (966642/1228928)
01/14/2023 13:55:31 - INFO - __main__ -   test: [epoch: 2 | batch: 9700/10010 ] | Loss: 0.857 | Acc: 78.653% (976657/1241728)
01/14/2023 13:59:54 - INFO - __main__ -   test: [epoch: 2 | batch: 9800/10010 ] | Loss: 0.857 | Acc: 78.652% (986708/1254528)
01/14/2023 14:04:16 - INFO - __main__ -   test: [epoch: 2 | batch: 9900/10010 ] | Loss: 0.857 | Acc: 78.653% (996792/1267328)
01/14/2023 14:08:38 - INFO - __main__ -   test: [epoch: 2 | batch: 10000/10010 ] | Loss: 0.857 | Acc: 78.650% (1006824/1280128)
01/14/2023 14:09:02 - INFO - __main__ -   Saving Checkpoint
01/14/2023 14:09:04 - INFO - __main__ -   test: [batch: 0/391 ] | Loss: 0.472 | Acc: 85.938% (110/128)/ 97.656% (125/128)
01/14/2023 14:09:07 - INFO - __main__ -   test: [batch: 1/391 ] | Loss: 0.470 | Acc: 85.547% (219/256)/ 98.047% (251/256)
01/14/2023 14:09:09 - INFO - __main__ -   test: [batch: 2/391 ] | Loss: 0.624 | Acc: 81.771% (314/384)/ 95.833% (368/384)
01/14/2023 14:09:11 - INFO - __main__ -   test: [batch: 3/391 ] | Loss: 0.579 | Acc: 83.789% (429/512)/ 96.484% (494/512)
01/14/2023 14:09:13 - INFO - __main__ -   test: [batch: 4/391 ] | Loss: 0.496 | Acc: 86.250% (552/640)/ 97.188% (622/640)
01/14/2023 14:09:15 - INFO - __main__ -   test: [batch: 5/391 ] | Loss: 0.438 | Acc: 87.500% (672/768)/ 97.656% (750/768)
01/14/2023 14:09:17 - INFO - __main__ -   test: [batch: 6/391 ] | Loss: 0.431 | Acc: 87.835% (787/896)/ 97.545% (874/896)
01/14/2023 14:09:20 - INFO - __main__ -   test: [batch: 7/391 ] | Loss: 0.411 | Acc: 88.867% (910/1024)/ 97.656% (1000/1024)
01/14/2023 14:09:22 - INFO - __main__ -   test: [batch: 8/391 ] | Loss: 0.428 | Acc: 88.802% (1023/1152)/ 97.569% (1124/1152)
01/14/2023 14:09:24 - INFO - __main__ -   test: [batch: 9/391 ] | Loss: 0.407 | Acc: 89.375% (1144/1280)/ 97.656% (1250/1280)
01/14/2023 14:09:26 - INFO - __main__ -   test: [batch: 10/391 ] | Loss: 0.456 | Acc: 87.997% (1239/1408)/ 97.585% (1374/1408)
01/14/2023 14:09:28 - INFO - __main__ -   test: [batch: 11/391 ] | Loss: 0.459 | Acc: 88.216% (1355/1536)/ 97.526% (1498/1536)
01/14/2023 14:09:30 - INFO - __main__ -   test: [batch: 12/391 ] | Loss: 0.504 | Acc: 87.079% (1449/1664)/ 97.236% (1618/1664)
01/14/2023 14:09:33 - INFO - __main__ -   test: [batch: 13/391 ] | Loss: 0.553 | Acc: 85.547% (1533/1792)/ 96.708% (1733/1792)
01/14/2023 14:09:35 - INFO - __main__ -   test: [batch: 14/391 ] | Loss: 0.573 | Acc: 84.792% (1628/1920)/ 96.771% (1858/1920)
01/14/2023 14:09:37 - INFO - __main__ -   test: [batch: 15/391 ] | Loss: 0.582 | Acc: 84.424% (1729/2048)/ 96.826% (1983/2048)
01/14/2023 14:09:39 - INFO - __main__ -   test: [batch: 16/391 ] | Loss: 0.592 | Acc: 84.237% (1833/2176)/ 96.645% (2103/2176)
01/14/2023 14:09:41 - INFO - __main__ -   test: [batch: 17/391 ] | Loss: 0.617 | Acc: 83.811% (1931/2304)/ 96.181% (2216/2304)
01/14/2023 14:09:43 - INFO - __main__ -   test: [batch: 18/391 ] | Loss: 0.640 | Acc: 83.347% (2027/2432)/ 96.012% (2335/2432)
01/14/2023 14:09:46 - INFO - __main__ -   test: [batch: 19/391 ] | Loss: 0.647 | Acc: 83.086% (2127/2560)/ 95.938% (2456/2560)
01/14/2023 14:09:48 - INFO - __main__ -   test: [batch: 20/391 ] | Loss: 0.645 | Acc: 83.185% (2236/2688)/ 95.833% (2576/2688)
01/14/2023 14:09:50 - INFO - __main__ -   test: [batch: 21/391 ] | Loss: 0.678 | Acc: 82.493% (2323/2816)/ 95.668% (2694/2816)
01/14/2023 14:09:52 - INFO - __main__ -   test: [batch: 22/391 ] | Loss: 0.677 | Acc: 82.337% (2424/2944)/ 95.652% (2816/2944)
01/14/2023 14:09:54 - INFO - __main__ -   test: [batch: 23/391 ] | Loss: 0.721 | Acc: 81.510% (2504/3072)/ 95.378% (2930/3072)
01/14/2023 14:09:56 - INFO - __main__ -   test: [batch: 24/391 ] | Loss: 0.740 | Acc: 81.031% (2593/3200)/ 95.188% (3046/3200)
01/14/2023 14:09:58 - INFO - __main__ -   test: [batch: 25/391 ] | Loss: 0.763 | Acc: 80.439% (2677/3328)/ 94.982% (3161/3328)
01/14/2023 14:10:01 - INFO - __main__ -   test: [batch: 26/391 ] | Loss: 0.776 | Acc: 79.716% (2755/3456)/ 94.994% (3283/3456)
01/14/2023 14:10:03 - INFO - __main__ -   test: [batch: 27/391 ] | Loss: 0.758 | Acc: 80.246% (2876/3584)/ 95.033% (3406/3584)
01/14/2023 14:10:05 - INFO - __main__ -   test: [batch: 28/391 ] | Loss: 0.767 | Acc: 79.714% (2959/3712)/ 95.097% (3530/3712)
01/14/2023 14:10:07 - INFO - __main__ -   test: [batch: 29/391 ] | Loss: 0.760 | Acc: 79.870% (3067/3840)/ 95.208% (3656/3840)
01/14/2023 14:10:09 - INFO - __main__ -   test: [batch: 30/391 ] | Loss: 0.773 | Acc: 79.839% (3168/3968)/ 95.086% (3773/3968)
01/14/2023 14:10:11 - INFO - __main__ -   test: [batch: 31/391 ] | Loss: 0.768 | Acc: 80.054% (3279/4096)/ 95.142% (3897/4096)
01/14/2023 14:10:13 - INFO - __main__ -   test: [batch: 32/391 ] | Loss: 0.754 | Acc: 80.374% (3395/4224)/ 95.241% (4023/4224)
01/14/2023 14:10:15 - INFO - __main__ -   test: [batch: 33/391 ] | Loss: 0.748 | Acc: 80.584% (3507/4352)/ 95.290% (4147/4352)
01/14/2023 14:10:18 - INFO - __main__ -   test: [batch: 34/391 ] | Loss: 0.735 | Acc: 80.982% (3628/4480)/ 95.335% (4271/4480)
01/14/2023 14:10:20 - INFO - __main__ -   test: [batch: 35/391 ] | Loss: 0.723 | Acc: 81.359% (3749/4608)/ 95.356% (4394/4608)
01/14/2023 14:10:22 - INFO - __main__ -   test: [batch: 36/391 ] | Loss: 0.708 | Acc: 81.799% (3874/4736)/ 95.460% (4521/4736)
01/14/2023 14:10:24 - INFO - __main__ -   test: [batch: 37/391 ] | Loss: 0.699 | Acc: 82.093% (3993/4864)/ 95.518% (4646/4864)
01/14/2023 14:10:26 - INFO - __main__ -   test: [batch: 38/391 ] | Loss: 0.691 | Acc: 82.232% (4105/4992)/ 95.573% (4771/4992)
01/14/2023 14:10:28 - INFO - __main__ -   test: [batch: 39/391 ] | Loss: 0.687 | Acc: 82.266% (4212/5120)/ 95.605% (4895/5120)
01/14/2023 14:10:30 - INFO - __main__ -   test: [batch: 40/391 ] | Loss: 0.685 | Acc: 82.393% (4324/5248)/ 95.541% (5014/5248)
01/14/2023 14:10:32 - INFO - __main__ -   test: [batch: 41/391 ] | Loss: 0.689 | Acc: 82.459% (4433/5376)/ 95.480% (5133/5376)
01/14/2023 14:10:34 - INFO - __main__ -   test: [batch: 42/391 ] | Loss: 0.688 | Acc: 82.431% (4537/5504)/ 95.494% (5256/5504)
01/14/2023 14:10:36 - INFO - __main__ -   test: [batch: 43/391 ] | Loss: 0.686 | Acc: 82.493% (4646/5632)/ 95.472% (5377/5632)
01/14/2023 14:10:38 - INFO - __main__ -   test: [batch: 44/391 ] | Loss: 0.689 | Acc: 82.483% (4751/5760)/ 95.399% (5495/5760)
01/14/2023 14:10:40 - INFO - __main__ -   test: [batch: 45/391 ] | Loss: 0.686 | Acc: 82.660% (4867/5888)/ 95.397% (5617/5888)
01/14/2023 14:10:43 - INFO - __main__ -   test: [batch: 46/391 ] | Loss: 0.689 | Acc: 82.646% (4972/6016)/ 95.412% (5740/6016)
01/14/2023 14:10:45 - INFO - __main__ -   test: [batch: 47/391 ] | Loss: 0.690 | Acc: 82.568% (5073/6144)/ 95.475% (5866/6144)
01/14/2023 14:10:47 - INFO - __main__ -   test: [batch: 48/391 ] | Loss: 0.696 | Acc: 82.430% (5170/6272)/ 95.440% (5986/6272)
01/14/2023 14:10:49 - INFO - __main__ -   test: [batch: 49/391 ] | Loss: 0.699 | Acc: 82.484% (5279/6400)/ 95.344% (6102/6400)
01/14/2023 14:10:51 - INFO - __main__ -   test: [batch: 50/391 ] | Loss: 0.690 | Acc: 82.690% (5398/6528)/ 95.389% (6227/6528)
01/14/2023 14:10:53 - INFO - __main__ -   test: [batch: 51/391 ] | Loss: 0.683 | Acc: 82.888% (5517/6656)/ 95.463% (6354/6656)
01/14/2023 14:10:55 - INFO - __main__ -   test: [batch: 52/391 ] | Loss: 0.681 | Acc: 82.960% (5628/6784)/ 95.489% (6478/6784)
01/14/2023 14:10:57 - INFO - __main__ -   test: [batch: 53/391 ] | Loss: 0.671 | Acc: 83.174% (5749/6912)/ 95.558% (6605/6912)
01/14/2023 14:11:00 - INFO - __main__ -   test: [batch: 54/391 ] | Loss: 0.664 | Acc: 83.324% (5866/7040)/ 95.597% (6730/7040)
01/14/2023 14:11:02 - INFO - __main__ -   test: [batch: 55/391 ] | Loss: 0.658 | Acc: 83.468% (5983/7168)/ 95.633% (6855/7168)
01/14/2023 14:11:04 - INFO - __main__ -   test: [batch: 56/391 ] | Loss: 0.650 | Acc: 83.676% (6105/7296)/ 95.683% (6981/7296)
01/14/2023 14:11:06 - INFO - __main__ -   test: [batch: 57/391 ] | Loss: 0.643 | Acc: 83.877% (6227/7424)/ 95.717% (7106/7424)
01/14/2023 14:11:08 - INFO - __main__ -   test: [batch: 58/391 ] | Loss: 0.641 | Acc: 83.938% (6339/7552)/ 95.710% (7228/7552)
01/14/2023 14:11:10 - INFO - __main__ -   test: [batch: 59/391 ] | Loss: 0.643 | Acc: 83.854% (6440/7680)/ 95.729% (7352/7680)
01/14/2023 14:11:12 - INFO - __main__ -   test: [batch: 60/391 ] | Loss: 0.648 | Acc: 83.747% (6539/7808)/ 95.697% (7472/7808)
01/14/2023 14:11:15 - INFO - __main__ -   test: [batch: 61/391 ] | Loss: 0.648 | Acc: 83.745% (6646/7936)/ 95.716% (7596/7936)
01/14/2023 14:11:17 - INFO - __main__ -   test: [batch: 62/391 ] | Loss: 0.647 | Acc: 83.693% (6749/8064)/ 95.722% (7719/8064)
01/14/2023 14:11:19 - INFO - __main__ -   test: [batch: 63/391 ] | Loss: 0.655 | Acc: 83.557% (6845/8192)/ 95.667% (7837/8192)
01/14/2023 14:11:21 - INFO - __main__ -   test: [batch: 64/391 ] | Loss: 0.664 | Acc: 83.365% (6936/8320)/ 95.613% (7955/8320)
01/14/2023 14:11:23 - INFO - __main__ -   test: [batch: 65/391 ] | Loss: 0.677 | Acc: 82.872% (7001/8448)/ 95.537% (8071/8448)
01/14/2023 14:11:25 - INFO - __main__ -   test: [batch: 66/391 ] | Loss: 0.682 | Acc: 82.859% (7106/8576)/ 95.499% (8190/8576)
01/14/2023 14:11:27 - INFO - __main__ -   test: [batch: 67/391 ] | Loss: 0.684 | Acc: 82.801% (7207/8704)/ 95.519% (8314/8704)
01/14/2023 14:11:29 - INFO - __main__ -   test: [batch: 68/391 ] | Loss: 0.685 | Acc: 82.745% (7308/8832)/ 95.539% (8438/8832)
01/14/2023 14:11:32 - INFO - __main__ -   test: [batch: 69/391 ] | Loss: 0.680 | Acc: 82.835% (7422/8960)/ 95.580% (8564/8960)
01/14/2023 14:11:34 - INFO - __main__ -   test: [batch: 70/391 ] | Loss: 0.682 | Acc: 82.768% (7522/9088)/ 95.577% (8686/9088)
01/14/2023 14:11:36 - INFO - __main__ -   test: [batch: 71/391 ] | Loss: 0.680 | Acc: 82.802% (7631/9216)/ 95.595% (8810/9216)
01/14/2023 14:11:38 - INFO - __main__ -   test: [batch: 72/391 ] | Loss: 0.682 | Acc: 82.695% (7727/9344)/ 95.612% (8934/9344)
01/14/2023 14:11:40 - INFO - __main__ -   test: [batch: 73/391 ] | Loss: 0.686 | Acc: 82.580% (7822/9472)/ 95.619% (9057/9472)
01/14/2023 14:11:42 - INFO - __main__ -   test: [batch: 74/391 ] | Loss: 0.687 | Acc: 82.562% (7926/9600)/ 95.604% (9178/9600)
01/14/2023 14:11:45 - INFO - __main__ -   test: [batch: 75/391 ] | Loss: 0.693 | Acc: 82.360% (8012/9728)/ 95.590% (9299/9728)
01/14/2023 14:11:47 - INFO - __main__ -   test: [batch: 76/391 ] | Loss: 0.693 | Acc: 82.336% (8115/9856)/ 95.586% (9421/9856)
01/14/2023 14:11:49 - INFO - __main__ -   test: [batch: 77/391 ] | Loss: 0.693 | Acc: 82.312% (8218/9984)/ 95.613% (9546/9984)
01/14/2023 14:11:51 - INFO - __main__ -   test: [batch: 78/391 ] | Loss: 0.693 | Acc: 82.199% (8312/10112)/ 95.649% (9672/10112)
01/14/2023 14:11:53 - INFO - __main__ -   test: [batch: 79/391 ] | Loss: 0.691 | Acc: 82.197% (8417/10240)/ 95.674% (9797/10240)
01/14/2023 14:11:55 - INFO - __main__ -   test: [batch: 80/391 ] | Loss: 0.691 | Acc: 82.166% (8519/10368)/ 95.669% (9919/10368)
01/14/2023 14:11:57 - INFO - __main__ -   test: [batch: 81/391 ] | Loss: 0.691 | Acc: 82.174% (8625/10496)/ 95.694% (10044/10496)
01/14/2023 14:11:59 - INFO - __main__ -   test: [batch: 82/391 ] | Loss: 0.690 | Acc: 82.191% (8732/10624)/ 95.689% (10166/10624)
01/14/2023 14:12:01 - INFO - __main__ -   test: [batch: 83/391 ] | Loss: 0.690 | Acc: 82.236% (8842/10752)/ 95.675% (10287/10752)
01/14/2023 14:12:04 - INFO - __main__ -   test: [batch: 84/391 ] | Loss: 0.686 | Acc: 82.344% (8959/10880)/ 95.708% (10413/10880)
01/14/2023 14:12:06 - INFO - __main__ -   test: [batch: 85/391 ] | Loss: 0.684 | Acc: 82.358% (9066/11008)/ 95.749% (10540/11008)
01/14/2023 14:12:08 - INFO - __main__ -   test: [batch: 86/391 ] | Loss: 0.688 | Acc: 82.283% (9163/11136)/ 95.726% (10660/11136)
01/14/2023 14:12:10 - INFO - __main__ -   test: [batch: 87/391 ] | Loss: 0.686 | Acc: 82.333% (9274/11264)/ 95.730% (10783/11264)
01/14/2023 14:12:12 - INFO - __main__ -   test: [batch: 88/391 ] | Loss: 0.694 | Acc: 82.268% (9372/11392)/ 95.664% (10898/11392)
01/14/2023 14:12:14 - INFO - __main__ -   test: [batch: 89/391 ] | Loss: 0.691 | Acc: 82.326% (9484/11520)/ 95.677% (11022/11520)
01/14/2023 14:12:16 - INFO - __main__ -   test: [batch: 90/391 ] | Loss: 0.692 | Acc: 82.254% (9581/11648)/ 95.682% (11145/11648)
01/14/2023 14:12:18 - INFO - __main__ -   test: [batch: 91/391 ] | Loss: 0.691 | Acc: 82.286% (9690/11776)/ 95.686% (11268/11776)
01/14/2023 14:12:20 - INFO - __main__ -   test: [batch: 92/391 ] | Loss: 0.693 | Acc: 82.266% (9793/11904)/ 95.665% (11388/11904)
01/14/2023 14:12:23 - INFO - __main__ -   test: [batch: 93/391 ] | Loss: 0.696 | Acc: 82.064% (9874/12032)/ 95.695% (11514/12032)
01/14/2023 14:12:25 - INFO - __main__ -   test: [batch: 94/391 ] | Loss: 0.699 | Acc: 81.949% (9965/12160)/ 95.707% (11638/12160)
01/14/2023 14:12:27 - INFO - __main__ -   test: [batch: 95/391 ] | Loss: 0.696 | Acc: 82.007% (10077/12288)/ 95.711% (11761/12288)
01/14/2023 14:12:29 - INFO - __main__ -   test: [batch: 96/391 ] | Loss: 0.700 | Acc: 81.918% (10171/12416)/ 95.723% (11885/12416)
01/14/2023 14:12:31 - INFO - __main__ -   test: [batch: 97/391 ] | Loss: 0.701 | Acc: 81.736% (10253/12544)/ 95.751% (12011/12544)
01/14/2023 14:12:33 - INFO - __main__ -   test: [batch: 98/391 ] | Loss: 0.698 | Acc: 81.842% (10371/12672)/ 95.786% (12138/12672)
01/14/2023 14:12:36 - INFO - __main__ -   test: [batch: 99/391 ] | Loss: 0.692 | Acc: 81.984% (10494/12800)/ 95.828% (12266/12800)
01/14/2023 14:12:38 - INFO - __main__ -   test: [batch: 100/391 ] | Loss: 0.691 | Acc: 82.039% (10606/12928)/ 95.846% (12391/12928)
01/14/2023 14:12:40 - INFO - __main__ -   test: [batch: 101/391 ] | Loss: 0.687 | Acc: 82.138% (10724/13056)/ 95.879% (12518/13056)
01/14/2023 14:12:42 - INFO - __main__ -   test: [batch: 102/391 ] | Loss: 0.683 | Acc: 82.266% (10846/13184)/ 95.904% (12644/13184)
01/14/2023 14:12:44 - INFO - __main__ -   test: [batch: 103/391 ] | Loss: 0.685 | Acc: 82.091% (10928/13312)/ 95.928% (12770/13312)
01/14/2023 14:12:46 - INFO - __main__ -   test: [batch: 104/391 ] | Loss: 0.685 | Acc: 82.039% (11026/13440)/ 95.923% (12892/13440)
01/14/2023 14:12:48 - INFO - __main__ -   test: [batch: 105/391 ] | Loss: 0.685 | Acc: 82.017% (11128/13568)/ 95.932% (13016/13568)
01/14/2023 14:12:50 - INFO - __main__ -   test: [batch: 106/391 ] | Loss: 0.693 | Acc: 81.900% (11217/13696)/ 95.867% (13130/13696)
01/14/2023 14:12:53 - INFO - __main__ -   test: [batch: 107/391 ] | Loss: 0.690 | Acc: 82.017% (11338/13824)/ 95.898% (13257/13824)
01/14/2023 14:12:55 - INFO - __main__ -   test: [batch: 108/391 ] | Loss: 0.693 | Acc: 81.881% (11424/13952)/ 95.900% (13380/13952)
01/14/2023 14:12:57 - INFO - __main__ -   test: [batch: 109/391 ] | Loss: 0.694 | Acc: 81.861% (11526/14080)/ 95.902% (13503/14080)
01/14/2023 14:12:59 - INFO - __main__ -   test: [batch: 110/391 ] | Loss: 0.697 | Acc: 81.672% (11604/14208)/ 95.911% (13627/14208)
01/14/2023 14:13:01 - INFO - __main__ -   test: [batch: 111/391 ] | Loss: 0.700 | Acc: 81.634% (11703/14336)/ 95.878% (13745/14336)
01/14/2023 14:13:03 - INFO - __main__ -   test: [batch: 112/391 ] | Loss: 0.700 | Acc: 81.651% (11810/14464)/ 95.893% (13870/14464)
01/14/2023 14:13:06 - INFO - __main__ -   test: [batch: 113/391 ] | Loss: 0.699 | Acc: 81.682% (11919/14592)/ 95.895% (13993/14592)
01/14/2023 14:13:08 - INFO - __main__ -   test: [batch: 114/391 ] | Loss: 0.696 | Acc: 81.773% (12037/14720)/ 95.924% (14120/14720)
01/14/2023 14:13:10 - INFO - __main__ -   test: [batch: 115/391 ] | Loss: 0.693 | Acc: 81.856% (12154/14848)/ 95.946% (14246/14848)
01/14/2023 14:13:12 - INFO - __main__ -   test: [batch: 116/391 ] | Loss: 0.697 | Acc: 81.824% (12254/14976)/ 95.913% (14364/14976)
01/14/2023 14:13:14 - INFO - __main__ -   test: [batch: 117/391 ] | Loss: 0.695 | Acc: 81.839% (12361/15104)/ 95.928% (14489/15104)
01/14/2023 14:13:16 - INFO - __main__ -   test: [batch: 118/391 ] | Loss: 0.698 | Acc: 81.736% (12450/15232)/ 95.943% (14614/15232)
01/14/2023 14:13:19 - INFO - __main__ -   test: [batch: 119/391 ] | Loss: 0.696 | Acc: 81.784% (12562/15360)/ 95.964% (14740/15360)
01/14/2023 14:13:21 - INFO - __main__ -   test: [batch: 120/391 ] | Loss: 0.695 | Acc: 81.825% (12673/15488)/ 95.978% (14865/15488)
01/14/2023 14:13:23 - INFO - __main__ -   test: [batch: 121/391 ] | Loss: 0.700 | Acc: 81.698% (12758/15616)/ 95.946% (14983/15616)
01/14/2023 14:13:25 - INFO - __main__ -   test: [batch: 122/391 ] | Loss: 0.702 | Acc: 81.637% (12853/15744)/ 95.916% (15101/15744)
01/14/2023 14:13:27 - INFO - __main__ -   test: [batch: 123/391 ] | Loss: 0.703 | Acc: 81.653% (12960/15872)/ 95.917% (15224/15872)
01/14/2023 14:13:29 - INFO - __main__ -   test: [batch: 124/391 ] | Loss: 0.701 | Acc: 81.669% (13067/16000)/ 95.931% (15349/16000)
01/14/2023 14:13:32 - INFO - __main__ -   test: [batch: 125/391 ] | Loss: 0.697 | Acc: 81.783% (13190/16128)/ 95.957% (15476/16128)
01/14/2023 14:13:34 - INFO - __main__ -   test: [batch: 126/391 ] | Loss: 0.694 | Acc: 81.871% (13309/16256)/ 95.971% (15601/16256)
01/14/2023 14:13:36 - INFO - __main__ -   test: [batch: 127/391 ] | Loss: 0.692 | Acc: 81.952% (13427/16384)/ 95.978% (15725/16384)
01/14/2023 14:13:38 - INFO - __main__ -   test: [batch: 128/391 ] | Loss: 0.693 | Acc: 81.934% (13529/16512)/ 95.954% (15844/16512)
01/14/2023 14:13:40 - INFO - __main__ -   test: [batch: 129/391 ] | Loss: 0.691 | Acc: 81.947% (13636/16640)/ 95.968% (15969/16640)
01/14/2023 14:13:42 - INFO - __main__ -   test: [batch: 130/391 ] | Loss: 0.688 | Acc: 82.043% (13757/16768)/ 95.986% (16095/16768)
01/14/2023 14:13:44 - INFO - __main__ -   test: [batch: 131/391 ] | Loss: 0.688 | Acc: 82.085% (13869/16896)/ 95.993% (16219/16896)
01/14/2023 14:13:46 - INFO - __main__ -   test: [batch: 132/391 ] | Loss: 0.685 | Acc: 82.149% (13985/17024)/ 96.006% (16344/17024)
01/14/2023 14:13:48 - INFO - __main__ -   test: [batch: 133/391 ] | Loss: 0.686 | Acc: 82.095% (14081/17152)/ 95.995% (16465/17152)
01/14/2023 14:13:50 - INFO - __main__ -   test: [batch: 134/391 ] | Loss: 0.684 | Acc: 82.118% (14190/17280)/ 96.013% (16591/17280)
01/14/2023 14:13:52 - INFO - __main__ -   test: [batch: 135/391 ] | Loss: 0.685 | Acc: 82.112% (14294/17408)/ 96.008% (16713/17408)
01/14/2023 14:13:55 - INFO - __main__ -   test: [batch: 136/391 ] | Loss: 0.683 | Acc: 82.071% (14392/17536)/ 96.025% (16839/17536)
01/14/2023 14:13:57 - INFO - __main__ -   test: [batch: 137/391 ] | Loss: 0.681 | Acc: 82.122% (14506/17664)/ 96.054% (16967/17664)
01/14/2023 14:13:59 - INFO - __main__ -   test: [batch: 138/391 ] | Loss: 0.680 | Acc: 82.155% (14617/17792)/ 96.066% (17092/17792)
01/14/2023 14:14:01 - INFO - __main__ -   test: [batch: 139/391 ] | Loss: 0.686 | Acc: 81.998% (14694/17920)/ 96.060% (17214/17920)
01/14/2023 14:14:03 - INFO - __main__ -   test: [batch: 140/391 ] | Loss: 0.688 | Acc: 81.920% (14785/18048)/ 96.055% (17336/18048)
01/14/2023 14:14:05 - INFO - __main__ -   test: [batch: 141/391 ] | Loss: 0.686 | Acc: 81.960% (14897/18176)/ 96.066% (17461/18176)
01/14/2023 14:14:07 - INFO - __main__ -   test: [batch: 142/391 ] | Loss: 0.684 | Acc: 82.037% (15016/18304)/ 96.083% (17587/18304)
01/14/2023 14:14:09 - INFO - __main__ -   test: [batch: 143/391 ] | Loss: 0.685 | Acc: 82.053% (15124/18432)/ 96.061% (17706/18432)
01/14/2023 14:14:11 - INFO - __main__ -   test: [batch: 144/391 ] | Loss: 0.687 | Acc: 82.020% (15223/18560)/ 96.029% (17823/18560)
01/14/2023 14:14:14 - INFO - __main__ -   test: [batch: 145/391 ] | Loss: 0.690 | Acc: 81.999% (15324/18688)/ 96.003% (17941/18688)
01/14/2023 14:14:16 - INFO - __main__ -   test: [batch: 146/391 ] | Loss: 0.691 | Acc: 81.978% (15425/18816)/ 95.987% (18061/18816)
01/14/2023 14:14:18 - INFO - __main__ -   test: [batch: 147/391 ] | Loss: 0.691 | Acc: 81.973% (15529/18944)/ 95.967% (18180/18944)
01/14/2023 14:14:20 - INFO - __main__ -   test: [batch: 148/391 ] | Loss: 0.693 | Acc: 81.900% (15620/19072)/ 95.957% (18301/19072)
01/14/2023 14:14:22 - INFO - __main__ -   test: [batch: 149/391 ] | Loss: 0.696 | Acc: 81.802% (15706/19200)/ 95.922% (18417/19200)
01/14/2023 14:14:24 - INFO - __main__ -   test: [batch: 150/391 ] | Loss: 0.697 | Acc: 81.731% (15797/19328)/ 95.928% (18541/19328)
01/14/2023 14:14:26 - INFO - __main__ -   test: [batch: 151/391 ] | Loss: 0.697 | Acc: 81.764% (15908/19456)/ 95.929% (18664/19456)
01/14/2023 14:14:29 - INFO - __main__ -   test: [batch: 152/391 ] | Loss: 0.698 | Acc: 81.745% (16009/19584)/ 95.920% (18785/19584)
01/14/2023 14:14:31 - INFO - __main__ -   test: [batch: 153/391 ] | Loss: 0.696 | Acc: 81.808% (16126/19712)/ 95.921% (18908/19712)
01/14/2023 14:14:33 - INFO - __main__ -   test: [batch: 154/391 ] | Loss: 0.696 | Acc: 81.799% (16229/19840)/ 95.892% (19025/19840)
01/14/2023 14:14:35 - INFO - __main__ -   test: [batch: 155/391 ] | Loss: 0.697 | Acc: 81.816% (16337/19968)/ 95.888% (19147/19968)
01/14/2023 14:14:37 - INFO - __main__ -   test: [batch: 156/391 ] | Loss: 0.699 | Acc: 81.738% (16426/20096)/ 95.865% (19265/20096)
01/14/2023 14:14:39 - INFO - __main__ -   test: [batch: 157/391 ] | Loss: 0.701 | Acc: 81.730% (16529/20224)/ 95.847% (19384/20224)
01/14/2023 14:14:41 - INFO - __main__ -   test: [batch: 158/391 ] | Loss: 0.702 | Acc: 81.692% (16626/20352)/ 95.819% (19501/20352)
01/14/2023 14:14:43 - INFO - __main__ -   test: [batch: 159/391 ] | Loss: 0.703 | Acc: 81.665% (16725/20480)/ 95.815% (19623/20480)
01/14/2023 14:14:46 - INFO - __main__ -   test: [batch: 160/391 ] | Loss: 0.704 | Acc: 81.648% (16826/20608)/ 95.803% (19743/20608)
01/14/2023 14:14:48 - INFO - __main__ -   test: [batch: 161/391 ] | Loss: 0.713 | Acc: 81.424% (16884/20736)/ 95.713% (19847/20736)
01/14/2023 14:14:50 - INFO - __main__ -   test: [batch: 162/391 ] | Loss: 0.719 | Acc: 81.322% (16967/20864)/ 95.648% (19956/20864)
01/14/2023 14:14:52 - INFO - __main__ -   test: [batch: 163/391 ] | Loss: 0.721 | Acc: 81.274% (17061/20992)/ 95.641% (20077/20992)
01/14/2023 14:14:54 - INFO - __main__ -   test: [batch: 164/391 ] | Loss: 0.721 | Acc: 81.264% (17163/21120)/ 95.658% (20203/21120)
01/14/2023 14:14:56 - INFO - __main__ -   test: [batch: 165/391 ] | Loss: 0.723 | Acc: 81.179% (17249/21248)/ 95.651% (20324/21248)
01/14/2023 14:14:58 - INFO - __main__ -   test: [batch: 166/391 ] | Loss: 0.723 | Acc: 81.194% (17356/21376)/ 95.640% (20444/21376)
01/14/2023 14:15:01 - INFO - __main__ -   test: [batch: 167/391 ] | Loss: 0.725 | Acc: 81.134% (17447/21504)/ 95.624% (20563/21504)
01/14/2023 14:15:03 - INFO - __main__ -   test: [batch: 168/391 ] | Loss: 0.724 | Acc: 81.139% (17552/21632)/ 95.622% (20685/21632)
01/14/2023 14:15:05 - INFO - __main__ -   test: [batch: 169/391 ] | Loss: 0.726 | Acc: 81.103% (17648/21760)/ 95.593% (20801/21760)
01/14/2023 14:15:07 - INFO - __main__ -   test: [batch: 170/391 ] | Loss: 0.731 | Acc: 80.994% (17728/21888)/ 95.550% (20914/21888)
01/14/2023 14:15:09 - INFO - __main__ -   test: [batch: 171/391 ] | Loss: 0.734 | Acc: 80.932% (17818/22016)/ 95.521% (21030/22016)
01/14/2023 14:15:12 - INFO - __main__ -   test: [batch: 172/391 ] | Loss: 0.735 | Acc: 80.889% (17912/22144)/ 95.520% (21152/22144)
01/14/2023 14:15:14 - INFO - __main__ -   test: [batch: 173/391 ] | Loss: 0.737 | Acc: 80.837% (18004/22272)/ 95.492% (21268/22272)
01/14/2023 14:15:16 - INFO - __main__ -   test: [batch: 174/391 ] | Loss: 0.741 | Acc: 80.750% (18088/22400)/ 95.446% (21380/22400)
01/14/2023 14:15:18 - INFO - __main__ -   test: [batch: 175/391 ] | Loss: 0.740 | Acc: 80.802% (18203/22528)/ 95.450% (21503/22528)
01/14/2023 14:15:20 - INFO - __main__ -   test: [batch: 176/391 ] | Loss: 0.740 | Acc: 80.795% (18305/22656)/ 95.432% (21621/22656)
01/14/2023 14:15:22 - INFO - __main__ -   test: [batch: 177/391 ] | Loss: 0.744 | Acc: 80.728% (18393/22784)/ 95.383% (21732/22784)
01/14/2023 14:15:24 - INFO - __main__ -   test: [batch: 178/391 ] | Loss: 0.746 | Acc: 80.704% (18491/22912)/ 95.356% (21848/22912)
01/14/2023 14:15:27 - INFO - __main__ -   test: [batch: 179/391 ] | Loss: 0.749 | Acc: 80.647% (18581/23040)/ 95.326% (21963/23040)
01/14/2023 14:15:29 - INFO - __main__ -   test: [batch: 180/391 ] | Loss: 0.755 | Acc: 80.521% (18655/23168)/ 95.282% (22075/23168)
01/14/2023 14:15:31 - INFO - __main__ -   test: [batch: 181/391 ] | Loss: 0.761 | Acc: 80.383% (18726/23296)/ 95.227% (22184/23296)
01/14/2023 14:15:33 - INFO - __main__ -   test: [batch: 182/391 ] | Loss: 0.760 | Acc: 80.396% (18832/23424)/ 95.227% (22306/23424)
01/14/2023 14:15:35 - INFO - __main__ -   test: [batch: 183/391 ] | Loss: 0.767 | Acc: 80.278% (18907/23552)/ 95.138% (22407/23552)
01/14/2023 14:15:37 - INFO - __main__ -   test: [batch: 184/391 ] | Loss: 0.767 | Acc: 80.300% (19015/23680)/ 95.122% (22525/23680)
01/14/2023 14:15:40 - INFO - __main__ -   test: [batch: 185/391 ] | Loss: 0.766 | Acc: 80.305% (19119/23808)/ 95.115% (22645/23808)
01/14/2023 14:15:42 - INFO - __main__ -   test: [batch: 186/391 ] | Loss: 0.769 | Acc: 80.268% (19213/23936)/ 95.083% (22759/23936)
01/14/2023 14:15:44 - INFO - __main__ -   test: [batch: 187/391 ] | Loss: 0.774 | Acc: 80.153% (19288/24064)/ 95.059% (22875/24064)
01/14/2023 14:15:46 - INFO - __main__ -   test: [batch: 188/391 ] | Loss: 0.778 | Acc: 80.022% (19359/24192)/ 95.040% (22992/24192)
01/14/2023 14:15:48 - INFO - __main__ -   test: [batch: 189/391 ] | Loss: 0.779 | Acc: 79.963% (19447/24320)/ 95.041% (23114/24320)
01/14/2023 14:15:50 - INFO - __main__ -   test: [batch: 190/391 ] | Loss: 0.784 | Acc: 79.872% (19527/24448)/ 95.018% (23230/24448)
01/14/2023 14:15:52 - INFO - __main__ -   test: [batch: 191/391 ] | Loss: 0.786 | Acc: 79.838% (19621/24576)/ 95.003% (23348/24576)
01/14/2023 14:15:55 - INFO - __main__ -   test: [batch: 192/391 ] | Loss: 0.792 | Acc: 79.732% (19697/24704)/ 94.920% (23449/24704)
01/14/2023 14:15:57 - INFO - __main__ -   test: [batch: 193/391 ] | Loss: 0.792 | Acc: 79.756% (19805/24832)/ 94.914% (23569/24832)
01/14/2023 14:15:59 - INFO - __main__ -   test: [batch: 194/391 ] | Loss: 0.795 | Acc: 79.696% (19892/24960)/ 94.896% (23686/24960)
01/14/2023 14:16:01 - INFO - __main__ -   test: [batch: 195/391 ] | Loss: 0.800 | Acc: 79.620% (19975/25088)/ 94.834% (23792/25088)
01/14/2023 14:16:03 - INFO - __main__ -   test: [batch: 196/391 ] | Loss: 0.803 | Acc: 79.513% (20050/25216)/ 94.793% (23903/25216)
01/14/2023 14:16:05 - INFO - __main__ -   test: [batch: 197/391 ] | Loss: 0.807 | Acc: 79.439% (20133/25344)/ 94.772% (24019/25344)
01/14/2023 14:16:07 - INFO - __main__ -   test: [batch: 198/391 ] | Loss: 0.809 | Acc: 79.405% (20226/25472)/ 94.739% (24132/25472)
01/14/2023 14:16:10 - INFO - __main__ -   test: [batch: 199/391 ] | Loss: 0.809 | Acc: 79.391% (20324/25600)/ 94.742% (24254/25600)
01/14/2023 14:16:12 - INFO - __main__ -   test: [batch: 200/391 ] | Loss: 0.809 | Acc: 79.349% (20415/25728)/ 94.737% (24374/25728)
01/14/2023 14:16:14 - INFO - __main__ -   test: [batch: 201/391 ] | Loss: 0.812 | Acc: 79.258% (20493/25856)/ 94.701% (24486/25856)
01/14/2023 14:16:16 - INFO - __main__ -   test: [batch: 202/391 ] | Loss: 0.813 | Acc: 79.241% (20590/25984)/ 94.693% (24605/25984)
01/14/2023 14:16:18 - INFO - __main__ -   test: [batch: 203/391 ] | Loss: 0.814 | Acc: 79.232% (20689/26112)/ 94.677% (24722/26112)
01/14/2023 14:16:20 - INFO - __main__ -   test: [batch: 204/391 ] | Loss: 0.817 | Acc: 79.150% (20769/26240)/ 94.657% (24838/26240)
01/14/2023 14:16:22 - INFO - __main__ -   test: [batch: 205/391 ] | Loss: 0.820 | Acc: 79.069% (20849/26368)/ 94.626% (24951/26368)
01/14/2023 14:16:24 - INFO - __main__ -   test: [batch: 206/391 ] | Loss: 0.822 | Acc: 79.046% (20944/26496)/ 94.622% (25071/26496)
01/14/2023 14:16:27 - INFO - __main__ -   test: [batch: 207/391 ] | Loss: 0.825 | Acc: 78.959% (21022/26624)/ 94.580% (25181/26624)
01/14/2023 14:16:29 - INFO - __main__ -   test: [batch: 208/391 ] | Loss: 0.827 | Acc: 78.917% (21112/26752)/ 94.557% (25296/26752)
01/14/2023 14:16:31 - INFO - __main__ -   test: [batch: 209/391 ] | Loss: 0.827 | Acc: 78.917% (21213/26880)/ 94.576% (25422/26880)
01/14/2023 14:16:33 - INFO - __main__ -   test: [batch: 210/391 ] | Loss: 0.828 | Acc: 78.903% (21310/27008)/ 94.557% (25538/27008)
01/14/2023 14:16:35 - INFO - __main__ -   test: [batch: 211/391 ] | Loss: 0.830 | Acc: 78.851% (21397/27136)/ 94.535% (25653/27136)
01/14/2023 14:16:37 - INFO - __main__ -   test: [batch: 212/391 ] | Loss: 0.832 | Acc: 78.796% (21483/27264)/ 94.513% (25768/27264)
01/14/2023 14:16:39 - INFO - __main__ -   test: [batch: 213/391 ] | Loss: 0.832 | Acc: 78.797% (21584/27392)/ 94.520% (25891/27392)
01/14/2023 14:16:41 - INFO - __main__ -   test: [batch: 214/391 ] | Loss: 0.833 | Acc: 78.772% (21678/27520)/ 94.520% (26012/27520)
01/14/2023 14:16:43 - INFO - __main__ -   test: [batch: 215/391 ] | Loss: 0.832 | Acc: 78.791% (21784/27648)/ 94.517% (26132/27648)
01/14/2023 14:16:45 - INFO - __main__ -   test: [batch: 216/391 ] | Loss: 0.831 | Acc: 78.831% (21896/27776)/ 94.535% (26258/27776)
01/14/2023 14:16:47 - INFO - __main__ -   test: [batch: 217/391 ] | Loss: 0.836 | Acc: 78.759% (21977/27904)/ 94.477% (26363/27904)
01/14/2023 14:16:50 - INFO - __main__ -   test: [batch: 218/391 ] | Loss: 0.838 | Acc: 78.706% (22063/28032)/ 94.438% (26473/28032)
01/14/2023 14:16:52 - INFO - __main__ -   test: [batch: 219/391 ] | Loss: 0.837 | Acc: 78.736% (22172/28160)/ 94.442% (26595/28160)
01/14/2023 14:16:54 - INFO - __main__ -   test: [batch: 220/391 ] | Loss: 0.835 | Acc: 78.772% (22283/28288)/ 94.461% (26721/28288)
01/14/2023 14:16:56 - INFO - __main__ -   test: [batch: 221/391 ] | Loss: 0.837 | Acc: 78.730% (22372/28416)/ 94.443% (26837/28416)
01/14/2023 14:16:58 - INFO - __main__ -   test: [batch: 222/391 ] | Loss: 0.835 | Acc: 78.787% (22489/28544)/ 94.458% (26962/28544)
01/14/2023 14:17:00 - INFO - __main__ -   test: [batch: 223/391 ] | Loss: 0.835 | Acc: 78.784% (22589/28672)/ 94.451% (27081/28672)
01/14/2023 14:17:02 - INFO - __main__ -   test: [batch: 224/391 ] | Loss: 0.835 | Acc: 78.802% (22695/28800)/ 94.444% (27200/28800)
01/14/2023 14:17:04 - INFO - __main__ -   test: [batch: 225/391 ] | Loss: 0.834 | Acc: 78.803% (22796/28928)/ 94.445% (27321/28928)
01/14/2023 14:17:07 - INFO - __main__ -   test: [batch: 226/391 ] | Loss: 0.834 | Acc: 78.786% (22892/29056)/ 94.456% (27445/29056)
01/14/2023 14:17:09 - INFO - __main__ -   test: [batch: 227/391 ] | Loss: 0.837 | Acc: 78.769% (22988/29184)/ 94.446% (27563/29184)
01/14/2023 14:17:11 - INFO - __main__ -   test: [batch: 228/391 ] | Loss: 0.841 | Acc: 78.691% (23066/29312)/ 94.385% (27666/29312)
01/14/2023 14:17:13 - INFO - __main__ -   test: [batch: 229/391 ] | Loss: 0.844 | Acc: 78.635% (23150/29440)/ 94.341% (27774/29440)
01/14/2023 14:17:16 - INFO - __main__ -   test: [batch: 230/391 ] | Loss: 0.847 | Acc: 78.578% (23234/29568)/ 94.305% (27884/29568)
01/14/2023 14:17:18 - INFO - __main__ -   test: [batch: 231/391 ] | Loss: 0.847 | Acc: 78.570% (23332/29696)/ 94.289% (28000/29696)
01/14/2023 14:17:20 - INFO - __main__ -   test: [batch: 232/391 ] | Loss: 0.846 | Acc: 78.598% (23441/29824)/ 94.303% (28125/29824)
01/14/2023 14:17:22 - INFO - __main__ -   test: [batch: 233/391 ] | Loss: 0.848 | Acc: 78.546% (23526/29952)/ 94.281% (28239/29952)
01/14/2023 14:17:24 - INFO - __main__ -   test: [batch: 234/391 ] | Loss: 0.855 | Acc: 78.428% (23591/30080)/ 94.209% (28338/30080)
01/14/2023 14:17:26 - INFO - __main__ -   test: [batch: 235/391 ] | Loss: 0.855 | Acc: 78.413% (23687/30208)/ 94.204% (28457/30208)
01/14/2023 14:17:29 - INFO - __main__ -   test: [batch: 236/391 ] | Loss: 0.853 | Acc: 78.458% (23801/30336)/ 94.211% (28580/30336)
01/14/2023 14:17:31 - INFO - __main__ -   test: [batch: 237/391 ] | Loss: 0.854 | Acc: 78.463% (23903/30464)/ 94.183% (28692/30464)
01/14/2023 14:17:33 - INFO - __main__ -   test: [batch: 238/391 ] | Loss: 0.852 | Acc: 78.504% (24016/30592)/ 94.191% (28815/30592)
01/14/2023 14:17:35 - INFO - __main__ -   test: [batch: 239/391 ] | Loss: 0.851 | Acc: 78.542% (24128/30720)/ 94.193% (28936/30720)
01/14/2023 14:17:37 - INFO - __main__ -   test: [batch: 240/391 ] | Loss: 0.851 | Acc: 78.559% (24234/30848)/ 94.178% (29052/30848)
01/14/2023 14:17:39 - INFO - __main__ -   test: [batch: 241/391 ] | Loss: 0.854 | Acc: 78.464% (24305/30976)/ 94.141% (29161/30976)
01/14/2023 14:17:41 - INFO - __main__ -   test: [batch: 242/391 ] | Loss: 0.857 | Acc: 78.360% (24373/31104)/ 94.120% (29275/31104)
01/14/2023 14:17:43 - INFO - __main__ -   test: [batch: 243/391 ] | Loss: 0.863 | Acc: 78.231% (24433/31232)/ 94.054% (29375/31232)
01/14/2023 14:17:45 - INFO - __main__ -   test: [batch: 244/391 ] | Loss: 0.863 | Acc: 78.240% (24536/31360)/ 94.050% (29494/31360)
01/14/2023 14:17:48 - INFO - __main__ -   test: [batch: 245/391 ] | Loss: 0.862 | Acc: 78.255% (24641/31488)/ 94.042% (29612/31488)
01/14/2023 14:17:50 - INFO - __main__ -   test: [batch: 246/391 ] | Loss: 0.863 | Acc: 78.233% (24734/31616)/ 94.032% (29729/31616)
01/14/2023 14:17:52 - INFO - __main__ -   test: [batch: 247/391 ] | Loss: 0.868 | Acc: 78.144% (24806/31744)/ 93.958% (29826/31744)
01/14/2023 14:17:54 - INFO - __main__ -   test: [batch: 248/391 ] | Loss: 0.869 | Acc: 78.103% (24893/31872)/ 93.948% (29943/31872)
01/14/2023 14:17:56 - INFO - __main__ -   test: [batch: 249/391 ] | Loss: 0.871 | Acc: 77.975% (24952/32000)/ 93.938% (30060/32000)
01/14/2023 14:17:58 - INFO - __main__ -   test: [batch: 250/391 ] | Loss: 0.870 | Acc: 78.010% (25063/32128)/ 93.952% (30185/32128)
01/14/2023 14:18:00 - INFO - __main__ -   test: [batch: 251/391 ] | Loss: 0.872 | Acc: 77.970% (25150/32256)/ 93.924% (30296/32256)
01/14/2023 14:18:02 - INFO - __main__ -   test: [batch: 252/391 ] | Loss: 0.871 | Acc: 77.998% (25259/32384)/ 93.917% (30414/32384)
01/14/2023 14:18:04 - INFO - __main__ -   test: [batch: 253/391 ] | Loss: 0.873 | Acc: 77.968% (25349/32512)/ 93.901% (30529/32512)
01/14/2023 14:18:06 - INFO - __main__ -   test: [batch: 254/391 ] | Loss: 0.877 | Acc: 77.904% (25428/32640)/ 93.851% (30633/32640)
01/14/2023 14:18:08 - INFO - __main__ -   test: [batch: 255/391 ] | Loss: 0.879 | Acc: 77.869% (25516/32768)/ 93.842% (30750/32768)
01/14/2023 14:18:11 - INFO - __main__ -   test: [batch: 256/391 ] | Loss: 0.884 | Acc: 77.748% (25576/32896)/ 93.811% (30860/32896)
01/14/2023 14:18:13 - INFO - __main__ -   test: [batch: 257/391 ] | Loss: 0.885 | Acc: 77.737% (25672/33024)/ 93.798% (30976/33024)
01/14/2023 14:18:15 - INFO - __main__ -   test: [batch: 258/391 ] | Loss: 0.885 | Acc: 77.727% (25768/33152)/ 93.786% (31092/33152)
01/14/2023 14:18:17 - INFO - __main__ -   test: [batch: 259/391 ] | Loss: 0.889 | Acc: 77.605% (25827/33280)/ 93.774% (31208/33280)
01/14/2023 14:18:19 - INFO - __main__ -   test: [batch: 260/391 ] | Loss: 0.890 | Acc: 77.574% (25916/33408)/ 93.765% (31325/33408)
01/14/2023 14:18:21 - INFO - __main__ -   test: [batch: 261/391 ] | Loss: 0.888 | Acc: 77.615% (26029/33536)/ 93.786% (31452/33536)
01/14/2023 14:18:23 - INFO - __main__ -   test: [batch: 262/391 ] | Loss: 0.887 | Acc: 77.632% (26134/33664)/ 93.792% (31574/33664)
01/14/2023 14:18:25 - INFO - __main__ -   test: [batch: 263/391 ] | Loss: 0.890 | Acc: 77.554% (26207/33792)/ 93.765% (31685/33792)
01/14/2023 14:18:27 - INFO - __main__ -   test: [batch: 264/391 ] | Loss: 0.895 | Acc: 77.494% (26286/33920)/ 93.706% (31785/33920)
01/14/2023 14:18:29 - INFO - __main__ -   test: [batch: 265/391 ] | Loss: 0.894 | Acc: 77.523% (26395/34048)/ 93.703% (31904/34048)
01/14/2023 14:18:32 - INFO - __main__ -   test: [batch: 266/391 ] | Loss: 0.897 | Acc: 77.455% (26471/34176)/ 93.686% (32018/34176)
01/14/2023 14:18:34 - INFO - __main__ -   test: [batch: 267/391 ] | Loss: 0.896 | Acc: 77.487% (26581/34304)/ 93.686% (32138/34304)
01/14/2023 14:18:36 - INFO - __main__ -   test: [batch: 268/391 ] | Loss: 0.895 | Acc: 77.503% (26686/34432)/ 93.683% (32257/34432)
01/14/2023 14:18:38 - INFO - __main__ -   test: [batch: 269/391 ] | Loss: 0.897 | Acc: 77.454% (26768/34560)/ 93.663% (32370/34560)
01/14/2023 14:18:40 - INFO - __main__ -   test: [batch: 270/391 ] | Loss: 0.900 | Acc: 77.390% (26845/34688)/ 93.632% (32479/34688)
01/14/2023 14:18:42 - INFO - __main__ -   test: [batch: 271/391 ] | Loss: 0.900 | Acc: 77.407% (26950/34816)/ 93.626% (32597/34816)
01/14/2023 14:18:44 - INFO - __main__ -   test: [batch: 272/391 ] | Loss: 0.902 | Acc: 77.370% (27036/34944)/ 93.621% (32715/34944)
01/14/2023 14:18:46 - INFO - __main__ -   test: [batch: 273/391 ] | Loss: 0.902 | Acc: 77.375% (27137/35072)/ 93.610% (32831/35072)
01/14/2023 14:18:49 - INFO - __main__ -   test: [batch: 274/391 ] | Loss: 0.902 | Acc: 77.375% (27236/35200)/ 93.616% (32953/35200)
01/14/2023 14:18:51 - INFO - __main__ -   test: [batch: 275/391 ] | Loss: 0.904 | Acc: 77.344% (27324/35328)/ 93.608% (33070/35328)
01/14/2023 14:18:53 - INFO - __main__ -   test: [batch: 276/391 ] | Loss: 0.904 | Acc: 77.293% (27405/35456)/ 93.601% (33187/35456)
01/14/2023 14:18:55 - INFO - __main__ -   test: [batch: 277/391 ] | Loss: 0.905 | Acc: 77.262% (27493/35584)/ 93.590% (33303/35584)
01/14/2023 14:18:57 - INFO - __main__ -   test: [batch: 278/391 ] | Loss: 0.905 | Acc: 77.265% (27593/35712)/ 93.588% (33422/35712)
01/14/2023 14:19:00 - INFO - __main__ -   test: [batch: 279/391 ] | Loss: 0.905 | Acc: 77.274% (27695/35840)/ 93.583% (33540/35840)
01/14/2023 14:19:02 - INFO - __main__ -   test: [batch: 280/391 ] | Loss: 0.906 | Acc: 77.255% (27787/35968)/ 93.578% (33658/35968)
01/14/2023 14:19:04 - INFO - __main__ -   test: [batch: 281/391 ] | Loss: 0.906 | Acc: 77.266% (27890/36096)/ 93.573% (33776/36096)
01/14/2023 14:19:06 - INFO - __main__ -   test: [batch: 282/391 ] | Loss: 0.905 | Acc: 77.302% (28002/36224)/ 93.579% (33898/36224)
01/14/2023 14:19:08 - INFO - __main__ -   test: [batch: 283/391 ] | Loss: 0.906 | Acc: 77.278% (28092/36352)/ 93.574% (34016/36352)
01/14/2023 14:19:10 - INFO - __main__ -   test: [batch: 284/391 ] | Loss: 0.909 | Acc: 77.226% (28172/36480)/ 93.544% (34125/36480)
01/14/2023 14:19:12 - INFO - __main__ -   test: [batch: 285/391 ] | Loss: 0.912 | Acc: 77.185% (28256/36608)/ 93.501% (34229/36608)
01/14/2023 14:19:14 - INFO - __main__ -   test: [batch: 286/391 ] | Loss: 0.913 | Acc: 77.153% (28343/36736)/ 93.491% (34345/36736)
01/14/2023 14:19:15 - INFO - __main__ -   test: [batch: 287/391 ] | Loss: 0.913 | Acc: 77.154% (28442/36864)/ 93.487% (34463/36864)
01/14/2023 14:19:16 - INFO - __main__ -   test: [batch: 288/391 ] | Loss: 0.912 | Acc: 77.173% (28548/36992)/ 93.490% (34584/36992)
01/14/2023 14:19:17 - INFO - __main__ -   test: [batch: 289/391 ] | Loss: 0.914 | Acc: 77.134% (28632/37120)/ 93.459% (34692/37120)
01/14/2023 14:19:19 - INFO - __main__ -   test: [batch: 290/391 ] | Loss: 0.915 | Acc: 77.056% (28702/37248)/ 93.457% (34811/37248)
01/14/2023 14:19:20 - INFO - __main__ -   test: [batch: 291/391 ] | Loss: 0.916 | Acc: 77.068% (28805/37376)/ 93.453% (34929/37376)
01/14/2023 14:19:21 - INFO - __main__ -   test: [batch: 292/391 ] | Loss: 0.917 | Acc: 77.013% (28883/37504)/ 93.435% (35042/37504)
01/14/2023 14:19:22 - INFO - __main__ -   test: [batch: 293/391 ] | Loss: 0.917 | Acc: 77.006% (28979/37632)/ 93.431% (35160/37632)
01/14/2023 14:19:23 - INFO - __main__ -   test: [batch: 294/391 ] | Loss: 0.918 | Acc: 76.989% (29071/37760)/ 93.427% (35278/37760)
01/14/2023 14:19:24 - INFO - __main__ -   test: [batch: 295/391 ] | Loss: 0.917 | Acc: 77.022% (29182/37888)/ 93.431% (35399/37888)
01/14/2023 14:19:25 - INFO - __main__ -   test: [batch: 296/391 ] | Loss: 0.918 | Acc: 77.020% (29280/38016)/ 93.421% (35515/38016)
01/14/2023 14:19:26 - INFO - __main__ -   test: [batch: 297/391 ] | Loss: 0.920 | Acc: 76.995% (29369/38144)/ 93.399% (35626/38144)
01/14/2023 14:19:27 - INFO - __main__ -   test: [batch: 298/391 ] | Loss: 0.922 | Acc: 76.965% (29456/38272)/ 93.376% (35737/38272)
01/14/2023 14:19:28 - INFO - __main__ -   test: [batch: 299/391 ] | Loss: 0.923 | Acc: 76.956% (29551/38400)/ 93.354% (35848/38400)
01/14/2023 14:19:29 - INFO - __main__ -   test: [batch: 300/391 ] | Loss: 0.923 | Acc: 76.960% (29651/38528)/ 93.350% (35966/38528)
01/14/2023 14:19:30 - INFO - __main__ -   test: [batch: 301/391 ] | Loss: 0.923 | Acc: 76.943% (29743/38656)/ 93.336% (36080/38656)
01/14/2023 14:19:31 - INFO - __main__ -   test: [batch: 302/391 ] | Loss: 0.925 | Acc: 76.908% (29828/38784)/ 93.319% (36193/38784)
01/14/2023 14:19:32 - INFO - __main__ -   test: [batch: 303/391 ] | Loss: 0.926 | Acc: 76.884% (29917/38912)/ 93.303% (36306/38912)
01/14/2023 14:19:33 - INFO - __main__ -   test: [batch: 304/391 ] | Loss: 0.925 | Acc: 76.903% (30023/39040)/ 93.307% (36427/39040)
01/14/2023 14:19:34 - INFO - __main__ -   test: [batch: 305/391 ] | Loss: 0.926 | Acc: 76.859% (30104/39168)/ 93.301% (36544/39168)
01/14/2023 14:19:35 - INFO - __main__ -   test: [batch: 306/391 ] | Loss: 0.928 | Acc: 76.840% (30195/39296)/ 93.279% (36655/39296)
01/14/2023 14:19:36 - INFO - __main__ -   test: [batch: 307/391 ] | Loss: 0.928 | Acc: 76.842% (30294/39424)/ 93.263% (36768/39424)
01/14/2023 14:19:37 - INFO - __main__ -   test: [batch: 308/391 ] | Loss: 0.929 | Acc: 76.828% (30387/39552)/ 93.254% (36884/39552)
01/14/2023 14:19:37 - INFO - __main__ -   test: [batch: 309/391 ] | Loss: 0.930 | Acc: 76.825% (30484/39680)/ 93.236% (36996/39680)
01/14/2023 14:19:38 - INFO - __main__ -   test: [batch: 310/391 ] | Loss: 0.930 | Acc: 76.819% (30580/39808)/ 93.225% (37111/39808)
01/14/2023 14:19:39 - INFO - __main__ -   test: [batch: 311/391 ] | Loss: 0.932 | Acc: 76.785% (30665/39936)/ 93.214% (37226/39936)
01/14/2023 14:19:40 - INFO - __main__ -   test: [batch: 312/391 ] | Loss: 0.934 | Acc: 76.767% (30756/40064)/ 93.193% (37337/40064)
01/14/2023 14:19:41 - INFO - __main__ -   test: [batch: 313/391 ] | Loss: 0.932 | Acc: 76.806% (30870/40192)/ 93.210% (37463/40192)
01/14/2023 14:19:42 - INFO - __main__ -   test: [batch: 314/391 ] | Loss: 0.932 | Acc: 76.806% (30968/40320)/ 93.207% (37581/40320)
01/14/2023 14:19:43 - INFO - __main__ -   test: [batch: 315/391 ] | Loss: 0.933 | Acc: 76.785% (31058/40448)/ 93.194% (37695/40448)
01/14/2023 14:19:44 - INFO - __main__ -   test: [batch: 316/391 ] | Loss: 0.935 | Acc: 76.693% (31119/40576)/ 93.176% (37807/40576)
01/14/2023 14:19:45 - INFO - __main__ -   test: [batch: 317/391 ] | Loss: 0.937 | Acc: 76.651% (31200/40704)/ 93.153% (37917/40704)
01/14/2023 14:19:46 - INFO - __main__ -   test: [batch: 318/391 ] | Loss: 0.936 | Acc: 76.680% (31310/40832)/ 93.170% (38043/40832)
01/14/2023 14:19:47 - INFO - __main__ -   test: [batch: 319/391 ] | Loss: 0.939 | Acc: 76.624% (31385/40960)/ 93.142% (38151/40960)
01/14/2023 14:19:48 - INFO - __main__ -   test: [batch: 320/391 ] | Loss: 0.937 | Acc: 76.660% (31498/41088)/ 93.151% (38274/41088)
01/14/2023 14:19:49 - INFO - __main__ -   test: [batch: 321/391 ] | Loss: 0.937 | Acc: 76.679% (31604/41216)/ 93.143% (38390/41216)
01/14/2023 14:19:50 - INFO - __main__ -   test: [batch: 322/391 ] | Loss: 0.939 | Acc: 76.652% (31691/41344)/ 93.138% (38507/41344)
01/14/2023 14:19:51 - INFO - __main__ -   test: [batch: 323/391 ] | Loss: 0.941 | Acc: 76.616% (31774/41472)/ 93.113% (38616/41472)
01/14/2023 14:19:52 - INFO - __main__ -   test: [batch: 324/391 ] | Loss: 0.941 | Acc: 76.613% (31871/41600)/ 93.106% (38732/41600)
01/14/2023 14:19:53 - INFO - __main__ -   test: [batch: 325/391 ] | Loss: 0.941 | Acc: 76.618% (31971/41728)/ 93.103% (38850/41728)
01/14/2023 14:19:54 - INFO - __main__ -   test: [batch: 326/391 ] | Loss: 0.944 | Acc: 76.536% (32035/41856)/ 93.074% (38957/41856)
01/14/2023 14:19:55 - INFO - __main__ -   test: [batch: 327/391 ] | Loss: 0.947 | Acc: 76.455% (32099/41984)/ 93.040% (39062/41984)
01/14/2023 14:19:56 - INFO - __main__ -   test: [batch: 328/391 ] | Loss: 0.949 | Acc: 76.406% (32176/42112)/ 93.019% (39172/42112)
01/14/2023 14:19:57 - INFO - __main__ -   test: [batch: 329/391 ] | Loss: 0.949 | Acc: 76.404% (32273/42240)/ 93.016% (39290/42240)
01/14/2023 14:19:58 - INFO - __main__ -   test: [batch: 330/391 ] | Loss: 0.951 | Acc: 76.362% (32353/42368)/ 92.990% (39398/42368)
01/14/2023 14:19:59 - INFO - __main__ -   test: [batch: 331/391 ] | Loss: 0.951 | Acc: 76.332% (32438/42496)/ 93.004% (39523/42496)
01/14/2023 14:20:00 - INFO - __main__ -   test: [batch: 332/391 ] | Loss: 0.952 | Acc: 76.323% (32532/42624)/ 93.004% (39642/42624)
01/14/2023 14:20:01 - INFO - __main__ -   test: [batch: 333/391 ] | Loss: 0.950 | Acc: 76.352% (32642/42752)/ 93.013% (39765/42752)
01/14/2023 14:20:02 - INFO - __main__ -   test: [batch: 334/391 ] | Loss: 0.952 | Acc: 76.320% (32726/42880)/ 92.994% (39876/42880)
01/14/2023 14:20:03 - INFO - __main__ -   test: [batch: 335/391 ] | Loss: 0.953 | Acc: 76.300% (32815/43008)/ 92.980% (39989/43008)
01/14/2023 14:20:04 - INFO - __main__ -   test: [batch: 336/391 ] | Loss: 0.954 | Acc: 76.254% (32893/43136)/ 92.966% (40102/43136)
01/14/2023 14:20:05 - INFO - __main__ -   test: [batch: 337/391 ] | Loss: 0.955 | Acc: 76.248% (32988/43264)/ 92.962% (40219/43264)
01/14/2023 14:20:06 - INFO - __main__ -   test: [batch: 338/391 ] | Loss: 0.954 | Acc: 76.233% (33079/43392)/ 92.971% (40342/43392)
01/14/2023 14:20:07 - INFO - __main__ -   test: [batch: 339/391 ] | Loss: 0.957 | Acc: 76.197% (33161/43520)/ 92.946% (40450/43520)
01/14/2023 14:20:08 - INFO - __main__ -   test: [batch: 340/391 ] | Loss: 0.957 | Acc: 76.189% (33255/43648)/ 92.955% (40573/43648)
01/14/2023 14:20:09 - INFO - __main__ -   test: [batch: 341/391 ] | Loss: 0.955 | Acc: 76.229% (33370/43776)/ 92.973% (40700/43776)
01/14/2023 14:20:10 - INFO - __main__ -   test: [batch: 342/391 ] | Loss: 0.956 | Acc: 76.173% (33443/43904)/ 92.966% (40816/43904)
01/14/2023 14:20:11 - INFO - __main__ -   test: [batch: 343/391 ] | Loss: 0.955 | Acc: 76.172% (33540/44032)/ 92.969% (40936/44032)
01/14/2023 14:20:12 - INFO - __main__ -   test: [batch: 344/391 ] | Loss: 0.956 | Acc: 76.171% (33637/44160)/ 92.962% (41052/44160)
01/14/2023 14:20:13 - INFO - __main__ -   test: [batch: 345/391 ] | Loss: 0.960 | Acc: 76.104% (33705/44288)/ 92.919% (41152/44288)
01/14/2023 14:20:14 - INFO - __main__ -   test: [batch: 346/391 ] | Loss: 0.961 | Acc: 76.087% (33795/44416)/ 92.903% (41264/44416)
01/14/2023 14:20:15 - INFO - __main__ -   test: [batch: 347/391 ] | Loss: 0.961 | Acc: 76.102% (33899/44544)/ 92.913% (41387/44544)
01/14/2023 14:20:16 - INFO - __main__ -   test: [batch: 348/391 ] | Loss: 0.962 | Acc: 76.079% (33986/44672)/ 92.886% (41494/44672)
01/14/2023 14:20:16 - INFO - __main__ -   test: [batch: 349/391 ] | Loss: 0.962 | Acc: 76.078% (34083/44800)/ 92.891% (41615/44800)
01/14/2023 14:20:18 - INFO - __main__ -   test: [batch: 350/391 ] | Loss: 0.961 | Acc: 76.093% (34187/44928)/ 92.889% (41733/44928)
01/14/2023 14:20:19 - INFO - __main__ -   test: [batch: 351/391 ] | Loss: 0.964 | Acc: 76.036% (34259/45056)/ 92.876% (41846/45056)
01/14/2023 14:20:20 - INFO - __main__ -   test: [batch: 352/391 ] | Loss: 0.964 | Acc: 76.036% (34356/45184)/ 92.874% (41964/45184)
01/14/2023 14:20:21 - INFO - __main__ -   test: [batch: 353/391 ] | Loss: 0.966 | Acc: 75.993% (34434/45312)/ 92.836% (42066/45312)
01/14/2023 14:20:22 - INFO - __main__ -   test: [batch: 354/391 ] | Loss: 0.969 | Acc: 75.935% (34505/45440)/ 92.815% (42175/45440)
01/14/2023 14:20:23 - INFO - __main__ -   test: [batch: 355/391 ] | Loss: 0.971 | Acc: 75.871% (34573/45568)/ 92.809% (42291/45568)
01/14/2023 14:20:24 - INFO - __main__ -   test: [batch: 356/391 ] | Loss: 0.972 | Acc: 75.871% (34670/45696)/ 92.809% (42410/45696)
01/14/2023 14:20:25 - INFO - __main__ -   test: [batch: 357/391 ] | Loss: 0.970 | Acc: 75.906% (34783/45824)/ 92.823% (42535/45824)
01/14/2023 14:20:26 - INFO - __main__ -   test: [batch: 358/391 ] | Loss: 0.969 | Acc: 75.934% (34893/45952)/ 92.825% (42655/45952)
01/14/2023 14:20:27 - INFO - __main__ -   test: [batch: 359/391 ] | Loss: 0.970 | Acc: 75.942% (34994/46080)/ 92.823% (42773/46080)
01/14/2023 14:20:27 - INFO - __main__ -   test: [batch: 360/391 ] | Loss: 0.972 | Acc: 75.911% (35077/46208)/ 92.815% (42888/46208)
01/14/2023 14:20:28 - INFO - __main__ -   test: [batch: 361/391 ] | Loss: 0.972 | Acc: 75.911% (35174/46336)/ 92.822% (43010/46336)
01/14/2023 14:20:29 - INFO - __main__ -   test: [batch: 362/391 ] | Loss: 0.971 | Acc: 75.908% (35270/46464)/ 92.835% (43135/46464)
01/14/2023 14:20:30 - INFO - __main__ -   test: [batch: 363/391 ] | Loss: 0.972 | Acc: 75.899% (35363/46592)/ 92.825% (43249/46592)
01/14/2023 14:20:32 - INFO - __main__ -   test: [batch: 364/391 ] | Loss: 0.970 | Acc: 75.927% (35473/46720)/ 92.834% (43372/46720)
01/14/2023 14:20:32 - INFO - __main__ -   test: [batch: 365/391 ] | Loss: 0.970 | Acc: 75.943% (35578/46848)/ 92.841% (43494/46848)
01/14/2023 14:20:33 - INFO - __main__ -   test: [batch: 366/391 ] | Loss: 0.968 | Acc: 75.981% (35693/46976)/ 92.858% (43621/46976)
01/14/2023 14:20:34 - INFO - __main__ -   test: [batch: 367/391 ] | Loss: 0.967 | Acc: 76.006% (35802/47104)/ 92.871% (43746/47104)
01/14/2023 14:20:35 - INFO - __main__ -   test: [batch: 368/391 ] | Loss: 0.967 | Acc: 76.012% (35902/47232)/ 92.880% (43869/47232)
01/14/2023 14:20:36 - INFO - __main__ -   test: [batch: 369/391 ] | Loss: 0.966 | Acc: 76.030% (36008/47360)/ 92.889% (43992/47360)
01/14/2023 14:20:37 - INFO - __main__ -   test: [batch: 370/391 ] | Loss: 0.966 | Acc: 76.017% (36099/47488)/ 92.889% (44111/47488)
01/14/2023 14:20:38 - INFO - __main__ -   test: [batch: 371/391 ] | Loss: 0.966 | Acc: 76.021% (36198/47616)/ 92.891% (44231/47616)
01/14/2023 14:20:39 - INFO - __main__ -   test: [batch: 372/391 ] | Loss: 0.964 | Acc: 76.066% (36317/47744)/ 92.906% (44357/47744)
01/14/2023 14:20:40 - INFO - __main__ -   test: [batch: 373/391 ] | Loss: 0.963 | Acc: 76.103% (36432/47872)/ 92.914% (44480/47872)
01/14/2023 14:20:41 - INFO - __main__ -   test: [batch: 374/391 ] | Loss: 0.962 | Acc: 76.133% (36544/48000)/ 92.921% (44602/48000)
01/14/2023 14:20:42 - INFO - __main__ -   test: [batch: 375/391 ] | Loss: 0.965 | Acc: 76.066% (36609/48128)/ 92.886% (44704/48128)
01/14/2023 14:20:43 - INFO - __main__ -   test: [batch: 376/391 ] | Loss: 0.965 | Acc: 76.061% (36704/48256)/ 92.871% (44816/48256)
01/14/2023 14:20:44 - INFO - __main__ -   test: [batch: 377/391 ] | Loss: 0.966 | Acc: 76.046% (36794/48384)/ 92.865% (44932/48384)
01/14/2023 14:20:44 - INFO - __main__ -   test: [batch: 378/391 ] | Loss: 0.969 | Acc: 75.973% (36856/48512)/ 92.829% (45033/48512)
01/14/2023 14:20:45 - INFO - __main__ -   test: [batch: 379/391 ] | Loss: 0.969 | Acc: 75.954% (36944/48640)/ 92.837% (45156/48640)
01/14/2023 14:20:45 - INFO - __main__ -   test: [batch: 380/391 ] | Loss: 0.969 | Acc: 75.949% (37039/48768)/ 92.846% (45279/48768)
01/14/2023 14:20:46 - INFO - __main__ -   test: [batch: 381/391 ] | Loss: 0.971 | Acc: 75.906% (37115/48896)/ 92.842% (45396/48896)
01/14/2023 14:20:46 - INFO - __main__ -   test: [batch: 382/391 ] | Loss: 0.972 | Acc: 75.871% (37195/49024)/ 92.826% (45507/49024)
01/14/2023 14:20:46 - INFO - __main__ -   test: [batch: 383/391 ] | Loss: 0.972 | Acc: 75.881% (37297/49152)/ 92.820% (45623/49152)
01/14/2023 14:20:47 - INFO - __main__ -   test: [batch: 384/391 ] | Loss: 0.970 | Acc: 75.921% (37414/49280)/ 92.833% (45748/49280)
01/14/2023 14:20:47 - INFO - __main__ -   test: [batch: 385/391 ] | Loss: 0.969 | Acc: 75.933% (37517/49408)/ 92.843% (45872/49408)
01/14/2023 14:20:47 - INFO - __main__ -   test: [batch: 386/391 ] | Loss: 0.967 | Acc: 75.977% (37636/49536)/ 92.860% (45999/49536)
01/14/2023 14:20:48 - INFO - __main__ -   test: [batch: 387/391 ] | Loss: 0.966 | Acc: 76.017% (37753/49664)/ 92.870% (46123/49664)
01/14/2023 14:20:48 - INFO - __main__ -   test: [batch: 388/391 ] | Loss: 0.964 | Acc: 76.062% (37873/49792)/ 92.882% (46248/49792)
01/14/2023 14:20:48 - INFO - __main__ -   test: [batch: 389/391 ] | Loss: 0.963 | Acc: 76.066% (37972/49920)/ 92.885% (46368/49920)
01/14/2023 14:20:49 - INFO - __main__ -   test: [batch: 390/391 ] | Loss: 0.965 | Acc: 76.020% (38010/50000)/ 92.876% (46438/50000)
01/14/2023 14:20:49 - INFO - __main__ -   Final accuracy: 76.020
