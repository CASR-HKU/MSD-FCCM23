/home/jjc/miniconda3/envs/ant_quant/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
Namespace(a_low=75, a_up=150, abit=8, batch_size=512, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='4', epoch=5, layer_8bit_l=None, layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='resnet18', percent=100, ptq=False, resume=False, search=False, tag='', train=True, w_low=75, w_up=150, wbit=8)
01/04/2023 17:58:50 - INFO - __main__ -   output/resnet18_imagenet/int_W8A8_39173/gpu_0
01/04/2023 17:58:50 - INFO - __main__ -   Namespace(a_low=75, a_up=150, abit=8, batch_size=512, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='4', epoch=5, layer_8bit_l=None, layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='resnet18', percent=100, ptq=False, resume=False, search=False, tag='', train=True, w_low=75, w_up=150, wbit=8)
01/04/2023 17:58:50 - INFO - __main__ -   ==> Preparing data..
01/04/2023 17:58:52 - INFO - __main__ -   ==> Setting quantizer..
Namespace(a_low=75, a_up=150, abit=8, batch_size=512, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='4', epoch=5, layer_8bit_l=None, layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='resnet18', percent=100, ptq=False, resume=False, search=False, tag='', train=True, w_low=75, w_up=150, wbit=8)
01/04/2023 17:58:52 - INFO - __main__ -   Namespace(a_low=75, a_up=150, abit=8, batch_size=512, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='4', epoch=5, layer_8bit_l=None, layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='resnet18', percent=100, ptq=False, resume=False, search=False, tag='', train=True, w_low=75, w_up=150, wbit=8)
01/04/2023 17:58:52 - INFO - __main__ -   ==> Building model..
ResNet(
  (conv1): Conv2dQuantizer(
    (quant_weight): TensorQuantizer()
    (quant_input): TensorQuantizer()
  )
  (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): LinearQuantizer(
    (quant_weight): TensorQuantizer()
    (quant_input): TensorQuantizer()
  )
)
01/04/2023 17:58:52 - INFO - __main__ -   {'milestones': Counter({2: 1, 3: 1, 4: 1}), 'gamma': 0.1, 'base_lrs': [0.001], 'last_epoch': 0, '_step_count': 1, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.001]}
01/04/2023 17:58:52 - INFO - __main__ -   
Epoch: 0
csd_eb2 search, INT   core: 0.000972
csd_eb3 search, INT   core: 0.000097
lsb eb2 search, INT   core: 0.003856
lsb eb3 search, INT   core: 0.003207
Layer quant EB csd_eb3
int	8-bit 	 conv1.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.001638
csd_eb3 search, INT   core: 0.000090
lsb eb2 search, INT   core: 0.005074
lsb eb3 search, INT   core: 0.001434
Layer quant EB abit-1
int	8-bit 	 conv1.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000207
csd_eb3 search, INT   core: 0.000046
lsb eb2 search, INT   core: 0.000990
lsb eb3 search, INT   core: 0.001827
Layer quant EB csd_eb3
int	8-bit 	 layer1.0.conv1.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000396
csd_eb3 search, INT   core: 0.000126
lsb eb2 search, INT   core: 0.001294
lsb eb3 search, INT   core: 0.000810
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv1.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000151
csd_eb3 search, INT   core: 0.000021
lsb eb2 search, INT   core: 0.000629
lsb eb3 search, INT   core: 0.000710
Layer quant EB csd_eb3
int	8-bit 	 layer1.0.conv2.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000090
csd_eb3 search, INT   core: 0.000065
lsb eb2 search, INT   core: 0.000294
lsb eb3 search, INT   core: 0.000559
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv2.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000189
csd_eb3 search, INT   core: 0.000032
lsb eb2 search, INT   core: 0.000819
lsb eb3 search, INT   core: 0.001094
Layer quant EB csd_eb3
int	8-bit 	 layer1.1.conv1.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000552
csd_eb3 search, INT   core: 0.000203
lsb eb2 search, INT   core: 0.001871
lsb eb3 search, INT   core: 0.001511
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv1.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000143
csd_eb3 search, INT   core: 0.000018
lsb eb2 search, INT   core: 0.000566
lsb eb3 search, INT   core: 0.000533
Layer quant EB csd_eb3
int	8-bit 	 layer1.1.conv2.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000055
csd_eb3 search, INT   core: 0.000025
lsb eb2 search, INT   core: 0.000187
lsb eb3 search, INT   core: 0.000199
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv2.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000258
csd_eb3 search, INT   core: 0.000035
lsb eb2 search, INT   core: 0.001048
lsb eb3 search, INT   core: 0.001082
Layer quant EB csd_eb3
int	8-bit 	 layer2.0.conv1.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000776
csd_eb3 search, INT   core: 0.000255
lsb eb2 search, INT   core: 0.002624
lsb eb3 search, INT   core: 0.001984
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv1.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000189
csd_eb3 search, INT   core: 0.000033
lsb eb2 search, INT   core: 0.000807
lsb eb3 search, INT   core: 0.001105
Layer quant EB csd_eb3
int	8-bit 	 layer2.0.conv2.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000070
csd_eb3 search, INT   core: 0.000031
lsb eb2 search, INT   core: 0.000236
lsb eb3 search, INT   core: 0.000284
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv2.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000390
csd_eb3 search, INT   core: 0.000054
lsb eb2 search, INT   core: 0.001741
lsb eb3 search, INT   core: 0.002206
Layer quant EB csd_eb3
int	8-bit 	 layer2.0.downsample.0.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000776
csd_eb3 search, INT   core: 0.000255
lsb eb2 search, INT   core: 0.002624
lsb eb3 search, INT   core: 0.001984
Layer quant EB abit-1
int	8-bit 	 layer2.0.downsample.0.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000193
csd_eb3 search, INT   core: 0.000035
lsb eb2 search, INT   core: 0.000843
lsb eb3 search, INT   core: 0.001209
Layer quant EB csd_eb3
int	8-bit 	 layer2.1.conv1.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000206
csd_eb3 search, INT   core: 0.000119
lsb eb2 search, INT   core: 0.000688
lsb eb3 search, INT   core: 0.001100
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv1.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000144
csd_eb3 search, INT   core: 0.000018
lsb eb2 search, INT   core: 0.000581
lsb eb3 search, INT   core: 0.000554
Layer quant EB csd_eb3
int	8-bit 	 layer2.1.conv2.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000035
csd_eb3 search, INT   core: 0.000018
lsb eb2 search, INT   core: 0.000117
lsb eb3 search, INT   core: 0.000180
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv2.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000271
csd_eb3 search, INT   core: 0.000043
lsb eb2 search, INT   core: 0.001135
lsb eb3 search, INT   core: 0.001394
Layer quant EB csd_eb3
int	8-bit 	 layer3.0.conv1.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000229
csd_eb3 search, INT   core: 0.000122
lsb eb2 search, INT   core: 0.000762
lsb eb3 search, INT   core: 0.001172
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv1.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000216
csd_eb3 search, INT   core: 0.000039
lsb eb2 search, INT   core: 0.000943
lsb eb3 search, INT   core: 0.001345
Layer quant EB csd_eb3
int	8-bit 	 layer3.0.conv2.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000054
csd_eb3 search, INT   core: 0.000021
lsb eb2 search, INT   core: 0.000178
lsb eb3 search, INT   core: 0.000184
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv2.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000242
csd_eb3 search, INT   core: 0.000027
lsb eb2 search, INT   core: 0.000974
lsb eb3 search, INT   core: 0.000831
Layer quant EB csd_eb3
int	8-bit 	 layer3.0.downsample.0.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000229
csd_eb3 search, INT   core: 0.000122
lsb eb2 search, INT   core: 0.000762
lsb eb3 search, INT   core: 0.001172
Layer quant EB abit-1
int	8-bit 	 layer3.0.downsample.0.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000170
csd_eb3 search, INT   core: 0.000027
lsb eb2 search, INT   core: 0.000715
lsb eb3 search, INT   core: 0.000862
Layer quant EB csd_eb3
int	8-bit 	 layer3.1.conv1.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000118
csd_eb3 search, INT   core: 0.000070
lsb eb2 search, INT   core: 0.000391
lsb eb3 search, INT   core: 0.000655
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv1.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000145
csd_eb3 search, INT   core: 0.000020
lsb eb2 search, INT   core: 0.000584
lsb eb3 search, INT   core: 0.000599
Layer quant EB csd_eb3
int	8-bit 	 layer3.1.conv2.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000026
csd_eb3 search, INT   core: 0.000018
lsb eb2 search, INT   core: 0.000084
lsb eb3 search, INT   core: 0.000195
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv2.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000267
csd_eb3 search, INT   core: 0.000036
lsb eb2 search, INT   core: 0.001070
lsb eb3 search, INT   core: 0.001061
Layer quant EB csd_eb3
int	8-bit 	 layer4.0.conv1.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000142
csd_eb3 search, INT   core: 0.000098
lsb eb2 search, INT   core: 0.000468
lsb eb3 search, INT   core: 0.001024
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv1.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000214
csd_eb3 search, INT   core: 0.000034
lsb eb2 search, INT   core: 0.000898
lsb eb3 search, INT   core: 0.001117
Layer quant EB csd_eb3
int	8-bit 	 layer4.0.conv2.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000017
csd_eb3 search, INT   core: 0.000008
lsb eb2 search, INT   core: 0.000055
lsb eb3 search, INT   core: 0.000077
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv2.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000569
csd_eb3 search, INT   core: 0.000066
lsb eb2 search, INT   core: 0.002252
lsb eb3 search, INT   core: 0.002050
Layer quant EB csd_eb3
int	8-bit 	 layer4.0.downsample.0.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000142
csd_eb3 search, INT   core: 0.000098
lsb eb2 search, INT   core: 0.000468
lsb eb3 search, INT   core: 0.001024
Layer quant EB abit-1
int	8-bit 	 layer4.0.downsample.0.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000223
csd_eb3 search, INT   core: 0.000027
lsb eb2 search, INT   core: 0.000873
lsb eb3 search, INT   core: 0.000784
Layer quant EB csd_eb3
int	8-bit 	 layer4.1.conv1.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000077
csd_eb3 search, INT   core: 0.000036
lsb eb2 search, INT   core: 0.000253
lsb eb3 search, INT   core: 0.000345
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv1.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000122
csd_eb3 search, INT   core: 0.000019
lsb eb2 search, INT   core: 0.000503
lsb eb3 search, INT   core: 0.000608
Layer quant EB csd_eb3
int	8-bit 	 layer4.1.conv2.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000018
csd_eb3 search, INT   core: 0.000010
lsb eb2 search, INT   core: 0.000060
lsb eb3 search, INT   core: 0.000104
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv2.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.005469
csd_eb3 search, INT   core: 0.000777
lsb eb2 search, INT   core: 0.022789
lsb eb3 search, INT   core: 0.023027
Layer quant EB csd_eb3
int	8-bit 	 fc.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.002423
csd_eb3 search, INT   core: 0.000741
lsb eb2 search, INT   core: 0.007829
lsb eb3 search, INT   core: 0.006859
Layer quant EB abit-1
int	8-bit 	 fc.quant_input,
set init to 1
01/04/2023 18:00:42 - INFO - __main__ -   test: [epoch: 0 | batch: 0/2503 ] | Loss: 1.170 | Acc: 73.047% (374/512)
01/04/2023 18:01:12 - INFO - __main__ -   test: [epoch: 0 | batch: 100/2503 ] | Loss: 1.241 | Acc: 70.343% (36376/51712)
01/04/2023 18:01:42 - INFO - __main__ -   test: [epoch: 0 | batch: 200/2503 ] | Loss: 1.253 | Acc: 70.176% (72220/102912)
01/04/2023 18:02:11 - INFO - __main__ -   test: [epoch: 0 | batch: 300/2503 ] | Loss: 1.252 | Acc: 70.167% (108135/154112)
01/04/2023 18:02:41 - INFO - __main__ -   test: [epoch: 0 | batch: 400/2503 ] | Loss: 1.249 | Acc: 70.194% (144117/205312)
01/04/2023 18:03:11 - INFO - __main__ -   test: [epoch: 0 | batch: 500/2503 ] | Loss: 1.244 | Acc: 70.317% (180372/256512)
01/04/2023 18:03:40 - INFO - __main__ -   test: [epoch: 0 | batch: 600/2503 ] | Loss: 1.244 | Acc: 70.261% (216200/307712)
01/04/2023 18:04:10 - INFO - __main__ -   test: [epoch: 0 | batch: 700/2503 ] | Loss: 1.245 | Acc: 70.211% (251995/358912)
01/04/2023 18:04:39 - INFO - __main__ -   test: [epoch: 0 | batch: 800/2503 ] | Loss: 1.245 | Acc: 70.235% (288042/410112)
01/04/2023 18:05:09 - INFO - __main__ -   test: [epoch: 0 | batch: 900/2503 ] | Loss: 1.246 | Acc: 70.221% (323936/461312)
01/04/2023 18:05:39 - INFO - __main__ -   test: [epoch: 0 | batch: 1000/2503 ] | Loss: 1.247 | Acc: 70.182% (359693/512512)
01/04/2023 18:06:08 - INFO - __main__ -   test: [epoch: 0 | batch: 1100/2503 ] | Loss: 1.247 | Acc: 70.194% (395694/563712)
01/04/2023 18:06:38 - INFO - __main__ -   test: [epoch: 0 | batch: 1200/2503 ] | Loss: 1.247 | Acc: 70.172% (431498/614912)
01/04/2023 18:07:07 - INFO - __main__ -   test: [epoch: 0 | batch: 1300/2503 ] | Loss: 1.247 | Acc: 70.187% (467522/666112)
01/04/2023 18:07:37 - INFO - __main__ -   test: [epoch: 0 | batch: 1400/2503 ] | Loss: 1.247 | Acc: 70.198% (503537/717312)
01/04/2023 18:08:07 - INFO - __main__ -   test: [epoch: 0 | batch: 1500/2503 ] | Loss: 1.247 | Acc: 70.214% (539602/768512)
01/04/2023 18:08:36 - INFO - __main__ -   test: [epoch: 0 | batch: 1600/2503 ] | Loss: 1.247 | Acc: 70.194% (575386/819712)
01/04/2023 18:09:06 - INFO - __main__ -   test: [epoch: 0 | batch: 1700/2503 ] | Loss: 1.248 | Acc: 70.199% (611372/870912)
01/04/2023 18:09:36 - INFO - __main__ -   test: [epoch: 0 | batch: 1800/2503 ] | Loss: 1.247 | Acc: 70.207% (647385/922112)
01/04/2023 18:10:05 - INFO - __main__ -   test: [epoch: 0 | batch: 1900/2503 ] | Loss: 1.247 | Acc: 70.212% (683386/973312)
01/04/2023 18:10:35 - INFO - __main__ -   test: [epoch: 0 | batch: 2000/2503 ] | Loss: 1.247 | Acc: 70.211% (719322/1024512)
01/04/2023 18:11:05 - INFO - __main__ -   test: [epoch: 0 | batch: 2100/2503 ] | Loss: 1.247 | Acc: 70.223% (755397/1075712)
01/04/2023 18:11:34 - INFO - __main__ -   test: [epoch: 0 | batch: 2200/2503 ] | Loss: 1.247 | Acc: 70.237% (791507/1126912)
01/04/2023 18:12:02 - INFO - __main__ -   test: [epoch: 0 | batch: 2300/2503 ] | Loss: 1.247 | Acc: 70.233% (827418/1178112)
01/04/2023 18:12:31 - INFO - __main__ -   test: [epoch: 0 | batch: 2400/2503 ] | Loss: 1.247 | Acc: 70.227% (863311/1229312)
01/04/2023 18:13:00 - INFO - __main__ -   test: [epoch: 0 | batch: 2500/2503 ] | Loss: 1.247 | Acc: 70.224% (899224/1280512)
01/04/2023 18:13:01 - INFO - __main__ -   Saving Checkpoint
01/04/2023 18:13:01 - INFO - __main__ -   test: [batch: 0/98 ] | Loss: 0.766 | Acc: 79.883% (409/512)/ 94.141% (482/512)
01/04/2023 18:13:01 - INFO - __main__ -   test: [batch: 1/98 ] | Loss: 0.586 | Acc: 84.863% (869/1024)/ 95.215% (975/1024)
01/04/2023 18:13:02 - INFO - __main__ -   test: [batch: 2/98 ] | Loss: 0.628 | Acc: 84.896% (1304/1536)/ 94.987% (1459/1536)
01/04/2023 18:13:02 - INFO - __main__ -   test: [batch: 3/98 ] | Loss: 0.780 | Acc: 80.273% (1644/2048)/ 93.896% (1923/2048)
01/04/2023 18:13:02 - INFO - __main__ -   test: [batch: 4/98 ] | Loss: 0.850 | Acc: 78.789% (2017/2560)/ 93.164% (2385/2560)
01/04/2023 18:13:02 - INFO - __main__ -   test: [batch: 5/98 ] | Loss: 0.943 | Acc: 76.530% (2351/3072)/ 92.415% (2839/3072)
01/04/2023 18:13:03 - INFO - __main__ -   test: [batch: 6/98 ] | Loss: 0.997 | Acc: 74.693% (2677/3584)/ 91.574% (3282/3584)
01/04/2023 18:13:03 - INFO - __main__ -   test: [batch: 7/98 ] | Loss: 1.003 | Acc: 74.658% (3058/4096)/ 91.772% (3759/4096)
01/04/2023 18:13:03 - INFO - __main__ -   test: [batch: 8/98 ] | Loss: 0.945 | Acc: 76.063% (3505/4608)/ 92.339% (4255/4608)
01/04/2023 18:13:04 - INFO - __main__ -   test: [batch: 9/98 ] | Loss: 0.914 | Acc: 76.699% (3927/5120)/ 92.539% (4738/5120)
01/04/2023 18:13:04 - INFO - __main__ -   test: [batch: 10/98 ] | Loss: 0.915 | Acc: 76.864% (4329/5632)/ 92.507% (5210/5632)
01/04/2023 18:13:04 - INFO - __main__ -   test: [batch: 11/98 ] | Loss: 0.932 | Acc: 76.579% (4705/6144)/ 92.236% (5667/6144)
01/04/2023 18:13:05 - INFO - __main__ -   test: [batch: 12/98 ] | Loss: 0.925 | Acc: 76.773% (5110/6656)/ 92.263% (6141/6656)
01/04/2023 18:13:05 - INFO - __main__ -   test: [batch: 13/98 ] | Loss: 0.894 | Acc: 77.511% (5556/7168)/ 92.550% (6634/7168)
01/04/2023 18:13:05 - INFO - __main__ -   test: [batch: 14/98 ] | Loss: 0.875 | Acc: 77.904% (5983/7680)/ 92.747% (7123/7680)
01/04/2023 18:13:05 - INFO - __main__ -   test: [batch: 15/98 ] | Loss: 0.880 | Acc: 77.808% (6374/8192)/ 92.761% (7599/8192)
01/04/2023 18:13:06 - INFO - __main__ -   test: [batch: 16/98 ] | Loss: 0.906 | Acc: 77.068% (6708/8704)/ 92.659% (8065/8704)
01/04/2023 18:13:06 - INFO - __main__ -   test: [batch: 17/98 ] | Loss: 0.905 | Acc: 76.964% (7093/9216)/ 92.828% (8555/9216)
01/04/2023 18:13:06 - INFO - __main__ -   test: [batch: 18/98 ] | Loss: 0.922 | Acc: 76.377% (7430/9728)/ 92.712% (9019/9728)
01/04/2023 18:13:07 - INFO - __main__ -   test: [batch: 19/98 ] | Loss: 0.920 | Acc: 76.211% (7804/10240)/ 92.891% (9512/10240)
01/04/2023 18:13:07 - INFO - __main__ -   test: [batch: 20/98 ] | Loss: 0.918 | Acc: 76.209% (8194/10752)/ 92.932% (9992/10752)
01/04/2023 18:13:07 - INFO - __main__ -   test: [batch: 21/98 ] | Loss: 0.917 | Acc: 76.252% (8589/11264)/ 92.907% (10465/11264)
01/04/2023 18:13:08 - INFO - __main__ -   test: [batch: 22/98 ] | Loss: 0.921 | Acc: 76.044% (8955/11776)/ 92.952% (10946/11776)
01/04/2023 18:13:08 - INFO - __main__ -   test: [batch: 23/98 ] | Loss: 0.921 | Acc: 75.863% (9322/12288)/ 93.042% (11433/12288)
01/04/2023 18:13:08 - INFO - __main__ -   test: [batch: 24/98 ] | Loss: 0.917 | Acc: 75.828% (9706/12800)/ 93.141% (11922/12800)
01/04/2023 18:13:08 - INFO - __main__ -   test: [batch: 25/98 ] | Loss: 0.910 | Acc: 75.931% (10108/13312)/ 93.239% (12412/13312)
01/04/2023 18:13:09 - INFO - __main__ -   test: [batch: 26/98 ] | Loss: 0.914 | Acc: 75.883% (10490/13824)/ 93.258% (12892/13824)
01/04/2023 18:13:09 - INFO - __main__ -   test: [batch: 27/98 ] | Loss: 0.921 | Acc: 75.579% (10835/14336)/ 93.255% (13369/14336)
01/04/2023 18:13:09 - INFO - __main__ -   test: [batch: 28/98 ] | Loss: 0.912 | Acc: 75.889% (11268/14848)/ 93.366% (13863/14848)
01/04/2023 18:13:10 - INFO - __main__ -   test: [batch: 29/98 ] | Loss: 0.917 | Acc: 75.840% (11649/15360)/ 93.366% (14341/15360)
01/04/2023 18:13:10 - INFO - __main__ -   test: [batch: 30/98 ] | Loss: 0.926 | Acc: 75.699% (12015/15872)/ 93.315% (14811/15872)
01/04/2023 18:13:10 - INFO - __main__ -   test: [batch: 31/98 ] | Loss: 0.912 | Acc: 76.068% (12463/16384)/ 93.420% (15306/16384)
01/04/2023 18:13:11 - INFO - __main__ -   test: [batch: 32/98 ] | Loss: 0.906 | Acc: 76.296% (12891/16896)/ 93.496% (15797/16896)
01/04/2023 18:13:11 - INFO - __main__ -   test: [batch: 33/98 ] | Loss: 0.902 | Acc: 76.390% (13298/17408)/ 93.509% (16278/17408)
01/04/2023 18:13:11 - INFO - __main__ -   test: [batch: 34/98 ] | Loss: 0.905 | Acc: 76.250% (13664/17920)/ 93.544% (16763/17920)
01/04/2023 18:13:11 - INFO - __main__ -   test: [batch: 35/98 ] | Loss: 0.900 | Acc: 76.378% (14078/18432)/ 93.576% (17248/18432)
01/04/2023 18:13:12 - INFO - __main__ -   test: [batch: 36/98 ] | Loss: 0.904 | Acc: 76.336% (14461/18944)/ 93.523% (17717/18944)
01/04/2023 18:13:12 - INFO - __main__ -   test: [batch: 37/98 ] | Loss: 0.907 | Acc: 76.234% (14832/19456)/ 93.544% (18200/19456)
01/04/2023 18:13:12 - INFO - __main__ -   test: [batch: 38/98 ] | Loss: 0.909 | Acc: 76.257% (15227/19968)/ 93.485% (18667/19968)
01/04/2023 18:13:13 - INFO - __main__ -   test: [batch: 39/98 ] | Loss: 0.916 | Acc: 76.133% (15592/20480)/ 93.408% (19130/20480)
01/04/2023 18:13:13 - INFO - __main__ -   test: [batch: 40/98 ] | Loss: 0.935 | Acc: 75.753% (15902/20992)/ 93.178% (19560/20992)
01/04/2023 18:13:13 - INFO - __main__ -   test: [batch: 41/98 ] | Loss: 0.941 | Acc: 75.632% (16264/21504)/ 93.104% (20021/21504)
01/04/2023 18:13:14 - INFO - __main__ -   test: [batch: 42/98 ] | Loss: 0.954 | Acc: 75.363% (16592/22016)/ 92.937% (20461/22016)
01/04/2023 18:13:14 - INFO - __main__ -   test: [batch: 43/98 ] | Loss: 0.963 | Acc: 75.160% (16932/22528)/ 92.813% (20909/22528)
01/04/2023 18:13:14 - INFO - __main__ -   test: [batch: 44/98 ] | Loss: 0.976 | Acc: 74.935% (17265/23040)/ 92.613% (21338/23040)
01/04/2023 18:13:14 - INFO - __main__ -   test: [batch: 45/98 ] | Loss: 0.996 | Acc: 74.554% (17559/23552)/ 92.366% (21754/23552)
01/04/2023 18:13:15 - INFO - __main__ -   test: [batch: 46/98 ] | Loss: 1.006 | Acc: 74.435% (17912/24064)/ 92.200% (22187/24064)
01/04/2023 18:13:15 - INFO - __main__ -   test: [batch: 47/98 ] | Loss: 1.019 | Acc: 74.084% (18207/24576)/ 92.065% (22626/24576)
01/04/2023 18:13:15 - INFO - __main__ -   test: [batch: 48/98 ] | Loss: 1.034 | Acc: 73.816% (18519/25088)/ 91.849% (23043/25088)
01/04/2023 18:13:16 - INFO - __main__ -   test: [batch: 49/98 ] | Loss: 1.046 | Acc: 73.531% (18824/25600)/ 91.719% (23480/25600)
01/04/2023 18:13:16 - INFO - __main__ -   test: [batch: 50/98 ] | Loss: 1.054 | Acc: 73.353% (19154/26112)/ 91.640% (23929/26112)
01/04/2023 18:13:16 - INFO - __main__ -   test: [batch: 51/98 ] | Loss: 1.064 | Acc: 73.118% (19467/26624)/ 91.541% (24372/26624)
01/04/2023 18:13:16 - INFO - __main__ -   test: [batch: 52/98 ] | Loss: 1.071 | Acc: 72.988% (19806/27136)/ 91.425% (24809/27136)
01/04/2023 18:13:17 - INFO - __main__ -   test: [batch: 53/98 ] | Loss: 1.075 | Acc: 72.891% (20153/27648)/ 91.377% (25264/27648)
01/04/2023 18:13:17 - INFO - __main__ -   test: [batch: 54/98 ] | Loss: 1.079 | Acc: 72.844% (20513/28160)/ 91.289% (25707/28160)
01/04/2023 18:13:17 - INFO - __main__ -   test: [batch: 55/98 ] | Loss: 1.078 | Acc: 72.925% (20909/28672)/ 91.298% (26177/28672)
01/04/2023 18:13:18 - INFO - __main__ -   test: [batch: 56/98 ] | Loss: 1.078 | Acc: 72.927% (21283/29184)/ 91.283% (26640/29184)
01/04/2023 18:13:18 - INFO - __main__ -   test: [batch: 57/98 ] | Loss: 1.090 | Acc: 72.687% (21585/29696)/ 91.100% (27053/29696)
01/04/2023 18:13:18 - INFO - __main__ -   test: [batch: 58/98 ] | Loss: 1.100 | Acc: 72.497% (21900/30208)/ 90.969% (27480/30208)
01/04/2023 18:13:19 - INFO - __main__ -   test: [batch: 59/98 ] | Loss: 1.097 | Acc: 72.627% (22311/30720)/ 90.970% (27946/30720)
01/04/2023 18:13:19 - INFO - __main__ -   test: [batch: 60/98 ] | Loss: 1.113 | Acc: 72.240% (22562/31232)/ 90.750% (28343/31232)
01/04/2023 18:13:19 - INFO - __main__ -   test: [batch: 61/98 ] | Loss: 1.122 | Acc: 72.146% (22902/31744)/ 90.593% (28758/31744)
01/04/2023 18:13:19 - INFO - __main__ -   test: [batch: 62/98 ] | Loss: 1.128 | Acc: 71.931% (23202/32256)/ 90.547% (29207/32256)
01/04/2023 18:13:20 - INFO - __main__ -   test: [batch: 63/98 ] | Loss: 1.136 | Acc: 71.841% (23541/32768)/ 90.430% (29632/32768)
01/04/2023 18:13:20 - INFO - __main__ -   test: [batch: 64/98 ] | Loss: 1.144 | Acc: 71.632% (23839/33280)/ 90.373% (30076/33280)
01/04/2023 18:13:20 - INFO - __main__ -   test: [batch: 65/98 ] | Loss: 1.145 | Acc: 71.600% (24195/33792)/ 90.365% (30536/33792)
01/04/2023 18:13:21 - INFO - __main__ -   test: [batch: 66/98 ] | Loss: 1.150 | Acc: 71.557% (24547/34304)/ 90.281% (30970/34304)
01/04/2023 18:13:21 - INFO - __main__ -   test: [batch: 67/98 ] | Loss: 1.156 | Acc: 71.436% (24871/34816)/ 90.237% (31417/34816)
01/04/2023 18:13:21 - INFO - __main__ -   test: [batch: 68/98 ] | Loss: 1.159 | Acc: 71.360% (25210/35328)/ 90.217% (31872/35328)
01/04/2023 18:13:22 - INFO - __main__ -   test: [batch: 69/98 ] | Loss: 1.162 | Acc: 71.323% (25562/35840)/ 90.184% (32322/35840)
01/04/2023 18:13:22 - INFO - __main__ -   test: [batch: 70/98 ] | Loss: 1.163 | Acc: 71.297% (25918/36352)/ 90.146% (32770/36352)
01/04/2023 18:13:22 - INFO - __main__ -   test: [batch: 71/98 ] | Loss: 1.172 | Acc: 71.137% (26224/36864)/ 89.998% (33177/36864)
01/04/2023 18:13:22 - INFO - __main__ -   test: [batch: 72/98 ] | Loss: 1.178 | Acc: 70.979% (26529/37376)/ 89.887% (33596/37376)
01/04/2023 18:13:23 - INFO - __main__ -   test: [batch: 73/98 ] | Loss: 1.182 | Acc: 70.867% (26850/37888)/ 89.833% (34036/37888)
01/04/2023 18:13:23 - INFO - __main__ -   test: [batch: 74/98 ] | Loss: 1.189 | Acc: 70.755% (27170/38400)/ 89.734% (34458/38400)
01/04/2023 18:13:23 - INFO - __main__ -   test: [batch: 75/98 ] | Loss: 1.192 | Acc: 70.737% (27525/38912)/ 89.666% (34891/38912)
01/04/2023 18:13:24 - INFO - __main__ -   test: [batch: 76/98 ] | Loss: 1.195 | Acc: 70.668% (27860/39424)/ 89.618% (35331/39424)
01/04/2023 18:13:24 - INFO - __main__ -   test: [batch: 77/98 ] | Loss: 1.201 | Acc: 70.590% (28191/39936)/ 89.553% (35764/39936)
01/04/2023 18:13:24 - INFO - __main__ -   test: [batch: 78/98 ] | Loss: 1.202 | Acc: 70.602% (28557/40448)/ 89.522% (36210/40448)
01/04/2023 18:13:25 - INFO - __main__ -   test: [batch: 79/98 ] | Loss: 1.210 | Acc: 70.439% (28852/40960)/ 89.456% (36641/40960)
01/04/2023 18:13:25 - INFO - __main__ -   test: [batch: 80/98 ] | Loss: 1.213 | Acc: 70.416% (29203/41472)/ 89.422% (37085/41472)
01/04/2023 18:13:25 - INFO - __main__ -   test: [batch: 81/98 ] | Loss: 1.222 | Acc: 70.222% (29482/41984)/ 89.286% (37486/41984)
01/04/2023 18:13:25 - INFO - __main__ -   test: [batch: 82/98 ] | Loss: 1.227 | Acc: 70.080% (29781/42496)/ 89.204% (37908/42496)
01/04/2023 18:13:26 - INFO - __main__ -   test: [batch: 83/98 ] | Loss: 1.229 | Acc: 70.047% (30126/43008)/ 89.162% (38347/43008)
01/04/2023 18:13:26 - INFO - __main__ -   test: [batch: 84/98 ] | Loss: 1.233 | Acc: 69.952% (30443/43520)/ 89.115% (38783/43520)
01/04/2023 18:13:26 - INFO - __main__ -   test: [batch: 85/98 ] | Loss: 1.233 | Acc: 69.931% (30792/44032)/ 89.110% (39237/44032)
01/04/2023 18:13:27 - INFO - __main__ -   test: [batch: 86/98 ] | Loss: 1.238 | Acc: 69.861% (31119/44544)/ 89.027% (39656/44544)
01/04/2023 18:13:27 - INFO - __main__ -   test: [batch: 87/98 ] | Loss: 1.241 | Acc: 69.793% (31446/45056)/ 88.996% (40098/45056)
01/04/2023 18:13:27 - INFO - __main__ -   test: [batch: 88/98 ] | Loss: 1.251 | Acc: 69.573% (31703/45568)/ 88.880% (40501/45568)
01/04/2023 18:13:28 - INFO - __main__ -   test: [batch: 89/98 ] | Loss: 1.249 | Acc: 69.655% (32097/46080)/ 88.911% (40970/46080)
01/04/2023 18:13:28 - INFO - __main__ -   test: [batch: 90/98 ] | Loss: 1.250 | Acc: 69.576% (32417/46592)/ 88.917% (41428/46592)
01/04/2023 18:13:28 - INFO - __main__ -   test: [batch: 91/98 ] | Loss: 1.246 | Acc: 69.659% (32812/47104)/ 88.965% (41906/47104)
01/04/2023 18:13:28 - INFO - __main__ -   test: [batch: 92/98 ] | Loss: 1.243 | Acc: 69.695% (33186/47616)/ 89.016% (42386/47616)
01/04/2023 18:13:29 - INFO - __main__ -   test: [batch: 93/98 ] | Loss: 1.242 | Acc: 69.747% (33568/48128)/ 89.013% (42840/48128)
01/04/2023 18:13:29 - INFO - __main__ -   test: [batch: 94/98 ] | Loss: 1.247 | Acc: 69.630% (33868/48640)/ 88.958% (43269/48640)
01/04/2023 18:13:29 - INFO - __main__ -   test: [batch: 95/98 ] | Loss: 1.249 | Acc: 69.574% (34197/49152)/ 88.965% (43728/49152)
01/04/2023 18:13:30 - INFO - __main__ -   test: [batch: 96/98 ] | Loss: 1.241 | Acc: 69.759% (34645/49664)/ 89.044% (44223/49664)
01/04/2023 18:13:30 - INFO - __main__ -   test: [batch: 97/98 ] | Loss: 1.239 | Acc: 69.774% (34887/50000)/ 89.056% (44528/50000)
01/04/2023 18:13:30 - INFO - __main__ -   Final accuracy: 69.774
01/04/2023 18:13:30 - INFO - __main__ -   {'milestones': Counter({2: 1, 3: 1, 4: 1}), 'gamma': 0.1, 'base_lrs': [0.001], 'last_epoch': 1, '_step_count': 2, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.001]}
01/04/2023 18:13:30 - INFO - __main__ -   
Epoch: 1
01/04/2023 18:13:31 - INFO - __main__ -   test: [epoch: 1 | batch: 0/2503 ] | Loss: 1.214 | Acc: 73.242% (375/512)
01/04/2023 18:13:59 - INFO - __main__ -   test: [epoch: 1 | batch: 100/2503 ] | Loss: 1.236 | Acc: 70.458% (36435/51712)
01/04/2023 18:14:28 - INFO - __main__ -   test: [epoch: 1 | batch: 200/2503 ] | Loss: 1.248 | Acc: 70.245% (72291/102912)
01/04/2023 18:14:57 - INFO - __main__ -   test: [epoch: 1 | batch: 300/2503 ] | Loss: 1.251 | Acc: 70.159% (108124/154112)
01/04/2023 18:15:26 - INFO - __main__ -   test: [epoch: 1 | batch: 400/2503 ] | Loss: 1.249 | Acc: 70.185% (144098/205312)
01/04/2023 18:15:55 - INFO - __main__ -   test: [epoch: 1 | batch: 500/2503 ] | Loss: 1.244 | Acc: 70.307% (180345/256512)
01/04/2023 18:16:25 - INFO - __main__ -   test: [epoch: 1 | batch: 600/2503 ] | Loss: 1.243 | Acc: 70.312% (216357/307712)
01/04/2023 18:16:55 - INFO - __main__ -   test: [epoch: 1 | batch: 700/2503 ] | Loss: 1.245 | Acc: 70.246% (252121/358912)
01/04/2023 18:17:25 - INFO - __main__ -   test: [epoch: 1 | batch: 800/2503 ] | Loss: 1.244 | Acc: 70.283% (288237/410112)
01/04/2023 18:17:54 - INFO - __main__ -   test: [epoch: 1 | batch: 900/2503 ] | Loss: 1.244 | Acc: 70.295% (324280/461312)
01/04/2023 18:18:25 - INFO - __main__ -   test: [epoch: 1 | batch: 1000/2503 ] | Loss: 1.246 | Acc: 70.262% (360099/512512)
01/04/2023 18:18:54 - INFO - __main__ -   test: [epoch: 1 | batch: 1100/2503 ] | Loss: 1.246 | Acc: 70.287% (396219/563712)
01/04/2023 18:19:24 - INFO - __main__ -   test: [epoch: 1 | batch: 1200/2503 ] | Loss: 1.246 | Acc: 70.287% (432202/614912)
01/04/2023 18:19:54 - INFO - __main__ -   test: [epoch: 1 | batch: 1300/2503 ] | Loss: 1.246 | Acc: 70.281% (468151/666112)
01/04/2023 18:20:23 - INFO - __main__ -   test: [epoch: 1 | batch: 1400/2503 ] | Loss: 1.246 | Acc: 70.278% (504109/717312)
01/04/2023 18:20:53 - INFO - __main__ -   test: [epoch: 1 | batch: 1500/2503 ] | Loss: 1.246 | Acc: 70.277% (540087/768512)
01/04/2023 18:21:23 - INFO - __main__ -   test: [epoch: 1 | batch: 1600/2503 ] | Loss: 1.247 | Acc: 70.284% (576125/819712)
01/04/2023 18:21:52 - INFO - __main__ -   test: [epoch: 1 | batch: 1700/2503 ] | Loss: 1.247 | Acc: 70.286% (612132/870912)
01/04/2023 18:22:22 - INFO - __main__ -   test: [epoch: 1 | batch: 1800/2503 ] | Loss: 1.247 | Acc: 70.276% (648027/922112)
01/04/2023 18:22:52 - INFO - __main__ -   test: [epoch: 1 | batch: 1900/2503 ] | Loss: 1.246 | Acc: 70.276% (684008/973312)
01/04/2023 18:23:22 - INFO - __main__ -   test: [epoch: 1 | batch: 2000/2503 ] | Loss: 1.247 | Acc: 70.269% (719918/1024512)
01/04/2023 18:23:51 - INFO - __main__ -   test: [epoch: 1 | batch: 2100/2503 ] | Loss: 1.247 | Acc: 70.276% (755971/1075712)
01/04/2023 18:24:21 - INFO - __main__ -   test: [epoch: 1 | batch: 2200/2503 ] | Loss: 1.246 | Acc: 70.293% (792144/1126912)
01/04/2023 18:24:50 - INFO - __main__ -   test: [epoch: 1 | batch: 2300/2503 ] | Loss: 1.246 | Acc: 70.283% (828013/1178112)
01/04/2023 18:25:20 - INFO - __main__ -   test: [epoch: 1 | batch: 2400/2503 ] | Loss: 1.246 | Acc: 70.284% (864011/1229312)
01/04/2023 18:25:50 - INFO - __main__ -   test: [epoch: 1 | batch: 2500/2503 ] | Loss: 1.246 | Acc: 70.290% (900066/1280512)
01/04/2023 18:25:50 - INFO - __main__ -   Saving Checkpoint
01/04/2023 18:25:51 - INFO - __main__ -   test: [batch: 0/98 ] | Loss: 0.767 | Acc: 79.492% (407/512)/ 94.141% (482/512)
01/04/2023 18:25:51 - INFO - __main__ -   test: [batch: 1/98 ] | Loss: 0.588 | Acc: 84.961% (870/1024)/ 95.410% (977/1024)
01/04/2023 18:25:51 - INFO - __main__ -   test: [batch: 2/98 ] | Loss: 0.631 | Acc: 84.961% (1305/1536)/ 95.117% (1461/1536)
01/04/2023 18:25:52 - INFO - __main__ -   test: [batch: 3/98 ] | Loss: 0.783 | Acc: 80.420% (1647/2048)/ 93.896% (1923/2048)
01/04/2023 18:25:52 - INFO - __main__ -   test: [batch: 4/98 ] | Loss: 0.853 | Acc: 78.867% (2019/2560)/ 93.242% (2387/2560)
01/04/2023 18:25:52 - INFO - __main__ -   test: [batch: 5/98 ] | Loss: 0.945 | Acc: 76.660% (2355/3072)/ 92.611% (2845/3072)
01/04/2023 18:25:52 - INFO - __main__ -   test: [batch: 6/98 ] | Loss: 0.999 | Acc: 74.749% (2679/3584)/ 91.769% (3289/3584)
01/04/2023 18:25:53 - INFO - __main__ -   test: [batch: 7/98 ] | Loss: 1.004 | Acc: 74.609% (3056/4096)/ 91.943% (3766/4096)
01/04/2023 18:25:53 - INFO - __main__ -   test: [batch: 8/98 ] | Loss: 0.946 | Acc: 76.042% (3504/4608)/ 92.470% (4261/4608)
01/04/2023 18:25:53 - INFO - __main__ -   test: [batch: 9/98 ] | Loss: 0.915 | Acc: 76.699% (3927/5120)/ 92.695% (4746/5120)
01/04/2023 18:25:54 - INFO - __main__ -   test: [batch: 10/98 ] | Loss: 0.916 | Acc: 76.900% (4331/5632)/ 92.614% (5216/5632)
01/04/2023 18:25:54 - INFO - __main__ -   test: [batch: 11/98 ] | Loss: 0.932 | Acc: 76.611% (4707/6144)/ 92.367% (5675/6144)
01/04/2023 18:25:54 - INFO - __main__ -   test: [batch: 12/98 ] | Loss: 0.924 | Acc: 76.818% (5113/6656)/ 92.383% (6149/6656)
01/04/2023 18:25:55 - INFO - __main__ -   test: [batch: 13/98 ] | Loss: 0.894 | Acc: 77.539% (5558/7168)/ 92.662% (6642/7168)
01/04/2023 18:25:55 - INFO - __main__ -   test: [batch: 14/98 ] | Loss: 0.876 | Acc: 77.917% (5984/7680)/ 92.826% (7129/7680)
01/04/2023 18:25:55 - INFO - __main__ -   test: [batch: 15/98 ] | Loss: 0.880 | Acc: 77.869% (6379/8192)/ 92.847% (7606/8192)
01/04/2023 18:25:56 - INFO - __main__ -   test: [batch: 16/98 ] | Loss: 0.906 | Acc: 77.022% (6704/8704)/ 92.727% (8071/8704)
01/04/2023 18:25:56 - INFO - __main__ -   test: [batch: 17/98 ] | Loss: 0.905 | Acc: 76.921% (7089/9216)/ 92.893% (8561/9216)
01/04/2023 18:25:56 - INFO - __main__ -   test: [batch: 18/98 ] | Loss: 0.923 | Acc: 76.295% (7422/9728)/ 92.794% (9027/9728)
01/04/2023 18:25:57 - INFO - __main__ -   test: [batch: 19/98 ] | Loss: 0.920 | Acc: 76.162% (7799/10240)/ 92.969% (9520/10240)
01/04/2023 18:25:57 - INFO - __main__ -   test: [batch: 20/98 ] | Loss: 0.919 | Acc: 76.181% (8191/10752)/ 92.997% (9999/10752)
01/04/2023 18:25:57 - INFO - __main__ -   test: [batch: 21/98 ] | Loss: 0.918 | Acc: 76.234% (8587/11264)/ 92.995% (10475/11264)
01/04/2023 18:25:57 - INFO - __main__ -   test: [batch: 22/98 ] | Loss: 0.922 | Acc: 76.028% (8953/11776)/ 93.037% (10956/11776)
01/04/2023 18:25:58 - INFO - __main__ -   test: [batch: 23/98 ] | Loss: 0.922 | Acc: 75.838% (9319/12288)/ 93.132% (11444/12288)
01/04/2023 18:25:58 - INFO - __main__ -   test: [batch: 24/98 ] | Loss: 0.917 | Acc: 75.805% (9703/12800)/ 93.219% (11932/12800)
01/04/2023 18:25:58 - INFO - __main__ -   test: [batch: 25/98 ] | Loss: 0.910 | Acc: 75.931% (10108/13312)/ 93.322% (12423/13312)
01/04/2023 18:25:59 - INFO - __main__ -   test: [batch: 26/98 ] | Loss: 0.915 | Acc: 75.854% (10486/13824)/ 93.323% (12901/13824)
01/04/2023 18:25:59 - INFO - __main__ -   test: [batch: 27/98 ] | Loss: 0.922 | Acc: 75.551% (10831/14336)/ 93.318% (13378/14336)
01/04/2023 18:25:59 - INFO - __main__ -   test: [batch: 28/98 ] | Loss: 0.912 | Acc: 75.869% (11265/14848)/ 93.420% (13871/14848)
01/04/2023 18:26:00 - INFO - __main__ -   test: [batch: 29/98 ] | Loss: 0.917 | Acc: 75.814% (11645/15360)/ 93.424% (14350/15360)
01/04/2023 18:26:00 - INFO - __main__ -   test: [batch: 30/98 ] | Loss: 0.926 | Acc: 75.668% (12010/15872)/ 93.378% (14821/15872)
01/04/2023 18:26:00 - INFO - __main__ -   test: [batch: 31/98 ] | Loss: 0.912 | Acc: 76.050% (12460/16384)/ 93.481% (15316/16384)
01/04/2023 18:26:01 - INFO - __main__ -   test: [batch: 32/98 ] | Loss: 0.906 | Acc: 76.267% (12886/16896)/ 93.549% (15806/16896)
01/04/2023 18:26:01 - INFO - __main__ -   test: [batch: 33/98 ] | Loss: 0.902 | Acc: 76.338% (13289/17408)/ 93.560% (16287/17408)
01/04/2023 18:26:01 - INFO - __main__ -   test: [batch: 34/98 ] | Loss: 0.905 | Acc: 76.177% (13651/17920)/ 93.583% (16770/17920)
01/04/2023 18:26:01 - INFO - __main__ -   test: [batch: 35/98 ] | Loss: 0.900 | Acc: 76.318% (14067/18432)/ 93.609% (17254/18432)
01/04/2023 18:26:02 - INFO - __main__ -   test: [batch: 36/98 ] | Loss: 0.905 | Acc: 76.267% (14448/18944)/ 93.560% (17724/18944)
01/04/2023 18:26:02 - INFO - __main__ -   test: [batch: 37/98 ] | Loss: 0.907 | Acc: 76.151% (14816/19456)/ 93.570% (18205/19456)
01/04/2023 18:26:02 - INFO - __main__ -   test: [batch: 38/98 ] | Loss: 0.909 | Acc: 76.172% (15210/19968)/ 93.515% (18673/19968)
01/04/2023 18:26:03 - INFO - __main__ -   test: [batch: 39/98 ] | Loss: 0.916 | Acc: 76.050% (15575/20480)/ 93.433% (19135/20480)
01/04/2023 18:26:03 - INFO - __main__ -   test: [batch: 40/98 ] | Loss: 0.935 | Acc: 75.681% (15887/20992)/ 93.212% (19567/20992)
01/04/2023 18:26:03 - INFO - __main__ -   test: [batch: 41/98 ] | Loss: 0.941 | Acc: 75.567% (16250/21504)/ 93.127% (20026/21504)
01/04/2023 18:26:04 - INFO - __main__ -   test: [batch: 42/98 ] | Loss: 0.954 | Acc: 75.273% (16572/22016)/ 92.978% (20470/22016)
01/04/2023 18:26:04 - INFO - __main__ -   test: [batch: 43/98 ] | Loss: 0.963 | Acc: 75.071% (16912/22528)/ 92.853% (20918/22528)
01/04/2023 18:26:04 - INFO - __main__ -   test: [batch: 44/98 ] | Loss: 0.976 | Acc: 74.852% (17246/23040)/ 92.669% (21351/23040)
01/04/2023 18:26:05 - INFO - __main__ -   test: [batch: 45/98 ] | Loss: 0.995 | Acc: 74.465% (17538/23552)/ 92.421% (21767/23552)
01/04/2023 18:26:05 - INFO - __main__ -   test: [batch: 46/98 ] | Loss: 1.005 | Acc: 74.348% (17891/24064)/ 92.271% (22204/24064)
01/04/2023 18:26:05 - INFO - __main__ -   test: [batch: 47/98 ] | Loss: 1.019 | Acc: 73.999% (18186/24576)/ 92.143% (22645/24576)
01/04/2023 18:26:05 - INFO - __main__ -   test: [batch: 48/98 ] | Loss: 1.034 | Acc: 73.728% (18497/25088)/ 91.912% (23059/25088)
01/04/2023 18:26:06 - INFO - __main__ -   test: [batch: 49/98 ] | Loss: 1.046 | Acc: 73.461% (18806/25600)/ 91.785% (23497/25600)
01/04/2023 18:26:06 - INFO - __main__ -   test: [batch: 50/98 ] | Loss: 1.054 | Acc: 73.307% (19142/26112)/ 91.701% (23945/26112)
01/04/2023 18:26:06 - INFO - __main__ -   test: [batch: 51/98 ] | Loss: 1.064 | Acc: 73.069% (19454/26624)/ 91.598% (24387/26624)
01/04/2023 18:26:07 - INFO - __main__ -   test: [batch: 52/98 ] | Loss: 1.070 | Acc: 72.933% (19791/27136)/ 91.487% (24826/27136)
01/04/2023 18:26:07 - INFO - __main__ -   test: [batch: 53/98 ] | Loss: 1.075 | Acc: 72.841% (20139/27648)/ 91.442% (25282/27648)
01/04/2023 18:26:07 - INFO - __main__ -   test: [batch: 54/98 ] | Loss: 1.079 | Acc: 72.791% (20498/28160)/ 91.349% (25724/28160)
01/04/2023 18:26:08 - INFO - __main__ -   test: [batch: 55/98 ] | Loss: 1.078 | Acc: 72.872% (20894/28672)/ 91.361% (26195/28672)
01/04/2023 18:26:08 - INFO - __main__ -   test: [batch: 56/98 ] | Loss: 1.079 | Acc: 72.869% (21266/29184)/ 91.341% (26657/29184)
01/04/2023 18:26:08 - INFO - __main__ -   test: [batch: 57/98 ] | Loss: 1.090 | Acc: 72.623% (21566/29696)/ 91.160% (27071/29696)
01/04/2023 18:26:09 - INFO - __main__ -   test: [batch: 58/98 ] | Loss: 1.100 | Acc: 72.458% (21888/30208)/ 91.039% (27501/30208)
01/04/2023 18:26:09 - INFO - __main__ -   test: [batch: 59/98 ] | Loss: 1.097 | Acc: 72.578% (22296/30720)/ 91.035% (27966/30720)
01/04/2023 18:26:09 - INFO - __main__ -   test: [batch: 60/98 ] | Loss: 1.113 | Acc: 72.198% (22549/31232)/ 90.827% (28367/31232)
01/04/2023 18:26:09 - INFO - __main__ -   test: [batch: 61/98 ] | Loss: 1.122 | Acc: 72.096% (22886/31744)/ 90.669% (28782/31744)
01/04/2023 18:26:10 - INFO - __main__ -   test: [batch: 62/98 ] | Loss: 1.128 | Acc: 71.887% (23188/32256)/ 90.619% (29230/32256)
01/04/2023 18:26:10 - INFO - __main__ -   test: [batch: 63/98 ] | Loss: 1.136 | Acc: 71.796% (23526/32768)/ 90.497% (29654/32768)
01/04/2023 18:26:10 - INFO - __main__ -   test: [batch: 64/98 ] | Loss: 1.144 | Acc: 71.584% (23823/33280)/ 90.433% (30096/33280)
01/04/2023 18:26:11 - INFO - __main__ -   test: [batch: 65/98 ] | Loss: 1.145 | Acc: 71.558% (24181/33792)/ 90.424% (30556/33792)
01/04/2023 18:26:11 - INFO - __main__ -   test: [batch: 66/98 ] | Loss: 1.150 | Acc: 71.511% (24531/34304)/ 90.328% (30986/34304)
01/04/2023 18:26:11 - INFO - __main__ -   test: [batch: 67/98 ] | Loss: 1.156 | Acc: 71.387% (24854/34816)/ 90.286% (31434/34816)
01/04/2023 18:26:12 - INFO - __main__ -   test: [batch: 68/98 ] | Loss: 1.159 | Acc: 71.309% (25192/35328)/ 90.268% (31890/35328)
01/04/2023 18:26:12 - INFO - __main__ -   test: [batch: 69/98 ] | Loss: 1.162 | Acc: 71.275% (25545/35840)/ 90.232% (32339/35840)
01/04/2023 18:26:12 - INFO - __main__ -   test: [batch: 70/98 ] | Loss: 1.163 | Acc: 71.240% (25897/36352)/ 90.190% (32786/36352)
01/04/2023 18:26:12 - INFO - __main__ -   test: [batch: 71/98 ] | Loss: 1.172 | Acc: 71.077% (26202/36864)/ 90.042% (33193/36864)
01/04/2023 18:26:13 - INFO - __main__ -   test: [batch: 72/98 ] | Loss: 1.178 | Acc: 70.912% (26504/37376)/ 89.929% (33612/37376)
01/04/2023 18:26:13 - INFO - __main__ -   test: [batch: 73/98 ] | Loss: 1.182 | Acc: 70.803% (26826/37888)/ 89.875% (34052/37888)
01/04/2023 18:26:13 - INFO - __main__ -   test: [batch: 74/98 ] | Loss: 1.188 | Acc: 70.706% (27151/38400)/ 89.779% (34475/38400)
01/04/2023 18:26:14 - INFO - __main__ -   test: [batch: 75/98 ] | Loss: 1.192 | Acc: 70.685% (27505/38912)/ 89.708% (34907/38912)
01/04/2023 18:26:14 - INFO - __main__ -   test: [batch: 76/98 ] | Loss: 1.195 | Acc: 70.622% (27842/39424)/ 89.656% (35346/39424)
01/04/2023 18:26:14 - INFO - __main__ -   test: [batch: 77/98 ] | Loss: 1.201 | Acc: 70.555% (28177/39936)/ 89.588% (35778/39936)
01/04/2023 18:26:15 - INFO - __main__ -   test: [batch: 78/98 ] | Loss: 1.202 | Acc: 70.572% (28545/40448)/ 89.562% (36226/40448)
01/04/2023 18:26:15 - INFO - __main__ -   test: [batch: 79/98 ] | Loss: 1.209 | Acc: 70.410% (28840/40960)/ 89.492% (36656/40960)
01/04/2023 18:26:15 - INFO - __main__ -   test: [batch: 80/98 ] | Loss: 1.213 | Acc: 70.385% (29190/41472)/ 89.460% (37101/41472)
01/04/2023 18:26:16 - INFO - __main__ -   test: [batch: 81/98 ] | Loss: 1.221 | Acc: 70.191% (29469/41984)/ 89.325% (37502/41984)
01/04/2023 18:26:16 - INFO - __main__ -   test: [batch: 82/98 ] | Loss: 1.227 | Acc: 70.044% (29766/42496)/ 89.239% (37923/42496)
01/04/2023 18:26:16 - INFO - __main__ -   test: [batch: 83/98 ] | Loss: 1.229 | Acc: 70.015% (30112/43008)/ 89.193% (38360/43008)
01/04/2023 18:26:16 - INFO - __main__ -   test: [batch: 84/98 ] | Loss: 1.233 | Acc: 69.920% (30429/43520)/ 89.154% (38800/43520)
01/04/2023 18:26:17 - INFO - __main__ -   test: [batch: 85/98 ] | Loss: 1.233 | Acc: 69.892% (30775/44032)/ 89.149% (39254/44032)
01/04/2023 18:26:17 - INFO - __main__ -   test: [batch: 86/98 ] | Loss: 1.237 | Acc: 69.828% (31104/44544)/ 89.069% (39675/44544)
01/04/2023 18:26:17 - INFO - __main__ -   test: [batch: 87/98 ] | Loss: 1.241 | Acc: 69.767% (31434/45056)/ 89.043% (40119/45056)
01/04/2023 18:26:18 - INFO - __main__ -   test: [batch: 88/98 ] | Loss: 1.251 | Acc: 69.553% (31694/45568)/ 88.931% (40524/45568)
01/04/2023 18:26:18 - INFO - __main__ -   test: [batch: 89/98 ] | Loss: 1.248 | Acc: 69.638% (32089/46080)/ 88.963% (40994/46080)
01/04/2023 18:26:18 - INFO - __main__ -   test: [batch: 90/98 ] | Loss: 1.250 | Acc: 69.555% (32407/46592)/ 88.962% (41449/46592)
01/04/2023 18:26:19 - INFO - __main__ -   test: [batch: 91/98 ] | Loss: 1.246 | Acc: 69.637% (32802/47104)/ 89.005% (41925/47104)
01/04/2023 18:26:19 - INFO - __main__ -   test: [batch: 92/98 ] | Loss: 1.243 | Acc: 69.676% (33177/47616)/ 89.054% (42404/47616)
01/04/2023 18:26:19 - INFO - __main__ -   test: [batch: 93/98 ] | Loss: 1.241 | Acc: 69.727% (33558/48128)/ 89.050% (42858/48128)
01/04/2023 18:26:20 - INFO - __main__ -   test: [batch: 94/98 ] | Loss: 1.246 | Acc: 69.611% (33859/48640)/ 88.988% (43284/48640)
01/04/2023 18:26:20 - INFO - __main__ -   test: [batch: 95/98 ] | Loss: 1.248 | Acc: 69.562% (34191/49152)/ 88.995% (43743/49152)
01/04/2023 18:26:20 - INFO - __main__ -   test: [batch: 96/98 ] | Loss: 1.240 | Acc: 69.753% (34642/49664)/ 89.077% (44239/49664)
01/04/2023 18:26:20 - INFO - __main__ -   test: [batch: 97/98 ] | Loss: 1.239 | Acc: 69.768% (34884/50000)/ 89.086% (44543/50000)
01/04/2023 18:26:20 - INFO - __main__ -   Final accuracy: 69.768
01/04/2023 18:26:20 - INFO - __main__ -   {'milestones': Counter({2: 1, 3: 1, 4: 1}), 'gamma': 0.1, 'base_lrs': [0.001], 'last_epoch': 2, '_step_count': 3, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.0001]}
01/04/2023 18:26:20 - INFO - __main__ -   
Epoch: 2
01/04/2023 18:26:21 - INFO - __main__ -   test: [epoch: 2 | batch: 0/2503 ] | Loss: 1.205 | Acc: 70.898% (363/512)
01/04/2023 18:26:51 - INFO - __main__ -   test: [epoch: 2 | batch: 100/2503 ] | Loss: 1.248 | Acc: 70.179% (36291/51712)
01/04/2023 18:27:20 - INFO - __main__ -   test: [epoch: 2 | batch: 200/2503 ] | Loss: 1.249 | Acc: 70.294% (72341/102912)
01/04/2023 18:27:50 - INFO - __main__ -   test: [epoch: 2 | batch: 300/2503 ] | Loss: 1.254 | Acc: 70.163% (108130/154112)
01/04/2023 18:28:20 - INFO - __main__ -   test: [epoch: 2 | batch: 400/2503 ] | Loss: 1.251 | Acc: 70.183% (144094/205312)
01/04/2023 18:28:49 - INFO - __main__ -   test: [epoch: 2 | batch: 500/2503 ] | Loss: 1.248 | Acc: 70.270% (180251/256512)
WARNING:torch.distributed.elastic.agent.server.api:Received 2 death signal, shutting down workers
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 535485 closing signal SIGINT
Traceback (most recent call last):
  File "main.py", line 300, in <module>
    train(epoch)
  File "main.py", line 204, in train
    outputs = model(inputs)
  File "/home/jjc/miniconda3/envs/ant_quant/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jjc/miniconda3/envs/ant_quant/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 965, in forward
    output = self.module(*inputs, **kwargs)
  File "/home/jjc/miniconda3/envs/ant_quant/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jjc/miniconda3/envs/ant_quant/lib/python3.8/site-packages/torchvision/models/resnet.py", line 283, in forward
    return self._forward_impl(x)
  File "/home/jjc/miniconda3/envs/ant_quant/lib/python3.8/site-packages/torchvision/models/resnet.py", line 273, in _forward_impl
    x = self.layer3(x)
  File "/home/jjc/miniconda3/envs/ant_quant/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jjc/miniconda3/envs/ant_quant/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/jjc/miniconda3/envs/ant_quant/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jjc/miniconda3/envs/ant_quant/lib/python3.8/site-packages/torchvision/models/resnet.py", line 90, in forward
    out = self.conv1(x)
  File "/home/jjc/miniconda3/envs/ant_quant/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jjc/hamha_quant/hamha_quant/ant_quantization/ImageNet/../antquant/quant_modules.py", line 1719, in forward
    layer_weight = layer_weight.to("cuda")
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/jjc/miniconda3/envs/ant_quant/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/jjc/miniconda3/envs/ant_quant/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/jjc/miniconda3/envs/ant_quant/lib/python3.8/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/home/jjc/miniconda3/envs/ant_quant/lib/python3.8/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/home/jjc/miniconda3/envs/ant_quant/lib/python3.8/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/home/jjc/miniconda3/envs/ant_quant/lib/python3.8/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/home/jjc/miniconda3/envs/ant_quant/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/jjc/miniconda3/envs/ant_quant/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 236, in launch_agent
    result = agent.run()
  File "/home/jjc/miniconda3/envs/ant_quant/lib/python3.8/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/jjc/miniconda3/envs/ant_quant/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/home/jjc/miniconda3/envs/ant_quant/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 850, in _invoke_run
    time.sleep(monitor_interval)
  File "/home/jjc/miniconda3/envs/ant_quant/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 60, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 535478 got signal: 2
