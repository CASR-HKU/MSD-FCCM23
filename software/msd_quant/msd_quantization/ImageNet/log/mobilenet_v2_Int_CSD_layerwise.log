/home/jjc/miniconda3/envs/ant_quant/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
Namespace(a_low=75, a_up=150, abit=8, batch_size=256, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='sel-2-3-4-5', epoch=5, layer_8bit_l=None, layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='mobilenet_v2', percent=100, ptq=False, resume=False, search=False, tag='', train=True, w_low=75, w_up=150, wbit=8)
01/04/2023 22:30:49 - INFO - __main__ -   output/mobilenet_v2_imagenet/int_W8A8_42442/gpu_0
01/04/2023 22:30:49 - INFO - __main__ -   Namespace(a_low=75, a_up=150, abit=8, batch_size=256, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='sel-2-3-4-5', epoch=5, layer_8bit_l=None, layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='mobilenet_v2', percent=100, ptq=False, resume=False, search=False, tag='', train=True, w_low=75, w_up=150, wbit=8)
01/04/2023 22:30:49 - INFO - __main__ -   ==> Preparing data..
01/04/2023 22:30:51 - INFO - __main__ -   ==> Setting quantizer..
Namespace(a_low=75, a_up=150, abit=8, batch_size=256, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='sel-2-3-4-5', epoch=5, layer_8bit_l=None, layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='mobilenet_v2', percent=100, ptq=False, resume=False, search=False, tag='', train=True, w_low=75, w_up=150, wbit=8)
01/04/2023 22:30:51 - INFO - __main__ -   Namespace(a_low=75, a_up=150, abit=8, batch_size=256, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='sel-2-3-4-5', epoch=5, layer_8bit_l=None, layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='mobilenet_v2', percent=100, ptq=False, resume=False, search=False, tag='', train=True, w_low=75, w_up=150, wbit=8)
01/04/2023 22:30:51 - INFO - __main__ -   ==> Building model..
MobileNetV2(
  (features): Sequential(
    (0): Sequential(
      (0): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (1): SyncBatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
    )
    (1): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (2): SyncBatchNorm(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (3): SyncBatchNorm(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (3): SyncBatchNorm(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (3): SyncBatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (3): SyncBatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (3): SyncBatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (7): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (3): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (8): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (3): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (9): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (3): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (10): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (3): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (11): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (3): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (12): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (3): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (13): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (3): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (14): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (3): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (15): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (3): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (16): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (3): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (17): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (3): SyncBatchNorm(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (18): Sequential(
      (0): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (1): SyncBatchNorm(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
    )
  )
  (classifier): Sequential(
    (0): Dropout(p=0.2, inplace=False)
    (1): LinearQuantizer(
      (quant_weight): TensorQuantizer()
      (quant_input): TensorQuantizer()
    )
  )
)
01/04/2023 22:30:51 - INFO - __main__ -   {'milestones': Counter({2: 1, 3: 1, 4: 1}), 'gamma': 0.1, 'base_lrs': [0.0005], 'last_epoch': 0, '_step_count': 1, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.0005]}
01/04/2023 22:30:51 - INFO - __main__ -   
Epoch: 0
csd_eb2 search, INT   core: 0.000880
csd_eb3 search, INT   core: 0.000056
lsb eb2 search, INT   core: 0.002759
lsb eb3 search, INT   core: 0.001927
Layer quant EB csd_eb3
int	8-bit 	 features.0.0.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.001650
csd_eb3 search, INT   core: 0.000090
lsb eb2 search, INT   core: 0.005046
lsb eb3 search, INT   core: 0.001438
Layer quant EB abit-1
int	8-bit 	 features.0.0.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.001495
csd_eb3 search, INT   core: 0.000123
lsb eb2 search, INT   core: 0.004494
lsb eb3 search, INT   core: 0.003731
Layer quant EB csd_eb3
int	8-bit 	 features.1.conv.0.0.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000230
csd_eb3 search, INT   core: 0.000057
lsb eb2 search, INT   core: 0.000773
lsb eb3 search, INT   core: 0.000443
Layer quant EB abit-1
int	8-bit 	 features.1.conv.0.0.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000421
csd_eb3 search, INT   core: 0.000037
lsb eb2 search, INT   core: 0.001426
lsb eb3 search, INT   core: 0.000987
Layer quant EB csd_eb3
int	8-bit 	 features.1.conv.1.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000832
csd_eb3 search, INT   core: 0.000123
lsb eb2 search, INT   core: 0.002583
lsb eb3 search, INT   core: 0.000875
Layer quant EB abit-1
int	8-bit 	 features.1.conv.1.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.001263
csd_eb3 search, INT   core: 0.000067
lsb eb2 search, INT   core: 0.003894
lsb eb3 search, INT   core: 0.001739
Layer quant EB csd_eb3
int	8-bit 	 features.2.conv.0.0.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000589
csd_eb3 search, INT   core: 0.000263
lsb eb2 search, INT   core: 0.004203
lsb eb3 search, INT   core: 0.012872
Layer quant EB abit-1
int	8-bit 	 features.2.conv.0.0.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000664
csd_eb3 search, INT   core: 0.000034
lsb eb2 search, INT   core: 0.002137
lsb eb3 search, INT   core: 0.000784
Layer quant EB csd_eb3
int	8-bit 	 features.2.conv.1.0.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000183
csd_eb3 search, INT   core: 0.000124
lsb eb2 search, INT   core: 0.000608
lsb eb3 search, INT   core: 0.001121
Layer quant EB abit-1
int	8-bit 	 features.2.conv.1.0.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000528
csd_eb3 search, INT   core: 0.000038
lsb eb2 search, INT   core: 0.001902
lsb eb3 search, INT   core: 0.000978
Layer quant EB csd_eb3
int	8-bit 	 features.2.conv.2.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000225
csd_eb3 search, INT   core: 0.000115
lsb eb2 search, INT   core: 0.000822
lsb eb3 search, INT   core: 0.001147
Layer quant EB abit-1
int	8-bit 	 features.2.conv.2.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.001073
csd_eb3 search, INT   core: 0.000060
lsb eb2 search, INT   core: 0.003477
lsb eb3 search, INT   core: 0.001617
Layer quant EB csd_eb3
int	8-bit 	 features.3.conv.0.0.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000414
csd_eb3 search, INT   core: 0.000144
lsb eb2 search, INT   core: 0.002325
lsb eb3 search, INT   core: 0.006189
Layer quant EB abit-1
int	8-bit 	 features.3.conv.0.0.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000874
csd_eb3 search, INT   core: 0.000084
lsb eb2 search, INT   core: 0.003483
lsb eb3 search, INT   core: 0.002865
Layer quant EB csd_eb3
int	8-bit 	 features.3.conv.1.0.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000070
csd_eb3 search, INT   core: 0.000019
lsb eb2 search, INT   core: 0.000240
lsb eb3 search, INT   core: 0.000146
Layer quant EB abit-1
int	8-bit 	 features.3.conv.1.0.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000314
csd_eb3 search, INT   core: 0.000024
lsb eb2 search, INT   core: 0.001164
lsb eb3 search, INT   core: 0.000604
Layer quant EB csd_eb3
int	8-bit 	 features.3.conv.2.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000112
csd_eb3 search, INT   core: 0.000038
lsb eb2 search, INT   core: 0.000359
lsb eb3 search, INT   core: 0.000357
Layer quant EB abit-1
int	8-bit 	 features.3.conv.2.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.001806
csd_eb3 search, INT   core: 0.000104
lsb eb2 search, INT   core: 0.005805
lsb eb3 search, INT   core: 0.002904
Layer quant EB csd_eb3
int	8-bit 	 features.4.conv.0.0.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000998
csd_eb3 search, INT   core: 0.000295
lsb eb2 search, INT   core: 0.005283
lsb eb3 search, INT   core: 0.012201
Layer quant EB abit-1
int	8-bit 	 features.4.conv.0.0.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000751
csd_eb3 search, INT   core: 0.000027
lsb eb2 search, INT   core: 0.002135
lsb eb3 search, INT   core: 0.000441
Layer quant EB csd_eb3
int	8-bit 	 features.4.conv.1.0.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000074
csd_eb3 search, INT   core: 0.000033
lsb eb2 search, INT   core: 0.000250
lsb eb3 search, INT   core: 0.000267
Layer quant EB abit-1
int	8-bit 	 features.4.conv.1.0.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000558
csd_eb3 search, INT   core: 0.000041
lsb eb2 search, INT   core: 0.001935
lsb eb3 search, INT   core: 0.001088
Layer quant EB csd_eb3
int	8-bit 	 features.4.conv.2.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000175
csd_eb3 search, INT   core: 0.000045
lsb eb2 search, INT   core: 0.000586
lsb eb3 search, INT   core: 0.000386
Layer quant EB abit-1
int	8-bit 	 features.4.conv.2.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000987
csd_eb3 search, INT   core: 0.000053
lsb eb2 search, INT   core: 0.003131
lsb eb3 search, INT   core: 0.001279
Layer quant EB csd_eb3
int	8-bit 	 features.5.conv.0.0.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000495
csd_eb3 search, INT   core: 0.000088
lsb eb2 search, INT   core: 0.002145
lsb eb3 search, INT   core: 0.002951
Layer quant EB abit-1
int	8-bit 	 features.5.conv.0.0.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000634
csd_eb3 search, INT   core: 0.000066
lsb eb2 search, INT   core: 0.002391
lsb eb3 search, INT   core: 0.002293
Layer quant EB csd_eb3
int	8-bit 	 features.5.conv.1.0.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000056
csd_eb3 search, INT   core: 0.000011
lsb eb2 search, INT   core: 0.000186
lsb eb3 search, INT   core: 0.000088
Layer quant EB abit-1
int	8-bit 	 features.5.conv.1.0.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000242
csd_eb3 search, INT   core: 0.000016
lsb eb2 search, INT   core: 0.000850
lsb eb3 search, INT   core: 0.000383
Layer quant EB csd_eb3
int	8-bit 	 features.5.conv.2.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000047
csd_eb3 search, INT   core: 0.000014
lsb eb2 search, INT   core: 0.000149
lsb eb3 search, INT   core: 0.000120
Layer quant EB abit-1
int	8-bit 	 features.5.conv.2.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000857
csd_eb3 search, INT   core: 0.000049
lsb eb2 search, INT   core: 0.002874
lsb eb3 search, INT   core: 0.001214
Layer quant EB csd_eb3
int	8-bit 	 features.6.conv.0.0.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000677
csd_eb3 search, INT   core: 0.000117
lsb eb2 search, INT   core: 0.002934
lsb eb3 search, INT   core: 0.003960
Layer quant EB abit-1
int	8-bit 	 features.6.conv.0.0.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000597
csd_eb3 search, INT   core: 0.000045
lsb eb2 search, INT   core: 0.002066
lsb eb3 search, INT   core: 0.001356
Layer quant EB csd_eb3
int	8-bit 	 features.6.conv.1.0.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000038
csd_eb3 search, INT   core: 0.000010
lsb eb2 search, INT   core: 0.000127
lsb eb3 search, INT   core: 0.000074
Layer quant EB abit-1
int	8-bit 	 features.6.conv.1.0.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000211
csd_eb3 search, INT   core: 0.000015
lsb eb2 search, INT   core: 0.000747
lsb eb3 search, INT   core: 0.000368
Layer quant EB csd_eb3
int	8-bit 	 features.6.conv.2.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000046
csd_eb3 search, INT   core: 0.000016
lsb eb2 search, INT   core: 0.000152
lsb eb3 search, INT   core: 0.000142
Layer quant EB abit-1
int	8-bit 	 features.6.conv.2.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.001961
csd_eb3 search, INT   core: 0.000118
lsb eb2 search, INT   core: 0.006667
lsb eb3 search, INT   core: 0.003012
Layer quant EB csd_eb3
int	8-bit 	 features.7.conv.0.0.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000928
csd_eb3 search, INT   core: 0.000244
lsb eb2 search, INT   core: 0.004630
lsb eb3 search, INT   core: 0.009365
Layer quant EB abit-1
int	8-bit 	 features.7.conv.0.0.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000942
csd_eb3 search, INT   core: 0.000032
lsb eb2 search, INT   core: 0.002659
lsb eb3 search, INT   core: 0.000515
Layer quant EB csd_eb3
int	8-bit 	 features.7.conv.1.0.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000053
csd_eb3 search, INT   core: 0.000019
lsb eb2 search, INT   core: 0.000180
lsb eb3 search, INT   core: 0.000149
Layer quant EB abit-1
int	8-bit 	 features.7.conv.1.0.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000696
csd_eb3 search, INT   core: 0.000047
lsb eb2 search, INT   core: 0.002386
lsb eb3 search, INT   core: 0.001173
Layer quant EB csd_eb3
int	8-bit 	 features.7.conv.2.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000189
csd_eb3 search, INT   core: 0.000046
lsb eb2 search, INT   core: 0.000612
lsb eb3 search, INT   core: 0.000390
Layer quant EB abit-1
int	8-bit 	 features.7.conv.2.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.001028
csd_eb3 search, INT   core: 0.000062
lsb eb2 search, INT   core: 0.003478
lsb eb3 search, INT   core: 0.001521
Layer quant EB csd_eb3
int	8-bit 	 features.8.conv.0.0.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000345
csd_eb3 search, INT   core: 0.000065
lsb eb2 search, INT   core: 0.001531
lsb eb3 search, INT   core: 0.002213
Layer quant EB abit-1
int	8-bit 	 features.8.conv.0.0.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000812
csd_eb3 search, INT   core: 0.000076
lsb eb2 search, INT   core: 0.002847
lsb eb3 search, INT   core: 0.002296
Layer quant EB csd_eb3
int	8-bit 	 features.8.conv.1.0.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000034
csd_eb3 search, INT   core: 0.000005
lsb eb2 search, INT   core: 0.000112
lsb eb3 search, INT   core: 0.000043
Layer quant EB abit-1
int	8-bit 	 features.8.conv.1.0.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000233
csd_eb3 search, INT   core: 0.000016
lsb eb2 search, INT   core: 0.000818
lsb eb3 search, INT   core: 0.000396
Layer quant EB csd_eb3
int	8-bit 	 features.8.conv.2.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000027
csd_eb3 search, INT   core: 0.000007
lsb eb2 search, INT   core: 0.000089
lsb eb3 search, INT   core: 0.000062
Layer quant EB abit-1
int	8-bit 	 features.8.conv.2.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.001016
csd_eb3 search, INT   core: 0.000059
lsb eb2 search, INT   core: 0.003435
lsb eb3 search, INT   core: 0.001450
Layer quant EB csd_eb3
int	8-bit 	 features.9.conv.0.0.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000486
csd_eb3 search, INT   core: 0.000076
lsb eb2 search, INT   core: 0.002033
lsb eb3 search, INT   core: 0.002458
Layer quant EB abit-1
int	8-bit 	 features.9.conv.0.0.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000650
csd_eb3 search, INT   core: 0.000051
lsb eb2 search, INT   core: 0.002194
lsb eb3 search, INT   core: 0.001644
Layer quant EB csd_eb3
int	8-bit 	 features.9.conv.1.0.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000022
csd_eb3 search, INT   core: 0.000004
lsb eb2 search, INT   core: 0.000071
lsb eb3 search, INT   core: 0.000035
Layer quant EB abit-1
int	8-bit 	 features.9.conv.1.0.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000205
csd_eb3 search, INT   core: 0.000015
lsb eb2 search, INT   core: 0.000734
lsb eb3 search, INT   core: 0.000372
Layer quant EB csd_eb3
int	8-bit 	 features.9.conv.2.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000028
csd_eb3 search, INT   core: 0.000007
lsb eb2 search, INT   core: 0.000090
lsb eb3 search, INT   core: 0.000059
Layer quant EB abit-1
int	8-bit 	 features.9.conv.2.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.001056
csd_eb3 search, INT   core: 0.000061
lsb eb2 search, INT   core: 0.003558
lsb eb3 search, INT   core: 0.001474
Layer quant EB csd_eb3
int	8-bit 	 features.10.conv.0.0.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000509
csd_eb3 search, INT   core: 0.000079
lsb eb2 search, INT   core: 0.002125
lsb eb3 search, INT   core: 0.002556
Layer quant EB abit-1
int	8-bit 	 features.10.conv.0.0.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000577
csd_eb3 search, INT   core: 0.000040
lsb eb2 search, INT   core: 0.001776
lsb eb3 search, INT   core: 0.001199
Layer quant EB csd_eb3
int	8-bit 	 features.10.conv.1.0.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000018
csd_eb3 search, INT   core: 0.000005
lsb eb2 search, INT   core: 0.000059
lsb eb3 search, INT   core: 0.000039
Layer quant EB abit-1
int	8-bit 	 features.10.conv.1.0.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000215
csd_eb3 search, INT   core: 0.000018
lsb eb2 search, INT   core: 0.000789
lsb eb3 search, INT   core: 0.000459
Layer quant EB csd_eb3
int	8-bit 	 features.10.conv.2.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000064
csd_eb3 search, INT   core: 0.000053
lsb eb2 search, INT   core: 0.000204
lsb eb3 search, INT   core: 0.000542
Layer quant EB abit-1
int	8-bit 	 features.10.conv.2.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.002089
csd_eb3 search, INT   core: 0.000126
lsb eb2 search, INT   core: 0.007118
lsb eb3 search, INT   core: 0.003214
Layer quant EB csd_eb3
int	8-bit 	 features.11.conv.0.0.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000538
csd_eb3 search, INT   core: 0.000104
lsb eb2 search, INT   core: 0.002407
lsb eb3 search, INT   core: 0.003598
Layer quant EB abit-1
int	8-bit 	 features.11.conv.0.0.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000650
csd_eb3 search, INT   core: 0.000104
lsb eb2 search, INT   core: 0.003005
lsb eb3 search, INT   core: 0.005165
Layer quant EB csd_eb3
int	8-bit 	 features.11.conv.1.0.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000036
csd_eb3 search, INT   core: 0.000011
lsb eb2 search, INT   core: 0.000121
lsb eb3 search, INT   core: 0.000085
Layer quant EB abit-1
int	8-bit 	 features.11.conv.1.0.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000547
csd_eb3 search, INT   core: 0.000041
lsb eb2 search, INT   core: 0.002005
lsb eb3 search, INT   core: 0.001000
Layer quant EB csd_eb3
int	8-bit 	 features.11.conv.2.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000096
csd_eb3 search, INT   core: 0.000035
lsb eb2 search, INT   core: 0.000320
lsb eb3 search, INT   core: 0.000283
Layer quant EB abit-1
int	8-bit 	 features.11.conv.2.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.001526
csd_eb3 search, INT   core: 0.000092
lsb eb2 search, INT   core: 0.005266
lsb eb3 search, INT   core: 0.002216
Layer quant EB csd_eb3
int	8-bit 	 features.12.conv.0.0.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000219
csd_eb3 search, INT   core: 0.000059
lsb eb2 search, INT   core: 0.001106
lsb eb3 search, INT   core: 0.002263
Layer quant EB abit-1
int	8-bit 	 features.12.conv.0.0.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000830
csd_eb3 search, INT   core: 0.000064
lsb eb2 search, INT   core: 0.002767
lsb eb3 search, INT   core: 0.001996
Layer quant EB csd_eb3
int	8-bit 	 features.12.conv.1.0.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000020
csd_eb3 search, INT   core: 0.000006
lsb eb2 search, INT   core: 0.000065
lsb eb3 search, INT   core: 0.000052
Layer quant EB abit-1
int	8-bit 	 features.12.conv.1.0.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000310
csd_eb3 search, INT   core: 0.000023
lsb eb2 search, INT   core: 0.001125
lsb eb3 search, INT   core: 0.000570
Layer quant EB csd_eb3
int	8-bit 	 features.12.conv.2.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000083
csd_eb3 search, INT   core: 0.000074
lsb eb2 search, INT   core: 0.000268
lsb eb3 search, INT   core: 0.000822
Layer quant EB abit-1
int	8-bit 	 features.12.conv.2.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.001601
csd_eb3 search, INT   core: 0.000096
lsb eb2 search, INT   core: 0.005406
lsb eb3 search, INT   core: 0.002260
Layer quant EB csd_eb3
int	8-bit 	 features.13.conv.0.0.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000333
csd_eb3 search, INT   core: 0.000094
lsb eb2 search, INT   core: 0.001724
lsb eb3 search, INT   core: 0.003706
Layer quant EB abit-1
int	8-bit 	 features.13.conv.0.0.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000726
csd_eb3 search, INT   core: 0.000054
lsb eb2 search, INT   core: 0.002464
lsb eb3 search, INT   core: 0.001575
Layer quant EB csd_eb3
int	8-bit 	 features.13.conv.1.0.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000052
csd_eb3 search, INT   core: 0.000049
lsb eb2 search, INT   core: 0.000162
lsb eb3 search, INT   core: 0.000616
Layer quant EB abit-1
int	8-bit 	 features.13.conv.1.0.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000301
csd_eb3 search, INT   core: 0.000028
lsb eb2 search, INT   core: 0.001146
lsb eb3 search, INT   core: 0.000750
Layer quant EB csd_eb3
int	8-bit 	 features.13.conv.2.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000085
csd_eb3 search, INT   core: 0.000075
lsb eb2 search, INT   core: 0.000271
lsb eb3 search, INT   core: 0.000805
Layer quant EB abit-1
int	8-bit 	 features.13.conv.2.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.002587
csd_eb3 search, INT   core: 0.000166
lsb eb2 search, INT   core: 0.008871
lsb eb3 search, INT   core: 0.004107
Layer quant EB csd_eb3
int	8-bit 	 features.14.conv.0.0.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000644
csd_eb3 search, INT   core: 0.000377
lsb eb2 search, INT   core: 0.004974
lsb eb3 search, INT   core: 0.018904
Layer quant EB abit-1
int	8-bit 	 features.14.conv.0.0.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.001429
csd_eb3 search, INT   core: 0.000044
lsb eb2 search, INT   core: 0.004060
lsb eb3 search, INT   core: 0.000705
Layer quant EB csd_eb3
int	8-bit 	 features.14.conv.1.0.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000032
csd_eb3 search, INT   core: 0.000021
lsb eb2 search, INT   core: 0.000105
lsb eb3 search, INT   core: 0.000216
Layer quant EB abit-1
int	8-bit 	 features.14.conv.1.0.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000752
csd_eb3 search, INT   core: 0.000056
lsb eb2 search, INT   core: 0.002709
lsb eb3 search, INT   core: 0.001353
Layer quant EB csd_eb3
int	8-bit 	 features.14.conv.2.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000203
csd_eb3 search, INT   core: 0.000148
lsb eb2 search, INT   core: 0.000583
lsb eb3 search, INT   core: 0.001131
Layer quant EB abit-1
int	8-bit 	 features.14.conv.2.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.001918
csd_eb3 search, INT   core: 0.000119
lsb eb2 search, INT   core: 0.006660
lsb eb3 search, INT   core: 0.002781
Layer quant EB csd_eb3
int	8-bit 	 features.15.conv.0.0.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000509
csd_eb3 search, INT   core: 0.000481
lsb eb2 search, INT   core: 0.007230
lsb eb3 search, INT   core: 0.022652
Layer quant EB abit-1
int	8-bit 	 features.15.conv.0.0.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.001773
csd_eb3 search, INT   core: 0.000092
lsb eb2 search, INT   core: 0.005241
lsb eb3 search, INT   core: 0.002236
Layer quant EB csd_eb3
int	8-bit 	 features.15.conv.1.0.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000083
csd_eb3 search, INT   core: 0.000078
lsb eb2 search, INT   core: 0.000257
lsb eb3 search, INT   core: 0.001042
Layer quant EB abit-1
int	8-bit 	 features.15.conv.1.0.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000399
csd_eb3 search, INT   core: 0.000031
lsb eb2 search, INT   core: 0.001456
lsb eb3 search, INT   core: 0.000768
Layer quant EB csd_eb3
int	8-bit 	 features.15.conv.2.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000083
csd_eb3 search, INT   core: 0.000073
lsb eb2 search, INT   core: 0.000264
lsb eb3 search, INT   core: 0.000847
Layer quant EB abit-1
int	8-bit 	 features.15.conv.2.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.002380
csd_eb3 search, INT   core: 0.000147
lsb eb2 search, INT   core: 0.008145
lsb eb3 search, INT   core: 0.003405
Layer quant EB csd_eb3
int	8-bit 	 features.16.conv.0.0.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.001288
csd_eb3 search, INT   core: 0.001242
lsb eb2 search, INT   core: 0.018745
lsb eb3 search, INT   core: 0.049033
Layer quant EB abit-1
int	8-bit 	 features.16.conv.0.0.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.001295
csd_eb3 search, INT   core: 0.000074
lsb eb2 search, INT   core: 0.003929
lsb eb3 search, INT   core: 0.001914
Layer quant EB csd_eb3
int	8-bit 	 features.16.conv.1.0.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000067
csd_eb3 search, INT   core: 0.000059
lsb eb2 search, INT   core: 0.000208
lsb eb3 search, INT   core: 0.000755
Layer quant EB abit-1
int	8-bit 	 features.16.conv.1.0.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000424
csd_eb3 search, INT   core: 0.000034
lsb eb2 search, INT   core: 0.001546
lsb eb3 search, INT   core: 0.000819
Layer quant EB csd_eb3
int	8-bit 	 features.16.conv.2.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000100
csd_eb3 search, INT   core: 0.000081
lsb eb2 search, INT   core: 0.000317
lsb eb3 search, INT   core: 0.000927
Layer quant EB abit-1
int	8-bit 	 features.16.conv.2.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.003127
csd_eb3 search, INT   core: 0.000195
lsb eb2 search, INT   core: 0.010793
lsb eb3 search, INT   core: 0.004524
Layer quant EB csd_eb3
int	8-bit 	 features.17.conv.0.0.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.005487
csd_eb3 search, INT   core: 0.005227
lsb eb2 search, INT   core: 0.069815
lsb eb3 search, INT   core: 0.118475
Layer quant EB abit-1
int	8-bit 	 features.17.conv.0.0.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000275
csd_eb3 search, INT   core: 0.000007
lsb eb2 search, INT   core: 0.000475
lsb eb3 search, INT   core: 0.000091
Layer quant EB csd_eb3
int	8-bit 	 features.17.conv.1.0.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000020
csd_eb3 search, INT   core: 0.000018
lsb eb2 search, INT   core: 0.000064
lsb eb3 search, INT   core: 0.000233
Layer quant EB abit-1
int	8-bit 	 features.17.conv.1.0.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.000684
csd_eb3 search, INT   core: 0.000049
lsb eb2 search, INT   core: 0.002462
lsb eb3 search, INT   core: 0.001151
Layer quant EB csd_eb3
int	8-bit 	 features.17.conv.2.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000064
csd_eb3 search, INT   core: 0.000050
lsb eb2 search, INT   core: 0.000414
lsb eb3 search, INT   core: 0.000724
Layer quant EB abit-1
int	8-bit 	 features.17.conv.2.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.001328
csd_eb3 search, INT   core: 0.000088
lsb eb2 search, INT   core: 0.004686
lsb eb3 search, INT   core: 0.002067
Layer quant EB csd_eb3
int	8-bit 	 features.18.0.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000083
csd_eb3 search, INT   core: 0.000020
lsb eb2 search, INT   core: 0.000404
lsb eb3 search, INT   core: 0.000773
Layer quant EB abit-1
int	8-bit 	 features.18.0.quant_input,
set init to 1
csd_eb2 search, INT   core: 0.003764
csd_eb3 search, INT   core: 0.000249
lsb eb2 search, INT   core: 0.013392
lsb eb3 search, INT   core: 0.005589
Layer quant EB csd_eb3
int	8-bit 	 classifier.1.quant_weight,
set init to 1
csd_eb2 search, INT   core: 0.000947
csd_eb3 search, INT   core: 0.000148
lsb eb2 search, INT   core: 0.003004
lsb eb3 search, INT   core: 0.001392
Layer quant EB abit-1
int	8-bit 	 classifier.1.quant_input,
set init to 1
01/04/2023 22:33:45 - INFO - __main__ -   test: [epoch: 0 | batch: 0/5005 ] | Loss: 1.123 | Acc: 74.609% (191/256)
01/04/2023 22:34:22 - INFO - __main__ -   test: [epoch: 0 | batch: 100/5005 ] | Loss: 1.168 | Acc: 71.693% (18537/25856)
01/04/2023 22:34:59 - INFO - __main__ -   test: [epoch: 0 | batch: 200/5005 ] | Loss: 1.181 | Acc: 71.331% (36704/51456)
01/04/2023 22:35:37 - INFO - __main__ -   test: [epoch: 0 | batch: 300/5005 ] | Loss: 1.181 | Acc: 71.349% (54979/77056)
01/04/2023 22:36:14 - INFO - __main__ -   test: [epoch: 0 | batch: 400/5005 ] | Loss: 1.186 | Acc: 71.288% (73181/102656)
01/04/2023 22:36:51 - INFO - __main__ -   test: [epoch: 0 | batch: 500/5005 ] | Loss: 1.187 | Acc: 71.265% (91402/128256)
01/04/2023 22:37:28 - INFO - __main__ -   test: [epoch: 0 | batch: 600/5005 ] | Loss: 1.190 | Acc: 71.209% (109559/153856)
01/04/2023 22:38:05 - INFO - __main__ -   test: [epoch: 0 | batch: 700/5005 ] | Loss: 1.189 | Acc: 71.289% (127932/179456)
01/04/2023 22:38:42 - INFO - __main__ -   test: [epoch: 0 | batch: 800/5005 ] | Loss: 1.190 | Acc: 71.282% (146169/205056)
01/04/2023 22:39:19 - INFO - __main__ -   test: [epoch: 0 | batch: 900/5005 ] | Loss: 1.189 | Acc: 71.343% (164557/230656)
01/04/2023 22:39:56 - INFO - __main__ -   test: [epoch: 0 | batch: 1000/5005 ] | Loss: 1.188 | Acc: 71.328% (182783/256256)
01/04/2023 22:40:34 - INFO - __main__ -   test: [epoch: 0 | batch: 1100/5005 ] | Loss: 1.188 | Acc: 71.315% (201006/281856)
01/04/2023 22:41:11 - INFO - __main__ -   test: [epoch: 0 | batch: 1200/5005 ] | Loss: 1.187 | Acc: 71.366% (219419/307456)
01/04/2023 22:41:47 - INFO - __main__ -   test: [epoch: 0 | batch: 1300/5005 ] | Loss: 1.187 | Acc: 71.362% (237675/333056)
01/04/2023 22:42:24 - INFO - __main__ -   test: [epoch: 0 | batch: 1400/5005 ] | Loss: 1.187 | Acc: 71.323% (255803/358656)
01/04/2023 22:43:01 - INFO - __main__ -   test: [epoch: 0 | batch: 1500/5005 ] | Loss: 1.187 | Acc: 71.306% (273996/384256)
01/04/2023 22:43:37 - INFO - __main__ -   test: [epoch: 0 | batch: 1600/5005 ] | Loss: 1.189 | Acc: 71.270% (292105/409856)
01/04/2023 22:44:14 - INFO - __main__ -   test: [epoch: 0 | batch: 1700/5005 ] | Loss: 1.188 | Acc: 71.291% (310440/435456)
01/04/2023 22:44:51 - INFO - __main__ -   test: [epoch: 0 | batch: 1800/5005 ] | Loss: 1.188 | Acc: 71.309% (328773/461056)
01/04/2023 22:45:27 - INFO - __main__ -   test: [epoch: 0 | batch: 1900/5005 ] | Loss: 1.187 | Acc: 71.300% (346986/486656)
01/04/2023 22:46:04 - INFO - __main__ -   test: [epoch: 0 | batch: 2000/5005 ] | Loss: 1.187 | Acc: 71.307% (365273/512256)
01/04/2023 22:46:40 - INFO - __main__ -   test: [epoch: 0 | batch: 2100/5005 ] | Loss: 1.187 | Acc: 71.300% (383489/537856)
01/04/2023 22:47:17 - INFO - __main__ -   test: [epoch: 0 | batch: 2200/5005 ] | Loss: 1.187 | Acc: 71.310% (401802/563456)
01/04/2023 22:47:53 - INFO - __main__ -   test: [epoch: 0 | batch: 2300/5005 ] | Loss: 1.187 | Acc: 71.297% (419980/589056)
01/04/2023 22:48:30 - INFO - __main__ -   test: [epoch: 0 | batch: 2400/5005 ] | Loss: 1.187 | Acc: 71.287% (438171/614656)
01/04/2023 22:49:06 - INFO - __main__ -   test: [epoch: 0 | batch: 2500/5005 ] | Loss: 1.187 | Acc: 71.312% (456577/640256)
01/04/2023 22:49:43 - INFO - __main__ -   test: [epoch: 0 | batch: 2600/5005 ] | Loss: 1.187 | Acc: 71.306% (474795/665856)
01/04/2023 22:50:20 - INFO - __main__ -   test: [epoch: 0 | batch: 2700/5005 ] | Loss: 1.187 | Acc: 71.291% (492945/691456)
01/04/2023 22:50:56 - INFO - __main__ -   test: [epoch: 0 | batch: 2800/5005 ] | Loss: 1.188 | Acc: 71.274% (511074/717056)
01/04/2023 22:51:33 - INFO - __main__ -   test: [epoch: 0 | batch: 2900/5005 ] | Loss: 1.187 | Acc: 71.282% (529378/742656)
01/04/2023 22:52:09 - INFO - __main__ -   test: [epoch: 0 | batch: 3000/5005 ] | Loss: 1.187 | Acc: 71.291% (547701/768256)
01/04/2023 22:52:46 - INFO - __main__ -   test: [epoch: 0 | batch: 3100/5005 ] | Loss: 1.188 | Acc: 71.276% (565831/793856)
01/04/2023 22:53:22 - INFO - __main__ -   test: [epoch: 0 | batch: 3200/5005 ] | Loss: 1.188 | Acc: 71.269% (584016/819456)
01/04/2023 22:53:59 - INFO - __main__ -   test: [epoch: 0 | batch: 3300/5005 ] | Loss: 1.188 | Acc: 71.265% (602229/845056)
01/04/2023 22:54:36 - INFO - __main__ -   test: [epoch: 0 | batch: 3400/5005 ] | Loss: 1.188 | Acc: 71.272% (620535/870656)
01/04/2023 22:55:12 - INFO - __main__ -   test: [epoch: 0 | batch: 3500/5005 ] | Loss: 1.188 | Acc: 71.264% (638708/896256)
01/04/2023 22:55:49 - INFO - __main__ -   test: [epoch: 0 | batch: 3600/5005 ] | Loss: 1.188 | Acc: 71.269% (656995/921856)
01/04/2023 22:56:25 - INFO - __main__ -   test: [epoch: 0 | batch: 3700/5005 ] | Loss: 1.188 | Acc: 71.274% (675294/947456)
01/04/2023 22:57:02 - INFO - __main__ -   test: [epoch: 0 | batch: 3800/5005 ] | Loss: 1.188 | Acc: 71.274% (693532/973056)
01/04/2023 22:57:38 - INFO - __main__ -   test: [epoch: 0 | batch: 3900/5005 ] | Loss: 1.188 | Acc: 71.269% (711735/998656)
01/04/2023 22:58:15 - INFO - __main__ -   test: [epoch: 0 | batch: 4000/5005 ] | Loss: 1.188 | Acc: 71.265% (729940/1024256)
01/04/2023 22:58:51 - INFO - __main__ -   test: [epoch: 0 | batch: 4100/5005 ] | Loss: 1.187 | Acc: 71.275% (748288/1049856)
01/04/2023 22:59:28 - INFO - __main__ -   test: [epoch: 0 | batch: 4200/5005 ] | Loss: 1.188 | Acc: 71.269% (766462/1075456)
01/04/2023 23:00:05 - INFO - __main__ -   test: [epoch: 0 | batch: 4300/5005 ] | Loss: 1.187 | Acc: 71.277% (784798/1101056)
01/04/2023 23:00:41 - INFO - __main__ -   test: [epoch: 0 | batch: 4400/5005 ] | Loss: 1.187 | Acc: 71.285% (803138/1126656)
01/04/2023 23:01:18 - INFO - __main__ -   test: [epoch: 0 | batch: 4500/5005 ] | Loss: 1.187 | Acc: 71.284% (821372/1152256)
01/04/2023 23:01:54 - INFO - __main__ -   test: [epoch: 0 | batch: 4600/5005 ] | Loss: 1.187 | Acc: 71.285% (839632/1177856)
01/04/2023 23:02:31 - INFO - __main__ -   test: [epoch: 0 | batch: 4700/5005 ] | Loss: 1.187 | Acc: 71.286% (857892/1203456)
01/04/2023 23:03:07 - INFO - __main__ -   test: [epoch: 0 | batch: 4800/5005 ] | Loss: 1.187 | Acc: 71.285% (876128/1229056)
01/04/2023 23:03:44 - INFO - __main__ -   test: [epoch: 0 | batch: 4900/5005 ] | Loss: 1.188 | Acc: 71.283% (894354/1254656)
01/04/2023 23:04:21 - INFO - __main__ -   test: [epoch: 0 | batch: 5000/5005 ] | Loss: 1.188 | Acc: 71.279% (912559/1280256)
01/04/2023 23:04:22 - INFO - __main__ -   Saving Checkpoint
01/04/2023 23:04:23 - INFO - __main__ -   test: [batch: 0/196 ] | Loss: 0.663 | Acc: 81.641% (209/256)/ 94.922% (243/256)
01/04/2023 23:04:23 - INFO - __main__ -   test: [batch: 1/196 ] | Loss: 0.702 | Acc: 80.859% (414/512)/ 94.141% (482/512)
01/04/2023 23:04:23 - INFO - __main__ -   test: [batch: 2/196 ] | Loss: 0.571 | Acc: 84.635% (650/768)/ 95.312% (732/768)
01/04/2023 23:04:24 - INFO - __main__ -   test: [batch: 3/196 ] | Loss: 0.561 | Acc: 85.449% (875/1024)/ 95.410% (977/1024)
01/04/2023 23:04:24 - INFO - __main__ -   test: [batch: 4/196 ] | Loss: 0.539 | Acc: 86.094% (1102/1280)/ 95.703% (1225/1280)
01/04/2023 23:04:24 - INFO - __main__ -   test: [batch: 5/196 ] | Loss: 0.592 | Acc: 85.352% (1311/1536)/ 95.312% (1464/1536)
01/04/2023 23:04:25 - INFO - __main__ -   test: [batch: 6/196 ] | Loss: 0.701 | Acc: 82.366% (1476/1792)/ 94.475% (1693/1792)
01/04/2023 23:04:25 - INFO - __main__ -   test: [batch: 7/196 ] | Loss: 0.733 | Acc: 80.664% (1652/2048)/ 94.531% (1936/2048)
01/04/2023 23:04:25 - INFO - __main__ -   test: [batch: 8/196 ] | Loss: 0.771 | Acc: 80.122% (1846/2304)/ 93.793% (2161/2304)
01/04/2023 23:04:26 - INFO - __main__ -   test: [batch: 9/196 ] | Loss: 0.803 | Acc: 79.219% (2028/2560)/ 93.594% (2396/2560)
01/04/2023 23:04:26 - INFO - __main__ -   test: [batch: 10/196 ] | Loss: 0.834 | Acc: 78.374% (2207/2816)/ 93.395% (2630/2816)
01/04/2023 23:04:26 - INFO - __main__ -   test: [batch: 11/196 ] | Loss: 0.878 | Acc: 77.409% (2378/3072)/ 93.164% (2862/3072)
01/04/2023 23:04:27 - INFO - __main__ -   test: [batch: 12/196 ] | Loss: 0.917 | Acc: 76.442% (2544/3328)/ 92.698% (3085/3328)
01/04/2023 23:04:27 - INFO - __main__ -   test: [batch: 13/196 ] | Loss: 0.927 | Acc: 75.949% (2722/3584)/ 92.690% (3322/3584)
01/04/2023 23:04:27 - INFO - __main__ -   test: [batch: 14/196 ] | Loss: 0.926 | Acc: 75.885% (2914/3840)/ 92.865% (3566/3840)
01/04/2023 23:04:28 - INFO - __main__ -   test: [batch: 15/196 ] | Loss: 0.935 | Acc: 76.025% (3114/4096)/ 92.749% (3799/4096)
01/04/2023 23:04:28 - INFO - __main__ -   test: [batch: 16/196 ] | Loss: 0.913 | Acc: 76.654% (3336/4352)/ 92.900% (4043/4352)
01/04/2023 23:04:28 - INFO - __main__ -   test: [batch: 17/196 ] | Loss: 0.884 | Acc: 77.517% (3572/4608)/ 93.099% (4290/4608)
01/04/2023 23:04:29 - INFO - __main__ -   test: [batch: 18/196 ] | Loss: 0.858 | Acc: 78.207% (3804/4864)/ 93.298% (4538/4864)
01/04/2023 23:04:29 - INFO - __main__ -   test: [batch: 19/196 ] | Loss: 0.847 | Acc: 78.418% (4015/5120)/ 93.379% (4781/5120)
01/04/2023 23:04:29 - INFO - __main__ -   test: [batch: 20/196 ] | Loss: 0.847 | Acc: 78.702% (4231/5376)/ 93.248% (5013/5376)
01/04/2023 23:04:30 - INFO - __main__ -   test: [batch: 21/196 ] | Loss: 0.845 | Acc: 78.764% (4436/5632)/ 93.324% (5256/5632)
01/04/2023 23:04:30 - INFO - __main__ -   test: [batch: 22/196 ] | Loss: 0.847 | Acc: 78.804% (4640/5888)/ 93.308% (5494/5888)
01/04/2023 23:04:30 - INFO - __main__ -   test: [batch: 23/196 ] | Loss: 0.849 | Acc: 78.743% (4838/6144)/ 93.359% (5736/6144)
01/04/2023 23:04:31 - INFO - __main__ -   test: [batch: 24/196 ] | Loss: 0.865 | Acc: 78.469% (5022/6400)/ 93.234% (5967/6400)
01/04/2023 23:04:31 - INFO - __main__ -   test: [batch: 25/196 ] | Loss: 0.848 | Acc: 78.936% (5254/6656)/ 93.329% (6212/6656)
01/04/2023 23:04:31 - INFO - __main__ -   test: [batch: 26/196 ] | Loss: 0.834 | Acc: 79.210% (5475/6912)/ 93.446% (6459/6912)
01/04/2023 23:04:32 - INFO - __main__ -   test: [batch: 27/196 ] | Loss: 0.819 | Acc: 79.506% (5699/7168)/ 93.499% (6702/7168)
01/04/2023 23:04:32 - INFO - __main__ -   test: [batch: 28/196 ] | Loss: 0.803 | Acc: 79.890% (5931/7424)/ 93.615% (6950/7424)
01/04/2023 23:04:32 - INFO - __main__ -   test: [batch: 29/196 ] | Loss: 0.804 | Acc: 79.740% (6124/7680)/ 93.620% (7190/7680)
01/04/2023 23:04:33 - INFO - __main__ -   test: [batch: 30/196 ] | Loss: 0.805 | Acc: 79.763% (6330/7936)/ 93.637% (7431/7936)
01/04/2023 23:04:33 - INFO - __main__ -   test: [batch: 31/196 ] | Loss: 0.814 | Acc: 79.480% (6511/8192)/ 93.604% (7668/8192)
01/04/2023 23:04:33 - INFO - __main__ -   test: [batch: 32/196 ] | Loss: 0.834 | Acc: 78.859% (6662/8448)/ 93.478% (7897/8448)
01/04/2023 23:04:34 - INFO - __main__ -   test: [batch: 33/196 ] | Loss: 0.841 | Acc: 78.676% (6848/8704)/ 93.451% (8134/8704)
01/04/2023 23:04:34 - INFO - __main__ -   test: [batch: 34/196 ] | Loss: 0.839 | Acc: 78.694% (7051/8960)/ 93.493% (8377/8960)
01/04/2023 23:04:34 - INFO - __main__ -   test: [batch: 35/196 ] | Loss: 0.842 | Acc: 78.559% (7240/9216)/ 93.533% (8620/9216)
01/04/2023 23:04:35 - INFO - __main__ -   test: [batch: 36/196 ] | Loss: 0.852 | Acc: 78.178% (7405/9472)/ 93.539% (8860/9472)
01/04/2023 23:04:35 - INFO - __main__ -   test: [batch: 37/196 ] | Loss: 0.859 | Acc: 77.991% (7587/9728)/ 93.400% (9086/9728)
01/04/2023 23:04:35 - INFO - __main__ -   test: [batch: 38/196 ] | Loss: 0.858 | Acc: 77.955% (7783/9984)/ 93.490% (9334/9984)
01/04/2023 23:04:36 - INFO - __main__ -   test: [batch: 39/196 ] | Loss: 0.859 | Acc: 77.871% (7974/10240)/ 93.584% (9583/10240)
01/04/2023 23:04:36 - INFO - __main__ -   test: [batch: 40/196 ] | Loss: 0.858 | Acc: 77.877% (8174/10496)/ 93.617% (9826/10496)
01/04/2023 23:04:36 - INFO - __main__ -   test: [batch: 41/196 ] | Loss: 0.858 | Acc: 77.874% (8373/10752)/ 93.592% (10063/10752)
01/04/2023 23:04:37 - INFO - __main__ -   test: [batch: 42/196 ] | Loss: 0.851 | Acc: 78.052% (8592/11008)/ 93.668% (10311/11008)
01/04/2023 23:04:37 - INFO - __main__ -   test: [batch: 43/196 ] | Loss: 0.856 | Acc: 77.947% (8780/11264)/ 93.599% (10543/11264)
01/04/2023 23:04:37 - INFO - __main__ -   test: [batch: 44/196 ] | Loss: 0.861 | Acc: 77.891% (8973/11520)/ 93.576% (10780/11520)
01/04/2023 23:04:38 - INFO - __main__ -   test: [batch: 45/196 ] | Loss: 0.863 | Acc: 77.743% (9155/11776)/ 93.597% (11022/11776)
01/04/2023 23:04:38 - INFO - __main__ -   test: [batch: 46/196 ] | Loss: 0.868 | Acc: 77.535% (9329/12032)/ 93.592% (11261/12032)
01/04/2023 23:04:38 - INFO - __main__ -   test: [batch: 47/196 ] | Loss: 0.866 | Acc: 77.490% (9522/12288)/ 93.628% (11505/12288)
01/04/2023 23:04:39 - INFO - __main__ -   test: [batch: 48/196 ] | Loss: 0.870 | Acc: 77.264% (9692/12544)/ 93.670% (11750/12544)
01/04/2023 23:04:39 - INFO - __main__ -   test: [batch: 49/196 ] | Loss: 0.861 | Acc: 77.500% (9920/12800)/ 93.742% (11999/12800)
01/04/2023 23:04:39 - INFO - __main__ -   test: [batch: 50/196 ] | Loss: 0.856 | Acc: 77.642% (10137/13056)/ 93.804% (12247/13056)
01/04/2023 23:04:40 - INFO - __main__ -   test: [batch: 51/196 ] | Loss: 0.854 | Acc: 77.637% (10335/13312)/ 93.848% (12493/13312)
01/04/2023 23:04:40 - INFO - __main__ -   test: [batch: 52/196 ] | Loss: 0.854 | Acc: 77.616% (10531/13568)/ 93.861% (12735/13568)
01/04/2023 23:04:40 - INFO - __main__ -   test: [batch: 53/196 ] | Loss: 0.857 | Acc: 77.626% (10731/13824)/ 93.830% (12971/13824)
01/04/2023 23:04:41 - INFO - __main__ -   test: [batch: 54/196 ] | Loss: 0.859 | Acc: 77.543% (10918/14080)/ 93.849% (13214/14080)
01/04/2023 23:04:41 - INFO - __main__ -   test: [batch: 55/196 ] | Loss: 0.865 | Acc: 77.302% (11082/14336)/ 93.813% (13449/14336)
01/04/2023 23:04:41 - INFO - __main__ -   test: [batch: 56/196 ] | Loss: 0.864 | Acc: 77.337% (11285/14592)/ 93.839% (13693/14592)
01/04/2023 23:04:42 - INFO - __main__ -   test: [batch: 57/196 ] | Loss: 0.856 | Acc: 77.566% (11517/14848)/ 93.918% (13945/14848)
01/04/2023 23:04:42 - INFO - __main__ -   test: [batch: 58/196 ] | Loss: 0.856 | Acc: 77.602% (11721/15104)/ 93.902% (14183/15104)
01/04/2023 23:04:42 - INFO - __main__ -   test: [batch: 59/196 ] | Loss: 0.858 | Acc: 77.539% (11910/15360)/ 93.952% (14431/15360)
01/04/2023 23:04:43 - INFO - __main__ -   test: [batch: 60/196 ] | Loss: 0.862 | Acc: 77.414% (12089/15616)/ 93.942% (14670/15616)
01/04/2023 23:04:43 - INFO - __main__ -   test: [batch: 61/196 ] | Loss: 0.867 | Acc: 77.287% (12267/15872)/ 93.889% (14902/15872)
01/04/2023 23:04:43 - INFO - __main__ -   test: [batch: 62/196 ] | Loss: 0.862 | Acc: 77.412% (12485/16128)/ 93.930% (15149/16128)
01/04/2023 23:04:44 - INFO - __main__ -   test: [batch: 63/196 ] | Loss: 0.855 | Acc: 77.594% (12713/16384)/ 93.970% (15396/16384)
01/04/2023 23:04:44 - INFO - __main__ -   test: [batch: 64/196 ] | Loss: 0.855 | Acc: 77.614% (12915/16640)/ 93.984% (15639/16640)
01/04/2023 23:04:44 - INFO - __main__ -   test: [batch: 65/196 ] | Loss: 0.851 | Acc: 77.723% (13132/16896)/ 94.028% (15887/16896)
01/04/2023 23:04:45 - INFO - __main__ -   test: [batch: 66/196 ] | Loss: 0.850 | Acc: 77.740% (13334/17152)/ 94.036% (16129/17152)
01/04/2023 23:04:45 - INFO - __main__ -   test: [batch: 67/196 ] | Loss: 0.848 | Acc: 77.740% (13533/17408)/ 94.072% (16376/17408)
01/04/2023 23:04:45 - INFO - __main__ -   test: [batch: 68/196 ] | Loss: 0.845 | Acc: 77.751% (13734/17664)/ 94.118% (16625/17664)
01/04/2023 23:04:46 - INFO - __main__ -   test: [batch: 69/196 ] | Loss: 0.849 | Acc: 77.640% (13913/17920)/ 94.113% (16865/17920)
01/04/2023 23:04:46 - INFO - __main__ -   test: [batch: 70/196 ] | Loss: 0.848 | Acc: 77.652% (14114/18176)/ 94.135% (17110/18176)
01/04/2023 23:04:46 - INFO - __main__ -   test: [batch: 71/196 ] | Loss: 0.845 | Acc: 77.745% (14330/18432)/ 94.146% (17353/18432)
01/04/2023 23:04:47 - INFO - __main__ -   test: [batch: 72/196 ] | Loss: 0.851 | Acc: 77.724% (14525/18688)/ 94.087% (17583/18688)
01/04/2023 23:04:47 - INFO - __main__ -   test: [batch: 73/196 ] | Loss: 0.852 | Acc: 77.734% (14726/18944)/ 94.046% (17816/18944)
01/04/2023 23:04:48 - INFO - __main__ -   test: [batch: 74/196 ] | Loss: 0.857 | Acc: 77.568% (14893/19200)/ 94.005% (18049/19200)
01/04/2023 23:04:48 - INFO - __main__ -   test: [batch: 75/196 ] | Loss: 0.857 | Acc: 77.565% (15091/19456)/ 94.007% (18290/19456)
01/04/2023 23:04:48 - INFO - __main__ -   test: [batch: 76/196 ] | Loss: 0.856 | Acc: 77.587% (15294/19712)/ 94.029% (18535/19712)
01/04/2023 23:04:49 - INFO - __main__ -   test: [batch: 77/196 ] | Loss: 0.858 | Acc: 77.604% (15496/19968)/ 93.980% (18766/19968)
01/04/2023 23:04:49 - INFO - __main__ -   test: [batch: 78/196 ] | Loss: 0.863 | Acc: 77.462% (15666/20224)/ 93.933% (18997/20224)
01/04/2023 23:04:49 - INFO - __main__ -   test: [batch: 79/196 ] | Loss: 0.866 | Acc: 77.417% (15855/20480)/ 93.926% (19236/20480)
01/04/2023 23:04:50 - INFO - __main__ -   test: [batch: 80/196 ] | Loss: 0.875 | Acc: 77.194% (16007/20736)/ 93.827% (19456/20736)
01/04/2023 23:04:50 - INFO - __main__ -   test: [batch: 81/196 ] | Loss: 0.884 | Acc: 77.015% (16167/20992)/ 93.726% (19675/20992)
01/04/2023 23:04:50 - INFO - __main__ -   test: [batch: 82/196 ] | Loss: 0.888 | Acc: 76.911% (16342/21248)/ 93.694% (19908/21248)
01/04/2023 23:04:51 - INFO - __main__ -   test: [batch: 83/196 ] | Loss: 0.890 | Acc: 76.888% (16534/21504)/ 93.676% (20144/21504)
01/04/2023 23:04:51 - INFO - __main__ -   test: [batch: 84/196 ] | Loss: 0.894 | Acc: 76.806% (16713/21760)/ 93.612% (20370/21760)
01/04/2023 23:04:51 - INFO - __main__ -   test: [batch: 85/196 ] | Loss: 0.903 | Acc: 76.603% (16865/22016)/ 93.514% (20588/22016)
01/04/2023 23:04:52 - INFO - __main__ -   test: [batch: 86/196 ] | Loss: 0.907 | Acc: 76.500% (17038/22272)/ 93.467% (20817/22272)
01/04/2023 23:04:52 - INFO - __main__ -   test: [batch: 87/196 ] | Loss: 0.911 | Acc: 76.407% (17213/22528)/ 93.408% (21043/22528)
01/04/2023 23:04:52 - INFO - __main__ -   test: [batch: 88/196 ] | Loss: 0.917 | Acc: 76.282% (17380/22784)/ 93.337% (21266/22784)
01/04/2023 23:04:53 - INFO - __main__ -   test: [batch: 89/196 ] | Loss: 0.923 | Acc: 76.168% (17549/23040)/ 93.247% (21484/23040)
01/04/2023 23:04:53 - INFO - __main__ -   test: [batch: 90/196 ] | Loss: 0.937 | Acc: 75.919% (17686/23296)/ 93.106% (21690/23296)
01/04/2023 23:04:53 - INFO - __main__ -   test: [batch: 91/196 ] | Loss: 0.943 | Acc: 75.819% (17857/23552)/ 93.003% (21904/23552)
01/04/2023 23:04:54 - INFO - __main__ -   test: [batch: 92/196 ] | Loss: 0.943 | Acc: 75.848% (18058/23808)/ 92.965% (22133/23808)
01/04/2023 23:04:54 - INFO - __main__ -   test: [batch: 93/196 ] | Loss: 0.951 | Acc: 75.698% (18216/24064)/ 92.861% (22346/24064)
01/04/2023 23:04:54 - INFO - __main__ -   test: [batch: 94/196 ] | Loss: 0.956 | Acc: 75.506% (18363/24320)/ 92.829% (22576/24320)
01/04/2023 23:04:55 - INFO - __main__ -   test: [batch: 95/196 ] | Loss: 0.963 | Acc: 75.387% (18527/24576)/ 92.757% (22796/24576)
01/04/2023 23:04:55 - INFO - __main__ -   test: [batch: 96/196 ] | Loss: 0.970 | Acc: 75.238% (18683/24832)/ 92.647% (23006/24832)
01/04/2023 23:04:55 - INFO - __main__ -   test: [batch: 97/196 ] | Loss: 0.979 | Acc: 75.068% (18833/25088)/ 92.534% (23215/25088)
01/04/2023 23:04:56 - INFO - __main__ -   test: [batch: 98/196 ] | Loss: 0.989 | Acc: 74.862% (18973/25344)/ 92.412% (23421/25344)
01/04/2023 23:04:56 - INFO - __main__ -   test: [batch: 99/196 ] | Loss: 0.990 | Acc: 74.797% (19148/25600)/ 92.387% (23651/25600)
01/04/2023 23:04:56 - INFO - __main__ -   test: [batch: 100/196 ] | Loss: 0.996 | Acc: 74.679% (19309/25856)/ 92.331% (23873/25856)
01/04/2023 23:04:57 - INFO - __main__ -   test: [batch: 101/196 ] | Loss: 0.996 | Acc: 74.671% (19498/26112)/ 92.314% (24105/26112)
01/04/2023 23:04:57 - INFO - __main__ -   test: [batch: 102/196 ] | Loss: 1.001 | Acc: 74.549% (19657/26368)/ 92.248% (24324/26368)
01/04/2023 23:04:57 - INFO - __main__ -   test: [batch: 103/196 ] | Loss: 1.006 | Acc: 74.470% (19827/26624)/ 92.199% (24547/26624)
01/04/2023 23:04:58 - INFO - __main__ -   test: [batch: 104/196 ] | Loss: 1.007 | Acc: 74.446% (20011/26880)/ 92.195% (24782/26880)
01/04/2023 23:04:58 - INFO - __main__ -   test: [batch: 105/196 ] | Loss: 1.012 | Acc: 74.337% (20172/27136)/ 92.125% (24999/27136)
01/04/2023 23:04:58 - INFO - __main__ -   test: [batch: 106/196 ] | Loss: 1.014 | Acc: 74.266% (20343/27392)/ 92.093% (25226/27392)
01/04/2023 23:04:59 - INFO - __main__ -   test: [batch: 107/196 ] | Loss: 1.015 | Acc: 74.266% (20533/27648)/ 92.075% (25457/27648)
01/04/2023 23:04:59 - INFO - __main__ -   test: [batch: 108/196 ] | Loss: 1.017 | Acc: 74.276% (20726/27904)/ 92.037% (25682/27904)
01/04/2023 23:04:59 - INFO - __main__ -   test: [batch: 109/196 ] | Loss: 1.019 | Acc: 74.251% (20909/28160)/ 91.985% (25903/28160)
01/04/2023 23:05:00 - INFO - __main__ -   test: [batch: 110/196 ] | Loss: 1.020 | Acc: 74.264% (21103/28416)/ 91.969% (26134/28416)
01/04/2023 23:05:00 - INFO - __main__ -   test: [batch: 111/196 ] | Loss: 1.018 | Acc: 74.313% (21307/28672)/ 91.982% (26373/28672)
01/04/2023 23:05:00 - INFO - __main__ -   test: [batch: 112/196 ] | Loss: 1.016 | Acc: 74.399% (21522/28928)/ 91.990% (26611/28928)
01/04/2023 23:05:01 - INFO - __main__ -   test: [batch: 113/196 ] | Loss: 1.017 | Acc: 74.380% (21707/29184)/ 92.016% (26854/29184)
01/04/2023 23:05:01 - INFO - __main__ -   test: [batch: 114/196 ] | Loss: 1.026 | Acc: 74.209% (21847/29440)/ 91.895% (27054/29440)
01/04/2023 23:05:01 - INFO - __main__ -   test: [batch: 115/196 ] | Loss: 1.031 | Acc: 74.084% (22000/29696)/ 91.820% (27267/29696)
01/04/2023 23:05:02 - INFO - __main__ -   test: [batch: 116/196 ] | Loss: 1.033 | Acc: 74.045% (22178/29952)/ 91.787% (27492/29952)
01/04/2023 23:05:02 - INFO - __main__ -   test: [batch: 117/196 ] | Loss: 1.039 | Acc: 73.924% (22331/30208)/ 91.714% (27705/30208)
01/04/2023 23:05:02 - INFO - __main__ -   test: [batch: 118/196 ] | Loss: 1.039 | Acc: 73.979% (22537/30464)/ 91.689% (27932/30464)
01/04/2023 23:05:03 - INFO - __main__ -   test: [batch: 119/196 ] | Loss: 1.037 | Acc: 74.059% (22751/30720)/ 91.699% (28170/30720)
01/04/2023 23:05:03 - INFO - __main__ -   test: [batch: 120/196 ] | Loss: 1.042 | Acc: 73.951% (22907/30976)/ 91.616% (28379/30976)
01/04/2023 23:05:03 - INFO - __main__ -   test: [batch: 121/196 ] | Loss: 1.051 | Acc: 73.726% (23026/31232)/ 91.502% (28578/31232)
01/04/2023 23:05:04 - INFO - __main__ -   test: [batch: 122/196 ] | Loss: 1.051 | Acc: 73.720% (23213/31488)/ 91.489% (28808/31488)
01/04/2023 23:05:04 - INFO - __main__ -   test: [batch: 123/196 ] | Loss: 1.059 | Acc: 73.617% (23369/31744)/ 91.387% (29010/31744)
01/04/2023 23:05:04 - INFO - __main__ -   test: [batch: 124/196 ] | Loss: 1.062 | Acc: 73.438% (23500/32000)/ 91.366% (29237/32000)
01/04/2023 23:05:05 - INFO - __main__ -   test: [batch: 125/196 ] | Loss: 1.063 | Acc: 73.419% (23682/32256)/ 91.350% (29466/32256)
01/04/2023 23:05:05 - INFO - __main__ -   test: [batch: 126/196 ] | Loss: 1.064 | Acc: 73.394% (23862/32512)/ 91.329% (29693/32512)
01/04/2023 23:05:05 - INFO - __main__ -   test: [batch: 127/196 ] | Loss: 1.071 | Acc: 73.297% (24018/32768)/ 91.263% (29905/32768)
01/04/2023 23:05:06 - INFO - __main__ -   test: [batch: 128/196 ] | Loss: 1.076 | Acc: 73.171% (24164/33024)/ 91.206% (30120/33024)
01/04/2023 23:05:06 - INFO - __main__ -   test: [batch: 129/196 ] | Loss: 1.079 | Acc: 73.062% (24315/33280)/ 91.190% (30348/33280)
01/04/2023 23:05:06 - INFO - __main__ -   test: [batch: 130/196 ] | Loss: 1.078 | Acc: 73.074% (24506/33536)/ 91.209% (30588/33536)
01/04/2023 23:05:07 - INFO - __main__ -   test: [batch: 131/196 ] | Loss: 1.080 | Acc: 73.005% (24670/33792)/ 91.187% (30814/33792)
01/04/2023 23:05:07 - INFO - __main__ -   test: [batch: 132/196 ] | Loss: 1.084 | Acc: 72.956% (24840/34048)/ 91.124% (31026/34048)
01/04/2023 23:05:07 - INFO - __main__ -   test: [batch: 133/196 ] | Loss: 1.086 | Acc: 72.927% (25017/34304)/ 91.094% (31249/34304)
01/04/2023 23:05:08 - INFO - __main__ -   test: [batch: 134/196 ] | Loss: 1.088 | Acc: 72.865% (25182/34560)/ 91.071% (31474/34560)
01/04/2023 23:05:08 - INFO - __main__ -   test: [batch: 135/196 ] | Loss: 1.092 | Acc: 72.817% (25352/34816)/ 91.024% (31691/34816)
01/04/2023 23:05:08 - INFO - __main__ -   test: [batch: 136/196 ] | Loss: 1.093 | Acc: 72.776% (25524/35072)/ 91.010% (31919/35072)
01/04/2023 23:05:09 - INFO - __main__ -   test: [batch: 137/196 ] | Loss: 1.095 | Acc: 72.758% (25704/35328)/ 90.993% (32146/35328)
01/04/2023 23:05:09 - INFO - __main__ -   test: [batch: 138/196 ] | Loss: 1.099 | Acc: 72.676% (25861/35584)/ 90.971% (32371/35584)
01/04/2023 23:05:09 - INFO - __main__ -   test: [batch: 139/196 ] | Loss: 1.099 | Acc: 72.690% (26052/35840)/ 90.968% (32603/35840)
01/04/2023 23:05:10 - INFO - __main__ -   test: [batch: 140/196 ] | Loss: 1.101 | Acc: 72.648% (26223/36096)/ 90.944% (32827/36096)
01/04/2023 23:05:10 - INFO - __main__ -   test: [batch: 141/196 ] | Loss: 1.100 | Acc: 72.681% (26421/36352)/ 90.941% (33059/36352)
01/04/2023 23:05:10 - INFO - __main__ -   test: [batch: 142/196 ] | Loss: 1.107 | Acc: 72.596% (26576/36608)/ 90.846% (33257/36608)
01/04/2023 23:05:11 - INFO - __main__ -   test: [batch: 143/196 ] | Loss: 1.109 | Acc: 72.561% (26749/36864)/ 90.815% (33478/36864)
01/04/2023 23:05:11 - INFO - __main__ -   test: [batch: 144/196 ] | Loss: 1.111 | Acc: 72.532% (26924/37120)/ 90.776% (33696/37120)
01/04/2023 23:05:11 - INFO - __main__ -   test: [batch: 145/196 ] | Loss: 1.113 | Acc: 72.450% (27079/37376)/ 90.761% (33923/37376)
01/04/2023 23:05:12 - INFO - __main__ -   test: [batch: 146/196 ] | Loss: 1.115 | Acc: 72.391% (27242/37632)/ 90.734% (34145/37632)
01/04/2023 23:05:12 - INFO - __main__ -   test: [batch: 147/196 ] | Loss: 1.115 | Acc: 72.371% (27420/37888)/ 90.731% (34376/37888)
01/04/2023 23:05:12 - INFO - __main__ -   test: [batch: 148/196 ] | Loss: 1.118 | Acc: 72.310% (27582/38144)/ 90.688% (34592/38144)
01/04/2023 23:05:13 - INFO - __main__ -   test: [batch: 149/196 ] | Loss: 1.121 | Acc: 72.271% (27752/38400)/ 90.656% (34812/38400)
01/04/2023 23:05:13 - INFO - __main__ -   test: [batch: 150/196 ] | Loss: 1.123 | Acc: 72.286% (27943/38656)/ 90.620% (35030/38656)
01/04/2023 23:05:13 - INFO - __main__ -   test: [batch: 151/196 ] | Loss: 1.125 | Acc: 72.253% (28115/38912)/ 90.574% (35244/38912)
01/04/2023 23:05:14 - INFO - __main__ -   test: [batch: 152/196 ] | Loss: 1.124 | Acc: 72.248% (28298/39168)/ 90.566% (35473/39168)
01/04/2023 23:05:14 - INFO - __main__ -   test: [batch: 153/196 ] | Loss: 1.128 | Acc: 72.192% (28461/39424)/ 90.513% (35684/39424)
01/04/2023 23:05:14 - INFO - __main__ -   test: [batch: 154/196 ] | Loss: 1.130 | Acc: 72.167% (28636/39680)/ 90.481% (35903/39680)
01/04/2023 23:05:15 - INFO - __main__ -   test: [batch: 155/196 ] | Loss: 1.133 | Acc: 72.125% (28804/39936)/ 90.430% (36114/39936)
01/04/2023 23:05:15 - INFO - __main__ -   test: [batch: 156/196 ] | Loss: 1.131 | Acc: 72.186% (29013/40192)/ 90.436% (36348/40192)
01/04/2023 23:05:15 - INFO - __main__ -   test: [batch: 157/196 ] | Loss: 1.134 | Acc: 72.149% (29183/40448)/ 90.402% (36566/40448)
01/04/2023 23:05:16 - INFO - __main__ -   test: [batch: 158/196 ] | Loss: 1.138 | Acc: 72.040% (29323/40704)/ 90.355% (36778/40704)
01/04/2023 23:05:16 - INFO - __main__ -   test: [batch: 159/196 ] | Loss: 1.140 | Acc: 71.992% (29488/40960)/ 90.332% (37000/40960)
01/04/2023 23:05:16 - INFO - __main__ -   test: [batch: 160/196 ] | Loss: 1.139 | Acc: 72.023% (29685/41216)/ 90.339% (37234/41216)
01/04/2023 23:05:17 - INFO - __main__ -   test: [batch: 161/196 ] | Loss: 1.143 | Acc: 71.962% (29844/41472)/ 90.304% (37451/41472)
01/04/2023 23:05:17 - INFO - __main__ -   test: [batch: 162/196 ] | Loss: 1.143 | Acc: 71.961% (30028/41728)/ 90.289% (37676/41728)
01/04/2023 23:05:17 - INFO - __main__ -   test: [batch: 163/196 ] | Loss: 1.151 | Acc: 71.754% (30125/41984)/ 90.194% (37867/41984)
01/04/2023 23:05:18 - INFO - __main__ -   test: [batch: 164/196 ] | Loss: 1.154 | Acc: 71.676% (30276/42240)/ 90.152% (38080/42240)
01/04/2023 23:05:18 - INFO - __main__ -   test: [batch: 165/196 ] | Loss: 1.157 | Acc: 71.586% (30421/42496)/ 90.119% (38297/42496)
01/04/2023 23:05:19 - INFO - __main__ -   test: [batch: 166/196 ] | Loss: 1.156 | Acc: 71.613% (30616/42752)/ 90.129% (38532/42752)
01/04/2023 23:05:19 - INFO - __main__ -   test: [batch: 167/196 ] | Loss: 1.158 | Acc: 71.559% (30776/43008)/ 90.074% (38739/43008)
01/04/2023 23:05:19 - INFO - __main__ -   test: [batch: 168/196 ] | Loss: 1.159 | Acc: 71.524% (30944/43264)/ 90.059% (38963/43264)
01/04/2023 23:05:20 - INFO - __main__ -   test: [batch: 169/196 ] | Loss: 1.162 | Acc: 71.484% (31110/43520)/ 90.028% (39180/43520)
01/04/2023 23:05:20 - INFO - __main__ -   test: [batch: 170/196 ] | Loss: 1.160 | Acc: 71.503% (31301/43776)/ 90.054% (39422/43776)
01/04/2023 23:05:20 - INFO - __main__ -   test: [batch: 171/196 ] | Loss: 1.161 | Acc: 71.464% (31467/44032)/ 90.028% (39641/44032)
01/04/2023 23:05:21 - INFO - __main__ -   test: [batch: 172/196 ] | Loss: 1.165 | Acc: 71.387% (31616/44288)/ 89.961% (39842/44288)
01/04/2023 23:05:21 - INFO - __main__ -   test: [batch: 173/196 ] | Loss: 1.165 | Acc: 71.392% (31801/44544)/ 89.969% (40076/44544)
01/04/2023 23:05:21 - INFO - __main__ -   test: [batch: 174/196 ] | Loss: 1.166 | Acc: 71.337% (31959/44800)/ 89.953% (40299/44800)
01/04/2023 23:05:22 - INFO - __main__ -   test: [batch: 175/196 ] | Loss: 1.168 | Acc: 71.298% (32124/45056)/ 89.935% (40521/45056)
01/04/2023 23:05:22 - INFO - __main__ -   test: [batch: 176/196 ] | Loss: 1.171 | Acc: 71.257% (32288/45312)/ 89.888% (40730/45312)
01/04/2023 23:05:22 - INFO - __main__ -   test: [batch: 177/196 ] | Loss: 1.177 | Acc: 71.103% (32400/45568)/ 89.848% (40942/45568)
01/04/2023 23:05:23 - INFO - __main__ -   test: [batch: 178/196 ] | Loss: 1.175 | Acc: 71.137% (32598/45824)/ 89.866% (41180/45824)
01/04/2023 23:05:23 - INFO - __main__ -   test: [batch: 179/196 ] | Loss: 1.174 | Acc: 71.159% (32790/46080)/ 89.874% (41414/46080)
01/04/2023 23:05:23 - INFO - __main__ -   test: [batch: 180/196 ] | Loss: 1.175 | Acc: 71.130% (32959/46336)/ 89.876% (41645/46336)
01/04/2023 23:05:24 - INFO - __main__ -   test: [batch: 181/196 ] | Loss: 1.175 | Acc: 71.115% (33134/46592)/ 89.882% (41878/46592)
01/04/2023 23:05:24 - INFO - __main__ -   test: [batch: 182/196 ] | Loss: 1.174 | Acc: 71.154% (33334/46848)/ 89.893% (42113/46848)
01/04/2023 23:05:24 - INFO - __main__ -   test: [batch: 183/196 ] | Loss: 1.170 | Acc: 71.219% (33547/47104)/ 89.933% (42362/47104)
01/04/2023 23:05:25 - INFO - __main__ -   test: [batch: 184/196 ] | Loss: 1.169 | Acc: 71.235% (33737/47360)/ 89.956% (42603/47360)
01/04/2023 23:05:25 - INFO - __main__ -   test: [batch: 185/196 ] | Loss: 1.168 | Acc: 71.241% (33922/47616)/ 89.978% (42844/47616)
01/04/2023 23:05:25 - INFO - __main__ -   test: [batch: 186/196 ] | Loss: 1.164 | Acc: 71.321% (34143/47872)/ 90.015% (43092/47872)
01/04/2023 23:05:26 - INFO - __main__ -   test: [batch: 187/196 ] | Loss: 1.166 | Acc: 71.285% (34308/48128)/ 89.991% (43311/48128)
01/04/2023 23:05:26 - INFO - __main__ -   test: [batch: 188/196 ] | Loss: 1.167 | Acc: 71.271% (34484/48384)/ 89.982% (43537/48384)
01/04/2023 23:05:26 - INFO - __main__ -   test: [batch: 189/196 ] | Loss: 1.171 | Acc: 71.162% (34613/48640)/ 89.947% (43750/48640)
01/04/2023 23:05:27 - INFO - __main__ -   test: [batch: 190/196 ] | Loss: 1.171 | Acc: 71.116% (34773/48896)/ 89.950% (43982/48896)
01/04/2023 23:05:27 - INFO - __main__ -   test: [batch: 191/196 ] | Loss: 1.172 | Acc: 71.108% (34951/49152)/ 89.939% (44207/49152)
01/04/2023 23:05:27 - INFO - __main__ -   test: [batch: 192/196 ] | Loss: 1.169 | Acc: 71.183% (35170/49408)/ 89.981% (44458/49408)
01/04/2023 23:05:28 - INFO - __main__ -   test: [batch: 193/196 ] | Loss: 1.165 | Acc: 71.289% (35405/49664)/ 90.015% (44705/49664)
01/04/2023 23:05:28 - INFO - __main__ -   test: [batch: 194/196 ] | Loss: 1.161 | Acc: 71.350% (35618/49920)/ 90.052% (44954/49920)
01/04/2023 23:05:28 - INFO - __main__ -   test: [batch: 195/196 ] | Loss: 1.167 | Acc: 71.296% (35648/50000)/ 90.038% (45019/50000)
01/04/2023 23:05:28 - INFO - __main__ -   Final accuracy: 71.296
01/04/2023 23:05:28 - INFO - __main__ -   {'milestones': Counter({2: 1, 3: 1, 4: 1}), 'gamma': 0.1, 'base_lrs': [0.0005], 'last_epoch': 1, '_step_count': 2, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.0005]}
01/04/2023 23:05:28 - INFO - __main__ -   
Epoch: 1
01/04/2023 23:05:29 - INFO - __main__ -   test: [epoch: 1 | batch: 0/5005 ] | Loss: 1.123 | Acc: 75.000% (192/256)
01/04/2023 23:06:05 - INFO - __main__ -   test: [epoch: 1 | batch: 100/5005 ] | Loss: 1.188 | Acc: 71.334% (18444/25856)
01/04/2023 23:06:42 - INFO - __main__ -   test: [epoch: 1 | batch: 200/5005 ] | Loss: 1.194 | Acc: 71.076% (36573/51456)
01/04/2023 23:07:18 - INFO - __main__ -   test: [epoch: 1 | batch: 300/5005 ] | Loss: 1.196 | Acc: 71.103% (54789/77056)
01/04/2023 23:07:55 - INFO - __main__ -   test: [epoch: 1 | batch: 400/5005 ] | Loss: 1.197 | Acc: 71.020% (72906/102656)
01/04/2023 23:08:31 - INFO - __main__ -   test: [epoch: 1 | batch: 500/5005 ] | Loss: 1.194 | Acc: 71.098% (91187/128256)
01/04/2023 23:09:08 - INFO - __main__ -   test: [epoch: 1 | batch: 600/5005 ] | Loss: 1.198 | Acc: 71.042% (109302/153856)
01/04/2023 23:09:44 - INFO - __main__ -   test: [epoch: 1 | batch: 700/5005 ] | Loss: 1.193 | Acc: 71.120% (127630/179456)
01/04/2023 23:10:21 - INFO - __main__ -   test: [epoch: 1 | batch: 800/5005 ] | Loss: 1.193 | Acc: 71.120% (145835/205056)
01/04/2023 23:10:57 - INFO - __main__ -   test: [epoch: 1 | batch: 900/5005 ] | Loss: 1.193 | Acc: 71.174% (164168/230656)
01/04/2023 23:11:34 - INFO - __main__ -   test: [epoch: 1 | batch: 1000/5005 ] | Loss: 1.193 | Acc: 71.159% (182350/256256)
01/04/2023 23:12:10 - INFO - __main__ -   test: [epoch: 1 | batch: 1100/5005 ] | Loss: 1.192 | Acc: 71.187% (200645/281856)
01/04/2023 23:12:47 - INFO - __main__ -   test: [epoch: 1 | batch: 1200/5005 ] | Loss: 1.192 | Acc: 71.196% (218896/307456)
01/04/2023 23:13:23 - INFO - __main__ -   test: [epoch: 1 | batch: 1300/5005 ] | Loss: 1.192 | Acc: 71.182% (237075/333056)
01/04/2023 23:14:00 - INFO - __main__ -   test: [epoch: 1 | batch: 1400/5005 ] | Loss: 1.191 | Acc: 71.177% (255281/358656)
01/04/2023 23:14:36 - INFO - __main__ -   test: [epoch: 1 | batch: 1500/5005 ] | Loss: 1.191 | Acc: 71.178% (273507/384256)
01/04/2023 23:15:13 - INFO - __main__ -   test: [epoch: 1 | batch: 1600/5005 ] | Loss: 1.191 | Acc: 71.185% (291754/409856)
01/04/2023 23:15:49 - INFO - __main__ -   test: [epoch: 1 | batch: 1700/5005 ] | Loss: 1.191 | Acc: 71.181% (309961/435456)
01/04/2023 23:16:26 - INFO - __main__ -   test: [epoch: 1 | batch: 1800/5005 ] | Loss: 1.190 | Acc: 71.192% (328235/461056)
01/04/2023 23:17:02 - INFO - __main__ -   test: [epoch: 1 | batch: 1900/5005 ] | Loss: 1.190 | Acc: 71.185% (346426/486656)
01/04/2023 23:17:39 - INFO - __main__ -   test: [epoch: 1 | batch: 2000/5005 ] | Loss: 1.190 | Acc: 71.166% (364551/512256)
01/04/2023 23:18:15 - INFO - __main__ -   test: [epoch: 1 | batch: 2100/5005 ] | Loss: 1.190 | Acc: 71.181% (382851/537856)
01/04/2023 23:18:52 - INFO - __main__ -   test: [epoch: 1 | batch: 2200/5005 ] | Loss: 1.190 | Acc: 71.198% (401170/563456)
01/04/2023 23:19:29 - INFO - __main__ -   test: [epoch: 1 | batch: 2300/5005 ] | Loss: 1.190 | Acc: 71.209% (419459/589056)
01/04/2023 23:20:05 - INFO - __main__ -   test: [epoch: 1 | batch: 2400/5005 ] | Loss: 1.189 | Acc: 71.223% (437775/614656)
01/04/2023 23:20:42 - INFO - __main__ -   test: [epoch: 1 | batch: 2500/5005 ] | Loss: 1.189 | Acc: 71.235% (456089/640256)
01/04/2023 23:21:18 - INFO - __main__ -   test: [epoch: 1 | batch: 2600/5005 ] | Loss: 1.189 | Acc: 71.231% (474299/665856)
01/04/2023 23:21:55 - INFO - __main__ -   test: [epoch: 1 | batch: 2700/5005 ] | Loss: 1.189 | Acc: 71.242% (492606/691456)
01/04/2023 23:22:31 - INFO - __main__ -   test: [epoch: 1 | batch: 2800/5005 ] | Loss: 1.189 | Acc: 71.240% (510833/717056)
01/04/2023 23:23:08 - INFO - __main__ -   test: [epoch: 1 | batch: 2900/5005 ] | Loss: 1.189 | Acc: 71.238% (529052/742656)
01/04/2023 23:23:44 - INFO - __main__ -   test: [epoch: 1 | batch: 3000/5005 ] | Loss: 1.189 | Acc: 71.242% (547323/768256)
01/04/2023 23:24:21 - INFO - __main__ -   test: [epoch: 1 | batch: 3100/5005 ] | Loss: 1.189 | Acc: 71.243% (565570/793856)
01/04/2023 23:24:57 - INFO - __main__ -   test: [epoch: 1 | batch: 3200/5005 ] | Loss: 1.189 | Acc: 71.230% (583700/819456)
01/04/2023 23:25:34 - INFO - __main__ -   test: [epoch: 1 | batch: 3300/5005 ] | Loss: 1.190 | Acc: 71.230% (601937/845056)
01/04/2023 23:26:10 - INFO - __main__ -   test: [epoch: 1 | batch: 3400/5005 ] | Loss: 1.189 | Acc: 71.231% (620175/870656)
01/04/2023 23:26:47 - INFO - __main__ -   test: [epoch: 1 | batch: 3500/5005 ] | Loss: 1.189 | Acc: 71.236% (638454/896256)
01/04/2023 23:27:23 - INFO - __main__ -   test: [epoch: 1 | batch: 3600/5005 ] | Loss: 1.189 | Acc: 71.245% (656772/921856)
01/04/2023 23:28:00 - INFO - __main__ -   test: [epoch: 1 | batch: 3700/5005 ] | Loss: 1.189 | Acc: 71.244% (675009/947456)
01/04/2023 23:28:36 - INFO - __main__ -   test: [epoch: 1 | batch: 3800/5005 ] | Loss: 1.189 | Acc: 71.237% (693175/973056)
01/04/2023 23:29:13 - INFO - __main__ -   test: [epoch: 1 | batch: 3900/5005 ] | Loss: 1.189 | Acc: 71.225% (711297/998656)
01/04/2023 23:29:49 - INFO - __main__ -   test: [epoch: 1 | batch: 4000/5005 ] | Loss: 1.189 | Acc: 71.221% (729486/1024256)
01/04/2023 23:30:26 - INFO - __main__ -   test: [epoch: 1 | batch: 4100/5005 ] | Loss: 1.189 | Acc: 71.232% (747835/1049856)
01/04/2023 23:31:02 - INFO - __main__ -   test: [epoch: 1 | batch: 4200/5005 ] | Loss: 1.189 | Acc: 71.233% (766075/1075456)
01/04/2023 23:31:39 - INFO - __main__ -   test: [epoch: 1 | batch: 4300/5005 ] | Loss: 1.189 | Acc: 71.243% (784430/1101056)
01/04/2023 23:32:15 - INFO - __main__ -   test: [epoch: 1 | batch: 4400/5005 ] | Loss: 1.189 | Acc: 71.247% (802711/1126656)
01/04/2023 23:32:52 - INFO - __main__ -   test: [epoch: 1 | batch: 4500/5005 ] | Loss: 1.189 | Acc: 71.242% (820892/1152256)
01/04/2023 23:33:28 - INFO - __main__ -   test: [epoch: 1 | batch: 4600/5005 ] | Loss: 1.189 | Acc: 71.235% (839047/1177856)
01/04/2023 23:34:05 - INFO - __main__ -   test: [epoch: 1 | batch: 4700/5005 ] | Loss: 1.189 | Acc: 71.239% (857332/1203456)
01/04/2023 23:34:41 - INFO - __main__ -   test: [epoch: 1 | batch: 4800/5005 ] | Loss: 1.189 | Acc: 71.237% (875548/1229056)
01/04/2023 23:35:18 - INFO - __main__ -   test: [epoch: 1 | batch: 4900/5005 ] | Loss: 1.190 | Acc: 71.230% (893692/1254656)
01/04/2023 23:35:54 - INFO - __main__ -   test: [epoch: 1 | batch: 5000/5005 ] | Loss: 1.190 | Acc: 71.230% (911932/1280256)
01/04/2023 23:35:56 - INFO - __main__ -   Saving Checkpoint
01/04/2023 23:35:56 - INFO - __main__ -   test: [batch: 0/196 ] | Loss: 0.645 | Acc: 81.250% (208/256)/ 96.484% (247/256)
01/04/2023 23:35:56 - INFO - __main__ -   test: [batch: 1/196 ] | Loss: 0.692 | Acc: 81.055% (415/512)/ 95.117% (487/512)
01/04/2023 23:35:57 - INFO - __main__ -   test: [batch: 2/196 ] | Loss: 0.570 | Acc: 84.635% (650/768)/ 96.224% (739/768)
01/04/2023 23:35:57 - INFO - __main__ -   test: [batch: 3/196 ] | Loss: 0.562 | Acc: 85.352% (874/1024)/ 95.898% (982/1024)
01/04/2023 23:35:57 - INFO - __main__ -   test: [batch: 4/196 ] | Loss: 0.539 | Acc: 86.172% (1103/1280)/ 96.016% (1229/1280)
01/04/2023 23:35:58 - INFO - __main__ -   test: [batch: 5/196 ] | Loss: 0.596 | Acc: 85.352% (1311/1536)/ 95.443% (1466/1536)
01/04/2023 23:35:58 - INFO - __main__ -   test: [batch: 6/196 ] | Loss: 0.711 | Acc: 82.310% (1475/1792)/ 94.420% (1692/1792)
01/04/2023 23:35:58 - INFO - __main__ -   test: [batch: 7/196 ] | Loss: 0.741 | Acc: 80.664% (1652/2048)/ 94.482% (1935/2048)
01/04/2023 23:35:59 - INFO - __main__ -   test: [batch: 8/196 ] | Loss: 0.778 | Acc: 79.948% (1842/2304)/ 93.793% (2161/2304)
01/04/2023 23:35:59 - INFO - __main__ -   test: [batch: 9/196 ] | Loss: 0.811 | Acc: 79.102% (2025/2560)/ 93.594% (2396/2560)
01/04/2023 23:35:59 - INFO - __main__ -   test: [batch: 10/196 ] | Loss: 0.839 | Acc: 78.196% (2202/2816)/ 93.501% (2633/2816)
01/04/2023 23:36:00 - INFO - __main__ -   test: [batch: 11/196 ] | Loss: 0.885 | Acc: 77.214% (2372/3072)/ 93.229% (2864/3072)
01/04/2023 23:36:00 - INFO - __main__ -   test: [batch: 12/196 ] | Loss: 0.925 | Acc: 76.322% (2540/3328)/ 92.728% (3086/3328)
01/04/2023 23:36:00 - INFO - __main__ -   test: [batch: 13/196 ] | Loss: 0.934 | Acc: 75.837% (2718/3584)/ 92.578% (3318/3584)
01/04/2023 23:36:01 - INFO - __main__ -   test: [batch: 14/196 ] | Loss: 0.934 | Acc: 75.755% (2909/3840)/ 92.786% (3563/3840)
01/04/2023 23:36:01 - INFO - __main__ -   test: [batch: 15/196 ] | Loss: 0.943 | Acc: 75.928% (3110/4096)/ 92.578% (3792/4096)
01/04/2023 23:36:01 - INFO - __main__ -   test: [batch: 16/196 ] | Loss: 0.921 | Acc: 76.471% (3328/4352)/ 92.785% (4038/4352)
01/04/2023 23:36:02 - INFO - __main__ -   test: [batch: 17/196 ] | Loss: 0.891 | Acc: 77.344% (3564/4608)/ 92.969% (4284/4608)
01/04/2023 23:36:02 - INFO - __main__ -   test: [batch: 18/196 ] | Loss: 0.865 | Acc: 78.043% (3796/4864)/ 93.195% (4533/4864)
01/04/2023 23:36:02 - INFO - __main__ -   test: [batch: 19/196 ] | Loss: 0.853 | Acc: 78.301% (4009/5120)/ 93.281% (4776/5120)
01/04/2023 23:36:03 - INFO - __main__ -   test: [batch: 20/196 ] | Loss: 0.853 | Acc: 78.516% (4221/5376)/ 93.192% (5010/5376)
01/04/2023 23:36:03 - INFO - __main__ -   test: [batch: 21/196 ] | Loss: 0.851 | Acc: 78.569% (4425/5632)/ 93.306% (5255/5632)
01/04/2023 23:36:03 - INFO - __main__ -   test: [batch: 22/196 ] | Loss: 0.853 | Acc: 78.550% (4625/5888)/ 93.274% (5492/5888)
01/04/2023 23:36:04 - INFO - __main__ -   test: [batch: 23/196 ] | Loss: 0.855 | Acc: 78.516% (4824/6144)/ 93.294% (5732/6144)
01/04/2023 23:36:04 - INFO - __main__ -   test: [batch: 24/196 ] | Loss: 0.871 | Acc: 78.266% (5009/6400)/ 93.172% (5963/6400)
01/04/2023 23:36:04 - INFO - __main__ -   test: [batch: 25/196 ] | Loss: 0.853 | Acc: 78.726% (5240/6656)/ 93.284% (6209/6656)
01/04/2023 23:36:05 - INFO - __main__ -   test: [batch: 26/196 ] | Loss: 0.840 | Acc: 78.993% (5460/6912)/ 93.345% (6452/6912)
01/04/2023 23:36:05 - INFO - __main__ -   test: [batch: 27/196 ] | Loss: 0.826 | Acc: 79.269% (5682/7168)/ 93.443% (6698/7168)
01/04/2023 23:36:05 - INFO - __main__ -   test: [batch: 28/196 ] | Loss: 0.810 | Acc: 79.701% (5917/7424)/ 93.548% (6945/7424)
01/04/2023 23:36:06 - INFO - __main__ -   test: [batch: 29/196 ] | Loss: 0.811 | Acc: 79.583% (6112/7680)/ 93.555% (7185/7680)
01/04/2023 23:36:06 - INFO - __main__ -   test: [batch: 30/196 ] | Loss: 0.811 | Acc: 79.574% (6315/7936)/ 93.574% (7426/7936)
01/04/2023 23:36:06 - INFO - __main__ -   test: [batch: 31/196 ] | Loss: 0.820 | Acc: 79.333% (6499/8192)/ 93.567% (7665/8192)
01/04/2023 23:36:07 - INFO - __main__ -   test: [batch: 32/196 ] | Loss: 0.841 | Acc: 78.717% (6650/8448)/ 93.419% (7892/8448)
01/04/2023 23:36:07 - INFO - __main__ -   test: [batch: 33/196 ] | Loss: 0.847 | Acc: 78.665% (6847/8704)/ 93.417% (8131/8704)
01/04/2023 23:36:08 - INFO - __main__ -   test: [batch: 34/196 ] | Loss: 0.845 | Acc: 78.705% (7052/8960)/ 93.471% (8375/8960)
01/04/2023 23:36:08 - INFO - __main__ -   test: [batch: 35/196 ] | Loss: 0.848 | Acc: 78.559% (7240/9216)/ 93.500% (8617/9216)
01/04/2023 23:36:08 - INFO - __main__ -   test: [batch: 36/196 ] | Loss: 0.858 | Acc: 78.146% (7402/9472)/ 93.528% (8859/9472)
01/04/2023 23:36:09 - INFO - __main__ -   test: [batch: 37/196 ] | Loss: 0.864 | Acc: 77.991% (7587/9728)/ 93.400% (9086/9728)
01/04/2023 23:36:09 - INFO - __main__ -   test: [batch: 38/196 ] | Loss: 0.864 | Acc: 77.975% (7785/9984)/ 93.480% (9333/9984)
01/04/2023 23:36:09 - INFO - __main__ -   test: [batch: 39/196 ] | Loss: 0.864 | Acc: 77.832% (7970/10240)/ 93.574% (9582/10240)
01/04/2023 23:36:10 - INFO - __main__ -   test: [batch: 40/196 ] | Loss: 0.863 | Acc: 77.849% (8171/10496)/ 93.598% (9824/10496)
01/04/2023 23:36:10 - INFO - __main__ -   test: [batch: 41/196 ] | Loss: 0.864 | Acc: 77.837% (8369/10752)/ 93.583% (10062/10752)
01/04/2023 23:36:10 - INFO - __main__ -   test: [batch: 42/196 ] | Loss: 0.856 | Acc: 78.025% (8589/11008)/ 93.668% (10311/11008)
01/04/2023 23:36:11 - INFO - __main__ -   test: [batch: 43/196 ] | Loss: 0.861 | Acc: 77.921% (8777/11264)/ 93.599% (10543/11264)
01/04/2023 23:36:11 - INFO - __main__ -   test: [batch: 44/196 ] | Loss: 0.866 | Acc: 77.891% (8973/11520)/ 93.594% (10782/11520)
01/04/2023 23:36:11 - INFO - __main__ -   test: [batch: 45/196 ] | Loss: 0.867 | Acc: 77.743% (9155/11776)/ 93.614% (11024/11776)
01/04/2023 23:36:12 - INFO - __main__ -   test: [batch: 46/196 ] | Loss: 0.872 | Acc: 77.502% (9325/12032)/ 93.600% (11262/12032)
01/04/2023 23:36:12 - INFO - __main__ -   test: [batch: 47/196 ] | Loss: 0.870 | Acc: 77.433% (9515/12288)/ 93.636% (11506/12288)
01/04/2023 23:36:12 - INFO - __main__ -   test: [batch: 48/196 ] | Loss: 0.874 | Acc: 77.168% (9680/12544)/ 93.662% (11749/12544)
01/04/2023 23:36:13 - INFO - __main__ -   test: [batch: 49/196 ] | Loss: 0.865 | Acc: 77.414% (9909/12800)/ 93.734% (11998/12800)
01/04/2023 23:36:13 - INFO - __main__ -   test: [batch: 50/196 ] | Loss: 0.859 | Acc: 77.566% (10127/13056)/ 93.781% (12244/13056)
01/04/2023 23:36:13 - INFO - __main__ -   test: [batch: 51/196 ] | Loss: 0.857 | Acc: 77.547% (10323/13312)/ 93.833% (12491/13312)
01/04/2023 23:36:14 - INFO - __main__ -   test: [batch: 52/196 ] | Loss: 0.857 | Acc: 77.528% (10519/13568)/ 93.838% (12732/13568)
01/04/2023 23:36:14 - INFO - __main__ -   test: [batch: 53/196 ] | Loss: 0.860 | Acc: 77.561% (10722/13824)/ 93.808% (12968/13824)
01/04/2023 23:36:14 - INFO - __main__ -   test: [batch: 54/196 ] | Loss: 0.862 | Acc: 77.479% (10909/14080)/ 93.828% (13211/14080)
01/04/2023 23:36:15 - INFO - __main__ -   test: [batch: 55/196 ] | Loss: 0.868 | Acc: 77.197% (11067/14336)/ 93.799% (13447/14336)
01/04/2023 23:36:15 - INFO - __main__ -   test: [batch: 56/196 ] | Loss: 0.867 | Acc: 77.255% (11273/14592)/ 93.819% (13690/14592)
01/04/2023 23:36:15 - INFO - __main__ -   test: [batch: 57/196 ] | Loss: 0.859 | Acc: 77.505% (11508/14848)/ 93.905% (13943/14848)
01/04/2023 23:36:16 - INFO - __main__ -   test: [batch: 58/196 ] | Loss: 0.860 | Acc: 77.529% (11710/15104)/ 93.889% (14181/15104)
01/04/2023 23:36:16 - INFO - __main__ -   test: [batch: 59/196 ] | Loss: 0.862 | Acc: 77.493% (11903/15360)/ 93.945% (14430/15360)
01/04/2023 23:36:16 - INFO - __main__ -   test: [batch: 60/196 ] | Loss: 0.866 | Acc: 77.357% (12080/15616)/ 93.936% (14669/15616)
01/04/2023 23:36:17 - INFO - __main__ -   test: [batch: 61/196 ] | Loss: 0.871 | Acc: 77.268% (12264/15872)/ 93.876% (14900/15872)
01/04/2023 23:36:17 - INFO - __main__ -   test: [batch: 62/196 ] | Loss: 0.866 | Acc: 77.387% (12481/16128)/ 93.911% (15146/16128)
01/04/2023 23:36:17 - INFO - __main__ -   test: [batch: 63/196 ] | Loss: 0.858 | Acc: 77.588% (12712/16384)/ 93.964% (15395/16384)
01/04/2023 23:36:18 - INFO - __main__ -   test: [batch: 64/196 ] | Loss: 0.858 | Acc: 77.608% (12914/16640)/ 93.966% (15636/16640)
01/04/2023 23:36:18 - INFO - __main__ -   test: [batch: 65/196 ] | Loss: 0.854 | Acc: 77.728% (13133/16896)/ 94.016% (15885/16896)
01/04/2023 23:36:18 - INFO - __main__ -   test: [batch: 66/196 ] | Loss: 0.853 | Acc: 77.775% (13340/17152)/ 94.030% (16128/17152)
01/04/2023 23:36:19 - INFO - __main__ -   test: [batch: 67/196 ] | Loss: 0.850 | Acc: 77.798% (13543/17408)/ 94.077% (16377/17408)
01/04/2023 23:36:19 - INFO - __main__ -   test: [batch: 68/196 ] | Loss: 0.847 | Acc: 77.791% (13741/17664)/ 94.118% (16625/17664)
01/04/2023 23:36:19 - INFO - __main__ -   test: [batch: 69/196 ] | Loss: 0.852 | Acc: 77.662% (13917/17920)/ 94.102% (16863/17920)
01/04/2023 23:36:20 - INFO - __main__ -   test: [batch: 70/196 ] | Loss: 0.851 | Acc: 77.668% (14117/18176)/ 94.119% (17107/18176)
01/04/2023 23:36:20 - INFO - __main__ -   test: [batch: 71/196 ] | Loss: 0.848 | Acc: 77.762% (14333/18432)/ 94.124% (17349/18432)
01/04/2023 23:36:20 - INFO - __main__ -   test: [batch: 72/196 ] | Loss: 0.853 | Acc: 77.718% (14524/18688)/ 94.076% (17581/18688)
01/04/2023 23:36:21 - INFO - __main__ -   test: [batch: 73/196 ] | Loss: 0.854 | Acc: 77.740% (14727/18944)/ 94.061% (17819/18944)
01/04/2023 23:36:21 - INFO - __main__ -   test: [batch: 74/196 ] | Loss: 0.859 | Acc: 77.568% (14893/19200)/ 94.016% (18051/19200)
01/04/2023 23:36:21 - INFO - __main__ -   test: [batch: 75/196 ] | Loss: 0.859 | Acc: 77.565% (15091/19456)/ 94.022% (18293/19456)
01/04/2023 23:36:22 - INFO - __main__ -   test: [batch: 76/196 ] | Loss: 0.858 | Acc: 77.582% (15293/19712)/ 94.044% (18538/19712)
01/04/2023 23:36:22 - INFO - __main__ -   test: [batch: 77/196 ] | Loss: 0.860 | Acc: 77.579% (15491/19968)/ 93.985% (18767/19968)
01/04/2023 23:36:22 - INFO - __main__ -   test: [batch: 78/196 ] | Loss: 0.866 | Acc: 77.438% (15661/20224)/ 93.888% (18988/20224)
01/04/2023 23:36:23 - INFO - __main__ -   test: [batch: 79/196 ] | Loss: 0.868 | Acc: 77.388% (15849/20480)/ 93.882% (19227/20480)
01/04/2023 23:36:23 - INFO - __main__ -   test: [batch: 80/196 ] | Loss: 0.878 | Acc: 77.160% (16000/20736)/ 93.789% (19448/20736)
01/04/2023 23:36:23 - INFO - __main__ -   test: [batch: 81/196 ] | Loss: 0.887 | Acc: 76.977% (16159/20992)/ 93.674% (19664/20992)
01/04/2023 23:36:24 - INFO - __main__ -   test: [batch: 82/196 ] | Loss: 0.891 | Acc: 76.864% (16332/21248)/ 93.632% (19895/21248)
01/04/2023 23:36:24 - INFO - __main__ -   test: [batch: 83/196 ] | Loss: 0.892 | Acc: 76.842% (16524/21504)/ 93.606% (20129/21504)
01/04/2023 23:36:24 - INFO - __main__ -   test: [batch: 84/196 ] | Loss: 0.896 | Acc: 76.765% (16704/21760)/ 93.539% (20354/21760)
01/04/2023 23:36:25 - INFO - __main__ -   test: [batch: 85/196 ] | Loss: 0.905 | Acc: 76.572% (16858/22016)/ 93.441% (20572/22016)
01/04/2023 23:36:25 - INFO - __main__ -   test: [batch: 86/196 ] | Loss: 0.910 | Acc: 76.464% (17030/22272)/ 93.377% (20797/22272)
01/04/2023 23:36:25 - INFO - __main__ -   test: [batch: 87/196 ] | Loss: 0.914 | Acc: 76.372% (17205/22528)/ 93.324% (21024/22528)
01/04/2023 23:36:26 - INFO - __main__ -   test: [batch: 88/196 ] | Loss: 0.919 | Acc: 76.233% (17369/22784)/ 93.250% (21246/22784)
01/04/2023 23:36:26 - INFO - __main__ -   test: [batch: 89/196 ] | Loss: 0.925 | Acc: 76.133% (17541/23040)/ 93.164% (21465/23040)
01/04/2023 23:36:26 - INFO - __main__ -   test: [batch: 90/196 ] | Loss: 0.938 | Acc: 75.876% (17676/23296)/ 93.042% (21675/23296)
01/04/2023 23:36:27 - INFO - __main__ -   test: [batch: 91/196 ] | Loss: 0.944 | Acc: 75.781% (17848/23552)/ 92.931% (21887/23552)
01/04/2023 23:36:27 - INFO - __main__ -   test: [batch: 92/196 ] | Loss: 0.944 | Acc: 75.811% (18049/23808)/ 92.889% (22115/23808)
01/04/2023 23:36:27 - INFO - __main__ -   test: [batch: 93/196 ] | Loss: 0.953 | Acc: 75.652% (18205/24064)/ 92.773% (22325/24064)
01/04/2023 23:36:28 - INFO - __main__ -   test: [batch: 94/196 ] | Loss: 0.957 | Acc: 75.473% (18355/24320)/ 92.755% (22558/24320)
01/04/2023 23:36:28 - INFO - __main__ -   test: [batch: 95/196 ] | Loss: 0.965 | Acc: 75.334% (18514/24576)/ 92.688% (22779/24576)
01/04/2023 23:36:28 - INFO - __main__ -   test: [batch: 96/196 ] | Loss: 0.972 | Acc: 75.177% (18668/24832)/ 92.590% (22992/24832)
01/04/2023 23:36:29 - INFO - __main__ -   test: [batch: 97/196 ] | Loss: 0.981 | Acc: 75.016% (18820/25088)/ 92.478% (23201/25088)
01/04/2023 23:36:29 - INFO - __main__ -   test: [batch: 98/196 ] | Loss: 0.991 | Acc: 74.822% (18963/25344)/ 92.365% (23409/25344)
01/04/2023 23:36:29 - INFO - __main__ -   test: [batch: 99/196 ] | Loss: 0.992 | Acc: 74.773% (19142/25600)/ 92.344% (23640/25600)
01/04/2023 23:36:30 - INFO - __main__ -   test: [batch: 100/196 ] | Loss: 0.997 | Acc: 74.660% (19304/25856)/ 92.280% (23860/25856)
01/04/2023 23:36:30 - INFO - __main__ -   test: [batch: 101/196 ] | Loss: 0.998 | Acc: 74.644% (19491/26112)/ 92.276% (24095/26112)
01/04/2023 23:36:30 - INFO - __main__ -   test: [batch: 102/196 ] | Loss: 1.003 | Acc: 74.515% (19648/26368)/ 92.214% (24315/26368)
01/04/2023 23:36:31 - INFO - __main__ -   test: [batch: 103/196 ] | Loss: 1.008 | Acc: 74.433% (19817/26624)/ 92.180% (24542/26624)
01/04/2023 23:36:31 - INFO - __main__ -   test: [batch: 104/196 ] | Loss: 1.009 | Acc: 74.408% (20001/26880)/ 92.176% (24777/26880)
01/04/2023 23:36:31 - INFO - __main__ -   test: [batch: 105/196 ] | Loss: 1.013 | Acc: 74.315% (20166/27136)/ 92.114% (24996/27136)
01/04/2023 23:36:32 - INFO - __main__ -   test: [batch: 106/196 ] | Loss: 1.015 | Acc: 74.263% (20342/27392)/ 92.074% (25221/27392)
01/04/2023 23:36:32 - INFO - __main__ -   test: [batch: 107/196 ] | Loss: 1.016 | Acc: 74.255% (20530/27648)/ 92.057% (25452/27648)
01/04/2023 23:36:32 - INFO - __main__ -   test: [batch: 108/196 ] | Loss: 1.018 | Acc: 74.244% (20717/27904)/ 92.019% (25677/27904)
01/04/2023 23:36:33 - INFO - __main__ -   test: [batch: 109/196 ] | Loss: 1.020 | Acc: 74.233% (20904/28160)/ 91.974% (25900/28160)
01/04/2023 23:36:33 - INFO - __main__ -   test: [batch: 110/196 ] | Loss: 1.020 | Acc: 74.247% (21098/28416)/ 91.959% (26131/28416)
01/04/2023 23:36:33 - INFO - __main__ -   test: [batch: 111/196 ] | Loss: 1.019 | Acc: 74.289% (21300/28672)/ 91.971% (26370/28672)
01/04/2023 23:36:34 - INFO - __main__ -   test: [batch: 112/196 ] | Loss: 1.017 | Acc: 74.367% (21513/28928)/ 91.980% (26608/28928)
01/04/2023 23:36:34 - INFO - __main__ -   test: [batch: 113/196 ] | Loss: 1.018 | Acc: 74.359% (21701/29184)/ 91.985% (26845/29184)
01/04/2023 23:36:34 - INFO - __main__ -   test: [batch: 114/196 ] | Loss: 1.027 | Acc: 74.185% (21840/29440)/ 91.865% (27045/29440)
01/04/2023 23:36:35 - INFO - __main__ -   test: [batch: 115/196 ] | Loss: 1.032 | Acc: 74.064% (21994/29696)/ 91.797% (27260/29696)
01/04/2023 23:36:35 - INFO - __main__ -   test: [batch: 116/196 ] | Loss: 1.034 | Acc: 74.035% (22175/29952)/ 91.763% (27485/29952)
01/04/2023 23:36:35 - INFO - __main__ -   test: [batch: 117/196 ] | Loss: 1.040 | Acc: 73.894% (22322/30208)/ 91.684% (27696/30208)
01/04/2023 23:36:36 - INFO - __main__ -   test: [batch: 118/196 ] | Loss: 1.039 | Acc: 73.940% (22525/30464)/ 91.672% (27927/30464)
01/04/2023 23:36:36 - INFO - __main__ -   test: [batch: 119/196 ] | Loss: 1.038 | Acc: 74.007% (22735/30720)/ 91.683% (28165/30720)
01/04/2023 23:36:36 - INFO - __main__ -   test: [batch: 120/196 ] | Loss: 1.043 | Acc: 73.906% (22893/30976)/ 91.610% (28377/30976)
01/04/2023 23:36:37 - INFO - __main__ -   test: [batch: 121/196 ] | Loss: 1.051 | Acc: 73.684% (23013/31232)/ 91.512% (28581/31232)
01/04/2023 23:36:37 - INFO - __main__ -   test: [batch: 122/196 ] | Loss: 1.051 | Acc: 73.692% (23204/31488)/ 91.498% (28811/31488)
01/04/2023 23:36:37 - INFO - __main__ -   test: [batch: 123/196 ] | Loss: 1.059 | Acc: 73.592% (23361/31744)/ 91.397% (29013/31744)
01/04/2023 23:36:38 - INFO - __main__ -   test: [batch: 124/196 ] | Loss: 1.063 | Acc: 73.425% (23496/32000)/ 91.375% (29240/32000)
01/04/2023 23:36:38 - INFO - __main__ -   test: [batch: 125/196 ] | Loss: 1.064 | Acc: 73.425% (23684/32256)/ 91.363% (29470/32256)
01/04/2023 23:36:38 - INFO - __main__ -   test: [batch: 126/196 ] | Loss: 1.065 | Acc: 73.404% (23865/32512)/ 91.336% (29695/32512)
01/04/2023 23:36:39 - INFO - __main__ -   test: [batch: 127/196 ] | Loss: 1.071 | Acc: 73.294% (24017/32768)/ 91.272% (29908/32768)
01/04/2023 23:36:39 - INFO - __main__ -   test: [batch: 128/196 ] | Loss: 1.076 | Acc: 73.168% (24163/33024)/ 91.215% (30123/33024)
01/04/2023 23:36:40 - INFO - __main__ -   test: [batch: 129/196 ] | Loss: 1.080 | Acc: 73.056% (24313/33280)/ 91.211% (30355/33280)
01/04/2023 23:36:40 - INFO - __main__ -   test: [batch: 130/196 ] | Loss: 1.078 | Acc: 73.068% (24504/33536)/ 91.227% (30594/33536)
01/04/2023 23:36:40 - INFO - __main__ -   test: [batch: 131/196 ] | Loss: 1.080 | Acc: 73.011% (24672/33792)/ 91.208% (30821/33792)
01/04/2023 23:36:41 - INFO - __main__ -   test: [batch: 132/196 ] | Loss: 1.085 | Acc: 72.956% (24840/34048)/ 91.130% (31028/34048)
01/04/2023 23:36:41 - INFO - __main__ -   test: [batch: 133/196 ] | Loss: 1.087 | Acc: 72.927% (25017/34304)/ 91.103% (31252/34304)
01/04/2023 23:36:41 - INFO - __main__ -   test: [batch: 134/196 ] | Loss: 1.089 | Acc: 72.870% (25184/34560)/ 91.079% (31477/34560)
01/04/2023 23:36:42 - INFO - __main__ -   test: [batch: 135/196 ] | Loss: 1.093 | Acc: 72.820% (25353/34816)/ 91.027% (31692/34816)
01/04/2023 23:36:42 - INFO - __main__ -   test: [batch: 136/196 ] | Loss: 1.094 | Acc: 72.785% (25527/35072)/ 91.016% (31921/35072)
01/04/2023 23:36:42 - INFO - __main__ -   test: [batch: 137/196 ] | Loss: 1.096 | Acc: 72.761% (25705/35328)/ 91.004% (32150/35328)
01/04/2023 23:36:43 - INFO - __main__ -   test: [batch: 138/196 ] | Loss: 1.099 | Acc: 72.690% (25866/35584)/ 90.985% (32376/35584)
01/04/2023 23:36:43 - INFO - __main__ -   test: [batch: 139/196 ] | Loss: 1.099 | Acc: 72.712% (26060/35840)/ 90.982% (32608/35840)
01/04/2023 23:36:43 - INFO - __main__ -   test: [batch: 140/196 ] | Loss: 1.101 | Acc: 72.673% (26232/36096)/ 90.960% (32833/36096)
01/04/2023 23:36:44 - INFO - __main__ -   test: [batch: 141/196 ] | Loss: 1.100 | Acc: 72.709% (26431/36352)/ 90.966% (33068/36352)
01/04/2023 23:36:44 - INFO - __main__ -   test: [batch: 142/196 ] | Loss: 1.107 | Acc: 72.623% (26586/36608)/ 90.857% (33261/36608)
01/04/2023 23:36:44 - INFO - __main__ -   test: [batch: 143/196 ] | Loss: 1.109 | Acc: 72.594% (26761/36864)/ 90.834% (33485/36864)
01/04/2023 23:36:45 - INFO - __main__ -   test: [batch: 144/196 ] | Loss: 1.111 | Acc: 72.543% (26928/37120)/ 90.800% (33705/37120)
01/04/2023 23:36:45 - INFO - __main__ -   test: [batch: 145/196 ] | Loss: 1.113 | Acc: 72.456% (27081/37376)/ 90.788% (33933/37376)
01/04/2023 23:36:45 - INFO - __main__ -   test: [batch: 146/196 ] | Loss: 1.115 | Acc: 72.388% (27241/37632)/ 90.763% (34156/37632)
01/04/2023 23:36:46 - INFO - __main__ -   test: [batch: 147/196 ] | Loss: 1.115 | Acc: 72.400% (27431/37888)/ 90.757% (34386/37888)
01/04/2023 23:36:46 - INFO - __main__ -   test: [batch: 148/196 ] | Loss: 1.118 | Acc: 72.352% (27598/38144)/ 90.722% (34605/38144)
01/04/2023 23:36:46 - INFO - __main__ -   test: [batch: 149/196 ] | Loss: 1.121 | Acc: 72.299% (27763/38400)/ 90.688% (34824/38400)
01/04/2023 23:36:47 - INFO - __main__ -   test: [batch: 150/196 ] | Loss: 1.123 | Acc: 72.310% (27952/38656)/ 90.661% (35046/38656)
01/04/2023 23:36:47 - INFO - __main__ -   test: [batch: 151/196 ] | Loss: 1.125 | Acc: 72.273% (28123/38912)/ 90.615% (35260/38912)
01/04/2023 23:36:47 - INFO - __main__ -   test: [batch: 152/196 ] | Loss: 1.125 | Acc: 72.268% (28306/39168)/ 90.605% (35488/39168)
01/04/2023 23:36:48 - INFO - __main__ -   test: [batch: 153/196 ] | Loss: 1.128 | Acc: 72.212% (28469/39424)/ 90.551% (35699/39424)
01/04/2023 23:36:48 - INFO - __main__ -   test: [batch: 154/196 ] | Loss: 1.130 | Acc: 72.172% (28638/39680)/ 90.512% (35915/39680)
01/04/2023 23:36:48 - INFO - __main__ -   test: [batch: 155/196 ] | Loss: 1.133 | Acc: 72.135% (28808/39936)/ 90.462% (36127/39936)
01/04/2023 23:36:49 - INFO - __main__ -   test: [batch: 156/196 ] | Loss: 1.132 | Acc: 72.186% (29013/40192)/ 90.478% (36365/40192)
01/04/2023 23:36:49 - INFO - __main__ -   test: [batch: 157/196 ] | Loss: 1.134 | Acc: 72.157% (29186/40448)/ 90.447% (36584/40448)
01/04/2023 23:36:49 - INFO - __main__ -   test: [batch: 158/196 ] | Loss: 1.139 | Acc: 72.045% (29325/40704)/ 90.392% (36793/40704)
01/04/2023 23:36:50 - INFO - __main__ -   test: [batch: 159/196 ] | Loss: 1.141 | Acc: 72.000% (29491/40960)/ 90.369% (37015/40960)
01/04/2023 23:36:50 - INFO - __main__ -   test: [batch: 160/196 ] | Loss: 1.140 | Acc: 72.030% (29688/41216)/ 90.380% (37251/41216)
01/04/2023 23:36:50 - INFO - __main__ -   test: [batch: 161/196 ] | Loss: 1.144 | Acc: 71.974% (29849/41472)/ 90.353% (37471/41472)
01/04/2023 23:36:51 - INFO - __main__ -   test: [batch: 162/196 ] | Loss: 1.144 | Acc: 71.973% (30033/41728)/ 90.345% (37699/41728)
01/04/2023 23:36:51 - INFO - __main__ -   test: [batch: 163/196 ] | Loss: 1.152 | Acc: 71.763% (30129/41984)/ 90.234% (37884/41984)
01/04/2023 23:36:51 - INFO - __main__ -   test: [batch: 164/196 ] | Loss: 1.155 | Acc: 71.697% (30285/42240)/ 90.189% (38096/42240)
01/04/2023 23:36:52 - INFO - __main__ -   test: [batch: 165/196 ] | Loss: 1.157 | Acc: 71.614% (30433/42496)/ 90.154% (38312/42496)
01/04/2023 23:36:52 - INFO - __main__ -   test: [batch: 166/196 ] | Loss: 1.156 | Acc: 71.634% (30625/42752)/ 90.160% (38545/42752)
01/04/2023 23:36:52 - INFO - __main__ -   test: [batch: 167/196 ] | Loss: 1.159 | Acc: 71.577% (30784/43008)/ 90.109% (38754/43008)
01/04/2023 23:36:53 - INFO - __main__ -   test: [batch: 168/196 ] | Loss: 1.160 | Acc: 71.544% (30953/43264)/ 90.100% (38981/43264)
01/04/2023 23:36:53 - INFO - __main__ -   test: [batch: 169/196 ] | Loss: 1.162 | Acc: 71.500% (31117/43520)/ 90.067% (39197/43520)
01/04/2023 23:36:53 - INFO - __main__ -   test: [batch: 170/196 ] | Loss: 1.160 | Acc: 71.528% (31312/43776)/ 90.102% (39443/43776)
01/04/2023 23:36:54 - INFO - __main__ -   test: [batch: 171/196 ] | Loss: 1.161 | Acc: 71.473% (31471/44032)/ 90.075% (39662/44032)
01/04/2023 23:36:54 - INFO - __main__ -   test: [batch: 172/196 ] | Loss: 1.166 | Acc: 71.401% (31622/44288)/ 90.013% (39865/44288)
01/04/2023 23:36:54 - INFO - __main__ -   test: [batch: 173/196 ] | Loss: 1.166 | Acc: 71.404% (31806/44544)/ 90.023% (40100/44544)
01/04/2023 23:36:55 - INFO - __main__ -   test: [batch: 174/196 ] | Loss: 1.167 | Acc: 71.348% (31964/44800)/ 90.009% (40324/44800)
01/04/2023 23:36:55 - INFO - __main__ -   test: [batch: 175/196 ] | Loss: 1.169 | Acc: 71.300% (32125/45056)/ 89.999% (40550/45056)
01/04/2023 23:36:55 - INFO - __main__ -   test: [batch: 176/196 ] | Loss: 1.172 | Acc: 71.266% (32292/45312)/ 89.945% (40756/45312)
01/04/2023 23:36:56 - INFO - __main__ -   test: [batch: 177/196 ] | Loss: 1.177 | Acc: 71.118% (32407/45568)/ 89.912% (40971/45568)
01/04/2023 23:36:56 - INFO - __main__ -   test: [batch: 178/196 ] | Loss: 1.176 | Acc: 71.164% (32610/45824)/ 89.933% (41211/45824)
01/04/2023 23:36:56 - INFO - __main__ -   test: [batch: 179/196 ] | Loss: 1.175 | Acc: 71.189% (32804/46080)/ 89.933% (41441/46080)
01/04/2023 23:36:57 - INFO - __main__ -   test: [batch: 180/196 ] | Loss: 1.176 | Acc: 71.167% (32976/46336)/ 89.939% (41674/46336)
01/04/2023 23:36:57 - INFO - __main__ -   test: [batch: 181/196 ] | Loss: 1.176 | Acc: 71.156% (33153/46592)/ 89.938% (41904/46592)
01/04/2023 23:36:57 - INFO - __main__ -   test: [batch: 182/196 ] | Loss: 1.175 | Acc: 71.192% (33352/46848)/ 89.948% (42139/46848)
01/04/2023 23:36:58 - INFO - __main__ -   test: [batch: 183/196 ] | Loss: 1.171 | Acc: 71.253% (33563/47104)/ 89.984% (42386/47104)
01/04/2023 23:36:58 - INFO - __main__ -   test: [batch: 184/196 ] | Loss: 1.170 | Acc: 71.267% (33752/47360)/ 90.002% (42625/47360)
01/04/2023 23:36:58 - INFO - __main__ -   test: [batch: 185/196 ] | Loss: 1.169 | Acc: 71.270% (33936/47616)/ 90.022% (42865/47616)
01/04/2023 23:36:59 - INFO - __main__ -   test: [batch: 186/196 ] | Loss: 1.165 | Acc: 71.359% (34161/47872)/ 90.059% (43113/47872)
01/04/2023 23:36:59 - INFO - __main__ -   test: [batch: 187/196 ] | Loss: 1.167 | Acc: 71.331% (34330/48128)/ 90.029% (43329/48128)
01/04/2023 23:36:59 - INFO - __main__ -   test: [batch: 188/196 ] | Loss: 1.168 | Acc: 71.319% (34507/48384)/ 90.013% (43552/48384)
01/04/2023 23:37:00 - INFO - __main__ -   test: [batch: 189/196 ] | Loss: 1.172 | Acc: 71.213% (34638/48640)/ 89.979% (43766/48640)
01/04/2023 23:37:00 - INFO - __main__ -   test: [batch: 190/196 ] | Loss: 1.173 | Acc: 71.188% (34808/48896)/ 89.985% (43999/48896)
01/04/2023 23:37:00 - INFO - __main__ -   test: [batch: 191/196 ] | Loss: 1.174 | Acc: 71.173% (34983/49152)/ 89.980% (44227/49152)
01/04/2023 23:37:01 - INFO - __main__ -   test: [batch: 192/196 ] | Loss: 1.170 | Acc: 71.248% (35202/49408)/ 90.022% (44478/49408)
01/04/2023 23:37:01 - INFO - __main__ -   test: [batch: 193/196 ] | Loss: 1.166 | Acc: 71.353% (35437/49664)/ 90.057% (44726/49664)
01/04/2023 23:37:01 - INFO - __main__ -   test: [batch: 194/196 ] | Loss: 1.162 | Acc: 71.414% (35650/49920)/ 90.092% (44974/49920)
01/04/2023 23:37:02 - INFO - __main__ -   test: [batch: 195/196 ] | Loss: 1.168 | Acc: 71.356% (35678/50000)/ 90.084% (45042/50000)
01/04/2023 23:37:02 - INFO - __main__ -   Final accuracy: 71.356
01/04/2023 23:37:02 - INFO - __main__ -   {'milestones': Counter({2: 1, 3: 1, 4: 1}), 'gamma': 0.1, 'base_lrs': [0.0005], 'last_epoch': 2, '_step_count': 3, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [5e-05]}
01/04/2023 23:37:02 - INFO - __main__ -   
Epoch: 2
01/04/2023 23:37:02 - INFO - __main__ -   test: [epoch: 2 | batch: 0/5005 ] | Loss: 1.124 | Acc: 72.266% (185/256)
01/04/2023 23:37:38 - INFO - __main__ -   test: [epoch: 2 | batch: 100/5005 ] | Loss: 1.169 | Acc: 71.419% (18466/25856)
01/04/2023 23:38:15 - INFO - __main__ -   test: [epoch: 2 | batch: 200/5005 ] | Loss: 1.177 | Acc: 71.350% (36714/51456)
01/04/2023 23:38:51 - INFO - __main__ -   test: [epoch: 2 | batch: 300/5005 ] | Loss: 1.186 | Acc: 71.295% (54937/77056)
01/04/2023 23:39:28 - INFO - __main__ -   test: [epoch: 2 | batch: 400/5005 ] | Loss: 1.187 | Acc: 71.200% (73091/102656)
01/04/2023 23:40:04 - INFO - __main__ -   test: [epoch: 2 | batch: 500/5005 ] | Loss: 1.186 | Acc: 71.289% (91432/128256)
01/04/2023 23:40:41 - INFO - __main__ -   test: [epoch: 2 | batch: 600/5005 ] | Loss: 1.189 | Acc: 71.243% (109611/153856)
01/04/2023 23:41:17 - INFO - __main__ -   test: [epoch: 2 | batch: 700/5005 ] | Loss: 1.187 | Acc: 71.319% (127987/179456)
01/04/2023 23:41:54 - INFO - __main__ -   test: [epoch: 2 | batch: 800/5005 ] | Loss: 1.186 | Acc: 71.295% (146195/205056)
01/04/2023 23:42:30 - INFO - __main__ -   test: [epoch: 2 | batch: 900/5005 ] | Loss: 1.185 | Acc: 71.320% (164505/230656)
01/04/2023 23:43:07 - INFO - __main__ -   test: [epoch: 2 | batch: 1000/5005 ] | Loss: 1.187 | Acc: 71.251% (182585/256256)
01/04/2023 23:43:43 - INFO - __main__ -   test: [epoch: 2 | batch: 1100/5005 ] | Loss: 1.187 | Acc: 71.229% (200762/281856)
01/04/2023 23:44:20 - INFO - __main__ -   test: [epoch: 2 | batch: 1200/5005 ] | Loss: 1.186 | Acc: 71.248% (219057/307456)
01/04/2023 23:44:56 - INFO - __main__ -   test: [epoch: 2 | batch: 1300/5005 ] | Loss: 1.187 | Acc: 71.282% (237410/333056)
01/04/2023 23:45:33 - INFO - __main__ -   test: [epoch: 2 | batch: 1400/5005 ] | Loss: 1.187 | Acc: 71.247% (255531/358656)
01/04/2023 23:46:09 - INFO - __main__ -   test: [epoch: 2 | batch: 1500/5005 ] | Loss: 1.187 | Acc: 71.250% (273784/384256)
01/04/2023 23:46:46 - INFO - __main__ -   test: [epoch: 2 | batch: 1600/5005 ] | Loss: 1.187 | Acc: 71.260% (292064/409856)
01/04/2023 23:47:22 - INFO - __main__ -   test: [epoch: 2 | batch: 1700/5005 ] | Loss: 1.187 | Acc: 71.248% (310252/435456)
01/04/2023 23:47:59 - INFO - __main__ -   test: [epoch: 2 | batch: 1800/5005 ] | Loss: 1.187 | Acc: 71.258% (328541/461056)
01/04/2023 23:48:35 - INFO - __main__ -   test: [epoch: 2 | batch: 1900/5005 ] | Loss: 1.187 | Acc: 71.232% (346653/486656)
01/04/2023 23:49:12 - INFO - __main__ -   test: [epoch: 2 | batch: 2000/5005 ] | Loss: 1.188 | Acc: 71.210% (364778/512256)
01/04/2023 23:49:48 - INFO - __main__ -   test: [epoch: 2 | batch: 2100/5005 ] | Loss: 1.188 | Acc: 71.210% (383007/537856)
01/04/2023 23:50:25 - INFO - __main__ -   test: [epoch: 2 | batch: 2200/5005 ] | Loss: 1.188 | Acc: 71.211% (401242/563456)
01/04/2023 23:51:01 - INFO - __main__ -   test: [epoch: 2 | batch: 2300/5005 ] | Loss: 1.188 | Acc: 71.204% (419429/589056)
01/04/2023 23:51:38 - INFO - __main__ -   test: [epoch: 2 | batch: 2400/5005 ] | Loss: 1.187 | Acc: 71.217% (437741/614656)
01/04/2023 23:52:14 - INFO - __main__ -   test: [epoch: 2 | batch: 2500/5005 ] | Loss: 1.187 | Acc: 71.225% (456021/640256)
01/04/2023 23:52:51 - INFO - __main__ -   test: [epoch: 2 | batch: 2600/5005 ] | Loss: 1.188 | Acc: 71.204% (474119/665856)
01/04/2023 23:53:27 - INFO - __main__ -   test: [epoch: 2 | batch: 2700/5005 ] | Loss: 1.188 | Acc: 71.198% (492306/691456)
01/04/2023 23:54:04 - INFO - __main__ -   test: [epoch: 2 | batch: 2800/5005 ] | Loss: 1.188 | Acc: 71.223% (510711/717056)
01/04/2023 23:54:40 - INFO - __main__ -   test: [epoch: 2 | batch: 2900/5005 ] | Loss: 1.188 | Acc: 71.224% (528947/742656)
01/04/2023 23:55:17 - INFO - __main__ -   test: [epoch: 2 | batch: 3000/5005 ] | Loss: 1.188 | Acc: 71.235% (547271/768256)
01/04/2023 23:55:53 - INFO - __main__ -   test: [epoch: 2 | batch: 3100/5005 ] | Loss: 1.189 | Acc: 71.230% (565460/793856)
01/04/2023 23:56:30 - INFO - __main__ -   test: [epoch: 2 | batch: 3200/5005 ] | Loss: 1.189 | Acc: 71.239% (583769/819456)
01/04/2023 23:57:06 - INFO - __main__ -   test: [epoch: 2 | batch: 3300/5005 ] | Loss: 1.188 | Acc: 71.234% (601971/845056)
01/04/2023 23:57:43 - INFO - __main__ -   test: [epoch: 2 | batch: 3400/5005 ] | Loss: 1.188 | Acc: 71.238% (620239/870656)
01/04/2023 23:58:19 - INFO - __main__ -   test: [epoch: 2 | batch: 3500/5005 ] | Loss: 1.188 | Acc: 71.233% (638427/896256)
01/04/2023 23:58:56 - INFO - __main__ -   test: [epoch: 2 | batch: 3600/5005 ] | Loss: 1.188 | Acc: 71.248% (656800/921856)
01/04/2023 23:59:32 - INFO - __main__ -   test: [epoch: 2 | batch: 3700/5005 ] | Loss: 1.188 | Acc: 71.242% (674986/947456)
01/05/2023 00:00:09 - INFO - __main__ -   test: [epoch: 2 | batch: 3800/5005 ] | Loss: 1.188 | Acc: 71.242% (693226/973056)
01/05/2023 00:00:45 - INFO - __main__ -   test: [epoch: 2 | batch: 3900/5005 ] | Loss: 1.189 | Acc: 71.229% (711331/998656)
01/05/2023 00:01:22 - INFO - __main__ -   test: [epoch: 2 | batch: 4000/5005 ] | Loss: 1.189 | Acc: 71.233% (729605/1024256)
01/05/2023 00:01:58 - INFO - __main__ -   test: [epoch: 2 | batch: 4100/5005 ] | Loss: 1.188 | Acc: 71.245% (747968/1049856)
01/05/2023 00:02:35 - INFO - __main__ -   test: [epoch: 2 | batch: 4200/5005 ] | Loss: 1.188 | Acc: 71.240% (766153/1075456)
01/05/2023 00:03:11 - INFO - __main__ -   test: [epoch: 2 | batch: 4300/5005 ] | Loss: 1.188 | Acc: 71.246% (784458/1101056)
01/05/2023 00:03:48 - INFO - __main__ -   test: [epoch: 2 | batch: 4400/5005 ] | Loss: 1.188 | Acc: 71.247% (802710/1126656)
01/05/2023 00:04:24 - INFO - __main__ -   test: [epoch: 2 | batch: 4500/5005 ] | Loss: 1.188 | Acc: 71.245% (820930/1152256)
01/05/2023 00:05:01 - INFO - __main__ -   test: [epoch: 2 | batch: 4600/5005 ] | Loss: 1.188 | Acc: 71.248% (839201/1177856)
01/05/2023 00:05:37 - INFO - __main__ -   test: [epoch: 2 | batch: 4700/5005 ] | Loss: 1.188 | Acc: 71.246% (857410/1203456)
01/05/2023 00:06:14 - INFO - __main__ -   test: [epoch: 2 | batch: 4800/5005 ] | Loss: 1.188 | Acc: 71.251% (875711/1229056)
01/05/2023 00:06:51 - INFO - __main__ -   test: [epoch: 2 | batch: 4900/5005 ] | Loss: 1.189 | Acc: 71.240% (893815/1254656)
01/05/2023 00:07:27 - INFO - __main__ -   test: [epoch: 2 | batch: 5000/5005 ] | Loss: 1.189 | Acc: 71.236% (911999/1280256)
01/05/2023 00:07:28 - INFO - __main__ -   Saving Checkpoint
01/05/2023 00:07:29 - INFO - __main__ -   test: [batch: 0/196 ] | Loss: 0.648 | Acc: 82.031% (210/256)/ 96.875% (248/256)
01/05/2023 00:07:29 - INFO - __main__ -   test: [batch: 1/196 ] | Loss: 0.694 | Acc: 81.445% (417/512)/ 95.312% (488/512)
01/05/2023 00:07:29 - INFO - __main__ -   test: [batch: 2/196 ] | Loss: 0.570 | Acc: 85.156% (654/768)/ 96.354% (740/768)
01/05/2023 00:07:30 - INFO - __main__ -   test: [batch: 3/196 ] | Loss: 0.558 | Acc: 85.938% (880/1024)/ 96.191% (985/1024)
01/05/2023 00:07:30 - INFO - __main__ -   test: [batch: 4/196 ] | Loss: 0.535 | Acc: 86.484% (1107/1280)/ 96.484% (1235/1280)
01/05/2023 00:07:30 - INFO - __main__ -   test: [batch: 5/196 ] | Loss: 0.590 | Acc: 85.547% (1314/1536)/ 95.768% (1471/1536)
01/05/2023 00:07:31 - INFO - __main__ -   test: [batch: 6/196 ] | Loss: 0.695 | Acc: 82.589% (1480/1792)/ 94.866% (1700/1792)
01/05/2023 00:07:31 - INFO - __main__ -   test: [batch: 7/196 ] | Loss: 0.729 | Acc: 80.908% (1657/2048)/ 94.824% (1942/2048)
01/05/2023 00:07:31 - INFO - __main__ -   test: [batch: 8/196 ] | Loss: 0.767 | Acc: 80.339% (1851/2304)/ 94.227% (2171/2304)
01/05/2023 00:07:32 - INFO - __main__ -   test: [batch: 9/196 ] | Loss: 0.802 | Acc: 79.531% (2036/2560)/ 93.906% (2404/2560)
01/05/2023 00:07:32 - INFO - __main__ -   test: [batch: 10/196 ] | Loss: 0.833 | Acc: 78.729% (2217/2816)/ 93.786% (2641/2816)
01/05/2023 00:07:32 - INFO - __main__ -   test: [batch: 11/196 ] | Loss: 0.879 | Acc: 77.702% (2387/3072)/ 93.522% (2873/3072)
01/05/2023 00:07:33 - INFO - __main__ -   test: [batch: 12/196 ] | Loss: 0.918 | Acc: 76.743% (2554/3328)/ 92.999% (3095/3328)
01/05/2023 00:07:33 - INFO - __main__ -   test: [batch: 13/196 ] | Loss: 0.928 | Acc: 76.228% (2732/3584)/ 92.857% (3328/3584)
01/05/2023 00:07:33 - INFO - __main__ -   test: [batch: 14/196 ] | Loss: 0.929 | Acc: 76.094% (2922/3840)/ 93.021% (3572/3840)
01/05/2023 00:07:34 - INFO - __main__ -   test: [batch: 15/196 ] | Loss: 0.938 | Acc: 76.147% (3119/4096)/ 92.725% (3798/4096)
01/05/2023 00:07:34 - INFO - __main__ -   test: [batch: 16/196 ] | Loss: 0.916 | Acc: 76.746% (3340/4352)/ 92.946% (4045/4352)
01/05/2023 00:07:34 - INFO - __main__ -   test: [batch: 17/196 ] | Loss: 0.887 | Acc: 77.669% (3579/4608)/ 93.142% (4292/4608)
01/05/2023 00:07:35 - INFO - __main__ -   test: [batch: 18/196 ] | Loss: 0.861 | Acc: 78.372% (3812/4864)/ 93.359% (4541/4864)
01/05/2023 00:07:35 - INFO - __main__ -   test: [batch: 19/196 ] | Loss: 0.849 | Acc: 78.594% (4024/5120)/ 93.438% (4784/5120)
01/05/2023 00:07:35 - INFO - __main__ -   test: [batch: 20/196 ] | Loss: 0.847 | Acc: 78.850% (4239/5376)/ 93.359% (5019/5376)
01/05/2023 00:07:36 - INFO - __main__ -   test: [batch: 21/196 ] | Loss: 0.844 | Acc: 78.924% (4445/5632)/ 93.448% (5263/5632)
01/05/2023 00:07:36 - INFO - __main__ -   test: [batch: 22/196 ] | Loss: 0.846 | Acc: 78.906% (4646/5888)/ 93.410% (5500/5888)
01/05/2023 00:07:36 - INFO - __main__ -   test: [batch: 23/196 ] | Loss: 0.849 | Acc: 78.857% (4845/6144)/ 93.457% (5742/6144)
01/05/2023 00:07:37 - INFO - __main__ -   test: [batch: 24/196 ] | Loss: 0.865 | Acc: 78.578% (5029/6400)/ 93.312% (5972/6400)
01/05/2023 00:07:37 - INFO - __main__ -   test: [batch: 25/196 ] | Loss: 0.848 | Acc: 79.041% (5261/6656)/ 93.434% (6219/6656)
01/05/2023 00:07:37 - INFO - __main__ -   test: [batch: 26/196 ] | Loss: 0.834 | Acc: 79.311% (5482/6912)/ 93.533% (6465/6912)
01/05/2023 00:07:38 - INFO - __main__ -   test: [batch: 27/196 ] | Loss: 0.820 | Acc: 79.590% (5705/7168)/ 93.597% (6709/7168)
01/05/2023 00:07:38 - INFO - __main__ -   test: [batch: 28/196 ] | Loss: 0.804 | Acc: 79.970% (5937/7424)/ 93.710% (6957/7424)
01/05/2023 00:07:39 - INFO - __main__ -   test: [batch: 29/196 ] | Loss: 0.806 | Acc: 79.870% (6134/7680)/ 93.724% (7198/7680)
01/05/2023 00:07:39 - INFO - __main__ -   test: [batch: 30/196 ] | Loss: 0.806 | Acc: 79.877% (6339/7936)/ 93.750% (7440/7936)
01/05/2023 00:07:39 - INFO - __main__ -   test: [batch: 31/196 ] | Loss: 0.815 | Acc: 79.626% (6523/8192)/ 93.689% (7675/8192)
01/05/2023 00:07:40 - INFO - __main__ -   test: [batch: 32/196 ] | Loss: 0.835 | Acc: 78.977% (6672/8448)/ 93.537% (7902/8448)
01/05/2023 00:07:40 - INFO - __main__ -   test: [batch: 33/196 ] | Loss: 0.842 | Acc: 78.803% (6859/8704)/ 93.509% (8139/8704)
01/05/2023 00:07:40 - INFO - __main__ -   test: [batch: 34/196 ] | Loss: 0.841 | Acc: 78.795% (7060/8960)/ 93.560% (8383/8960)
01/05/2023 00:07:41 - INFO - __main__ -   test: [batch: 35/196 ] | Loss: 0.843 | Acc: 78.657% (7249/9216)/ 93.587% (8625/9216)
01/05/2023 00:07:41 - INFO - __main__ -   test: [batch: 36/196 ] | Loss: 0.853 | Acc: 78.262% (7413/9472)/ 93.623% (8868/9472)
01/05/2023 00:07:41 - INFO - __main__ -   test: [batch: 37/196 ] | Loss: 0.860 | Acc: 78.074% (7595/9728)/ 93.503% (9096/9728)
01/05/2023 00:07:42 - INFO - __main__ -   test: [batch: 38/196 ] | Loss: 0.859 | Acc: 78.065% (7794/9984)/ 93.580% (9343/9984)
01/05/2023 00:07:42 - INFO - __main__ -   test: [batch: 39/196 ] | Loss: 0.860 | Acc: 77.939% (7981/10240)/ 93.662% (9591/10240)
01/05/2023 00:07:42 - INFO - __main__ -   test: [batch: 40/196 ] | Loss: 0.859 | Acc: 77.963% (8183/10496)/ 93.683% (9833/10496)
01/05/2023 00:07:43 - INFO - __main__ -   test: [batch: 41/196 ] | Loss: 0.860 | Acc: 77.967% (8383/10752)/ 93.638% (10068/10752)
01/05/2023 00:07:43 - INFO - __main__ -   test: [batch: 42/196 ] | Loss: 0.852 | Acc: 78.161% (8604/11008)/ 93.723% (10317/11008)
01/05/2023 00:07:43 - INFO - __main__ -   test: [batch: 43/196 ] | Loss: 0.857 | Acc: 78.098% (8797/11264)/ 93.661% (10550/11264)
01/05/2023 00:07:44 - INFO - __main__ -   test: [batch: 44/196 ] | Loss: 0.861 | Acc: 78.021% (8988/11520)/ 93.655% (10789/11520)
01/05/2023 00:07:44 - INFO - __main__ -   test: [batch: 45/196 ] | Loss: 0.863 | Acc: 77.862% (9169/11776)/ 93.674% (11031/11776)
01/05/2023 00:07:44 - INFO - __main__ -   test: [batch: 46/196 ] | Loss: 0.868 | Acc: 77.618% (9339/12032)/ 93.659% (11269/12032)
01/05/2023 00:07:45 - INFO - __main__ -   test: [batch: 47/196 ] | Loss: 0.866 | Acc: 77.563% (9531/12288)/ 93.693% (11513/12288)
01/05/2023 00:07:45 - INFO - __main__ -   test: [batch: 48/196 ] | Loss: 0.870 | Acc: 77.352% (9703/12544)/ 93.726% (11757/12544)
01/05/2023 00:07:45 - INFO - __main__ -   test: [batch: 49/196 ] | Loss: 0.861 | Acc: 77.586% (9931/12800)/ 93.797% (12006/12800)
01/05/2023 00:07:46 - INFO - __main__ -   test: [batch: 50/196 ] | Loss: 0.856 | Acc: 77.711% (10146/13056)/ 93.850% (12253/13056)
01/05/2023 00:07:46 - INFO - __main__ -   test: [batch: 51/196 ] | Loss: 0.854 | Acc: 77.689% (10342/13312)/ 93.908% (12501/13312)
01/05/2023 00:07:46 - INFO - __main__ -   test: [batch: 52/196 ] | Loss: 0.854 | Acc: 77.653% (10536/13568)/ 93.912% (12742/13568)
01/05/2023 00:07:47 - INFO - __main__ -   test: [batch: 53/196 ] | Loss: 0.858 | Acc: 77.662% (10736/13824)/ 93.880% (12978/13824)
01/05/2023 00:07:47 - INFO - __main__ -   test: [batch: 54/196 ] | Loss: 0.859 | Acc: 77.571% (10922/14080)/ 93.892% (13220/14080)
01/05/2023 00:07:47 - INFO - __main__ -   test: [batch: 55/196 ] | Loss: 0.865 | Acc: 77.358% (11090/14336)/ 93.862% (13456/14336)
01/05/2023 00:07:48 - INFO - __main__ -   test: [batch: 56/196 ] | Loss: 0.864 | Acc: 77.399% (11294/14592)/ 93.887% (13700/14592)
01/05/2023 00:07:48 - INFO - __main__ -   test: [batch: 57/196 ] | Loss: 0.856 | Acc: 77.633% (11527/14848)/ 93.966% (13952/14848)
01/05/2023 00:07:48 - INFO - __main__ -   test: [batch: 58/196 ] | Loss: 0.857 | Acc: 77.668% (11731/15104)/ 93.949% (14190/15104)
01/05/2023 00:07:49 - INFO - __main__ -   test: [batch: 59/196 ] | Loss: 0.859 | Acc: 77.604% (11920/15360)/ 93.984% (14436/15360)
01/05/2023 00:07:49 - INFO - __main__ -   test: [batch: 60/196 ] | Loss: 0.863 | Acc: 77.523% (12106/15616)/ 93.968% (14674/15616)
01/05/2023 00:07:49 - INFO - __main__ -   test: [batch: 61/196 ] | Loss: 0.868 | Acc: 77.419% (12288/15872)/ 93.914% (14906/15872)
01/05/2023 00:07:50 - INFO - __main__ -   test: [batch: 62/196 ] | Loss: 0.863 | Acc: 77.555% (12508/16128)/ 93.961% (15154/16128)
01/05/2023 00:07:50 - INFO - __main__ -   test: [batch: 63/196 ] | Loss: 0.856 | Acc: 77.734% (12736/16384)/ 94.000% (15401/16384)
01/05/2023 00:07:50 - INFO - __main__ -   test: [batch: 64/196 ] | Loss: 0.855 | Acc: 77.752% (12938/16640)/ 94.026% (15646/16640)
01/05/2023 00:07:51 - INFO - __main__ -   test: [batch: 65/196 ] | Loss: 0.851 | Acc: 77.882% (13159/16896)/ 94.081% (15896/16896)
01/05/2023 00:07:51 - INFO - __main__ -   test: [batch: 66/196 ] | Loss: 0.850 | Acc: 77.921% (13365/17152)/ 94.076% (16136/17152)
01/05/2023 00:07:51 - INFO - __main__ -   test: [batch: 67/196 ] | Loss: 0.848 | Acc: 77.958% (13571/17408)/ 94.106% (16382/17408)
01/05/2023 00:07:52 - INFO - __main__ -   test: [batch: 68/196 ] | Loss: 0.844 | Acc: 77.950% (13769/17664)/ 94.158% (16632/17664)
01/05/2023 00:07:52 - INFO - __main__ -   test: [batch: 69/196 ] | Loss: 0.848 | Acc: 77.835% (13948/17920)/ 94.157% (16873/17920)
01/05/2023 00:07:52 - INFO - __main__ -   test: [batch: 70/196 ] | Loss: 0.848 | Acc: 77.844% (14149/18176)/ 94.174% (17117/18176)
01/05/2023 00:07:53 - INFO - __main__ -   test: [batch: 71/196 ] | Loss: 0.845 | Acc: 77.951% (14368/18432)/ 94.184% (17360/18432)
01/05/2023 00:07:53 - INFO - __main__ -   test: [batch: 72/196 ] | Loss: 0.850 | Acc: 77.916% (14561/18688)/ 94.119% (17589/18688)
01/05/2023 00:07:53 - INFO - __main__ -   test: [batch: 73/196 ] | Loss: 0.851 | Acc: 77.951% (14767/18944)/ 94.083% (17823/18944)
01/05/2023 00:07:54 - INFO - __main__ -   test: [batch: 74/196 ] | Loss: 0.856 | Acc: 77.781% (14934/19200)/ 94.036% (18055/19200)
01/05/2023 00:07:54 - INFO - __main__ -   test: [batch: 75/196 ] | Loss: 0.856 | Acc: 77.791% (15135/19456)/ 94.048% (18298/19456)
01/05/2023 00:07:54 - INFO - __main__ -   test: [batch: 76/196 ] | Loss: 0.855 | Acc: 77.800% (15336/19712)/ 94.065% (18542/19712)
01/05/2023 00:07:55 - INFO - __main__ -   test: [batch: 77/196 ] | Loss: 0.857 | Acc: 77.789% (15533/19968)/ 94.010% (18772/19968)
01/05/2023 00:07:55 - INFO - __main__ -   test: [batch: 78/196 ] | Loss: 0.862 | Acc: 77.635% (15701/20224)/ 93.953% (19001/20224)
01/05/2023 00:07:55 - INFO - __main__ -   test: [batch: 79/196 ] | Loss: 0.865 | Acc: 77.578% (15888/20480)/ 93.945% (19240/20480)
01/05/2023 00:07:56 - INFO - __main__ -   test: [batch: 80/196 ] | Loss: 0.874 | Acc: 77.334% (16036/20736)/ 93.846% (19460/20736)
01/05/2023 00:07:56 - INFO - __main__ -   test: [batch: 81/196 ] | Loss: 0.883 | Acc: 77.163% (16198/20992)/ 93.740% (19678/20992)
01/05/2023 00:07:56 - INFO - __main__ -   test: [batch: 82/196 ] | Loss: 0.887 | Acc: 77.057% (16373/21248)/ 93.722% (19914/21248)
01/05/2023 00:07:57 - INFO - __main__ -   test: [batch: 83/196 ] | Loss: 0.889 | Acc: 77.023% (16563/21504)/ 93.703% (20150/21504)
01/05/2023 00:07:57 - INFO - __main__ -   test: [batch: 84/196 ] | Loss: 0.892 | Acc: 76.939% (16742/21760)/ 93.640% (20376/21760)
01/05/2023 00:07:57 - INFO - __main__ -   test: [batch: 85/196 ] | Loss: 0.901 | Acc: 76.758% (16899/22016)/ 93.550% (20596/22016)
01/05/2023 00:07:58 - INFO - __main__ -   test: [batch: 86/196 ] | Loss: 0.905 | Acc: 76.648% (17071/22272)/ 93.494% (20823/22272)
01/05/2023 00:07:58 - INFO - __main__ -   test: [batch: 87/196 ] | Loss: 0.909 | Acc: 76.540% (17243/22528)/ 93.453% (21053/22528)
01/05/2023 00:07:58 - INFO - __main__ -   test: [batch: 88/196 ] | Loss: 0.914 | Acc: 76.426% (17413/22784)/ 93.386% (21277/22784)
01/05/2023 00:07:59 - INFO - __main__ -   test: [batch: 89/196 ] | Loss: 0.921 | Acc: 76.302% (17580/23040)/ 93.303% (21497/23040)
01/05/2023 00:07:59 - INFO - __main__ -   test: [batch: 90/196 ] | Loss: 0.933 | Acc: 76.052% (17717/23296)/ 93.171% (21705/23296)
01/05/2023 00:07:59 - INFO - __main__ -   test: [batch: 91/196 ] | Loss: 0.940 | Acc: 75.943% (17886/23552)/ 93.066% (21919/23552)
01/05/2023 00:08:00 - INFO - __main__ -   test: [batch: 92/196 ] | Loss: 0.940 | Acc: 75.970% (18087/23808)/ 93.036% (22150/23808)
01/05/2023 00:08:00 - INFO - __main__ -   test: [batch: 93/196 ] | Loss: 0.948 | Acc: 75.827% (18247/24064)/ 92.915% (22359/24064)
01/05/2023 00:08:00 - INFO - __main__ -   test: [batch: 94/196 ] | Loss: 0.953 | Acc: 75.650% (18398/24320)/ 92.899% (22593/24320)
01/05/2023 00:08:01 - INFO - __main__ -   test: [batch: 95/196 ] | Loss: 0.960 | Acc: 75.525% (18561/24576)/ 92.826% (22813/24576)
01/05/2023 00:08:01 - INFO - __main__ -   test: [batch: 96/196 ] | Loss: 0.967 | Acc: 75.403% (18724/24832)/ 92.719% (23024/24832)
01/05/2023 00:08:01 - INFO - __main__ -   test: [batch: 97/196 ] | Loss: 0.975 | Acc: 75.247% (18878/25088)/ 92.634% (23240/25088)
01/05/2023 00:08:02 - INFO - __main__ -   test: [batch: 98/196 ] | Loss: 0.985 | Acc: 75.051% (19021/25344)/ 92.531% (23451/25344)
01/05/2023 00:08:02 - INFO - __main__ -   test: [batch: 99/196 ] | Loss: 0.987 | Acc: 74.988% (19197/25600)/ 92.504% (23681/25600)
01/05/2023 00:08:02 - INFO - __main__ -   test: [batch: 100/196 ] | Loss: 0.991 | Acc: 74.865% (19357/25856)/ 92.450% (23904/25856)
01/05/2023 00:08:03 - INFO - __main__ -   test: [batch: 101/196 ] | Loss: 0.992 | Acc: 74.851% (19545/26112)/ 92.429% (24135/26112)
01/05/2023 00:08:03 - INFO - __main__ -   test: [batch: 102/196 ] | Loss: 0.997 | Acc: 74.735% (19706/26368)/ 92.366% (24355/26368)
01/05/2023 00:08:03 - INFO - __main__ -   test: [batch: 103/196 ] | Loss: 1.002 | Acc: 74.643% (19873/26624)/ 92.326% (24581/26624)
01/05/2023 00:08:04 - INFO - __main__ -   test: [batch: 104/196 ] | Loss: 1.003 | Acc: 74.628% (20060/26880)/ 92.314% (24814/26880)
01/05/2023 00:08:04 - INFO - __main__ -   test: [batch: 105/196 ] | Loss: 1.008 | Acc: 74.525% (20223/27136)/ 92.239% (25030/27136)
01/05/2023 00:08:04 - INFO - __main__ -   test: [batch: 106/196 ] | Loss: 1.010 | Acc: 74.467% (20398/27392)/ 92.206% (25257/27392)
01/05/2023 00:08:05 - INFO - __main__ -   test: [batch: 107/196 ] | Loss: 1.011 | Acc: 74.461% (20587/27648)/ 92.184% (25487/27648)
01/05/2023 00:08:05 - INFO - __main__ -   test: [batch: 108/196 ] | Loss: 1.013 | Acc: 74.452% (20775/27904)/ 92.155% (25715/27904)
01/05/2023 00:08:05 - INFO - __main__ -   test: [batch: 109/196 ] | Loss: 1.016 | Acc: 74.418% (20956/28160)/ 92.106% (25937/28160)
01/05/2023 00:08:06 - INFO - __main__ -   test: [batch: 110/196 ] | Loss: 1.016 | Acc: 74.412% (21145/28416)/ 92.092% (26169/28416)
01/05/2023 00:08:06 - INFO - __main__ -   test: [batch: 111/196 ] | Loss: 1.015 | Acc: 74.442% (21344/28672)/ 92.111% (26410/28672)
01/05/2023 00:08:06 - INFO - __main__ -   test: [batch: 112/196 ] | Loss: 1.013 | Acc: 74.523% (21558/28928)/ 92.125% (26650/28928)
01/05/2023 00:08:07 - INFO - __main__ -   test: [batch: 113/196 ] | Loss: 1.014 | Acc: 74.493% (21740/29184)/ 92.129% (26887/29184)
01/05/2023 00:08:07 - INFO - __main__ -   test: [batch: 114/196 ] | Loss: 1.023 | Acc: 74.314% (21878/29440)/ 92.007% (27087/29440)
01/05/2023 00:08:07 - INFO - __main__ -   test: [batch: 115/196 ] | Loss: 1.027 | Acc: 74.199% (22034/29696)/ 91.928% (27299/29696)
01/05/2023 00:08:08 - INFO - __main__ -   test: [batch: 116/196 ] | Loss: 1.029 | Acc: 74.152% (22210/29952)/ 91.894% (27524/29952)
01/05/2023 00:08:08 - INFO - __main__ -   test: [batch: 117/196 ] | Loss: 1.036 | Acc: 74.020% (22360/30208)/ 91.820% (27737/30208)
01/05/2023 00:08:08 - INFO - __main__ -   test: [batch: 118/196 ] | Loss: 1.035 | Acc: 74.064% (22563/30464)/ 91.787% (27962/30464)
01/05/2023 00:08:09 - INFO - __main__ -   test: [batch: 119/196 ] | Loss: 1.034 | Acc: 74.137% (22775/30720)/ 91.797% (28200/30720)
01/05/2023 00:08:09 - INFO - __main__ -   test: [batch: 120/196 ] | Loss: 1.039 | Acc: 74.022% (22929/30976)/ 91.716% (28410/30976)
01/05/2023 00:08:09 - INFO - __main__ -   test: [batch: 121/196 ] | Loss: 1.047 | Acc: 73.796% (23048/31232)/ 91.608% (28611/31232)
01/05/2023 00:08:10 - INFO - __main__ -   test: [batch: 122/196 ] | Loss: 1.047 | Acc: 73.800% (23238/31488)/ 91.597% (28842/31488)
01/05/2023 00:08:10 - INFO - __main__ -   test: [batch: 123/196 ] | Loss: 1.055 | Acc: 73.696% (23394/31744)/ 91.494% (29044/31744)
01/05/2023 00:08:10 - INFO - __main__ -   test: [batch: 124/196 ] | Loss: 1.058 | Acc: 73.537% (23532/32000)/ 91.475% (29272/32000)
01/05/2023 00:08:11 - INFO - __main__ -   test: [batch: 125/196 ] | Loss: 1.060 | Acc: 73.524% (23716/32256)/ 91.468% (29504/32256)
01/05/2023 00:08:11 - INFO - __main__ -   test: [batch: 126/196 ] | Loss: 1.060 | Acc: 73.511% (23900/32512)/ 91.431% (29726/32512)
01/05/2023 00:08:11 - INFO - __main__ -   test: [batch: 127/196 ] | Loss: 1.067 | Acc: 73.416% (24057/32768)/ 91.360% (29937/32768)
01/05/2023 00:08:12 - INFO - __main__ -   test: [batch: 128/196 ] | Loss: 1.071 | Acc: 73.289% (24203/33024)/ 91.309% (30154/33024)
01/05/2023 00:08:12 - INFO - __main__ -   test: [batch: 129/196 ] | Loss: 1.075 | Acc: 73.200% (24361/33280)/ 91.292% (30382/33280)
01/05/2023 00:08:13 - INFO - __main__ -   test: [batch: 130/196 ] | Loss: 1.074 | Acc: 73.214% (24553/33536)/ 91.308% (30621/33536)
01/05/2023 00:08:13 - INFO - __main__ -   test: [batch: 131/196 ] | Loss: 1.076 | Acc: 73.145% (24717/33792)/ 91.291% (30849/33792)
01/05/2023 00:08:13 - INFO - __main__ -   test: [batch: 132/196 ] | Loss: 1.080 | Acc: 73.088% (24885/34048)/ 91.215% (31057/34048)
01/05/2023 00:08:14 - INFO - __main__ -   test: [batch: 133/196 ] | Loss: 1.082 | Acc: 73.067% (25065/34304)/ 91.196% (31284/34304)
01/05/2023 00:08:14 - INFO - __main__ -   test: [batch: 134/196 ] | Loss: 1.085 | Acc: 73.012% (25233/34560)/ 91.178% (31511/34560)
01/05/2023 00:08:14 - INFO - __main__ -   test: [batch: 135/196 ] | Loss: 1.088 | Acc: 72.958% (25401/34816)/ 91.128% (31727/34816)
01/05/2023 00:08:15 - INFO - __main__ -   test: [batch: 136/196 ] | Loss: 1.089 | Acc: 72.916% (25573/35072)/ 91.104% (31952/35072)
01/05/2023 00:08:15 - INFO - __main__ -   test: [batch: 137/196 ] | Loss: 1.091 | Acc: 72.897% (25753/35328)/ 91.089% (32180/35328)
01/05/2023 00:08:15 - INFO - __main__ -   test: [batch: 138/196 ] | Loss: 1.094 | Acc: 72.819% (25912/35584)/ 91.055% (32401/35584)
01/05/2023 00:08:16 - INFO - __main__ -   test: [batch: 139/196 ] | Loss: 1.094 | Acc: 72.843% (26107/35840)/ 91.052% (32633/35840)
01/05/2023 00:08:16 - INFO - __main__ -   test: [batch: 140/196 ] | Loss: 1.096 | Acc: 72.809% (26281/36096)/ 91.038% (32861/36096)
01/05/2023 00:08:16 - INFO - __main__ -   test: [batch: 141/196 ] | Loss: 1.095 | Acc: 72.846% (26481/36352)/ 91.038% (33094/36352)
01/05/2023 00:08:17 - INFO - __main__ -   test: [batch: 142/196 ] | Loss: 1.102 | Acc: 72.760% (26636/36608)/ 90.956% (33297/36608)
01/05/2023 00:08:17 - INFO - __main__ -   test: [batch: 143/196 ] | Loss: 1.104 | Acc: 72.721% (26808/36864)/ 90.923% (33518/36864)
01/05/2023 00:08:17 - INFO - __main__ -   test: [batch: 144/196 ] | Loss: 1.106 | Acc: 72.680% (26979/37120)/ 90.884% (33736/37120)
01/05/2023 00:08:18 - INFO - __main__ -   test: [batch: 145/196 ] | Loss: 1.108 | Acc: 72.597% (27134/37376)/ 90.868% (33963/37376)
01/05/2023 00:08:18 - INFO - __main__ -   test: [batch: 146/196 ] | Loss: 1.110 | Acc: 72.537% (27297/37632)/ 90.843% (34186/37632)
01/05/2023 00:08:18 - INFO - __main__ -   test: [batch: 147/196 ] | Loss: 1.110 | Acc: 72.527% (27479/37888)/ 90.836% (34416/37888)
01/05/2023 00:08:19 - INFO - __main__ -   test: [batch: 148/196 ] | Loss: 1.113 | Acc: 72.475% (27645/38144)/ 90.801% (34635/38144)
01/05/2023 00:08:19 - INFO - __main__ -   test: [batch: 149/196 ] | Loss: 1.116 | Acc: 72.430% (27813/38400)/ 90.760% (34852/38400)
01/05/2023 00:08:19 - INFO - __main__ -   test: [batch: 150/196 ] | Loss: 1.118 | Acc: 72.444% (28004/38656)/ 90.731% (35073/38656)
01/05/2023 00:08:20 - INFO - __main__ -   test: [batch: 151/196 ] | Loss: 1.120 | Acc: 72.397% (28171/38912)/ 90.700% (35293/38912)
01/05/2023 00:08:20 - INFO - __main__ -   test: [batch: 152/196 ] | Loss: 1.120 | Acc: 72.388% (28353/39168)/ 90.699% (35525/39168)
01/05/2023 00:08:20 - INFO - __main__ -   test: [batch: 153/196 ] | Loss: 1.123 | Acc: 72.347% (28522/39424)/ 90.638% (35733/39424)
01/05/2023 00:08:21 - INFO - __main__ -   test: [batch: 154/196 ] | Loss: 1.125 | Acc: 72.311% (28693/39680)/ 90.597% (35949/39680)
01/05/2023 00:08:21 - INFO - __main__ -   test: [batch: 155/196 ] | Loss: 1.128 | Acc: 72.278% (28865/39936)/ 90.545% (36160/39936)
01/05/2023 00:08:21 - INFO - __main__ -   test: [batch: 156/196 ] | Loss: 1.127 | Acc: 72.325% (29069/40192)/ 90.553% (36395/40192)
01/05/2023 00:08:22 - INFO - __main__ -   test: [batch: 157/196 ] | Loss: 1.129 | Acc: 72.283% (29237/40448)/ 90.526% (36616/40448)
01/05/2023 00:08:22 - INFO - __main__ -   test: [batch: 158/196 ] | Loss: 1.134 | Acc: 72.162% (29373/40704)/ 90.480% (36829/40704)
01/05/2023 00:08:22 - INFO - __main__ -   test: [batch: 159/196 ] | Loss: 1.136 | Acc: 72.117% (29539/40960)/ 90.457% (37051/40960)
01/05/2023 00:08:23 - INFO - __main__ -   test: [batch: 160/196 ] | Loss: 1.135 | Acc: 72.137% (29732/41216)/ 90.460% (37284/41216)
01/05/2023 00:08:23 - INFO - __main__ -   test: [batch: 161/196 ] | Loss: 1.139 | Acc: 72.070% (29889/41472)/ 90.427% (37502/41472)
01/05/2023 00:08:23 - INFO - __main__ -   test: [batch: 162/196 ] | Loss: 1.139 | Acc: 72.062% (30070/41728)/ 90.414% (37728/41728)
01/05/2023 00:08:24 - INFO - __main__ -   test: [batch: 163/196 ] | Loss: 1.147 | Acc: 71.844% (30163/41984)/ 90.303% (37913/41984)
01/05/2023 00:08:24 - INFO - __main__ -   test: [batch: 164/196 ] | Loss: 1.150 | Acc: 71.771% (30316/42240)/ 90.263% (38127/42240)
01/05/2023 00:08:24 - INFO - __main__ -   test: [batch: 165/196 ] | Loss: 1.153 | Acc: 71.689% (30465/42496)/ 90.225% (38342/42496)
01/05/2023 00:08:25 - INFO - __main__ -   test: [batch: 166/196 ] | Loss: 1.152 | Acc: 71.693% (30650/42752)/ 90.230% (38575/42752)
01/05/2023 00:08:25 - INFO - __main__ -   test: [batch: 167/196 ] | Loss: 1.155 | Acc: 71.640% (30811/43008)/ 90.176% (38783/43008)
01/05/2023 00:08:25 - INFO - __main__ -   test: [batch: 168/196 ] | Loss: 1.155 | Acc: 71.607% (30980/43264)/ 90.163% (39008/43264)
01/05/2023 00:08:26 - INFO - __main__ -   test: [batch: 169/196 ] | Loss: 1.158 | Acc: 71.556% (31141/43520)/ 90.131% (39225/43520)
01/05/2023 00:08:26 - INFO - __main__ -   test: [batch: 170/196 ] | Loss: 1.156 | Acc: 71.580% (31335/43776)/ 90.159% (39468/43776)
01/05/2023 00:08:26 - INFO - __main__ -   test: [batch: 171/196 ] | Loss: 1.157 | Acc: 71.546% (31503/44032)/ 90.141% (39691/44032)
01/05/2023 00:08:27 - INFO - __main__ -   test: [batch: 172/196 ] | Loss: 1.162 | Acc: 71.471% (31653/44288)/ 90.085% (39897/44288)
01/05/2023 00:08:27 - INFO - __main__ -   test: [batch: 173/196 ] | Loss: 1.161 | Acc: 71.478% (31839/44544)/ 90.088% (40129/44544)
01/05/2023 00:08:27 - INFO - __main__ -   test: [batch: 174/196 ] | Loss: 1.163 | Acc: 71.422% (31997/44800)/ 90.076% (40354/44800)
01/05/2023 00:08:28 - INFO - __main__ -   test: [batch: 175/196 ] | Loss: 1.165 | Acc: 71.380% (32161/45056)/ 90.063% (40579/45056)
01/05/2023 00:08:28 - INFO - __main__ -   test: [batch: 176/196 ] | Loss: 1.168 | Acc: 71.343% (32327/45312)/ 90.016% (40788/45312)
01/05/2023 00:08:28 - INFO - __main__ -   test: [batch: 177/196 ] | Loss: 1.173 | Acc: 71.184% (32437/45568)/ 89.975% (41000/45568)
01/05/2023 00:08:29 - INFO - __main__ -   test: [batch: 178/196 ] | Loss: 1.171 | Acc: 71.218% (32635/45824)/ 89.997% (41240/45824)
01/05/2023 00:08:29 - INFO - __main__ -   test: [batch: 179/196 ] | Loss: 1.171 | Acc: 71.239% (32827/46080)/ 89.998% (41471/46080)
01/05/2023 00:08:29 - INFO - __main__ -   test: [batch: 180/196 ] | Loss: 1.171 | Acc: 71.215% (32998/46336)/ 90.006% (41705/46336)
01/05/2023 00:08:30 - INFO - __main__ -   test: [batch: 181/196 ] | Loss: 1.172 | Acc: 71.197% (33172/46592)/ 90.005% (41935/46592)
01/05/2023 00:08:30 - INFO - __main__ -   test: [batch: 182/196 ] | Loss: 1.170 | Acc: 71.228% (33369/46848)/ 90.015% (42170/46848)
01/05/2023 00:08:30 - INFO - __main__ -   test: [batch: 183/196 ] | Loss: 1.167 | Acc: 71.287% (33579/47104)/ 90.050% (42417/47104)
01/05/2023 00:08:31 - INFO - __main__ -   test: [batch: 184/196 ] | Loss: 1.165 | Acc: 71.313% (33774/47360)/ 90.074% (42659/47360)
01/05/2023 00:08:31 - INFO - __main__ -   test: [batch: 185/196 ] | Loss: 1.165 | Acc: 71.312% (33956/47616)/ 90.098% (42901/47616)
01/05/2023 00:08:31 - INFO - __main__ -   test: [batch: 186/196 ] | Loss: 1.161 | Acc: 71.409% (34185/47872)/ 90.132% (43148/47872)
01/05/2023 00:08:32 - INFO - __main__ -   test: [batch: 187/196 ] | Loss: 1.163 | Acc: 71.370% (34349/48128)/ 90.106% (43366/48128)
01/05/2023 00:08:32 - INFO - __main__ -   test: [batch: 188/196 ] | Loss: 1.164 | Acc: 71.367% (34530/48384)/ 90.090% (43589/48384)
01/05/2023 00:08:32 - INFO - __main__ -   test: [batch: 189/196 ] | Loss: 1.167 | Acc: 71.266% (34664/48640)/ 90.056% (43803/48640)
01/05/2023 00:08:33 - INFO - __main__ -   test: [batch: 190/196 ] | Loss: 1.168 | Acc: 71.227% (34827/48896)/ 90.063% (44037/48896)
01/05/2023 00:08:33 - INFO - __main__ -   test: [batch: 191/196 ] | Loss: 1.169 | Acc: 71.218% (35005/49152)/ 90.051% (44262/49152)
01/05/2023 00:08:33 - INFO - __main__ -   test: [batch: 192/196 ] | Loss: 1.165 | Acc: 71.290% (35223/49408)/ 90.091% (44512/49408)
01/05/2023 00:08:34 - INFO - __main__ -   test: [batch: 193/196 ] | Loss: 1.161 | Acc: 71.398% (35459/49664)/ 90.124% (44759/49664)
01/05/2023 00:08:34 - INFO - __main__ -   test: [batch: 194/196 ] | Loss: 1.158 | Acc: 71.454% (35670/49920)/ 90.156% (45006/49920)
01/05/2023 00:08:34 - INFO - __main__ -   test: [batch: 195/196 ] | Loss: 1.163 | Acc: 71.394% (35697/50000)/ 90.140% (45070/50000)
01/05/2023 00:08:34 - INFO - __main__ -   Final accuracy: 71.394
01/05/2023 00:08:34 - INFO - __main__ -   {'milestones': Counter({2: 1, 3: 1, 4: 1}), 'gamma': 0.1, 'base_lrs': [0.0005], 'last_epoch': 3, '_step_count': 4, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [5e-06]}
01/05/2023 00:08:34 - INFO - __main__ -   
Epoch: 3
01/05/2023 00:08:35 - INFO - __main__ -   test: [epoch: 3 | batch: 0/5005 ] | Loss: 0.986 | Acc: 76.562% (196/256)
01/05/2023 00:09:11 - INFO - __main__ -   test: [epoch: 3 | batch: 100/5005 ] | Loss: 1.163 | Acc: 71.651% (18526/25856)
01/05/2023 00:09:48 - INFO - __main__ -   test: [epoch: 3 | batch: 200/5005 ] | Loss: 1.184 | Acc: 71.286% (36681/51456)
01/05/2023 00:10:24 - INFO - __main__ -   test: [epoch: 3 | batch: 300/5005 ] | Loss: 1.190 | Acc: 71.211% (54872/77056)
01/05/2023 00:11:01 - INFO - __main__ -   test: [epoch: 3 | batch: 400/5005 ] | Loss: 1.192 | Acc: 71.136% (73025/102656)
01/05/2023 00:11:37 - INFO - __main__ -   test: [epoch: 3 | batch: 500/5005 ] | Loss: 1.191 | Acc: 71.203% (91322/128256)
01/05/2023 00:12:14 - INFO - __main__ -   test: [epoch: 3 | batch: 600/5005 ] | Loss: 1.192 | Acc: 71.161% (109485/153856)
01/05/2023 00:12:50 - INFO - __main__ -   test: [epoch: 3 | batch: 700/5005 ] | Loss: 1.188 | Acc: 71.230% (127826/179456)
01/05/2023 00:13:27 - INFO - __main__ -   test: [epoch: 3 | batch: 800/5005 ] | Loss: 1.190 | Acc: 71.187% (145973/205056)
01/05/2023 00:14:03 - INFO - __main__ -   test: [epoch: 3 | batch: 900/5005 ] | Loss: 1.187 | Acc: 71.264% (164374/230656)
01/05/2023 00:14:40 - INFO - __main__ -   test: [epoch: 3 | batch: 1000/5005 ] | Loss: 1.187 | Acc: 71.257% (182601/256256)
01/05/2023 00:15:16 - INFO - __main__ -   test: [epoch: 3 | batch: 1100/5005 ] | Loss: 1.187 | Acc: 71.269% (200877/281856)
01/05/2023 00:15:53 - INFO - __main__ -   test: [epoch: 3 | batch: 1200/5005 ] | Loss: 1.187 | Acc: 71.274% (219137/307456)
01/05/2023 00:16:29 - INFO - __main__ -   test: [epoch: 3 | batch: 1300/5005 ] | Loss: 1.187 | Acc: 71.287% (237425/333056)
01/05/2023 00:17:06 - INFO - __main__ -   test: [epoch: 3 | batch: 1400/5005 ] | Loss: 1.187 | Acc: 71.289% (255683/358656)
01/05/2023 00:17:42 - INFO - __main__ -   test: [epoch: 3 | batch: 1500/5005 ] | Loss: 1.186 | Acc: 71.293% (273946/384256)
01/05/2023 00:18:19 - INFO - __main__ -   test: [epoch: 3 | batch: 1600/5005 ] | Loss: 1.187 | Acc: 71.263% (292074/409856)
01/05/2023 00:18:55 - INFO - __main__ -   test: [epoch: 3 | batch: 1700/5005 ] | Loss: 1.188 | Acc: 71.264% (310322/435456)
01/05/2023 00:19:32 - INFO - __main__ -   test: [epoch: 3 | batch: 1800/5005 ] | Loss: 1.187 | Acc: 71.281% (328645/461056)
01/05/2023 00:20:08 - INFO - __main__ -   test: [epoch: 3 | batch: 1900/5005 ] | Loss: 1.188 | Acc: 71.255% (346766/486656)
01/05/2023 00:20:44 - INFO - __main__ -   test: [epoch: 3 | batch: 2000/5005 ] | Loss: 1.187 | Acc: 71.247% (364965/512256)
01/05/2023 00:21:21 - INFO - __main__ -   test: [epoch: 3 | batch: 2100/5005 ] | Loss: 1.188 | Acc: 71.240% (383168/537856)
01/05/2023 00:21:57 - INFO - __main__ -   test: [epoch: 3 | batch: 2200/5005 ] | Loss: 1.188 | Acc: 71.252% (401476/563456)
01/05/2023 00:22:34 - INFO - __main__ -   test: [epoch: 3 | batch: 2300/5005 ] | Loss: 1.188 | Acc: 71.256% (419739/589056)
01/05/2023 00:23:10 - INFO - __main__ -   test: [epoch: 3 | batch: 2400/5005 ] | Loss: 1.188 | Acc: 71.269% (438062/614656)
01/05/2023 00:23:47 - INFO - __main__ -   test: [epoch: 3 | batch: 2500/5005 ] | Loss: 1.188 | Acc: 71.278% (456362/640256)
01/05/2023 00:24:23 - INFO - __main__ -   test: [epoch: 3 | batch: 2600/5005 ] | Loss: 1.189 | Acc: 71.281% (474631/665856)
01/05/2023 00:25:00 - INFO - __main__ -   test: [epoch: 3 | batch: 2700/5005 ] | Loss: 1.189 | Acc: 71.277% (492852/691456)
01/05/2023 00:25:36 - INFO - __main__ -   test: [epoch: 3 | batch: 2800/5005 ] | Loss: 1.189 | Acc: 71.284% (511145/717056)
01/05/2023 00:26:13 - INFO - __main__ -   test: [epoch: 3 | batch: 2900/5005 ] | Loss: 1.189 | Acc: 71.297% (529488/742656)
01/05/2023 00:26:49 - INFO - __main__ -   test: [epoch: 3 | batch: 3000/5005 ] | Loss: 1.189 | Acc: 71.303% (547787/768256)
01/05/2023 00:27:26 - INFO - __main__ -   test: [epoch: 3 | batch: 3100/5005 ] | Loss: 1.189 | Acc: 71.309% (566094/793856)
01/05/2023 00:28:02 - INFO - __main__ -   test: [epoch: 3 | batch: 3200/5005 ] | Loss: 1.189 | Acc: 71.309% (584344/819456)
01/05/2023 00:28:39 - INFO - __main__ -   test: [epoch: 3 | batch: 3300/5005 ] | Loss: 1.189 | Acc: 71.307% (602582/845056)
01/05/2023 00:29:15 - INFO - __main__ -   test: [epoch: 3 | batch: 3400/5005 ] | Loss: 1.189 | Acc: 71.303% (620804/870656)
01/05/2023 00:29:51 - INFO - __main__ -   test: [epoch: 3 | batch: 3500/5005 ] | Loss: 1.189 | Acc: 71.298% (639015/896256)
01/05/2023 00:30:28 - INFO - __main__ -   test: [epoch: 3 | batch: 3600/5005 ] | Loss: 1.189 | Acc: 71.300% (657285/921856)
01/05/2023 00:31:04 - INFO - __main__ -   test: [epoch: 3 | batch: 3700/5005 ] | Loss: 1.188 | Acc: 71.307% (675598/947456)
01/05/2023 00:31:41 - INFO - __main__ -   test: [epoch: 3 | batch: 3800/5005 ] | Loss: 1.188 | Acc: 71.306% (693850/973056)
01/05/2023 00:32:17 - INFO - __main__ -   test: [epoch: 3 | batch: 3900/5005 ] | Loss: 1.189 | Acc: 71.293% (711969/998656)
01/05/2023 00:32:54 - INFO - __main__ -   test: [epoch: 3 | batch: 4000/5005 ] | Loss: 1.189 | Acc: 71.285% (730142/1024256)
01/05/2023 00:33:30 - INFO - __main__ -   test: [epoch: 3 | batch: 4100/5005 ] | Loss: 1.188 | Acc: 71.293% (748472/1049856)
01/05/2023 00:34:07 - INFO - __main__ -   test: [epoch: 3 | batch: 4200/5005 ] | Loss: 1.189 | Acc: 71.294% (766739/1075456)
01/05/2023 00:34:43 - INFO - __main__ -   test: [epoch: 3 | batch: 4300/5005 ] | Loss: 1.189 | Acc: 71.293% (784975/1101056)
01/05/2023 00:35:20 - INFO - __main__ -   test: [epoch: 3 | batch: 4400/5005 ] | Loss: 1.188 | Acc: 71.297% (803270/1126656)
01/05/2023 00:35:56 - INFO - __main__ -   test: [epoch: 3 | batch: 4500/5005 ] | Loss: 1.189 | Acc: 71.289% (821435/1152256)
01/05/2023 00:36:33 - INFO - __main__ -   test: [epoch: 3 | batch: 4600/5005 ] | Loss: 1.189 | Acc: 71.279% (839562/1177856)
01/05/2023 00:37:09 - INFO - __main__ -   test: [epoch: 3 | batch: 4700/5005 ] | Loss: 1.189 | Acc: 71.278% (857797/1203456)
01/05/2023 00:37:46 - INFO - __main__ -   test: [epoch: 3 | batch: 4800/5005 ] | Loss: 1.189 | Acc: 71.269% (875940/1229056)
01/05/2023 00:38:22 - INFO - __main__ -   test: [epoch: 3 | batch: 4900/5005 ] | Loss: 1.189 | Acc: 71.271% (894208/1254656)
01/05/2023 00:38:59 - INFO - __main__ -   test: [epoch: 3 | batch: 5000/5005 ] | Loss: 1.189 | Acc: 71.269% (912427/1280256)
01/05/2023 00:39:00 - INFO - __main__ -   Saving Checkpoint
01/05/2023 00:39:00 - INFO - __main__ -   test: [batch: 0/196 ] | Loss: 0.657 | Acc: 82.422% (211/256)/ 96.484% (247/256)
01/05/2023 00:39:01 - INFO - __main__ -   test: [batch: 1/196 ] | Loss: 0.706 | Acc: 80.859% (414/512)/ 94.922% (486/512)
01/05/2023 00:39:01 - INFO - __main__ -   test: [batch: 2/196 ] | Loss: 0.572 | Acc: 84.766% (651/768)/ 96.094% (738/768)
01/05/2023 00:39:01 - INFO - __main__ -   test: [batch: 3/196 ] | Loss: 0.562 | Acc: 85.254% (873/1024)/ 95.898% (982/1024)
01/05/2023 00:39:02 - INFO - __main__ -   test: [batch: 4/196 ] | Loss: 0.542 | Acc: 86.094% (1102/1280)/ 95.859% (1227/1280)
01/05/2023 00:39:02 - INFO - __main__ -   test: [batch: 5/196 ] | Loss: 0.597 | Acc: 85.286% (1310/1536)/ 95.247% (1463/1536)
01/05/2023 00:39:02 - INFO - __main__ -   test: [batch: 6/196 ] | Loss: 0.708 | Acc: 82.143% (1472/1792)/ 94.252% (1689/1792)
01/05/2023 00:39:03 - INFO - __main__ -   test: [batch: 7/196 ] | Loss: 0.737 | Acc: 80.566% (1650/2048)/ 94.385% (1933/2048)
01/05/2023 00:39:03 - INFO - __main__ -   test: [batch: 8/196 ] | Loss: 0.777 | Acc: 79.905% (1841/2304)/ 93.750% (2160/2304)
01/05/2023 00:39:03 - INFO - __main__ -   test: [batch: 9/196 ] | Loss: 0.809 | Acc: 79.023% (2023/2560)/ 93.477% (2393/2560)
01/05/2023 00:39:04 - INFO - __main__ -   test: [batch: 10/196 ] | Loss: 0.836 | Acc: 78.054% (2198/2816)/ 93.395% (2630/2816)
01/05/2023 00:39:04 - INFO - __main__ -   test: [batch: 11/196 ] | Loss: 0.882 | Acc: 77.116% (2369/3072)/ 93.066% (2859/3072)
01/05/2023 00:39:04 - INFO - __main__ -   test: [batch: 12/196 ] | Loss: 0.920 | Acc: 76.172% (2535/3328)/ 92.578% (3081/3328)
01/05/2023 00:39:05 - INFO - __main__ -   test: [batch: 13/196 ] | Loss: 0.930 | Acc: 75.725% (2714/3584)/ 92.494% (3315/3584)
01/05/2023 00:39:05 - INFO - __main__ -   test: [batch: 14/196 ] | Loss: 0.928 | Acc: 75.651% (2905/3840)/ 92.708% (3560/3840)
01/05/2023 00:39:05 - INFO - __main__ -   test: [batch: 15/196 ] | Loss: 0.937 | Acc: 75.854% (3107/4096)/ 92.529% (3790/4096)
01/05/2023 00:39:06 - INFO - __main__ -   test: [batch: 16/196 ] | Loss: 0.915 | Acc: 76.494% (3329/4352)/ 92.716% (4035/4352)
01/05/2023 00:39:06 - INFO - __main__ -   test: [batch: 17/196 ] | Loss: 0.886 | Acc: 77.387% (3566/4608)/ 92.925% (4282/4608)
01/05/2023 00:39:06 - INFO - __main__ -   test: [batch: 18/196 ] | Loss: 0.859 | Acc: 78.125% (3800/4864)/ 93.154% (4531/4864)
01/05/2023 00:39:07 - INFO - __main__ -   test: [batch: 19/196 ] | Loss: 0.849 | Acc: 78.301% (4009/5120)/ 93.281% (4776/5120)
01/05/2023 00:39:07 - INFO - __main__ -   test: [batch: 20/196 ] | Loss: 0.848 | Acc: 78.516% (4221/5376)/ 93.192% (5010/5376)
01/05/2023 00:39:08 - INFO - __main__ -   test: [batch: 21/196 ] | Loss: 0.846 | Acc: 78.569% (4425/5632)/ 93.288% (5254/5632)
01/05/2023 00:39:08 - INFO - __main__ -   test: [batch: 22/196 ] | Loss: 0.847 | Acc: 78.618% (4629/5888)/ 93.240% (5490/5888)
01/05/2023 00:39:08 - INFO - __main__ -   test: [batch: 23/196 ] | Loss: 0.850 | Acc: 78.630% (4831/6144)/ 93.294% (5732/6144)
01/05/2023 00:39:09 - INFO - __main__ -   test: [batch: 24/196 ] | Loss: 0.864 | Acc: 78.422% (5019/6400)/ 93.188% (5964/6400)
01/05/2023 00:39:09 - INFO - __main__ -   test: [batch: 25/196 ] | Loss: 0.847 | Acc: 78.891% (5251/6656)/ 93.299% (6210/6656)
01/05/2023 00:39:09 - INFO - __main__ -   test: [batch: 26/196 ] | Loss: 0.834 | Acc: 79.167% (5472/6912)/ 93.388% (6455/6912)
01/05/2023 00:39:10 - INFO - __main__ -   test: [batch: 27/196 ] | Loss: 0.820 | Acc: 79.450% (5695/7168)/ 93.485% (6701/7168)
01/05/2023 00:39:10 - INFO - __main__ -   test: [batch: 28/196 ] | Loss: 0.804 | Acc: 79.863% (5929/7424)/ 93.602% (6949/7424)
01/05/2023 00:39:10 - INFO - __main__ -   test: [batch: 29/196 ] | Loss: 0.805 | Acc: 79.753% (6125/7680)/ 93.633% (7191/7680)
01/05/2023 00:39:11 - INFO - __main__ -   test: [batch: 30/196 ] | Loss: 0.806 | Acc: 79.713% (6326/7936)/ 93.649% (7432/7936)
01/05/2023 00:39:11 - INFO - __main__ -   test: [batch: 31/196 ] | Loss: 0.815 | Acc: 79.504% (6513/8192)/ 93.640% (7671/8192)
01/05/2023 00:39:11 - INFO - __main__ -   test: [batch: 32/196 ] | Loss: 0.835 | Acc: 78.847% (6661/8448)/ 93.513% (7900/8448)
01/05/2023 00:39:12 - INFO - __main__ -   test: [batch: 33/196 ] | Loss: 0.842 | Acc: 78.699% (6850/8704)/ 93.497% (8138/8704)
01/05/2023 00:39:12 - INFO - __main__ -   test: [batch: 34/196 ] | Loss: 0.840 | Acc: 78.728% (7054/8960)/ 93.538% (8381/8960)
01/05/2023 00:39:12 - INFO - __main__ -   test: [batch: 35/196 ] | Loss: 0.843 | Acc: 78.570% (7241/9216)/ 93.566% (8623/9216)
01/05/2023 00:39:13 - INFO - __main__ -   test: [batch: 36/196 ] | Loss: 0.853 | Acc: 78.252% (7412/9472)/ 93.571% (8863/9472)
01/05/2023 00:39:13 - INFO - __main__ -   test: [batch: 37/196 ] | Loss: 0.859 | Acc: 78.074% (7595/9728)/ 93.452% (9091/9728)
01/05/2023 00:39:13 - INFO - __main__ -   test: [batch: 38/196 ] | Loss: 0.860 | Acc: 78.035% (7791/9984)/ 93.530% (9338/9984)
01/05/2023 00:39:14 - INFO - __main__ -   test: [batch: 39/196 ] | Loss: 0.860 | Acc: 77.920% (7979/10240)/ 93.623% (9587/10240)
01/05/2023 00:39:14 - INFO - __main__ -   test: [batch: 40/196 ] | Loss: 0.859 | Acc: 77.944% (8181/10496)/ 93.645% (9829/10496)
01/05/2023 00:39:14 - INFO - __main__ -   test: [batch: 41/196 ] | Loss: 0.860 | Acc: 77.930% (8379/10752)/ 93.629% (10067/10752)
01/05/2023 00:39:15 - INFO - __main__ -   test: [batch: 42/196 ] | Loss: 0.852 | Acc: 78.125% (8600/11008)/ 93.741% (10319/11008)
01/05/2023 00:39:15 - INFO - __main__ -   test: [batch: 43/196 ] | Loss: 0.857 | Acc: 78.063% (8793/11264)/ 93.670% (10551/11264)
01/05/2023 00:39:15 - INFO - __main__ -   test: [batch: 44/196 ] | Loss: 0.862 | Acc: 78.021% (8988/11520)/ 93.637% (10787/11520)
01/05/2023 00:39:16 - INFO - __main__ -   test: [batch: 45/196 ] | Loss: 0.864 | Acc: 77.904% (9174/11776)/ 93.648% (11028/11776)
01/05/2023 00:39:16 - INFO - __main__ -   test: [batch: 46/196 ] | Loss: 0.869 | Acc: 77.685% (9347/12032)/ 93.650% (11268/12032)
01/05/2023 00:39:16 - INFO - __main__ -   test: [batch: 47/196 ] | Loss: 0.867 | Acc: 77.629% (9539/12288)/ 93.677% (11511/12288)
01/05/2023 00:39:17 - INFO - __main__ -   test: [batch: 48/196 ] | Loss: 0.870 | Acc: 77.368% (9705/12544)/ 93.718% (11756/12544)
01/05/2023 00:39:17 - INFO - __main__ -   test: [batch: 49/196 ] | Loss: 0.861 | Acc: 77.633% (9937/12800)/ 93.797% (12006/12800)
01/05/2023 00:39:17 - INFO - __main__ -   test: [batch: 50/196 ] | Loss: 0.856 | Acc: 77.780% (10155/13056)/ 93.857% (12254/13056)
01/05/2023 00:39:18 - INFO - __main__ -   test: [batch: 51/196 ] | Loss: 0.854 | Acc: 77.764% (10352/13312)/ 93.885% (12498/13312)
01/05/2023 00:39:18 - INFO - __main__ -   test: [batch: 52/196 ] | Loss: 0.854 | Acc: 77.742% (10548/13568)/ 93.905% (12741/13568)
01/05/2023 00:39:18 - INFO - __main__ -   test: [batch: 53/196 ] | Loss: 0.857 | Acc: 77.756% (10749/13824)/ 93.873% (12977/13824)
01/05/2023 00:39:19 - INFO - __main__ -   test: [batch: 54/196 ] | Loss: 0.858 | Acc: 77.670% (10936/14080)/ 93.899% (13221/14080)
01/05/2023 00:39:19 - INFO - __main__ -   test: [batch: 55/196 ] | Loss: 0.864 | Acc: 77.448% (11103/14336)/ 93.869% (13457/14336)
01/05/2023 00:39:19 - INFO - __main__ -   test: [batch: 56/196 ] | Loss: 0.863 | Acc: 77.488% (11307/14592)/ 93.901% (13702/14592)
01/05/2023 00:39:20 - INFO - __main__ -   test: [batch: 57/196 ] | Loss: 0.855 | Acc: 77.721% (11540/14848)/ 93.986% (13955/14848)
01/05/2023 00:39:20 - INFO - __main__ -   test: [batch: 58/196 ] | Loss: 0.856 | Acc: 77.748% (11743/15104)/ 93.955% (14191/15104)
01/05/2023 00:39:20 - INFO - __main__ -   test: [batch: 59/196 ] | Loss: 0.858 | Acc: 77.676% (11931/15360)/ 94.004% (14439/15360)
01/05/2023 00:39:21 - INFO - __main__ -   test: [batch: 60/196 ] | Loss: 0.862 | Acc: 77.561% (12112/15616)/ 93.968% (14674/15616)
01/05/2023 00:39:21 - INFO - __main__ -   test: [batch: 61/196 ] | Loss: 0.866 | Acc: 77.457% (12294/15872)/ 93.920% (14907/15872)
01/05/2023 00:39:21 - INFO - __main__ -   test: [batch: 62/196 ] | Loss: 0.861 | Acc: 77.573% (12511/16128)/ 93.955% (15153/16128)
01/05/2023 00:39:22 - INFO - __main__ -   test: [batch: 63/196 ] | Loss: 0.854 | Acc: 77.747% (12738/16384)/ 94.006% (15402/16384)
01/05/2023 00:39:22 - INFO - __main__ -   test: [batch: 64/196 ] | Loss: 0.854 | Acc: 77.794% (12945/16640)/ 94.032% (15647/16640)
01/05/2023 00:39:22 - INFO - __main__ -   test: [batch: 65/196 ] | Loss: 0.850 | Acc: 77.918% (13165/16896)/ 94.070% (15894/16896)
01/05/2023 00:39:23 - INFO - __main__ -   test: [batch: 66/196 ] | Loss: 0.849 | Acc: 77.950% (13370/17152)/ 94.082% (16137/17152)
01/05/2023 00:39:23 - INFO - __main__ -   test: [batch: 67/196 ] | Loss: 0.847 | Acc: 77.981% (13575/17408)/ 94.100% (16381/17408)
01/05/2023 00:39:23 - INFO - __main__ -   test: [batch: 68/196 ] | Loss: 0.844 | Acc: 77.961% (13771/17664)/ 94.141% (16629/17664)
01/05/2023 00:39:24 - INFO - __main__ -   test: [batch: 69/196 ] | Loss: 0.848 | Acc: 77.846% (13950/17920)/ 94.113% (16865/17920)
01/05/2023 00:39:24 - INFO - __main__ -   test: [batch: 70/196 ] | Loss: 0.847 | Acc: 77.833% (14147/18176)/ 94.124% (17108/18176)
01/05/2023 00:39:24 - INFO - __main__ -   test: [batch: 71/196 ] | Loss: 0.844 | Acc: 77.935% (14365/18432)/ 94.141% (17352/18432)
01/05/2023 00:39:25 - INFO - __main__ -   test: [batch: 72/196 ] | Loss: 0.849 | Acc: 77.922% (14562/18688)/ 94.082% (17582/18688)
01/05/2023 00:39:25 - INFO - __main__ -   test: [batch: 73/196 ] | Loss: 0.850 | Acc: 77.935% (14764/18944)/ 94.061% (17819/18944)
01/05/2023 00:39:25 - INFO - __main__ -   test: [batch: 74/196 ] | Loss: 0.856 | Acc: 77.771% (14932/19200)/ 94.010% (18050/19200)
01/05/2023 00:39:26 - INFO - __main__ -   test: [batch: 75/196 ] | Loss: 0.855 | Acc: 77.775% (15132/19456)/ 94.012% (18291/19456)
01/05/2023 00:39:26 - INFO - __main__ -   test: [batch: 76/196 ] | Loss: 0.854 | Acc: 77.800% (15336/19712)/ 94.019% (18533/19712)
01/05/2023 00:39:26 - INFO - __main__ -   test: [batch: 77/196 ] | Loss: 0.856 | Acc: 77.799% (15535/19968)/ 93.985% (18767/19968)
01/05/2023 00:39:27 - INFO - __main__ -   test: [batch: 78/196 ] | Loss: 0.862 | Acc: 77.640% (15702/20224)/ 93.893% (18989/20224)
01/05/2023 00:39:27 - INFO - __main__ -   test: [batch: 79/196 ] | Loss: 0.865 | Acc: 77.583% (15889/20480)/ 93.877% (19226/20480)
01/05/2023 00:39:27 - INFO - __main__ -   test: [batch: 80/196 ] | Loss: 0.874 | Acc: 77.363% (16042/20736)/ 93.789% (19448/20736)
01/05/2023 00:39:28 - INFO - __main__ -   test: [batch: 81/196 ] | Loss: 0.883 | Acc: 77.182% (16202/20992)/ 93.688% (19667/20992)
01/05/2023 00:39:28 - INFO - __main__ -   test: [batch: 82/196 ] | Loss: 0.887 | Acc: 77.061% (16374/21248)/ 93.656% (19900/21248)
01/05/2023 00:39:28 - INFO - __main__ -   test: [batch: 83/196 ] | Loss: 0.889 | Acc: 77.051% (16569/21504)/ 93.638% (20136/21504)
01/05/2023 00:39:29 - INFO - __main__ -   test: [batch: 84/196 ] | Loss: 0.892 | Acc: 76.976% (16750/21760)/ 93.571% (20361/21760)
01/05/2023 00:39:29 - INFO - __main__ -   test: [batch: 85/196 ] | Loss: 0.901 | Acc: 76.767% (16901/22016)/ 93.477% (20580/22016)
01/05/2023 00:39:29 - INFO - __main__ -   test: [batch: 86/196 ] | Loss: 0.905 | Acc: 76.652% (17072/22272)/ 93.404% (20803/22272)
01/05/2023 00:39:30 - INFO - __main__ -   test: [batch: 87/196 ] | Loss: 0.909 | Acc: 76.536% (17242/22528)/ 93.368% (21034/22528)
01/05/2023 00:39:30 - INFO - __main__ -   test: [batch: 88/196 ] | Loss: 0.915 | Acc: 76.391% (17405/22784)/ 93.298% (21257/22784)
01/05/2023 00:39:30 - INFO - __main__ -   test: [batch: 89/196 ] | Loss: 0.921 | Acc: 76.259% (17570/23040)/ 93.212% (21476/23040)
01/05/2023 00:39:31 - INFO - __main__ -   test: [batch: 90/196 ] | Loss: 0.934 | Acc: 76.009% (17707/23296)/ 93.076% (21683/23296)
01/05/2023 00:39:31 - INFO - __main__ -   test: [batch: 91/196 ] | Loss: 0.940 | Acc: 75.904% (17877/23552)/ 92.973% (21897/23552)
01/05/2023 00:39:31 - INFO - __main__ -   test: [batch: 92/196 ] | Loss: 0.940 | Acc: 75.937% (18079/23808)/ 92.931% (22125/23808)
01/05/2023 00:39:32 - INFO - __main__ -   test: [batch: 93/196 ] | Loss: 0.948 | Acc: 75.790% (18238/24064)/ 92.819% (22336/24064)
01/05/2023 00:39:32 - INFO - __main__ -   test: [batch: 94/196 ] | Loss: 0.953 | Acc: 75.617% (18390/24320)/ 92.792% (22567/24320)
01/05/2023 00:39:32 - INFO - __main__ -   test: [batch: 95/196 ] | Loss: 0.961 | Acc: 75.492% (18553/24576)/ 92.708% (22784/24576)
01/05/2023 00:39:33 - INFO - __main__ -   test: [batch: 96/196 ] | Loss: 0.968 | Acc: 75.354% (18712/24832)/ 92.614% (22998/24832)
01/05/2023 00:39:33 - INFO - __main__ -   test: [batch: 97/196 ] | Loss: 0.977 | Acc: 75.195% (18865/25088)/ 92.502% (23207/25088)
01/05/2023 00:39:33 - INFO - __main__ -   test: [batch: 98/196 ] | Loss: 0.987 | Acc: 74.980% (19003/25344)/ 92.385% (23414/25344)
01/05/2023 00:39:34 - INFO - __main__ -   test: [batch: 99/196 ] | Loss: 0.988 | Acc: 74.930% (19182/25600)/ 92.367% (23646/25600)
01/05/2023 00:39:34 - INFO - __main__ -   test: [batch: 100/196 ] | Loss: 0.993 | Acc: 74.803% (19341/25856)/ 92.307% (23867/25856)
01/05/2023 00:39:34 - INFO - __main__ -   test: [batch: 101/196 ] | Loss: 0.994 | Acc: 74.789% (19529/26112)/ 92.283% (24097/26112)
01/05/2023 00:39:35 - INFO - __main__ -   test: [batch: 102/196 ] | Loss: 0.999 | Acc: 74.670% (19689/26368)/ 92.222% (24317/26368)
01/05/2023 00:39:35 - INFO - __main__ -   test: [batch: 103/196 ] | Loss: 1.004 | Acc: 74.572% (19854/26624)/ 92.176% (24541/26624)
01/05/2023 00:39:35 - INFO - __main__ -   test: [batch: 104/196 ] | Loss: 1.005 | Acc: 74.550% (20039/26880)/ 92.165% (24774/26880)
01/05/2023 00:39:36 - INFO - __main__ -   test: [batch: 105/196 ] | Loss: 1.010 | Acc: 74.458% (20205/27136)/ 92.099% (24992/27136)
01/05/2023 00:39:36 - INFO - __main__ -   test: [batch: 106/196 ] | Loss: 1.012 | Acc: 74.398% (20379/27392)/ 92.071% (25220/27392)
01/05/2023 00:39:36 - INFO - __main__ -   test: [batch: 107/196 ] | Loss: 1.013 | Acc: 74.396% (20569/27648)/ 92.046% (25449/27648)
01/05/2023 00:39:37 - INFO - __main__ -   test: [batch: 108/196 ] | Loss: 1.015 | Acc: 74.391% (20758/27904)/ 92.019% (25677/27904)
01/05/2023 00:39:37 - INFO - __main__ -   test: [batch: 109/196 ] | Loss: 1.017 | Acc: 74.364% (20941/28160)/ 91.971% (25899/28160)
01/05/2023 00:39:37 - INFO - __main__ -   test: [batch: 110/196 ] | Loss: 1.018 | Acc: 74.363% (21131/28416)/ 91.955% (26130/28416)
01/05/2023 00:39:38 - INFO - __main__ -   test: [batch: 111/196 ] | Loss: 1.017 | Acc: 74.404% (21333/28672)/ 91.978% (26372/28672)
01/05/2023 00:39:38 - INFO - __main__ -   test: [batch: 112/196 ] | Loss: 1.014 | Acc: 74.488% (21548/28928)/ 91.987% (26610/28928)
01/05/2023 00:39:39 - INFO - __main__ -   test: [batch: 113/196 ] | Loss: 1.016 | Acc: 74.476% (21735/29184)/ 91.992% (26847/29184)
01/05/2023 00:39:39 - INFO - __main__ -   test: [batch: 114/196 ] | Loss: 1.024 | Acc: 74.307% (21876/29440)/ 91.872% (27047/29440)
01/05/2023 00:39:39 - INFO - __main__ -   test: [batch: 115/196 ] | Loss: 1.029 | Acc: 74.188% (22031/29696)/ 91.794% (27259/29696)
01/05/2023 00:39:40 - INFO - __main__ -   test: [batch: 116/196 ] | Loss: 1.031 | Acc: 74.152% (22210/29952)/ 91.747% (27480/29952)
01/05/2023 00:39:40 - INFO - __main__ -   test: [batch: 117/196 ] | Loss: 1.038 | Acc: 74.017% (22359/30208)/ 91.661% (27689/30208)
01/05/2023 00:39:40 - INFO - __main__ -   test: [batch: 118/196 ] | Loss: 1.037 | Acc: 74.074% (22566/30464)/ 91.639% (27917/30464)
01/05/2023 00:39:41 - INFO - __main__ -   test: [batch: 119/196 ] | Loss: 1.036 | Acc: 74.141% (22776/30720)/ 91.644% (28153/30720)
01/05/2023 00:39:41 - INFO - __main__ -   test: [batch: 120/196 ] | Loss: 1.041 | Acc: 74.044% (22936/30976)/ 91.568% (28364/30976)
01/05/2023 00:39:41 - INFO - __main__ -   test: [batch: 121/196 ] | Loss: 1.049 | Acc: 73.815% (23054/31232)/ 91.461% (28565/31232)
01/05/2023 00:39:42 - INFO - __main__ -   test: [batch: 122/196 ] | Loss: 1.049 | Acc: 73.834% (23249/31488)/ 91.438% (28792/31488)
01/05/2023 00:39:42 - INFO - __main__ -   test: [batch: 123/196 ] | Loss: 1.057 | Acc: 73.740% (23408/31744)/ 91.337% (28994/31744)
01/05/2023 00:39:42 - INFO - __main__ -   test: [batch: 124/196 ] | Loss: 1.060 | Acc: 73.578% (23545/32000)/ 91.309% (29219/32000)
01/05/2023 00:39:43 - INFO - __main__ -   test: [batch: 125/196 ] | Loss: 1.062 | Acc: 73.562% (23728/32256)/ 91.298% (29449/32256)
01/05/2023 00:39:43 - INFO - __main__ -   test: [batch: 126/196 ] | Loss: 1.063 | Acc: 73.545% (23911/32512)/ 91.268% (29673/32512)
01/05/2023 00:39:43 - INFO - __main__ -   test: [batch: 127/196 ] | Loss: 1.069 | Acc: 73.447% (24067/32768)/ 91.193% (29882/32768)
01/05/2023 00:39:44 - INFO - __main__ -   test: [batch: 128/196 ] | Loss: 1.074 | Acc: 73.332% (24217/33024)/ 91.146% (30100/33024)
01/05/2023 00:39:44 - INFO - __main__ -   test: [batch: 129/196 ] | Loss: 1.077 | Acc: 73.221% (24368/33280)/ 91.130% (30328/33280)
01/05/2023 00:39:44 - INFO - __main__ -   test: [batch: 130/196 ] | Loss: 1.076 | Acc: 73.220% (24555/33536)/ 91.150% (30568/33536)
01/05/2023 00:39:45 - INFO - __main__ -   test: [batch: 131/196 ] | Loss: 1.078 | Acc: 73.165% (24724/33792)/ 91.137% (30797/33792)
01/05/2023 00:39:45 - INFO - __main__ -   test: [batch: 132/196 ] | Loss: 1.083 | Acc: 73.097% (24888/34048)/ 91.066% (31006/34048)
01/05/2023 00:39:45 - INFO - __main__ -   test: [batch: 133/196 ] | Loss: 1.085 | Acc: 73.061% (25063/34304)/ 91.045% (31232/34304)
01/05/2023 00:39:46 - INFO - __main__ -   test: [batch: 134/196 ] | Loss: 1.087 | Acc: 73.003% (25230/34560)/ 91.024% (31458/34560)
01/05/2023 00:39:46 - INFO - __main__ -   test: [batch: 135/196 ] | Loss: 1.091 | Acc: 72.964% (25403/34816)/ 90.978% (31675/34816)
01/05/2023 00:39:46 - INFO - __main__ -   test: [batch: 136/196 ] | Loss: 1.092 | Acc: 72.913% (25572/35072)/ 90.956% (31900/35072)
01/05/2023 00:39:47 - INFO - __main__ -   test: [batch: 137/196 ] | Loss: 1.094 | Acc: 72.886% (25749/35328)/ 90.951% (32131/35328)
01/05/2023 00:39:47 - INFO - __main__ -   test: [batch: 138/196 ] | Loss: 1.097 | Acc: 72.802% (25906/35584)/ 90.926% (32355/35584)
01/05/2023 00:39:47 - INFO - __main__ -   test: [batch: 139/196 ] | Loss: 1.097 | Acc: 72.829% (26102/35840)/ 90.912% (32583/35840)
01/05/2023 00:39:48 - INFO - __main__ -   test: [batch: 140/196 ] | Loss: 1.099 | Acc: 72.789% (26274/36096)/ 90.894% (32809/36096)
01/05/2023 00:39:48 - INFO - __main__ -   test: [batch: 141/196 ] | Loss: 1.098 | Acc: 72.824% (26473/36352)/ 90.889% (33040/36352)
01/05/2023 00:39:48 - INFO - __main__ -   test: [batch: 142/196 ] | Loss: 1.105 | Acc: 72.738% (26628/36608)/ 90.805% (33242/36608)
01/05/2023 00:39:49 - INFO - __main__ -   test: [batch: 143/196 ] | Loss: 1.106 | Acc: 72.708% (26803/36864)/ 90.777% (33464/36864)
01/05/2023 00:39:49 - INFO - __main__ -   test: [batch: 144/196 ] | Loss: 1.108 | Acc: 72.686% (26981/37120)/ 90.738% (33682/37120)
01/05/2023 00:39:49 - INFO - __main__ -   test: [batch: 145/196 ] | Loss: 1.110 | Acc: 72.611% (27139/37376)/ 90.716% (33906/37376)
01/05/2023 00:39:50 - INFO - __main__ -   test: [batch: 146/196 ] | Loss: 1.113 | Acc: 72.534% (27296/37632)/ 90.699% (34132/37632)
01/05/2023 00:39:50 - INFO - __main__ -   test: [batch: 147/196 ] | Loss: 1.112 | Acc: 72.530% (27480/37888)/ 90.694% (34362/37888)
01/05/2023 00:39:50 - INFO - __main__ -   test: [batch: 148/196 ] | Loss: 1.116 | Acc: 72.475% (27645/38144)/ 90.659% (34581/38144)
01/05/2023 00:39:51 - INFO - __main__ -   test: [batch: 149/196 ] | Loss: 1.118 | Acc: 72.443% (27818/38400)/ 90.628% (34801/38400)
01/05/2023 00:39:51 - INFO - __main__ -   test: [batch: 150/196 ] | Loss: 1.120 | Acc: 72.457% (28009/38656)/ 90.602% (35023/38656)
01/05/2023 00:39:51 - INFO - __main__ -   test: [batch: 151/196 ] | Loss: 1.122 | Acc: 72.420% (28180/38912)/ 90.558% (35238/38912)
01/05/2023 00:39:52 - INFO - __main__ -   test: [batch: 152/196 ] | Loss: 1.122 | Acc: 72.411% (28362/39168)/ 90.554% (35468/39168)
01/05/2023 00:39:52 - INFO - __main__ -   test: [batch: 153/196 ] | Loss: 1.125 | Acc: 72.362% (28528/39424)/ 90.506% (35681/39424)
01/05/2023 00:39:52 - INFO - __main__ -   test: [batch: 154/196 ] | Loss: 1.127 | Acc: 72.336% (28703/39680)/ 90.474% (35900/39680)
01/05/2023 00:39:53 - INFO - __main__ -   test: [batch: 155/196 ] | Loss: 1.130 | Acc: 72.296% (28872/39936)/ 90.417% (36109/39936)
01/05/2023 00:39:53 - INFO - __main__ -   test: [batch: 156/196 ] | Loss: 1.129 | Acc: 72.345% (29077/40192)/ 90.431% (36346/40192)
01/05/2023 00:39:53 - INFO - __main__ -   test: [batch: 157/196 ] | Loss: 1.131 | Acc: 72.305% (29246/40448)/ 90.407% (36568/40448)
01/05/2023 00:39:54 - INFO - __main__ -   test: [batch: 158/196 ] | Loss: 1.136 | Acc: 72.199% (29388/40704)/ 90.357% (36779/40704)
01/05/2023 00:39:54 - INFO - __main__ -   test: [batch: 159/196 ] | Loss: 1.138 | Acc: 72.161% (29557/40960)/ 90.330% (36999/40960)
01/05/2023 00:39:54 - INFO - __main__ -   test: [batch: 160/196 ] | Loss: 1.137 | Acc: 72.173% (29747/41216)/ 90.327% (37229/41216)
01/05/2023 00:39:55 - INFO - __main__ -   test: [batch: 161/196 ] | Loss: 1.140 | Acc: 72.109% (29905/41472)/ 90.295% (37447/41472)
01/05/2023 00:39:55 - INFO - __main__ -   test: [batch: 162/196 ] | Loss: 1.141 | Acc: 72.115% (30092/41728)/ 90.292% (37677/41728)
01/05/2023 00:39:55 - INFO - __main__ -   test: [batch: 163/196 ] | Loss: 1.148 | Acc: 71.904% (30188/41984)/ 90.189% (37865/41984)
01/05/2023 00:39:56 - INFO - __main__ -   test: [batch: 164/196 ] | Loss: 1.151 | Acc: 71.825% (30339/42240)/ 90.156% (38082/42240)
01/05/2023 00:39:56 - INFO - __main__ -   test: [batch: 165/196 ] | Loss: 1.154 | Acc: 71.739% (30486/42496)/ 90.128% (38301/42496)
01/05/2023 00:39:56 - INFO - __main__ -   test: [batch: 166/196 ] | Loss: 1.153 | Acc: 71.749% (30674/42752)/ 90.134% (38534/42752)
01/05/2023 00:39:57 - INFO - __main__ -   test: [batch: 167/196 ] | Loss: 1.156 | Acc: 71.691% (30833/43008)/ 90.079% (38741/43008)
01/05/2023 00:39:57 - INFO - __main__ -   test: [batch: 168/196 ] | Loss: 1.157 | Acc: 71.655% (31001/43264)/ 90.070% (38968/43264)
01/05/2023 00:39:57 - INFO - __main__ -   test: [batch: 169/196 ] | Loss: 1.160 | Acc: 71.606% (31163/43520)/ 90.032% (39182/43520)
01/05/2023 00:39:58 - INFO - __main__ -   test: [batch: 170/196 ] | Loss: 1.157 | Acc: 71.635% (31359/43776)/ 90.068% (39428/43776)
01/05/2023 00:39:58 - INFO - __main__ -   test: [batch: 171/196 ] | Loss: 1.158 | Acc: 71.589% (31522/44032)/ 90.037% (39645/44032)
01/05/2023 00:39:58 - INFO - __main__ -   test: [batch: 172/196 ] | Loss: 1.163 | Acc: 71.518% (31674/44288)/ 89.970% (39846/44288)
01/05/2023 00:39:59 - INFO - __main__ -   test: [batch: 173/196 ] | Loss: 1.163 | Acc: 71.518% (31857/44544)/ 89.978% (40080/44544)
01/05/2023 00:39:59 - INFO - __main__ -   test: [batch: 174/196 ] | Loss: 1.164 | Acc: 71.458% (32013/44800)/ 89.969% (40306/44800)
01/05/2023 00:39:59 - INFO - __main__ -   test: [batch: 175/196 ] | Loss: 1.166 | Acc: 71.409% (32174/45056)/ 89.946% (40526/45056)
01/05/2023 00:40:00 - INFO - __main__ -   test: [batch: 176/196 ] | Loss: 1.169 | Acc: 71.372% (32340/45312)/ 89.892% (40732/45312)
01/05/2023 00:40:00 - INFO - __main__ -   test: [batch: 177/196 ] | Loss: 1.175 | Acc: 71.212% (32450/45568)/ 89.861% (40948/45568)
01/05/2023 00:40:00 - INFO - __main__ -   test: [batch: 178/196 ] | Loss: 1.173 | Acc: 71.255% (32652/45824)/ 89.881% (41187/45824)
01/05/2023 00:40:01 - INFO - __main__ -   test: [batch: 179/196 ] | Loss: 1.172 | Acc: 71.276% (32844/46080)/ 89.887% (41420/46080)
01/05/2023 00:40:01 - INFO - __main__ -   test: [batch: 180/196 ] | Loss: 1.173 | Acc: 71.262% (33020/46336)/ 89.896% (41654/46336)
01/05/2023 00:40:01 - INFO - __main__ -   test: [batch: 181/196 ] | Loss: 1.173 | Acc: 71.235% (33190/46592)/ 89.897% (41885/46592)
01/05/2023 00:40:02 - INFO - __main__ -   test: [batch: 182/196 ] | Loss: 1.172 | Acc: 71.262% (33385/46848)/ 89.910% (42121/46848)
01/05/2023 00:40:02 - INFO - __main__ -   test: [batch: 183/196 ] | Loss: 1.169 | Acc: 71.329% (33599/47104)/ 89.944% (42367/47104)
01/05/2023 00:40:02 - INFO - __main__ -   test: [batch: 184/196 ] | Loss: 1.167 | Acc: 71.358% (33795/47360)/ 89.970% (42610/47360)
01/05/2023 00:40:03 - INFO - __main__ -   test: [batch: 185/196 ] | Loss: 1.167 | Acc: 71.365% (33981/47616)/ 89.993% (42851/47616)
01/05/2023 00:40:03 - INFO - __main__ -   test: [batch: 186/196 ] | Loss: 1.163 | Acc: 71.451% (34205/47872)/ 90.030% (43099/47872)
01/05/2023 00:40:03 - INFO - __main__ -   test: [batch: 187/196 ] | Loss: 1.165 | Acc: 71.412% (34369/48128)/ 90.000% (43315/48128)
01/05/2023 00:40:04 - INFO - __main__ -   test: [batch: 188/196 ] | Loss: 1.165 | Acc: 71.404% (34548/48384)/ 89.990% (43541/48384)
01/05/2023 00:40:04 - INFO - __main__ -   test: [batch: 189/196 ] | Loss: 1.169 | Acc: 71.295% (34678/48640)/ 89.963% (43758/48640)
01/05/2023 00:40:04 - INFO - __main__ -   test: [batch: 190/196 ] | Loss: 1.170 | Acc: 71.251% (34839/48896)/ 89.964% (43989/48896)
01/05/2023 00:40:05 - INFO - __main__ -   test: [batch: 191/196 ] | Loss: 1.171 | Acc: 71.246% (35019/49152)/ 89.960% (44217/49152)
01/05/2023 00:40:05 - INFO - __main__ -   test: [batch: 192/196 ] | Loss: 1.167 | Acc: 71.320% (35238/49408)/ 90.000% (44467/49408)
01/05/2023 00:40:05 - INFO - __main__ -   test: [batch: 193/196 ] | Loss: 1.163 | Acc: 71.430% (35475/49664)/ 90.033% (44714/49664)
01/05/2023 00:40:06 - INFO - __main__ -   test: [batch: 194/196 ] | Loss: 1.160 | Acc: 71.494% (35690/49920)/ 90.066% (44961/49920)
01/05/2023 00:40:06 - INFO - __main__ -   test: [batch: 195/196 ] | Loss: 1.165 | Acc: 71.436% (35718/50000)/ 90.054% (45027/50000)
01/05/2023 00:40:06 - INFO - __main__ -   Final accuracy: 71.436
01/05/2023 00:40:06 - INFO - __main__ -   {'milestones': Counter({2: 1, 3: 1, 4: 1}), 'gamma': 0.1, 'base_lrs': [0.0005], 'last_epoch': 4, '_step_count': 5, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [5.000000000000001e-07]}
01/05/2023 00:40:06 - INFO - __main__ -   
Epoch: 4
01/05/2023 00:40:06 - INFO - __main__ -   test: [epoch: 4 | batch: 0/5005 ] | Loss: 1.372 | Acc: 68.750% (176/256)
01/05/2023 00:40:43 - INFO - __main__ -   test: [epoch: 4 | batch: 100/5005 ] | Loss: 1.164 | Acc: 71.848% (18577/25856)
01/05/2023 00:41:19 - INFO - __main__ -   test: [epoch: 4 | batch: 200/5005 ] | Loss: 1.177 | Acc: 71.418% (36749/51456)
01/05/2023 00:41:56 - INFO - __main__ -   test: [epoch: 4 | batch: 300/5005 ] | Loss: 1.182 | Acc: 71.285% (54929/77056)
01/05/2023 00:42:32 - INFO - __main__ -   test: [epoch: 4 | batch: 400/5005 ] | Loss: 1.185 | Acc: 71.261% (73154/102656)
01/05/2023 00:43:09 - INFO - __main__ -   test: [epoch: 4 | batch: 500/5005 ] | Loss: 1.186 | Acc: 71.211% (91333/128256)
01/05/2023 00:43:45 - INFO - __main__ -   test: [epoch: 4 | batch: 600/5005 ] | Loss: 1.189 | Acc: 71.144% (109460/153856)
01/05/2023 00:44:22 - INFO - __main__ -   test: [epoch: 4 | batch: 700/5005 ] | Loss: 1.188 | Acc: 71.163% (127707/179456)
01/05/2023 00:44:58 - INFO - __main__ -   test: [epoch: 4 | batch: 800/5005 ] | Loss: 1.188 | Acc: 71.201% (146001/205056)
01/05/2023 00:45:35 - INFO - __main__ -   test: [epoch: 4 | batch: 900/5005 ] | Loss: 1.185 | Acc: 71.276% (164402/230656)
01/05/2023 00:46:11 - INFO - __main__ -   test: [epoch: 4 | batch: 1000/5005 ] | Loss: 1.185 | Acc: 71.249% (182579/256256)
01/05/2023 00:46:48 - INFO - __main__ -   test: [epoch: 4 | batch: 1100/5005 ] | Loss: 1.185 | Acc: 71.215% (200723/281856)
01/05/2023 00:47:24 - INFO - __main__ -   test: [epoch: 4 | batch: 1200/5005 ] | Loss: 1.186 | Acc: 71.240% (219033/307456)
01/05/2023 00:48:01 - INFO - __main__ -   test: [epoch: 4 | batch: 1300/5005 ] | Loss: 1.187 | Acc: 71.200% (237136/333056)
01/05/2023 00:48:37 - INFO - __main__ -   test: [epoch: 4 | batch: 1400/5005 ] | Loss: 1.187 | Acc: 71.177% (255280/358656)
01/05/2023 00:49:14 - INFO - __main__ -   test: [epoch: 4 | batch: 1500/5005 ] | Loss: 1.188 | Acc: 71.165% (273457/384256)
01/05/2023 00:49:50 - INFO - __main__ -   test: [epoch: 4 | batch: 1600/5005 ] | Loss: 1.188 | Acc: 71.168% (291687/409856)
01/05/2023 00:50:26 - INFO - __main__ -   test: [epoch: 4 | batch: 1700/5005 ] | Loss: 1.189 | Acc: 71.136% (309764/435456)
01/05/2023 00:51:03 - INFO - __main__ -   test: [epoch: 4 | batch: 1800/5005 ] | Loss: 1.188 | Acc: 71.154% (328059/461056)
01/05/2023 00:51:39 - INFO - __main__ -   test: [epoch: 4 | batch: 1900/5005 ] | Loss: 1.188 | Acc: 71.152% (346265/486656)
01/05/2023 00:52:16 - INFO - __main__ -   test: [epoch: 4 | batch: 2000/5005 ] | Loss: 1.189 | Acc: 71.142% (364429/512256)
01/05/2023 00:52:52 - INFO - __main__ -   test: [epoch: 4 | batch: 2100/5005 ] | Loss: 1.188 | Acc: 71.146% (382661/537856)
01/05/2023 00:53:29 - INFO - __main__ -   test: [epoch: 4 | batch: 2200/5005 ] | Loss: 1.188 | Acc: 71.162% (400968/563456)
01/05/2023 00:54:05 - INFO - __main__ -   test: [epoch: 4 | batch: 2300/5005 ] | Loss: 1.188 | Acc: 71.179% (419285/589056)
01/05/2023 00:54:42 - INFO - __main__ -   test: [epoch: 4 | batch: 2400/5005 ] | Loss: 1.187 | Acc: 71.186% (437548/614656)
01/05/2023 00:55:18 - INFO - __main__ -   test: [epoch: 4 | batch: 2500/5005 ] | Loss: 1.187 | Acc: 71.193% (455815/640256)
01/05/2023 00:55:55 - INFO - __main__ -   test: [epoch: 4 | batch: 2600/5005 ] | Loss: 1.188 | Acc: 71.201% (474093/665856)
01/05/2023 00:56:31 - INFO - __main__ -   test: [epoch: 4 | batch: 2700/5005 ] | Loss: 1.188 | Acc: 71.193% (492269/691456)
01/05/2023 00:57:08 - INFO - __main__ -   test: [epoch: 4 | batch: 2800/5005 ] | Loss: 1.188 | Acc: 71.210% (510618/717056)
01/05/2023 00:57:44 - INFO - __main__ -   test: [epoch: 4 | batch: 2900/5005 ] | Loss: 1.187 | Acc: 71.210% (528845/742656)
01/05/2023 00:58:21 - INFO - __main__ -   test: [epoch: 4 | batch: 3000/5005 ] | Loss: 1.187 | Acc: 71.228% (547210/768256)
01/05/2023 00:58:57 - INFO - __main__ -   test: [epoch: 4 | batch: 3100/5005 ] | Loss: 1.187 | Acc: 71.226% (565433/793856)
01/05/2023 00:59:34 - INFO - __main__ -   test: [epoch: 4 | batch: 3200/5005 ] | Loss: 1.188 | Acc: 71.217% (583590/819456)
01/05/2023 01:00:10 - INFO - __main__ -   test: [epoch: 4 | batch: 3300/5005 ] | Loss: 1.188 | Acc: 71.229% (601923/845056)
01/05/2023 01:00:47 - INFO - __main__ -   test: [epoch: 4 | batch: 3400/5005 ] | Loss: 1.187 | Acc: 71.235% (620211/870656)
01/05/2023 01:01:23 - INFO - __main__ -   test: [epoch: 4 | batch: 3500/5005 ] | Loss: 1.187 | Acc: 71.235% (638448/896256)
01/05/2023 01:02:00 - INFO - __main__ -   test: [epoch: 4 | batch: 3600/5005 ] | Loss: 1.187 | Acc: 71.245% (656774/921856)
01/05/2023 01:02:37 - INFO - __main__ -   test: [epoch: 4 | batch: 3700/5005 ] | Loss: 1.187 | Acc: 71.238% (674950/947456)
01/05/2023 01:03:13 - INFO - __main__ -   test: [epoch: 4 | batch: 3800/5005 ] | Loss: 1.187 | Acc: 71.248% (693286/973056)
01/05/2023 01:03:50 - INFO - __main__ -   test: [epoch: 4 | batch: 3900/5005 ] | Loss: 1.187 | Acc: 71.235% (711391/998656)
01/05/2023 01:04:26 - INFO - __main__ -   test: [epoch: 4 | batch: 4000/5005 ] | Loss: 1.188 | Acc: 71.229% (729570/1024256)
01/05/2023 01:05:03 - INFO - __main__ -   test: [epoch: 4 | batch: 4100/5005 ] | Loss: 1.188 | Acc: 71.231% (747828/1049856)
01/05/2023 01:05:39 - INFO - __main__ -   test: [epoch: 4 | batch: 4200/5005 ] | Loss: 1.187 | Acc: 71.234% (766090/1075456)
01/05/2023 01:06:16 - INFO - __main__ -   test: [epoch: 4 | batch: 4300/5005 ] | Loss: 1.187 | Acc: 71.238% (784367/1101056)
01/05/2023 01:06:52 - INFO - __main__ -   test: [epoch: 4 | batch: 4400/5005 ] | Loss: 1.187 | Acc: 71.254% (802784/1126656)
01/05/2023 01:07:29 - INFO - __main__ -   test: [epoch: 4 | batch: 4500/5005 ] | Loss: 1.187 | Acc: 71.256% (821050/1152256)
01/05/2023 01:08:05 - INFO - __main__ -   test: [epoch: 4 | batch: 4600/5005 ] | Loss: 1.187 | Acc: 71.254% (839275/1177856)
01/05/2023 01:08:41 - INFO - __main__ -   test: [epoch: 4 | batch: 4700/5005 ] | Loss: 1.187 | Acc: 71.249% (857452/1203456)
01/05/2023 01:09:18 - INFO - __main__ -   test: [epoch: 4 | batch: 4800/5005 ] | Loss: 1.187 | Acc: 71.250% (875703/1229056)
01/05/2023 01:09:54 - INFO - __main__ -   test: [epoch: 4 | batch: 4900/5005 ] | Loss: 1.187 | Acc: 71.250% (893941/1254656)
01/05/2023 01:10:31 - INFO - __main__ -   test: [epoch: 4 | batch: 5000/5005 ] | Loss: 1.187 | Acc: 71.248% (912158/1280256)
01/05/2023 01:10:32 - INFO - __main__ -   Saving Checkpoint
01/05/2023 01:10:33 - INFO - __main__ -   test: [batch: 0/196 ] | Loss: 0.657 | Acc: 82.422% (211/256)/ 96.094% (246/256)
01/05/2023 01:10:33 - INFO - __main__ -   test: [batch: 1/196 ] | Loss: 0.707 | Acc: 81.445% (417/512)/ 94.922% (486/512)
01/05/2023 01:10:33 - INFO - __main__ -   test: [batch: 2/196 ] | Loss: 0.576 | Acc: 85.026% (653/768)/ 95.833% (736/768)
01/05/2023 01:10:34 - INFO - __main__ -   test: [batch: 3/196 ] | Loss: 0.562 | Acc: 85.840% (879/1024)/ 95.703% (980/1024)
01/05/2023 01:10:34 - INFO - __main__ -   test: [batch: 4/196 ] | Loss: 0.540 | Acc: 86.328% (1105/1280)/ 96.016% (1229/1280)
01/05/2023 01:10:34 - INFO - __main__ -   test: [batch: 5/196 ] | Loss: 0.595 | Acc: 85.417% (1312/1536)/ 95.443% (1466/1536)
01/05/2023 01:10:35 - INFO - __main__ -   test: [batch: 6/196 ] | Loss: 0.706 | Acc: 82.310% (1475/1792)/ 94.420% (1692/1792)
01/05/2023 01:10:35 - INFO - __main__ -   test: [batch: 7/196 ] | Loss: 0.738 | Acc: 80.615% (1651/2048)/ 94.482% (1935/2048)
01/05/2023 01:10:35 - INFO - __main__ -   test: [batch: 8/196 ] | Loss: 0.780 | Acc: 79.905% (1841/2304)/ 93.750% (2160/2304)
01/05/2023 01:10:36 - INFO - __main__ -   test: [batch: 9/196 ] | Loss: 0.813 | Acc: 79.023% (2023/2560)/ 93.398% (2391/2560)
01/05/2023 01:10:36 - INFO - __main__ -   test: [batch: 10/196 ] | Loss: 0.841 | Acc: 78.303% (2205/2816)/ 93.324% (2628/2816)
01/05/2023 01:10:36 - INFO - __main__ -   test: [batch: 11/196 ] | Loss: 0.886 | Acc: 77.474% (2380/3072)/ 93.099% (2860/3072)
01/05/2023 01:10:37 - INFO - __main__ -   test: [batch: 12/196 ] | Loss: 0.925 | Acc: 76.502% (2546/3328)/ 92.698% (3085/3328)
01/05/2023 01:10:37 - INFO - __main__ -   test: [batch: 13/196 ] | Loss: 0.937 | Acc: 75.893% (2720/3584)/ 92.634% (3320/3584)
01/05/2023 01:10:37 - INFO - __main__ -   test: [batch: 14/196 ] | Loss: 0.936 | Acc: 75.833% (2912/3840)/ 92.865% (3566/3840)
01/05/2023 01:10:38 - INFO - __main__ -   test: [batch: 15/196 ] | Loss: 0.943 | Acc: 75.928% (3110/4096)/ 92.700% (3797/4096)
01/05/2023 01:10:38 - INFO - __main__ -   test: [batch: 16/196 ] | Loss: 0.922 | Acc: 76.608% (3334/4352)/ 92.877% (4042/4352)
01/05/2023 01:10:38 - INFO - __main__ -   test: [batch: 17/196 ] | Loss: 0.893 | Acc: 77.517% (3572/4608)/ 93.077% (4289/4608)
01/05/2023 01:10:39 - INFO - __main__ -   test: [batch: 18/196 ] | Loss: 0.866 | Acc: 78.228% (3805/4864)/ 93.277% (4537/4864)
01/05/2023 01:10:39 - INFO - __main__ -   test: [batch: 19/196 ] | Loss: 0.853 | Acc: 78.438% (4016/5120)/ 93.379% (4781/5120)
01/05/2023 01:10:39 - INFO - __main__ -   test: [batch: 20/196 ] | Loss: 0.853 | Acc: 78.702% (4231/5376)/ 93.304% (5016/5376)
01/05/2023 01:10:40 - INFO - __main__ -   test: [batch: 21/196 ] | Loss: 0.850 | Acc: 78.800% (4438/5632)/ 93.395% (5260/5632)
01/05/2023 01:10:40 - INFO - __main__ -   test: [batch: 22/196 ] | Loss: 0.853 | Acc: 78.770% (4638/5888)/ 93.342% (5496/5888)
01/05/2023 01:10:40 - INFO - __main__ -   test: [batch: 23/196 ] | Loss: 0.856 | Acc: 78.695% (4835/6144)/ 93.392% (5738/6144)
01/05/2023 01:10:41 - INFO - __main__ -   test: [batch: 24/196 ] | Loss: 0.870 | Acc: 78.469% (5022/6400)/ 93.297% (5971/6400)
01/05/2023 01:10:41 - INFO - __main__ -   test: [batch: 25/196 ] | Loss: 0.852 | Acc: 78.951% (5255/6656)/ 93.434% (6219/6656)
01/05/2023 01:10:41 - INFO - __main__ -   test: [batch: 26/196 ] | Loss: 0.839 | Acc: 79.225% (5476/6912)/ 93.519% (6464/6912)
01/05/2023 01:10:42 - INFO - __main__ -   test: [batch: 27/196 ] | Loss: 0.825 | Acc: 79.520% (5700/7168)/ 93.583% (6708/7168)
01/05/2023 01:10:42 - INFO - __main__ -   test: [batch: 28/196 ] | Loss: 0.808 | Acc: 79.916% (5933/7424)/ 93.723% (6958/7424)
01/05/2023 01:10:42 - INFO - __main__ -   test: [batch: 29/196 ] | Loss: 0.808 | Acc: 79.779% (6127/7680)/ 93.763% (7201/7680)
01/05/2023 01:10:43 - INFO - __main__ -   test: [batch: 30/196 ] | Loss: 0.809 | Acc: 79.763% (6330/7936)/ 93.775% (7442/7936)
01/05/2023 01:10:43 - INFO - __main__ -   test: [batch: 31/196 ] | Loss: 0.817 | Acc: 79.541% (6516/8192)/ 93.713% (7677/8192)
01/05/2023 01:10:43 - INFO - __main__ -   test: [batch: 32/196 ] | Loss: 0.837 | Acc: 78.918% (6667/8448)/ 93.584% (7906/8448)
01/05/2023 01:10:44 - INFO - __main__ -   test: [batch: 33/196 ] | Loss: 0.844 | Acc: 78.791% (6858/8704)/ 93.555% (8143/8704)
01/05/2023 01:10:44 - INFO - __main__ -   test: [batch: 34/196 ] | Loss: 0.843 | Acc: 78.828% (7063/8960)/ 93.583% (8385/8960)
01/05/2023 01:10:44 - INFO - __main__ -   test: [batch: 35/196 ] | Loss: 0.844 | Acc: 78.722% (7255/9216)/ 93.609% (8627/9216)
01/05/2023 01:10:45 - INFO - __main__ -   test: [batch: 36/196 ] | Loss: 0.855 | Acc: 78.252% (7412/9472)/ 93.634% (8869/9472)
01/05/2023 01:10:45 - INFO - __main__ -   test: [batch: 37/196 ] | Loss: 0.861 | Acc: 78.104% (7598/9728)/ 93.555% (9101/9728)
01/05/2023 01:10:45 - INFO - __main__ -   test: [batch: 38/196 ] | Loss: 0.860 | Acc: 78.065% (7794/9984)/ 93.640% (9349/9984)
01/05/2023 01:10:46 - INFO - __main__ -   test: [batch: 39/196 ] | Loss: 0.861 | Acc: 77.930% (7980/10240)/ 93.711% (9596/10240)
01/05/2023 01:10:46 - INFO - __main__ -   test: [batch: 40/196 ] | Loss: 0.860 | Acc: 77.954% (8182/10496)/ 93.731% (9838/10496)
01/05/2023 01:10:46 - INFO - __main__ -   test: [batch: 41/196 ] | Loss: 0.861 | Acc: 77.958% (8382/10752)/ 93.685% (10073/10752)
01/05/2023 01:10:47 - INFO - __main__ -   test: [batch: 42/196 ] | Loss: 0.853 | Acc: 78.143% (8602/11008)/ 93.777% (10323/11008)
01/05/2023 01:10:47 - INFO - __main__ -   test: [batch: 43/196 ] | Loss: 0.858 | Acc: 78.045% (8791/11264)/ 93.714% (10556/11264)
01/05/2023 01:10:47 - INFO - __main__ -   test: [batch: 44/196 ] | Loss: 0.863 | Acc: 77.977% (8983/11520)/ 93.698% (10794/11520)
01/05/2023 01:10:48 - INFO - __main__ -   test: [batch: 45/196 ] | Loss: 0.865 | Acc: 77.828% (9165/11776)/ 93.716% (11036/11776)
01/05/2023 01:10:48 - INFO - __main__ -   test: [batch: 46/196 ] | Loss: 0.869 | Acc: 77.568% (9333/12032)/ 93.700% (11274/12032)
01/05/2023 01:10:49 - INFO - __main__ -   test: [batch: 47/196 ] | Loss: 0.867 | Acc: 77.515% (9525/12288)/ 93.734% (11518/12288)
01/05/2023 01:10:49 - INFO - __main__ -   test: [batch: 48/196 ] | Loss: 0.871 | Acc: 77.264% (9692/12544)/ 93.766% (11762/12544)
01/05/2023 01:10:49 - INFO - __main__ -   test: [batch: 49/196 ] | Loss: 0.862 | Acc: 77.516% (9922/12800)/ 93.844% (12012/12800)
01/05/2023 01:10:50 - INFO - __main__ -   test: [batch: 50/196 ] | Loss: 0.857 | Acc: 77.658% (10139/13056)/ 93.896% (12259/13056)
01/05/2023 01:10:50 - INFO - __main__ -   test: [batch: 51/196 ] | Loss: 0.855 | Acc: 77.637% (10335/13312)/ 93.953% (12507/13312)
01/05/2023 01:10:50 - INFO - __main__ -   test: [batch: 52/196 ] | Loss: 0.855 | Acc: 77.594% (10528/13568)/ 93.964% (12749/13568)
01/05/2023 01:10:51 - INFO - __main__ -   test: [batch: 53/196 ] | Loss: 0.859 | Acc: 77.582% (10725/13824)/ 93.931% (12985/13824)
01/05/2023 01:10:51 - INFO - __main__ -   test: [batch: 54/196 ] | Loss: 0.860 | Acc: 77.450% (10905/14080)/ 93.956% (13229/14080)
01/05/2023 01:10:51 - INFO - __main__ -   test: [batch: 55/196 ] | Loss: 0.866 | Acc: 77.225% (11071/14336)/ 93.917% (13464/14336)
01/05/2023 01:10:52 - INFO - __main__ -   test: [batch: 56/196 ] | Loss: 0.865 | Acc: 77.262% (11274/14592)/ 93.942% (13708/14592)
01/05/2023 01:10:52 - INFO - __main__ -   test: [batch: 57/196 ] | Loss: 0.858 | Acc: 77.519% (11510/14848)/ 94.019% (13960/14848)
01/05/2023 01:10:52 - INFO - __main__ -   test: [batch: 58/196 ] | Loss: 0.858 | Acc: 77.542% (11712/15104)/ 93.988% (14196/15104)
01/05/2023 01:10:53 - INFO - __main__ -   test: [batch: 59/196 ] | Loss: 0.860 | Acc: 77.474% (11900/15360)/ 94.023% (14442/15360)
01/05/2023 01:10:53 - INFO - __main__ -   test: [batch: 60/196 ] | Loss: 0.865 | Acc: 77.344% (12078/15616)/ 94.013% (14681/15616)
01/05/2023 01:10:53 - INFO - __main__ -   test: [batch: 61/196 ] | Loss: 0.869 | Acc: 77.243% (12260/15872)/ 93.964% (14914/15872)
01/05/2023 01:10:54 - INFO - __main__ -   test: [batch: 62/196 ] | Loss: 0.864 | Acc: 77.356% (12476/16128)/ 93.998% (15160/16128)
01/05/2023 01:10:54 - INFO - __main__ -   test: [batch: 63/196 ] | Loss: 0.857 | Acc: 77.545% (12705/16384)/ 94.049% (15409/16384)
01/05/2023 01:10:54 - INFO - __main__ -   test: [batch: 64/196 ] | Loss: 0.857 | Acc: 77.578% (12909/16640)/ 94.062% (15652/16640)
01/05/2023 01:10:55 - INFO - __main__ -   test: [batch: 65/196 ] | Loss: 0.853 | Acc: 77.693% (13127/16896)/ 94.105% (15900/16896)
01/05/2023 01:10:55 - INFO - __main__ -   test: [batch: 66/196 ] | Loss: 0.852 | Acc: 77.723% (13331/17152)/ 94.117% (16143/17152)
01/05/2023 01:10:55 - INFO - __main__ -   test: [batch: 67/196 ] | Loss: 0.850 | Acc: 77.746% (13534/17408)/ 94.141% (16388/17408)
01/05/2023 01:10:56 - INFO - __main__ -   test: [batch: 68/196 ] | Loss: 0.847 | Acc: 77.740% (13732/17664)/ 94.180% (16636/17664)
01/05/2023 01:10:56 - INFO - __main__ -   test: [batch: 69/196 ] | Loss: 0.851 | Acc: 77.623% (13910/17920)/ 94.169% (16875/17920)
01/05/2023 01:10:56 - INFO - __main__ -   test: [batch: 70/196 ] | Loss: 0.850 | Acc: 77.602% (14105/18176)/ 94.190% (17120/18176)
01/05/2023 01:10:57 - INFO - __main__ -   test: [batch: 71/196 ] | Loss: 0.847 | Acc: 77.713% (14324/18432)/ 94.195% (17362/18432)
01/05/2023 01:10:57 - INFO - __main__ -   test: [batch: 72/196 ] | Loss: 0.852 | Acc: 77.676% (14516/18688)/ 94.130% (17591/18688)
01/05/2023 01:10:57 - INFO - __main__ -   test: [batch: 73/196 ] | Loss: 0.853 | Acc: 77.697% (14719/18944)/ 94.098% (17826/18944)
01/05/2023 01:10:58 - INFO - __main__ -   test: [batch: 74/196 ] | Loss: 0.859 | Acc: 77.536% (14887/19200)/ 94.057% (18059/19200)
01/05/2023 01:10:58 - INFO - __main__ -   test: [batch: 75/196 ] | Loss: 0.858 | Acc: 77.544% (15087/19456)/ 94.058% (18300/19456)
01/05/2023 01:10:58 - INFO - __main__ -   test: [batch: 76/196 ] | Loss: 0.857 | Acc: 77.562% (15289/19712)/ 94.065% (18542/19712)
01/05/2023 01:10:59 - INFO - __main__ -   test: [batch: 77/196 ] | Loss: 0.859 | Acc: 77.564% (15488/19968)/ 94.035% (18777/19968)
01/05/2023 01:10:59 - INFO - __main__ -   test: [batch: 78/196 ] | Loss: 0.865 | Acc: 77.398% (15653/20224)/ 93.963% (19003/20224)
01/05/2023 01:10:59 - INFO - __main__ -   test: [batch: 79/196 ] | Loss: 0.867 | Acc: 77.349% (15841/20480)/ 93.940% (19239/20480)
01/05/2023 01:11:00 - INFO - __main__ -   test: [batch: 80/196 ] | Loss: 0.877 | Acc: 77.122% (15992/20736)/ 93.842% (19459/20736)
01/05/2023 01:11:00 - INFO - __main__ -   test: [batch: 81/196 ] | Loss: 0.885 | Acc: 76.939% (16151/20992)/ 93.731% (19676/20992)
01/05/2023 01:11:00 - INFO - __main__ -   test: [batch: 82/196 ] | Loss: 0.890 | Acc: 76.817% (16322/21248)/ 93.675% (19904/21248)
01/05/2023 01:11:01 - INFO - __main__ -   test: [batch: 83/196 ] | Loss: 0.891 | Acc: 76.790% (16513/21504)/ 93.638% (20136/21504)
01/05/2023 01:11:01 - INFO - __main__ -   test: [batch: 84/196 ] | Loss: 0.895 | Acc: 76.723% (16695/21760)/ 93.580% (20363/21760)
01/05/2023 01:11:01 - INFO - __main__ -   test: [batch: 85/196 ] | Loss: 0.904 | Acc: 76.531% (16849/22016)/ 93.473% (20579/22016)
01/05/2023 01:11:02 - INFO - __main__ -   test: [batch: 86/196 ] | Loss: 0.908 | Acc: 76.432% (17023/22272)/ 93.418% (20806/22272)
01/05/2023 01:11:02 - INFO - __main__ -   test: [batch: 87/196 ] | Loss: 0.912 | Acc: 76.323% (17194/22528)/ 93.364% (21033/22528)
01/05/2023 01:11:02 - INFO - __main__ -   test: [batch: 88/196 ] | Loss: 0.918 | Acc: 76.189% (17359/22784)/ 93.298% (21257/22784)
01/05/2023 01:11:03 - INFO - __main__ -   test: [batch: 89/196 ] | Loss: 0.924 | Acc: 76.089% (17531/23040)/ 93.212% (21476/23040)
01/05/2023 01:11:03 - INFO - __main__ -   test: [batch: 90/196 ] | Loss: 0.937 | Acc: 75.841% (17668/23296)/ 93.085% (21685/23296)
01/05/2023 01:11:03 - INFO - __main__ -   test: [batch: 91/196 ] | Loss: 0.944 | Acc: 75.747% (17840/23552)/ 92.973% (21897/23552)
01/05/2023 01:11:04 - INFO - __main__ -   test: [batch: 92/196 ] | Loss: 0.944 | Acc: 75.794% (18045/23808)/ 92.935% (22126/23808)
01/05/2023 01:11:04 - INFO - __main__ -   test: [batch: 93/196 ] | Loss: 0.952 | Acc: 75.657% (18206/24064)/ 92.832% (22339/24064)
01/05/2023 01:11:04 - INFO - __main__ -   test: [batch: 94/196 ] | Loss: 0.957 | Acc: 75.485% (18358/24320)/ 92.812% (22572/24320)
01/05/2023 01:11:05 - INFO - __main__ -   test: [batch: 95/196 ] | Loss: 0.964 | Acc: 75.342% (18516/24576)/ 92.737% (22791/24576)
01/05/2023 01:11:05 - INFO - __main__ -   test: [batch: 96/196 ] | Loss: 0.971 | Acc: 75.213% (18677/24832)/ 92.635% (23003/24832)
01/05/2023 01:11:05 - INFO - __main__ -   test: [batch: 97/196 ] | Loss: 0.980 | Acc: 75.072% (18834/25088)/ 92.530% (23214/25088)
01/05/2023 01:11:06 - INFO - __main__ -   test: [batch: 98/196 ] | Loss: 0.990 | Acc: 74.870% (18975/25344)/ 92.412% (23421/25344)
01/05/2023 01:11:06 - INFO - __main__ -   test: [batch: 99/196 ] | Loss: 0.991 | Acc: 74.809% (19151/25600)/ 92.383% (23650/25600)
01/05/2023 01:11:06 - INFO - __main__ -   test: [batch: 100/196 ] | Loss: 0.996 | Acc: 74.698% (19314/25856)/ 92.338% (23875/25856)
01/05/2023 01:11:07 - INFO - __main__ -   test: [batch: 101/196 ] | Loss: 0.997 | Acc: 74.690% (19503/26112)/ 92.325% (24108/26112)
01/05/2023 01:11:07 - INFO - __main__ -   test: [batch: 102/196 ] | Loss: 1.002 | Acc: 74.568% (19662/26368)/ 92.260% (24327/26368)
01/05/2023 01:11:07 - INFO - __main__ -   test: [batch: 103/196 ] | Loss: 1.007 | Acc: 74.478% (19829/26624)/ 92.221% (24553/26624)
01/05/2023 01:11:08 - INFO - __main__ -   test: [batch: 104/196 ] | Loss: 1.008 | Acc: 74.453% (20013/26880)/ 92.217% (24788/26880)
01/05/2023 01:11:08 - INFO - __main__ -   test: [batch: 105/196 ] | Loss: 1.012 | Acc: 74.348% (20175/27136)/ 92.136% (25002/27136)
01/05/2023 01:11:08 - INFO - __main__ -   test: [batch: 106/196 ] | Loss: 1.015 | Acc: 74.292% (20350/27392)/ 92.093% (25226/27392)
01/05/2023 01:11:09 - INFO - __main__ -   test: [batch: 107/196 ] | Loss: 1.016 | Acc: 74.280% (20537/27648)/ 92.061% (25453/27648)
01/05/2023 01:11:09 - INFO - __main__ -   test: [batch: 108/196 ] | Loss: 1.018 | Acc: 74.273% (20725/27904)/ 92.026% (25679/27904)
01/05/2023 01:11:09 - INFO - __main__ -   test: [batch: 109/196 ] | Loss: 1.020 | Acc: 74.244% (20907/28160)/ 91.974% (25900/28160)
01/05/2023 01:11:10 - INFO - __main__ -   test: [batch: 110/196 ] | Loss: 1.020 | Acc: 74.257% (21101/28416)/ 91.959% (26131/28416)
01/05/2023 01:11:10 - INFO - __main__ -   test: [batch: 111/196 ] | Loss: 1.019 | Acc: 74.306% (21305/28672)/ 91.968% (26369/28672)
01/05/2023 01:11:10 - INFO - __main__ -   test: [batch: 112/196 ] | Loss: 1.017 | Acc: 74.395% (21521/28928)/ 91.980% (26608/28928)
01/05/2023 01:11:11 - INFO - __main__ -   test: [batch: 113/196 ] | Loss: 1.018 | Acc: 74.383% (21708/29184)/ 91.978% (26843/29184)
01/05/2023 01:11:11 - INFO - __main__ -   test: [batch: 114/196 ] | Loss: 1.027 | Acc: 74.219% (21850/29440)/ 91.855% (27042/29440)
01/05/2023 01:11:11 - INFO - __main__ -   test: [batch: 115/196 ] | Loss: 1.032 | Acc: 74.108% (22007/29696)/ 91.777% (27254/29696)
01/05/2023 01:11:12 - INFO - __main__ -   test: [batch: 116/196 ] | Loss: 1.034 | Acc: 74.069% (22185/29952)/ 91.743% (27479/29952)
01/05/2023 01:11:12 - INFO - __main__ -   test: [batch: 117/196 ] | Loss: 1.041 | Acc: 73.934% (22334/30208)/ 91.651% (27686/30208)
01/05/2023 01:11:12 - INFO - __main__ -   test: [batch: 118/196 ] | Loss: 1.040 | Acc: 73.976% (22536/30464)/ 91.626% (27913/30464)
01/05/2023 01:11:13 - INFO - __main__ -   test: [batch: 119/196 ] | Loss: 1.039 | Acc: 74.046% (22747/30720)/ 91.637% (28151/30720)
01/05/2023 01:11:13 - INFO - __main__ -   test: [batch: 120/196 ] | Loss: 1.043 | Acc: 73.944% (22905/30976)/ 91.564% (28363/30976)
01/05/2023 01:11:13 - INFO - __main__ -   test: [batch: 121/196 ] | Loss: 1.052 | Acc: 73.729% (23027/31232)/ 91.461% (28565/31232)
01/05/2023 01:11:14 - INFO - __main__ -   test: [batch: 122/196 ] | Loss: 1.052 | Acc: 73.749% (23222/31488)/ 91.441% (28793/31488)
01/05/2023 01:11:14 - INFO - __main__ -   test: [batch: 123/196 ] | Loss: 1.060 | Acc: 73.645% (23378/31744)/ 91.337% (28994/31744)
01/05/2023 01:11:14 - INFO - __main__ -   test: [batch: 124/196 ] | Loss: 1.063 | Acc: 73.478% (23513/32000)/ 91.316% (29221/32000)
01/05/2023 01:11:15 - INFO - __main__ -   test: [batch: 125/196 ] | Loss: 1.064 | Acc: 73.469% (23698/32256)/ 91.307% (29452/32256)
01/05/2023 01:11:15 - INFO - __main__ -   test: [batch: 126/196 ] | Loss: 1.066 | Acc: 73.459% (23883/32512)/ 91.277% (29676/32512)
01/05/2023 01:11:15 - INFO - __main__ -   test: [batch: 127/196 ] | Loss: 1.072 | Acc: 73.355% (24037/32768)/ 91.208% (29887/32768)
01/05/2023 01:11:16 - INFO - __main__ -   test: [batch: 128/196 ] | Loss: 1.077 | Acc: 73.232% (24184/33024)/ 91.167% (30107/33024)
01/05/2023 01:11:16 - INFO - __main__ -   test: [batch: 129/196 ] | Loss: 1.080 | Acc: 73.122% (24335/33280)/ 91.157% (30337/33280)
01/05/2023 01:11:16 - INFO - __main__ -   test: [batch: 130/196 ] | Loss: 1.079 | Acc: 73.133% (24526/33536)/ 91.183% (30579/33536)
01/05/2023 01:11:17 - INFO - __main__ -   test: [batch: 131/196 ] | Loss: 1.081 | Acc: 73.068% (24691/33792)/ 91.172% (30809/33792)
01/05/2023 01:11:17 - INFO - __main__ -   test: [batch: 132/196 ] | Loss: 1.086 | Acc: 73.012% (24859/34048)/ 91.101% (31018/34048)
01/05/2023 01:11:17 - INFO - __main__ -   test: [batch: 133/196 ] | Loss: 1.088 | Acc: 72.980% (25035/34304)/ 91.077% (31243/34304)
01/05/2023 01:11:18 - INFO - __main__ -   test: [batch: 134/196 ] | Loss: 1.090 | Acc: 72.925% (25203/34560)/ 91.053% (31468/34560)
01/05/2023 01:11:18 - INFO - __main__ -   test: [batch: 135/196 ] | Loss: 1.094 | Acc: 72.880% (25374/34816)/ 91.010% (31686/34816)
01/05/2023 01:11:18 - INFO - __main__ -   test: [batch: 136/196 ] | Loss: 1.095 | Acc: 72.862% (25554/35072)/ 90.990% (31912/35072)
01/05/2023 01:11:19 - INFO - __main__ -   test: [batch: 137/196 ] | Loss: 1.097 | Acc: 72.820% (25726/35328)/ 90.970% (32138/35328)
01/05/2023 01:11:19 - INFO - __main__ -   test: [batch: 138/196 ] | Loss: 1.100 | Acc: 72.749% (25887/35584)/ 90.945% (32362/35584)
01/05/2023 01:11:19 - INFO - __main__ -   test: [batch: 139/196 ] | Loss: 1.100 | Acc: 72.768% (26080/35840)/ 90.940% (32593/35840)
01/05/2023 01:11:20 - INFO - __main__ -   test: [batch: 140/196 ] | Loss: 1.102 | Acc: 72.737% (26255/36096)/ 90.924% (32820/36096)
01/05/2023 01:11:20 - INFO - __main__ -   test: [batch: 141/196 ] | Loss: 1.101 | Acc: 72.780% (26457/36352)/ 90.922% (33052/36352)
01/05/2023 01:11:20 - INFO - __main__ -   test: [batch: 142/196 ] | Loss: 1.108 | Acc: 72.684% (26608/36608)/ 90.835% (33253/36608)
01/05/2023 01:11:21 - INFO - __main__ -   test: [batch: 143/196 ] | Loss: 1.109 | Acc: 72.654% (26783/36864)/ 90.809% (33476/36864)
01/05/2023 01:11:21 - INFO - __main__ -   test: [batch: 144/196 ] | Loss: 1.111 | Acc: 72.624% (26958/37120)/ 90.770% (33694/37120)
01/05/2023 01:11:21 - INFO - __main__ -   test: [batch: 145/196 ] | Loss: 1.113 | Acc: 72.544% (27114/37376)/ 90.756% (33921/37376)
01/05/2023 01:11:22 - INFO - __main__ -   test: [batch: 146/196 ] | Loss: 1.115 | Acc: 72.489% (27279/37632)/ 90.734% (34145/37632)
01/05/2023 01:11:22 - INFO - __main__ -   test: [batch: 147/196 ] | Loss: 1.115 | Acc: 72.469% (27457/37888)/ 90.736% (34378/37888)
01/05/2023 01:11:23 - INFO - __main__ -   test: [batch: 148/196 ] | Loss: 1.118 | Acc: 72.423% (27625/38144)/ 90.701% (34597/38144)
01/05/2023 01:11:23 - INFO - __main__ -   test: [batch: 149/196 ] | Loss: 1.121 | Acc: 72.375% (27792/38400)/ 90.667% (34816/38400)
01/05/2023 01:11:23 - INFO - __main__ -   test: [batch: 150/196 ] | Loss: 1.123 | Acc: 72.390% (27983/38656)/ 90.641% (35038/38656)
01/05/2023 01:11:24 - INFO - __main__ -   test: [batch: 151/196 ] | Loss: 1.125 | Acc: 72.340% (28149/38912)/ 90.597% (35253/38912)
01/05/2023 01:11:24 - INFO - __main__ -   test: [batch: 152/196 ] | Loss: 1.125 | Acc: 72.327% (28329/39168)/ 90.587% (35481/39168)
01/05/2023 01:11:24 - INFO - __main__ -   test: [batch: 153/196 ] | Loss: 1.128 | Acc: 72.276% (28494/39424)/ 90.544% (35696/39424)
01/05/2023 01:11:25 - INFO - __main__ -   test: [batch: 154/196 ] | Loss: 1.130 | Acc: 72.245% (28667/39680)/ 90.507% (35913/39680)
01/05/2023 01:11:25 - INFO - __main__ -   test: [batch: 155/196 ] | Loss: 1.133 | Acc: 72.211% (28838/39936)/ 90.452% (36123/39936)
01/05/2023 01:11:25 - INFO - __main__ -   test: [batch: 156/196 ] | Loss: 1.132 | Acc: 72.256% (29041/40192)/ 90.466% (36360/40192)
01/05/2023 01:11:26 - INFO - __main__ -   test: [batch: 157/196 ] | Loss: 1.134 | Acc: 72.219% (29211/40448)/ 90.435% (36579/40448)
01/05/2023 01:11:26 - INFO - __main__ -   test: [batch: 158/196 ] | Loss: 1.139 | Acc: 72.113% (29353/40704)/ 90.394% (36794/40704)
01/05/2023 01:11:26 - INFO - __main__ -   test: [batch: 159/196 ] | Loss: 1.141 | Acc: 72.080% (29524/40960)/ 90.371% (37016/40960)
01/05/2023 01:11:27 - INFO - __main__ -   test: [batch: 160/196 ] | Loss: 1.140 | Acc: 72.113% (29722/41216)/ 90.373% (37248/41216)
01/05/2023 01:11:27 - INFO - __main__ -   test: [batch: 161/196 ] | Loss: 1.144 | Acc: 72.039% (29876/41472)/ 90.340% (37466/41472)
01/05/2023 01:11:27 - INFO - __main__ -   test: [batch: 162/196 ] | Loss: 1.144 | Acc: 72.036% (30059/41728)/ 90.330% (37693/41728)
01/05/2023 01:11:28 - INFO - __main__ -   test: [batch: 163/196 ] | Loss: 1.151 | Acc: 71.820% (30153/41984)/ 90.220% (37878/41984)
01/05/2023 01:11:28 - INFO - __main__ -   test: [batch: 164/196 ] | Loss: 1.155 | Acc: 71.735% (30301/42240)/ 90.180% (38092/42240)
01/05/2023 01:11:28 - INFO - __main__ -   test: [batch: 165/196 ] | Loss: 1.157 | Acc: 71.659% (30452/42496)/ 90.145% (38308/42496)
01/05/2023 01:11:29 - INFO - __main__ -   test: [batch: 166/196 ] | Loss: 1.156 | Acc: 71.672% (30641/42752)/ 90.150% (38541/42752)
01/05/2023 01:11:29 - INFO - __main__ -   test: [batch: 167/196 ] | Loss: 1.159 | Acc: 71.615% (30800/43008)/ 90.095% (38748/43008)
01/05/2023 01:11:29 - INFO - __main__ -   test: [batch: 168/196 ] | Loss: 1.160 | Acc: 71.586% (30971/43264)/ 90.080% (38972/43264)
01/05/2023 01:11:30 - INFO - __main__ -   test: [batch: 169/196 ] | Loss: 1.162 | Acc: 71.540% (31134/43520)/ 90.044% (39187/43520)
01/05/2023 01:11:30 - INFO - __main__ -   test: [batch: 170/196 ] | Loss: 1.160 | Acc: 71.562% (31327/43776)/ 90.077% (39432/43776)
01/05/2023 01:11:30 - INFO - __main__ -   test: [batch: 171/196 ] | Loss: 1.161 | Acc: 71.507% (31486/44032)/ 90.048% (39650/44032)
01/05/2023 01:11:31 - INFO - __main__ -   test: [batch: 172/196 ] | Loss: 1.166 | Acc: 71.439% (31639/44288)/ 89.981% (39851/44288)
01/05/2023 01:11:31 - INFO - __main__ -   test: [batch: 173/196 ] | Loss: 1.166 | Acc: 71.439% (31822/44544)/ 89.990% (40085/44544)
01/05/2023 01:11:31 - INFO - __main__ -   test: [batch: 174/196 ] | Loss: 1.167 | Acc: 71.391% (31983/44800)/ 89.978% (40310/44800)
01/05/2023 01:11:32 - INFO - __main__ -   test: [batch: 175/196 ] | Loss: 1.169 | Acc: 71.336% (32141/45056)/ 89.961% (40533/45056)
01/05/2023 01:11:32 - INFO - __main__ -   test: [batch: 176/196 ] | Loss: 1.172 | Acc: 71.308% (32311/45312)/ 89.908% (40739/45312)
01/05/2023 01:11:32 - INFO - __main__ -   test: [batch: 177/196 ] | Loss: 1.177 | Acc: 71.153% (32423/45568)/ 89.870% (40952/45568)
01/05/2023 01:11:33 - INFO - __main__ -   test: [batch: 178/196 ] | Loss: 1.176 | Acc: 71.196% (32625/45824)/ 89.890% (41191/45824)
01/05/2023 01:11:33 - INFO - __main__ -   test: [batch: 179/196 ] | Loss: 1.175 | Acc: 71.217% (32817/46080)/ 89.896% (41424/46080)
01/05/2023 01:11:33 - INFO - __main__ -   test: [batch: 180/196 ] | Loss: 1.176 | Acc: 71.193% (32988/46336)/ 89.911% (41661/46336)
01/05/2023 01:11:34 - INFO - __main__ -   test: [batch: 181/196 ] | Loss: 1.176 | Acc: 71.180% (33164/46592)/ 89.906% (41889/46592)
01/05/2023 01:11:34 - INFO - __main__ -   test: [batch: 182/196 ] | Loss: 1.174 | Acc: 71.220% (33365/46848)/ 89.916% (42124/46848)
01/05/2023 01:11:34 - INFO - __main__ -   test: [batch: 183/196 ] | Loss: 1.171 | Acc: 71.285% (33578/47104)/ 89.952% (42371/47104)
01/05/2023 01:11:35 - INFO - __main__ -   test: [batch: 184/196 ] | Loss: 1.169 | Acc: 71.303% (33769/47360)/ 89.973% (42611/47360)
01/05/2023 01:11:35 - INFO - __main__ -   test: [batch: 185/196 ] | Loss: 1.169 | Acc: 71.297% (33949/47616)/ 89.995% (42852/47616)
01/05/2023 01:11:35 - INFO - __main__ -   test: [batch: 186/196 ] | Loss: 1.165 | Acc: 71.384% (34173/47872)/ 90.032% (43100/47872)
01/05/2023 01:11:36 - INFO - __main__ -   test: [batch: 187/196 ] | Loss: 1.167 | Acc: 71.347% (34338/48128)/ 90.004% (43317/48128)
01/05/2023 01:11:36 - INFO - __main__ -   test: [batch: 188/196 ] | Loss: 1.168 | Acc: 71.338% (34516/48384)/ 89.988% (43540/48384)
01/05/2023 01:11:36 - INFO - __main__ -   test: [batch: 189/196 ] | Loss: 1.172 | Acc: 71.234% (34648/48640)/ 89.957% (43755/48640)
01/05/2023 01:11:37 - INFO - __main__ -   test: [batch: 190/196 ] | Loss: 1.172 | Acc: 71.196% (34812/48896)/ 89.960% (43987/48896)
01/05/2023 01:11:37 - INFO - __main__ -   test: [batch: 191/196 ] | Loss: 1.173 | Acc: 71.195% (34994/49152)/ 89.954% (44214/49152)
01/05/2023 01:11:37 - INFO - __main__ -   test: [batch: 192/196 ] | Loss: 1.170 | Acc: 71.274% (35215/49408)/ 89.996% (44465/49408)
01/05/2023 01:11:38 - INFO - __main__ -   test: [batch: 193/196 ] | Loss: 1.165 | Acc: 71.382% (35451/49664)/ 90.025% (44710/49664)
01/05/2023 01:11:38 - INFO - __main__ -   test: [batch: 194/196 ] | Loss: 1.162 | Acc: 71.444% (35665/49920)/ 90.058% (44957/49920)
01/05/2023 01:11:38 - INFO - __main__ -   test: [batch: 195/196 ] | Loss: 1.168 | Acc: 71.392% (35696/50000)/ 90.042% (45021/50000)
01/05/2023 01:11:38 - INFO - __main__ -   Final accuracy: 71.392
