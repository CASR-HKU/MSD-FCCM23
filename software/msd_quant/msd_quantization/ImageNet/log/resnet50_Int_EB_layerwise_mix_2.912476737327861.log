/home/jjc/miniconda3/envs/ant_quant/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
Namespace(a_low=75, a_up=150, abit=8, batch_size=128, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='csd_eb2', epoch=3, layer_4bit_l='28,40,18,0,39,27,33,40,27,33,39,13,43,24,45,48,2,3,9,25,12', layer_8bit_l=None, layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='resnet50', percent=100, ptq=False, resume=False, search=False, tag='', train=True, w_low=75, w_up=150, wbit=8)
01/15/2023 02:49:27 - INFO - __main__ -   output/resnet50_imagenet/int_W8A8_681/gpu_0
01/15/2023 02:49:27 - INFO - __main__ -   Namespace(a_low=75, a_up=150, abit=8, batch_size=128, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='csd_eb2', epoch=3, layer_4bit_l='28,40,18,0,39,27,33,40,27,33,39,13,43,24,45,48,2,3,9,25,12', layer_8bit_l=None, layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='resnet50', percent=100, ptq=False, resume=False, search=False, tag='', train=True, w_low=75, w_up=150, wbit=8)
01/15/2023 02:49:27 - INFO - __main__ -   ==> Preparing data..
01/15/2023 02:49:30 - INFO - __main__ -   ==> Setting quantizer..
Namespace(a_low=75, a_up=150, abit=8, batch_size=128, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='csd_eb2', epoch=3, layer_4bit_l='28,40,18,0,39,27,33,40,27,33,39,13,43,24,45,48,2,3,9,25,12', layer_8bit_l=None, layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='resnet50', percent=100, ptq=False, resume=False, search=False, tag='', train=True, w_low=75, w_up=150, wbit=8)
01/15/2023 02:49:30 - INFO - __main__ -   Namespace(a_low=75, a_up=150, abit=8, batch_size=128, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='csd_eb2', epoch=3, layer_4bit_l='28,40,18,0,39,27,33,40,27,33,39,13,43,24,45,48,2,3,9,25,12', layer_8bit_l=None, layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='resnet50', percent=100, ptq=False, resume=False, search=False, tag='', train=True, w_low=75, w_up=150, wbit=8)
01/15/2023 02:49:30 - INFO - __main__ -   ==> Building model..
ResNet(
  (conv1): Conv2dQuantizer(
    (quant_weight): TensorQuantizer()
    (quant_input): TensorQuantizer()
  )
  (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (1): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (1): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): LinearQuantizer(
    (quant_weight): TensorQuantizer()
    (quant_input): TensorQuantizer()
  )
)
01/15/2023 02:49:31 - INFO - __main__ -   {'milestones': Counter({2: 1, 3: 1, 4: 1}), 'gamma': 0.1, 'base_lrs': [0.00025], 'last_epoch': 0, '_step_count': 1, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.00025]}
01/15/2023 02:49:31 - INFO - __main__ -   
Epoch: 0
Layer quant EB csd_eb2
int	8-bit 	 conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.3.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.3.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.3.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.3.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.3.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.3.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.4.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.4.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.4.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.5.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.5.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.5.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 fc.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 fc.quant_input,
set init to 1
------------- 8-bit EB1 Re-SET -------------
21
conv1.quant_weight 0
conv1.quant_input 0
layer1.0.conv2.quant_weight 2
layer1.0.conv2.quant_input 2
layer1.0.conv3.quant_weight 3
layer1.0.conv3.quant_input 3
layer1.2.conv2.quant_weight 9
layer1.2.conv2.quant_input 9
layer2.0.conv2.quant_weight 12
layer2.0.conv2.quant_input 12
layer2.0.conv3.quant_weight 13
layer2.0.conv3.quant_input 13
layer2.2.conv1.quant_weight 18
layer2.2.conv1.quant_input 18
layer3.0.conv1.quant_weight 24
layer3.0.conv1.quant_input 24
layer3.0.conv2.quant_weight 25
layer3.0.conv2.quant_input 25
layer3.0.downsample.0.quant_weight 27
layer3.0.downsample.0.quant_input 27
layer3.1.conv1.quant_weight 28
layer3.1.conv1.quant_input 28
layer3.2.conv3.quant_weight 33
layer3.2.conv3.quant_input 33
layer3.4.conv3.quant_weight 39
layer3.4.conv3.quant_input 39
layer3.5.conv1.quant_weight 40
layer3.5.conv1.quant_input 40
layer4.0.conv1.quant_weight 43
layer4.0.conv1.quant_input 43
layer4.0.conv3.quant_weight 45
layer4.0.conv3.quant_input 45
layer4.1.conv2.quant_weight 48
layer4.1.conv2.quant_input 48
------------- 8-bit EB1 Re-SET -------------
Layer quant EB csd_eb1
int	8-bit 	 conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer1.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer1.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer1.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer2.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer2.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer2.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.3.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.3.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.3.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv3.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer3.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer3.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer3.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer3.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer3.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.3.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.3.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.3.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.4.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.4.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv2.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer3.4.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv3.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer3.5.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.5.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.5.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv3.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer4.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer4.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer4.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 fc.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 fc.quant_input,
set init to 1
01/15/2023 02:50:36 - INFO - __main__ -   test: [epoch: 0 | batch: 0/10010 ] | Loss: 0.737 | Acc: 78.906% (101/128)
01/15/2023 02:53:59 - INFO - __main__ -   test: [epoch: 0 | batch: 100/10010 ] | Loss: 0.920 | Acc: 77.313% (9995/12928)
01/15/2023 02:57:30 - INFO - __main__ -   test: [epoch: 0 | batch: 200/10010 ] | Loss: 0.904 | Acc: 77.495% (19938/25728)
01/15/2023 03:01:01 - INFO - __main__ -   test: [epoch: 0 | batch: 300/10010 ] | Loss: 0.921 | Acc: 77.102% (29706/38528)
01/15/2023 03:04:32 - INFO - __main__ -   test: [epoch: 0 | batch: 400/10010 ] | Loss: 0.932 | Acc: 76.829% (39435/51328)
01/15/2023 03:08:03 - INFO - __main__ -   test: [epoch: 0 | batch: 500/10010 ] | Loss: 0.935 | Acc: 76.715% (49196/64128)
01/15/2023 03:11:36 - INFO - __main__ -   test: [epoch: 0 | batch: 600/10010 ] | Loss: 0.936 | Acc: 76.729% (59026/76928)
01/15/2023 03:15:07 - INFO - __main__ -   test: [epoch: 0 | batch: 700/10010 ] | Loss: 0.937 | Acc: 76.764% (68879/89728)
01/15/2023 03:18:39 - INFO - __main__ -   test: [epoch: 0 | batch: 800/10010 ] | Loss: 0.939 | Acc: 76.691% (78630/102528)
01/15/2023 03:22:10 - INFO - __main__ -   test: [epoch: 0 | batch: 900/10010 ] | Loss: 0.939 | Acc: 76.689% (88444/115328)
01/15/2023 03:25:41 - INFO - __main__ -   test: [epoch: 0 | batch: 1000/10010 ] | Loss: 0.940 | Acc: 76.695% (98268/128128)
01/15/2023 03:29:13 - INFO - __main__ -   test: [epoch: 0 | batch: 1100/10010 ] | Loss: 0.942 | Acc: 76.690% (108077/140928)
01/15/2023 03:32:46 - INFO - __main__ -   test: [epoch: 0 | batch: 1200/10010 ] | Loss: 0.943 | Acc: 76.685% (117887/153728)
01/15/2023 03:36:17 - INFO - __main__ -   test: [epoch: 0 | batch: 1300/10010 ] | Loss: 0.942 | Acc: 76.730% (127777/166528)
01/15/2023 03:39:48 - INFO - __main__ -   test: [epoch: 0 | batch: 1400/10010 ] | Loss: 0.943 | Acc: 76.738% (137612/179328)
01/15/2023 03:43:21 - INFO - __main__ -   test: [epoch: 0 | batch: 1500/10010 ] | Loss: 0.941 | Acc: 76.765% (147488/192128)
01/15/2023 03:46:52 - INFO - __main__ -   test: [epoch: 0 | batch: 1600/10010 ] | Loss: 0.942 | Acc: 76.749% (157281/204928)
01/15/2023 03:50:23 - INFO - __main__ -   test: [epoch: 0 | batch: 1700/10010 ] | Loss: 0.942 | Acc: 76.754% (167115/217728)
01/15/2023 03:53:54 - INFO - __main__ -   test: [epoch: 0 | batch: 1800/10010 ] | Loss: 0.940 | Acc: 76.793% (177030/230528)
01/15/2023 03:57:24 - INFO - __main__ -   test: [epoch: 0 | batch: 1900/10010 ] | Loss: 0.939 | Acc: 76.814% (186909/243328)
01/15/2023 04:00:56 - INFO - __main__ -   test: [epoch: 0 | batch: 2000/10010 ] | Loss: 0.939 | Acc: 76.822% (196763/256128)
01/15/2023 04:04:28 - INFO - __main__ -   test: [epoch: 0 | batch: 2100/10010 ] | Loss: 0.939 | Acc: 76.809% (206561/268928)
01/15/2023 04:07:59 - INFO - __main__ -   test: [epoch: 0 | batch: 2200/10010 ] | Loss: 0.940 | Acc: 76.799% (216364/281728)
01/15/2023 04:11:30 - INFO - __main__ -   test: [epoch: 0 | batch: 2300/10010 ] | Loss: 0.940 | Acc: 76.779% (226137/294528)
01/15/2023 04:15:02 - INFO - __main__ -   test: [epoch: 0 | batch: 2400/10010 ] | Loss: 0.940 | Acc: 76.775% (235950/307328)
01/15/2023 04:18:34 - INFO - __main__ -   test: [epoch: 0 | batch: 2500/10010 ] | Loss: 0.941 | Acc: 76.779% (245791/320128)
01/15/2023 04:22:05 - INFO - __main__ -   test: [epoch: 0 | batch: 2600/10010 ] | Loss: 0.941 | Acc: 76.769% (255586/332928)
01/15/2023 04:25:36 - INFO - __main__ -   test: [epoch: 0 | batch: 2700/10010 ] | Loss: 0.941 | Acc: 76.780% (265449/345728)
01/15/2023 04:29:08 - INFO - __main__ -   test: [epoch: 0 | batch: 2800/10010 ] | Loss: 0.941 | Acc: 76.768% (275236/358528)
01/15/2023 04:32:39 - INFO - __main__ -   test: [epoch: 0 | batch: 2900/10010 ] | Loss: 0.941 | Acc: 76.769% (285064/371328)
01/15/2023 04:36:11 - INFO - __main__ -   test: [epoch: 0 | batch: 3000/10010 ] | Loss: 0.940 | Acc: 76.764% (294872/384128)
01/15/2023 04:39:43 - INFO - __main__ -   test: [epoch: 0 | batch: 3100/10010 ] | Loss: 0.940 | Acc: 76.766% (304705/396928)
01/15/2023 04:43:15 - INFO - __main__ -   test: [epoch: 0 | batch: 3200/10010 ] | Loss: 0.941 | Acc: 76.761% (314512/409728)
01/15/2023 04:46:48 - INFO - __main__ -   test: [epoch: 0 | batch: 3300/10010 ] | Loss: 0.941 | Acc: 76.746% (324272/422528)
01/15/2023 04:50:19 - INFO - __main__ -   test: [epoch: 0 | batch: 3400/10010 ] | Loss: 0.941 | Acc: 76.759% (334152/435328)
01/15/2023 04:53:50 - INFO - __main__ -   test: [epoch: 0 | batch: 3500/10010 ] | Loss: 0.941 | Acc: 76.762% (343994/448128)
01/15/2023 04:57:20 - INFO - __main__ -   test: [epoch: 0 | batch: 3600/10010 ] | Loss: 0.941 | Acc: 76.756% (353790/460928)
01/15/2023 05:00:52 - INFO - __main__ -   test: [epoch: 0 | batch: 3700/10010 ] | Loss: 0.941 | Acc: 76.736% (363520/473728)
01/15/2023 05:04:24 - INFO - __main__ -   test: [epoch: 0 | batch: 3800/10010 ] | Loss: 0.941 | Acc: 76.740% (373361/486528)
01/15/2023 05:07:55 - INFO - __main__ -   test: [epoch: 0 | batch: 3900/10010 ] | Loss: 0.942 | Acc: 76.732% (383142/499328)
01/15/2023 05:11:26 - INFO - __main__ -   test: [epoch: 0 | batch: 4000/10010 ] | Loss: 0.942 | Acc: 76.724% (392927/512128)
01/15/2023 05:14:57 - INFO - __main__ -   test: [epoch: 0 | batch: 4100/10010 ] | Loss: 0.942 | Acc: 76.724% (402745/524928)
01/15/2023 05:18:30 - INFO - __main__ -   test: [epoch: 0 | batch: 4200/10010 ] | Loss: 0.942 | Acc: 76.725% (412574/537728)
01/15/2023 05:22:01 - INFO - __main__ -   test: [epoch: 0 | batch: 4300/10010 ] | Loss: 0.942 | Acc: 76.722% (422374/550528)
01/15/2023 05:25:33 - INFO - __main__ -   test: [epoch: 0 | batch: 4400/10010 ] | Loss: 0.942 | Acc: 76.722% (432198/563328)
01/15/2023 05:29:04 - INFO - __main__ -   test: [epoch: 0 | batch: 4500/10010 ] | Loss: 0.942 | Acc: 76.711% (441952/576128)
01/15/2023 05:32:35 - INFO - __main__ -   test: [epoch: 0 | batch: 4600/10010 ] | Loss: 0.942 | Acc: 76.716% (451803/588928)
01/15/2023 05:36:06 - INFO - __main__ -   test: [epoch: 0 | batch: 4700/10010 ] | Loss: 0.942 | Acc: 76.711% (461594/601728)
01/15/2023 05:39:38 - INFO - __main__ -   test: [epoch: 0 | batch: 4800/10010 ] | Loss: 0.942 | Acc: 76.715% (471434/614528)
01/15/2023 05:43:08 - INFO - __main__ -   test: [epoch: 0 | batch: 4900/10010 ] | Loss: 0.942 | Acc: 76.718% (481272/627328)
01/15/2023 05:46:40 - INFO - __main__ -   test: [epoch: 0 | batch: 5000/10010 ] | Loss: 0.941 | Acc: 76.728% (491160/640128)
01/15/2023 05:50:12 - INFO - __main__ -   test: [epoch: 0 | batch: 5100/10010 ] | Loss: 0.941 | Acc: 76.734% (501015/652928)
01/15/2023 05:53:45 - INFO - __main__ -   test: [epoch: 0 | batch: 5200/10010 ] | Loss: 0.941 | Acc: 76.733% (510835/665728)
01/15/2023 05:57:15 - INFO - __main__ -   test: [epoch: 0 | batch: 5300/10010 ] | Loss: 0.941 | Acc: 76.736% (520675/678528)
01/15/2023 06:00:47 - INFO - __main__ -   test: [epoch: 0 | batch: 5400/10010 ] | Loss: 0.942 | Acc: 76.729% (530448/691328)
01/15/2023 06:04:21 - INFO - __main__ -   test: [epoch: 0 | batch: 5500/10010 ] | Loss: 0.942 | Acc: 76.718% (540190/704128)
01/15/2023 06:07:52 - INFO - __main__ -   test: [epoch: 0 | batch: 5600/10010 ] | Loss: 0.942 | Acc: 76.726% (550071/716928)
01/15/2023 06:11:24 - INFO - __main__ -   test: [epoch: 0 | batch: 5700/10010 ] | Loss: 0.941 | Acc: 76.727% (559895/729728)
01/15/2023 06:14:55 - INFO - __main__ -   test: [epoch: 0 | batch: 5800/10010 ] | Loss: 0.941 | Acc: 76.731% (569746/742528)
01/15/2023 06:18:27 - INFO - __main__ -   test: [epoch: 0 | batch: 5900/10010 ] | Loss: 0.941 | Acc: 76.734% (579592/755328)
01/15/2023 06:21:58 - INFO - __main__ -   test: [epoch: 0 | batch: 6000/10010 ] | Loss: 0.942 | Acc: 76.732% (589399/768128)
01/15/2023 06:25:29 - INFO - __main__ -   test: [epoch: 0 | batch: 6100/10010 ] | Loss: 0.942 | Acc: 76.725% (599166/780928)
01/15/2023 06:29:02 - INFO - __main__ -   test: [epoch: 0 | batch: 6200/10010 ] | Loss: 0.942 | Acc: 76.722% (608964/793728)
01/15/2023 06:32:34 - INFO - __main__ -   test: [epoch: 0 | batch: 6300/10010 ] | Loss: 0.942 | Acc: 76.719% (618764/806528)
01/15/2023 06:36:06 - INFO - __main__ -   test: [epoch: 0 | batch: 6400/10010 ] | Loss: 0.942 | Acc: 76.716% (628559/819328)
01/15/2023 06:39:37 - INFO - __main__ -   test: [epoch: 0 | batch: 6500/10010 ] | Loss: 0.942 | Acc: 76.717% (638382/832128)
01/15/2023 06:43:10 - INFO - __main__ -   test: [epoch: 0 | batch: 6600/10010 ] | Loss: 0.942 | Acc: 76.714% (648179/844928)
01/15/2023 06:46:41 - INFO - __main__ -   test: [epoch: 0 | batch: 6700/10010 ] | Loss: 0.942 | Acc: 76.718% (658030/857728)
01/15/2023 06:50:12 - INFO - __main__ -   test: [epoch: 0 | batch: 6800/10010 ] | Loss: 0.942 | Acc: 76.714% (667814/870528)
01/15/2023 06:53:43 - INFO - __main__ -   test: [epoch: 0 | batch: 6900/10010 ] | Loss: 0.942 | Acc: 76.717% (677662/883328)
01/15/2023 06:57:15 - INFO - __main__ -   test: [epoch: 0 | batch: 7000/10010 ] | Loss: 0.942 | Acc: 76.710% (687419/896128)
01/15/2023 07:00:45 - INFO - __main__ -   test: [epoch: 0 | batch: 7100/10010 ] | Loss: 0.942 | Acc: 76.719% (697323/908928)
01/15/2023 07:04:19 - INFO - __main__ -   test: [epoch: 0 | batch: 7200/10010 ] | Loss: 0.941 | Acc: 76.728% (707220/921728)
01/15/2023 07:07:51 - INFO - __main__ -   test: [epoch: 0 | batch: 7300/10010 ] | Loss: 0.942 | Acc: 76.720% (716966/934528)
01/15/2023 07:11:23 - INFO - __main__ -   test: [epoch: 0 | batch: 7400/10010 ] | Loss: 0.942 | Acc: 76.712% (726713/947328)
01/15/2023 07:14:53 - INFO - __main__ -   test: [epoch: 0 | batch: 7500/10010 ] | Loss: 0.942 | Acc: 76.718% (736595/960128)
01/15/2023 07:18:25 - INFO - __main__ -   test: [epoch: 0 | batch: 7600/10010 ] | Loss: 0.942 | Acc: 76.714% (746374/972928)
01/15/2023 07:21:57 - INFO - __main__ -   test: [epoch: 0 | batch: 7700/10010 ] | Loss: 0.942 | Acc: 76.707% (756120/985728)
01/15/2023 07:25:29 - INFO - __main__ -   test: [epoch: 0 | batch: 7800/10010 ] | Loss: 0.942 | Acc: 76.703% (765903/998528)
01/15/2023 07:29:01 - INFO - __main__ -   test: [epoch: 0 | batch: 7900/10010 ] | Loss: 0.943 | Acc: 76.706% (775750/1011328)
01/15/2023 07:32:32 - INFO - __main__ -   test: [epoch: 0 | batch: 8000/10010 ] | Loss: 0.943 | Acc: 76.701% (785519/1024128)
01/15/2023 07:36:04 - INFO - __main__ -   test: [epoch: 0 | batch: 8100/10010 ] | Loss: 0.942 | Acc: 76.707% (795394/1036928)
01/15/2023 07:39:36 - INFO - __main__ -   test: [epoch: 0 | batch: 8200/10010 ] | Loss: 0.942 | Acc: 76.702% (805161/1049728)
01/15/2023 07:43:08 - INFO - __main__ -   test: [epoch: 0 | batch: 8300/10010 ] | Loss: 0.942 | Acc: 76.706% (815025/1062528)
01/15/2023 07:46:39 - INFO - __main__ -   test: [epoch: 0 | batch: 8400/10010 ] | Loss: 0.942 | Acc: 76.710% (824880/1075328)
01/15/2023 07:50:11 - INFO - __main__ -   test: [epoch: 0 | batch: 8500/10010 ] | Loss: 0.942 | Acc: 76.716% (834769/1088128)
01/15/2023 07:53:41 - INFO - __main__ -   test: [epoch: 0 | batch: 8600/10010 ] | Loss: 0.942 | Acc: 76.709% (844512/1100928)
01/15/2023 07:57:13 - INFO - __main__ -   test: [epoch: 0 | batch: 8700/10010 ] | Loss: 0.942 | Acc: 76.710% (854336/1113728)
01/15/2023 08:00:45 - INFO - __main__ -   test: [epoch: 0 | batch: 8800/10010 ] | Loss: 0.942 | Acc: 76.715% (864211/1126528)
01/15/2023 08:04:18 - INFO - __main__ -   test: [epoch: 0 | batch: 8900/10010 ] | Loss: 0.942 | Acc: 76.716% (874046/1139328)
01/15/2023 08:07:50 - INFO - __main__ -   test: [epoch: 0 | batch: 9000/10010 ] | Loss: 0.942 | Acc: 76.719% (883897/1152128)
01/15/2023 08:11:20 - INFO - __main__ -   test: [epoch: 0 | batch: 9100/10010 ] | Loss: 0.942 | Acc: 76.717% (893701/1164928)
01/15/2023 08:14:53 - INFO - __main__ -   test: [epoch: 0 | batch: 9200/10010 ] | Loss: 0.942 | Acc: 76.717% (903516/1177728)
01/15/2023 08:18:24 - INFO - __main__ -   test: [epoch: 0 | batch: 9300/10010 ] | Loss: 0.942 | Acc: 76.717% (913332/1190528)
01/15/2023 08:21:56 - INFO - __main__ -   test: [epoch: 0 | batch: 9400/10010 ] | Loss: 0.942 | Acc: 76.713% (923115/1203328)
01/15/2023 08:25:28 - INFO - __main__ -   test: [epoch: 0 | batch: 9500/10010 ] | Loss: 0.942 | Acc: 76.718% (932995/1216128)
01/15/2023 08:28:59 - INFO - __main__ -   test: [epoch: 0 | batch: 9600/10010 ] | Loss: 0.942 | Acc: 76.717% (942797/1228928)
01/15/2023 08:32:30 - INFO - __main__ -   test: [epoch: 0 | batch: 9700/10010 ] | Loss: 0.942 | Acc: 76.715% (952590/1241728)
01/15/2023 08:36:01 - INFO - __main__ -   test: [epoch: 0 | batch: 9800/10010 ] | Loss: 0.942 | Acc: 76.710% (962353/1254528)
01/15/2023 08:39:32 - INFO - __main__ -   test: [epoch: 0 | batch: 9900/10010 ] | Loss: 0.942 | Acc: 76.715% (972237/1267328)
01/15/2023 08:43:06 - INFO - __main__ -   test: [epoch: 0 | batch: 10000/10010 ] | Loss: 0.942 | Acc: 76.712% (982014/1280128)
01/15/2023 08:43:25 - INFO - __main__ -   Saving Checkpoint
01/15/2023 08:43:27 - INFO - __main__ -   test: [batch: 0/391 ] | Loss: 0.547 | Acc: 87.500% (112/128)/ 95.312% (122/128)
01/15/2023 08:43:29 - INFO - __main__ -   test: [batch: 1/391 ] | Loss: 0.563 | Acc: 85.156% (218/256)/ 96.484% (247/256)
01/15/2023 08:43:32 - INFO - __main__ -   test: [batch: 2/391 ] | Loss: 0.749 | Acc: 79.688% (306/384)/ 94.271% (362/384)
01/15/2023 08:43:34 - INFO - __main__ -   test: [batch: 3/391 ] | Loss: 0.684 | Acc: 82.422% (422/512)/ 94.922% (486/512)
01/15/2023 08:43:36 - INFO - __main__ -   test: [batch: 4/391 ] | Loss: 0.588 | Acc: 84.844% (543/640)/ 95.781% (613/640)
01/15/2023 08:43:38 - INFO - __main__ -   test: [batch: 5/391 ] | Loss: 0.521 | Acc: 86.198% (662/768)/ 96.484% (741/768)
01/15/2023 08:43:40 - INFO - __main__ -   test: [batch: 6/391 ] | Loss: 0.506 | Acc: 86.719% (777/896)/ 96.429% (864/896)
01/15/2023 08:43:42 - INFO - __main__ -   test: [batch: 7/391 ] | Loss: 0.484 | Acc: 87.695% (898/1024)/ 96.582% (989/1024)
01/15/2023 08:43:45 - INFO - __main__ -   test: [batch: 8/391 ] | Loss: 0.492 | Acc: 87.674% (1010/1152)/ 96.615% (1113/1152)
01/15/2023 08:43:47 - INFO - __main__ -   test: [batch: 9/391 ] | Loss: 0.467 | Acc: 88.359% (1131/1280)/ 96.797% (1239/1280)
01/15/2023 08:43:49 - INFO - __main__ -   test: [batch: 10/391 ] | Loss: 0.512 | Acc: 87.145% (1227/1408)/ 96.733% (1362/1408)
01/15/2023 08:43:51 - INFO - __main__ -   test: [batch: 11/391 ] | Loss: 0.513 | Acc: 87.370% (1342/1536)/ 96.615% (1484/1536)
01/15/2023 08:43:53 - INFO - __main__ -   test: [batch: 12/391 ] | Loss: 0.566 | Acc: 85.817% (1428/1664)/ 96.154% (1600/1664)
01/15/2023 08:43:55 - INFO - __main__ -   test: [batch: 13/391 ] | Loss: 0.625 | Acc: 84.319% (1511/1792)/ 95.368% (1709/1792)
01/15/2023 08:43:57 - INFO - __main__ -   test: [batch: 14/391 ] | Loss: 0.640 | Acc: 83.542% (1604/1920)/ 95.521% (1834/1920)
01/15/2023 08:43:59 - INFO - __main__ -   test: [batch: 15/391 ] | Loss: 0.645 | Acc: 82.959% (1699/2048)/ 95.654% (1959/2048)
01/15/2023 08:44:01 - INFO - __main__ -   test: [batch: 16/391 ] | Loss: 0.652 | Acc: 82.858% (1803/2176)/ 95.542% (2079/2176)
01/15/2023 08:44:04 - INFO - __main__ -   test: [batch: 17/391 ] | Loss: 0.678 | Acc: 82.378% (1898/2304)/ 95.139% (2192/2304)
01/15/2023 08:44:06 - INFO - __main__ -   test: [batch: 18/391 ] | Loss: 0.702 | Acc: 81.908% (1992/2432)/ 94.984% (2310/2432)
01/15/2023 08:44:08 - INFO - __main__ -   test: [batch: 19/391 ] | Loss: 0.710 | Acc: 81.602% (2089/2560)/ 94.844% (2428/2560)
01/15/2023 08:44:10 - INFO - __main__ -   test: [batch: 20/391 ] | Loss: 0.705 | Acc: 81.734% (2197/2688)/ 94.829% (2549/2688)
01/15/2023 08:44:12 - INFO - __main__ -   test: [batch: 21/391 ] | Loss: 0.742 | Acc: 80.788% (2275/2816)/ 94.673% (2666/2816)
01/15/2023 08:44:14 - INFO - __main__ -   test: [batch: 22/391 ] | Loss: 0.742 | Acc: 80.571% (2372/2944)/ 94.701% (2788/2944)
01/15/2023 08:44:16 - INFO - __main__ -   test: [batch: 23/391 ] | Loss: 0.783 | Acc: 79.753% (2450/3072)/ 94.368% (2899/3072)
01/15/2023 08:44:18 - INFO - __main__ -   test: [batch: 24/391 ] | Loss: 0.801 | Acc: 79.312% (2538/3200)/ 94.250% (3016/3200)
01/15/2023 08:44:21 - INFO - __main__ -   test: [batch: 25/391 ] | Loss: 0.828 | Acc: 78.576% (2615/3328)/ 93.990% (3128/3328)
01/15/2023 08:44:23 - INFO - __main__ -   test: [batch: 26/391 ] | Loss: 0.844 | Acc: 77.894% (2692/3456)/ 93.895% (3245/3456)
01/15/2023 08:44:25 - INFO - __main__ -   test: [batch: 27/391 ] | Loss: 0.826 | Acc: 78.376% (2809/3584)/ 93.945% (3367/3584)
01/15/2023 08:44:27 - INFO - __main__ -   test: [batch: 28/391 ] | Loss: 0.831 | Acc: 77.909% (2892/3712)/ 94.073% (3492/3712)
01/15/2023 08:44:29 - INFO - __main__ -   test: [batch: 29/391 ] | Loss: 0.823 | Acc: 78.047% (2997/3840)/ 94.219% (3618/3840)
01/15/2023 08:44:31 - INFO - __main__ -   test: [batch: 30/391 ] | Loss: 0.836 | Acc: 77.898% (3091/3968)/ 94.052% (3732/3968)
01/15/2023 08:44:33 - INFO - __main__ -   test: [batch: 31/391 ] | Loss: 0.831 | Acc: 78.125% (3200/4096)/ 94.141% (3856/4096)
01/15/2023 08:44:35 - INFO - __main__ -   test: [batch: 32/391 ] | Loss: 0.816 | Acc: 78.456% (3314/4224)/ 94.295% (3983/4224)
01/15/2023 08:44:37 - INFO - __main__ -   test: [batch: 33/391 ] | Loss: 0.809 | Acc: 78.653% (3423/4352)/ 94.347% (4106/4352)
01/15/2023 08:44:40 - INFO - __main__ -   test: [batch: 34/391 ] | Loss: 0.794 | Acc: 79.107% (3544/4480)/ 94.420% (4230/4480)
01/15/2023 08:44:42 - INFO - __main__ -   test: [batch: 35/391 ] | Loss: 0.780 | Acc: 79.470% (3662/4608)/ 94.466% (4353/4608)
01/15/2023 08:44:44 - INFO - __main__ -   test: [batch: 36/391 ] | Loss: 0.765 | Acc: 79.920% (3785/4736)/ 94.595% (4480/4736)
01/15/2023 08:44:46 - INFO - __main__ -   test: [batch: 37/391 ] | Loss: 0.755 | Acc: 80.222% (3902/4864)/ 94.675% (4605/4864)
01/15/2023 08:44:48 - INFO - __main__ -   test: [batch: 38/391 ] | Loss: 0.746 | Acc: 80.429% (4015/4992)/ 94.752% (4730/4992)
01/15/2023 08:44:50 - INFO - __main__ -   test: [batch: 39/391 ] | Loss: 0.741 | Acc: 80.625% (4128/5120)/ 94.785% (4853/5120)
01/15/2023 08:44:53 - INFO - __main__ -   test: [batch: 40/391 ] | Loss: 0.739 | Acc: 80.755% (4238/5248)/ 94.703% (4970/5248)
01/15/2023 08:44:55 - INFO - __main__ -   test: [batch: 41/391 ] | Loss: 0.741 | Acc: 80.915% (4350/5376)/ 94.624% (5087/5376)
01/15/2023 08:44:57 - INFO - __main__ -   test: [batch: 42/391 ] | Loss: 0.741 | Acc: 80.977% (4457/5504)/ 94.677% (5211/5504)
01/15/2023 08:44:59 - INFO - __main__ -   test: [batch: 43/391 ] | Loss: 0.740 | Acc: 81.001% (4562/5632)/ 94.673% (5332/5632)
01/15/2023 08:45:01 - INFO - __main__ -   test: [batch: 44/391 ] | Loss: 0.745 | Acc: 80.990% (4665/5760)/ 94.583% (5448/5760)
01/15/2023 08:45:03 - INFO - __main__ -   test: [batch: 45/391 ] | Loss: 0.741 | Acc: 81.114% (4776/5888)/ 94.633% (5572/5888)
01/15/2023 08:45:05 - INFO - __main__ -   test: [batch: 46/391 ] | Loss: 0.744 | Acc: 81.034% (4875/6016)/ 94.681% (5696/6016)
01/15/2023 08:45:07 - INFO - __main__ -   test: [batch: 47/391 ] | Loss: 0.747 | Acc: 80.908% (4971/6144)/ 94.727% (5820/6144)
01/15/2023 08:45:10 - INFO - __main__ -   test: [batch: 48/391 ] | Loss: 0.752 | Acc: 80.788% (5067/6272)/ 94.707% (5940/6272)
01/15/2023 08:45:12 - INFO - __main__ -   test: [batch: 49/391 ] | Loss: 0.756 | Acc: 80.766% (5169/6400)/ 94.641% (6057/6400)
01/15/2023 08:45:14 - INFO - __main__ -   test: [batch: 50/391 ] | Loss: 0.748 | Acc: 80.959% (5285/6528)/ 94.730% (6184/6528)
01/15/2023 08:45:16 - INFO - __main__ -   test: [batch: 51/391 ] | Loss: 0.739 | Acc: 81.175% (5403/6656)/ 94.817% (6311/6656)
01/15/2023 08:45:18 - INFO - __main__ -   test: [batch: 52/391 ] | Loss: 0.735 | Acc: 81.250% (5512/6784)/ 94.856% (6435/6784)
01/15/2023 08:45:21 - INFO - __main__ -   test: [batch: 53/391 ] | Loss: 0.726 | Acc: 81.481% (5632/6912)/ 94.936% (6562/6912)
01/15/2023 08:45:23 - INFO - __main__ -   test: [batch: 54/391 ] | Loss: 0.719 | Acc: 81.662% (5749/7040)/ 94.972% (6686/7040)
01/15/2023 08:45:25 - INFO - __main__ -   test: [batch: 55/391 ] | Loss: 0.712 | Acc: 81.808% (5864/7168)/ 95.020% (6811/7168)
01/15/2023 08:45:27 - INFO - __main__ -   test: [batch: 56/391 ] | Loss: 0.703 | Acc: 82.072% (5988/7296)/ 95.066% (6936/7296)
01/15/2023 08:45:29 - INFO - __main__ -   test: [batch: 57/391 ] | Loss: 0.696 | Acc: 82.274% (6108/7424)/ 95.110% (7061/7424)
01/15/2023 08:45:31 - INFO - __main__ -   test: [batch: 58/391 ] | Loss: 0.692 | Acc: 82.362% (6220/7552)/ 95.114% (7183/7552)
01/15/2023 08:45:33 - INFO - __main__ -   test: [batch: 59/391 ] | Loss: 0.694 | Acc: 82.266% (6318/7680)/ 95.104% (7304/7680)
01/15/2023 08:45:35 - INFO - __main__ -   test: [batch: 60/391 ] | Loss: 0.697 | Acc: 82.198% (6418/7808)/ 95.069% (7423/7808)
01/15/2023 08:45:37 - INFO - __main__ -   test: [batch: 61/391 ] | Loss: 0.697 | Acc: 82.195% (6523/7936)/ 95.098% (7547/7936)
01/15/2023 08:45:39 - INFO - __main__ -   test: [batch: 62/391 ] | Loss: 0.694 | Acc: 82.168% (6626/8064)/ 95.151% (7673/8064)
01/15/2023 08:45:41 - INFO - __main__ -   test: [batch: 63/391 ] | Loss: 0.702 | Acc: 82.043% (6721/8192)/ 95.093% (7790/8192)
01/15/2023 08:45:43 - INFO - __main__ -   test: [batch: 64/391 ] | Loss: 0.710 | Acc: 81.839% (6809/8320)/ 95.048% (7908/8320)
01/15/2023 08:45:46 - INFO - __main__ -   test: [batch: 65/391 ] | Loss: 0.722 | Acc: 81.416% (6878/8448)/ 94.969% (8023/8448)
01/15/2023 08:45:48 - INFO - __main__ -   test: [batch: 66/391 ] | Loss: 0.727 | Acc: 81.437% (6984/8576)/ 94.939% (8142/8576)
01/15/2023 08:45:50 - INFO - __main__ -   test: [batch: 67/391 ] | Loss: 0.730 | Acc: 81.399% (7085/8704)/ 94.933% (8263/8704)
01/15/2023 08:45:52 - INFO - __main__ -   test: [batch: 68/391 ] | Loss: 0.731 | Acc: 81.329% (7183/8832)/ 94.962% (8387/8832)
01/15/2023 08:45:54 - INFO - __main__ -   test: [batch: 69/391 ] | Loss: 0.727 | Acc: 81.429% (7296/8960)/ 95.011% (8513/8960)
01/15/2023 08:45:56 - INFO - __main__ -   test: [batch: 70/391 ] | Loss: 0.727 | Acc: 81.360% (7394/9088)/ 95.026% (8636/9088)
01/15/2023 08:45:58 - INFO - __main__ -   test: [batch: 71/391 ] | Loss: 0.725 | Acc: 81.456% (7507/9216)/ 95.020% (8757/9216)
01/15/2023 08:46:01 - INFO - __main__ -   test: [batch: 72/391 ] | Loss: 0.728 | Acc: 81.325% (7599/9344)/ 95.024% (8879/9344)
01/15/2023 08:46:03 - INFO - __main__ -   test: [batch: 73/391 ] | Loss: 0.731 | Acc: 81.208% (7692/9472)/ 95.017% (9000/9472)
01/15/2023 08:46:05 - INFO - __main__ -   test: [batch: 74/391 ] | Loss: 0.731 | Acc: 81.208% (7796/9600)/ 95.000% (9120/9600)
01/15/2023 08:46:07 - INFO - __main__ -   test: [batch: 75/391 ] | Loss: 0.737 | Acc: 81.003% (7880/9728)/ 94.984% (9240/9728)
01/15/2023 08:46:09 - INFO - __main__ -   test: [batch: 76/391 ] | Loss: 0.736 | Acc: 81.027% (7986/9856)/ 95.008% (9364/9856)
01/15/2023 08:46:12 - INFO - __main__ -   test: [batch: 77/391 ] | Loss: 0.737 | Acc: 80.950% (8082/9984)/ 95.042% (9489/9984)
01/15/2023 08:46:14 - INFO - __main__ -   test: [batch: 78/391 ] | Loss: 0.736 | Acc: 80.924% (8183/10112)/ 95.085% (9615/10112)
01/15/2023 08:46:16 - INFO - __main__ -   test: [batch: 79/391 ] | Loss: 0.735 | Acc: 80.908% (8285/10240)/ 95.107% (9739/10240)
01/15/2023 08:46:18 - INFO - __main__ -   test: [batch: 80/391 ] | Loss: 0.735 | Acc: 80.874% (8385/10368)/ 95.091% (9859/10368)
01/15/2023 08:46:20 - INFO - __main__ -   test: [batch: 81/391 ] | Loss: 0.735 | Acc: 80.878% (8489/10496)/ 95.112% (9983/10496)
01/15/2023 08:46:22 - INFO - __main__ -   test: [batch: 82/391 ] | Loss: 0.734 | Acc: 80.892% (8594/10624)/ 95.115% (10105/10624)
01/15/2023 08:46:24 - INFO - __main__ -   test: [batch: 83/391 ] | Loss: 0.733 | Acc: 80.934% (8702/10752)/ 95.089% (10224/10752)
01/15/2023 08:46:27 - INFO - __main__ -   test: [batch: 84/391 ] | Loss: 0.729 | Acc: 81.048% (8818/10880)/ 95.129% (10350/10880)
01/15/2023 08:46:29 - INFO - __main__ -   test: [batch: 85/391 ] | Loss: 0.728 | Acc: 81.014% (8918/11008)/ 95.176% (10477/11008)
01/15/2023 08:46:31 - INFO - __main__ -   test: [batch: 86/391 ] | Loss: 0.732 | Acc: 80.936% (9013/11136)/ 95.133% (10594/11136)
01/15/2023 08:46:33 - INFO - __main__ -   test: [batch: 87/391 ] | Loss: 0.731 | Acc: 80.984% (9122/11264)/ 95.126% (10715/11264)
01/15/2023 08:46:35 - INFO - __main__ -   test: [batch: 88/391 ] | Loss: 0.738 | Acc: 80.925% (9219/11392)/ 95.067% (10830/11392)
01/15/2023 08:46:37 - INFO - __main__ -   test: [batch: 89/391 ] | Loss: 0.736 | Acc: 80.955% (9326/11520)/ 95.078% (10953/11520)
01/15/2023 08:46:40 - INFO - __main__ -   test: [batch: 90/391 ] | Loss: 0.737 | Acc: 80.864% (9419/11648)/ 95.089% (11076/11648)
01/15/2023 08:46:42 - INFO - __main__ -   test: [batch: 91/391 ] | Loss: 0.735 | Acc: 80.927% (9530/11776)/ 95.109% (11200/11776)
01/15/2023 08:46:44 - INFO - __main__ -   test: [batch: 92/391 ] | Loss: 0.737 | Acc: 80.880% (9628/11904)/ 95.086% (11319/11904)
01/15/2023 08:46:46 - INFO - __main__ -   test: [batch: 93/391 ] | Loss: 0.740 | Acc: 80.677% (9707/12032)/ 95.130% (11446/12032)
01/15/2023 08:46:48 - INFO - __main__ -   test: [batch: 94/391 ] | Loss: 0.743 | Acc: 80.526% (9792/12160)/ 95.132% (11568/12160)
01/15/2023 08:46:50 - INFO - __main__ -   test: [batch: 95/391 ] | Loss: 0.740 | Acc: 80.583% (9902/12288)/ 95.150% (11692/12288)
01/15/2023 08:46:52 - INFO - __main__ -   test: [batch: 96/391 ] | Loss: 0.744 | Acc: 80.461% (9990/12416)/ 95.159% (11815/12416)
01/15/2023 08:46:54 - INFO - __main__ -   test: [batch: 97/391 ] | Loss: 0.746 | Acc: 80.277% (10070/12544)/ 95.177% (11939/12544)
01/15/2023 08:46:56 - INFO - __main__ -   test: [batch: 98/391 ] | Loss: 0.742 | Acc: 80.374% (10185/12672)/ 95.210% (12065/12672)
01/15/2023 08:46:58 - INFO - __main__ -   test: [batch: 99/391 ] | Loss: 0.736 | Acc: 80.531% (10308/12800)/ 95.258% (12193/12800)
01/15/2023 08:47:01 - INFO - __main__ -   test: [batch: 100/391 ] | Loss: 0.735 | Acc: 80.577% (10417/12928)/ 95.282% (12318/12928)
01/15/2023 08:47:03 - INFO - __main__ -   test: [batch: 101/391 ] | Loss: 0.730 | Acc: 80.699% (10536/13056)/ 95.312% (12444/13056)
01/15/2023 08:47:05 - INFO - __main__ -   test: [batch: 102/391 ] | Loss: 0.727 | Acc: 80.780% (10650/13184)/ 95.343% (12570/13184)
01/15/2023 08:47:07 - INFO - __main__ -   test: [batch: 103/391 ] | Loss: 0.729 | Acc: 80.619% (10732/13312)/ 95.343% (12692/13312)
01/15/2023 08:47:09 - INFO - __main__ -   test: [batch: 104/391 ] | Loss: 0.729 | Acc: 80.543% (10825/13440)/ 95.342% (12814/13440)
01/15/2023 08:47:11 - INFO - __main__ -   test: [batch: 105/391 ] | Loss: 0.730 | Acc: 80.542% (10928/13568)/ 95.349% (12937/13568)
01/15/2023 08:47:13 - INFO - __main__ -   test: [batch: 106/391 ] | Loss: 0.738 | Acc: 80.454% (11019/13696)/ 95.291% (13051/13696)
01/15/2023 08:47:15 - INFO - __main__ -   test: [batch: 107/391 ] | Loss: 0.734 | Acc: 80.584% (11140/13824)/ 95.327% (13178/13824)
01/15/2023 08:47:17 - INFO - __main__ -   test: [batch: 108/391 ] | Loss: 0.737 | Acc: 80.440% (11223/13952)/ 95.334% (13301/13952)
01/15/2023 08:47:20 - INFO - __main__ -   test: [batch: 109/391 ] | Loss: 0.737 | Acc: 80.455% (11328/14080)/ 95.334% (13423/14080)
01/15/2023 08:47:22 - INFO - __main__ -   test: [batch: 110/391 ] | Loss: 0.740 | Acc: 80.265% (11404/14208)/ 95.355% (13548/14208)
01/15/2023 08:47:24 - INFO - __main__ -   test: [batch: 111/391 ] | Loss: 0.743 | Acc: 80.218% (11500/14336)/ 95.319% (13665/14336)
01/15/2023 08:47:26 - INFO - __main__ -   test: [batch: 112/391 ] | Loss: 0.743 | Acc: 80.234% (11605/14464)/ 95.340% (13790/14464)
01/15/2023 08:47:28 - INFO - __main__ -   test: [batch: 113/391 ] | Loss: 0.742 | Acc: 80.270% (11713/14592)/ 95.347% (13913/14592)
01/15/2023 08:47:30 - INFO - __main__ -   test: [batch: 114/391 ] | Loss: 0.738 | Acc: 80.374% (11831/14720)/ 95.374% (14039/14720)
01/15/2023 08:47:32 - INFO - __main__ -   test: [batch: 115/391 ] | Loss: 0.735 | Acc: 80.455% (11946/14848)/ 95.393% (14164/14848)
01/15/2023 08:47:35 - INFO - __main__ -   test: [batch: 116/391 ] | Loss: 0.738 | Acc: 80.435% (12046/14976)/ 95.359% (14281/14976)
01/15/2023 08:47:37 - INFO - __main__ -   test: [batch: 117/391 ] | Loss: 0.737 | Acc: 80.462% (12153/15104)/ 95.379% (14406/15104)
01/15/2023 08:47:39 - INFO - __main__ -   test: [batch: 118/391 ] | Loss: 0.740 | Acc: 80.311% (12233/15232)/ 95.391% (14530/15232)
01/15/2023 08:47:41 - INFO - __main__ -   test: [batch: 119/391 ] | Loss: 0.738 | Acc: 80.358% (12343/15360)/ 95.417% (14656/15360)
01/15/2023 08:47:43 - INFO - __main__ -   test: [batch: 120/391 ] | Loss: 0.737 | Acc: 80.417% (12455/15488)/ 95.435% (14781/15488)
01/15/2023 08:47:45 - INFO - __main__ -   test: [batch: 121/391 ] | Loss: 0.742 | Acc: 80.289% (12538/15616)/ 95.396% (14897/15616)
01/15/2023 08:47:47 - INFO - __main__ -   test: [batch: 122/391 ] | Loss: 0.745 | Acc: 80.240% (12633/15744)/ 95.370% (15015/15744)
01/15/2023 08:47:49 - INFO - __main__ -   test: [batch: 123/391 ] | Loss: 0.746 | Acc: 80.261% (12739/15872)/ 95.350% (15134/15872)
01/15/2023 08:47:51 - INFO - __main__ -   test: [batch: 124/391 ] | Loss: 0.745 | Acc: 80.294% (12847/16000)/ 95.362% (15258/16000)
01/15/2023 08:47:53 - INFO - __main__ -   test: [batch: 125/391 ] | Loss: 0.741 | Acc: 80.394% (12966/16128)/ 95.393% (15385/16128)
01/15/2023 08:47:56 - INFO - __main__ -   test: [batch: 126/391 ] | Loss: 0.738 | Acc: 80.487% (13084/16256)/ 95.399% (15508/16256)
01/15/2023 08:47:58 - INFO - __main__ -   test: [batch: 127/391 ] | Loss: 0.736 | Acc: 80.560% (13199/16384)/ 95.410% (15632/16384)
01/15/2023 08:48:00 - INFO - __main__ -   test: [batch: 128/391 ] | Loss: 0.737 | Acc: 80.554% (13301/16512)/ 95.385% (15750/16512)
01/15/2023 08:48:02 - INFO - __main__ -   test: [batch: 129/391 ] | Loss: 0.735 | Acc: 80.595% (13411/16640)/ 95.415% (15877/16640)
01/15/2023 08:48:04 - INFO - __main__ -   test: [batch: 130/391 ] | Loss: 0.732 | Acc: 80.707% (13533/16768)/ 95.438% (16003/16768)
01/15/2023 08:48:06 - INFO - __main__ -   test: [batch: 131/391 ] | Loss: 0.731 | Acc: 80.759% (13645/16896)/ 95.449% (16127/16896)
01/15/2023 08:48:08 - INFO - __main__ -   test: [batch: 132/391 ] | Loss: 0.729 | Acc: 80.821% (13759/17024)/ 95.465% (16252/17024)
01/15/2023 08:48:10 - INFO - __main__ -   test: [batch: 133/391 ] | Loss: 0.731 | Acc: 80.766% (13853/17152)/ 95.452% (16372/17152)
01/15/2023 08:48:13 - INFO - __main__ -   test: [batch: 134/391 ] | Loss: 0.729 | Acc: 80.793% (13961/17280)/ 95.469% (16497/17280)
01/15/2023 08:48:15 - INFO - __main__ -   test: [batch: 135/391 ] | Loss: 0.729 | Acc: 80.779% (14062/17408)/ 95.473% (16620/17408)
01/15/2023 08:48:17 - INFO - __main__ -   test: [batch: 136/391 ] | Loss: 0.728 | Acc: 80.765% (14163/17536)/ 95.489% (16745/17536)
01/15/2023 08:48:19 - INFO - __main__ -   test: [batch: 137/391 ] | Loss: 0.725 | Acc: 80.814% (14275/17664)/ 95.522% (16873/17664)
01/15/2023 08:48:21 - INFO - __main__ -   test: [batch: 138/391 ] | Loss: 0.724 | Acc: 80.857% (14386/17792)/ 95.537% (16998/17792)
01/15/2023 08:48:23 - INFO - __main__ -   test: [batch: 139/391 ] | Loss: 0.729 | Acc: 80.720% (14465/17920)/ 95.525% (17118/17920)
01/15/2023 08:48:25 - INFO - __main__ -   test: [batch: 140/391 ] | Loss: 0.731 | Acc: 80.641% (14554/18048)/ 95.518% (17239/18048)
01/15/2023 08:48:27 - INFO - __main__ -   test: [batch: 141/391 ] | Loss: 0.729 | Acc: 80.672% (14663/18176)/ 95.533% (17364/18176)
01/15/2023 08:48:29 - INFO - __main__ -   test: [batch: 142/391 ] | Loss: 0.727 | Acc: 80.753% (14781/18304)/ 95.542% (17488/18304)
01/15/2023 08:48:31 - INFO - __main__ -   test: [batch: 143/391 ] | Loss: 0.727 | Acc: 80.778% (14889/18432)/ 95.535% (17609/18432)
01/15/2023 08:48:34 - INFO - __main__ -   test: [batch: 144/391 ] | Loss: 0.729 | Acc: 80.754% (14988/18560)/ 95.512% (17727/18560)
01/15/2023 08:48:36 - INFO - __main__ -   test: [batch: 145/391 ] | Loss: 0.732 | Acc: 80.736% (15088/18688)/ 95.489% (17845/18688)
01/15/2023 08:48:38 - INFO - __main__ -   test: [batch: 146/391 ] | Loss: 0.733 | Acc: 80.713% (15187/18816)/ 95.467% (17963/18816)
01/15/2023 08:48:40 - INFO - __main__ -   test: [batch: 147/391 ] | Loss: 0.734 | Acc: 80.696% (15287/18944)/ 95.444% (18081/18944)
01/15/2023 08:48:42 - INFO - __main__ -   test: [batch: 148/391 ] | Loss: 0.735 | Acc: 80.616% (15375/19072)/ 95.444% (18203/19072)
01/15/2023 08:48:44 - INFO - __main__ -   test: [batch: 149/391 ] | Loss: 0.739 | Acc: 80.531% (15462/19200)/ 95.406% (18318/19200)
01/15/2023 08:48:46 - INFO - __main__ -   test: [batch: 150/391 ] | Loss: 0.739 | Acc: 80.479% (15555/19328)/ 95.416% (18442/19328)
01/15/2023 08:48:48 - INFO - __main__ -   test: [batch: 151/391 ] | Loss: 0.739 | Acc: 80.520% (15666/19456)/ 95.410% (18563/19456)
01/15/2023 08:48:50 - INFO - __main__ -   test: [batch: 152/391 ] | Loss: 0.740 | Acc: 80.499% (15765/19584)/ 95.389% (18681/19584)
01/15/2023 08:48:52 - INFO - __main__ -   test: [batch: 153/391 ] | Loss: 0.739 | Acc: 80.555% (15879/19712)/ 95.399% (18805/19712)
01/15/2023 08:48:55 - INFO - __main__ -   test: [batch: 154/391 ] | Loss: 0.740 | Acc: 80.524% (15976/19840)/ 95.358% (18919/19840)
01/15/2023 08:48:57 - INFO - __main__ -   test: [batch: 155/391 ] | Loss: 0.741 | Acc: 80.529% (16080/19968)/ 95.343% (19038/19968)
01/15/2023 08:48:59 - INFO - __main__ -   test: [batch: 156/391 ] | Loss: 0.743 | Acc: 80.459% (16169/20096)/ 95.327% (19157/20096)
01/15/2023 08:49:01 - INFO - __main__ -   test: [batch: 157/391 ] | Loss: 0.745 | Acc: 80.409% (16262/20224)/ 95.303% (19274/20224)
01/15/2023 08:49:03 - INFO - __main__ -   test: [batch: 158/391 ] | Loss: 0.746 | Acc: 80.385% (16360/20352)/ 95.278% (19391/20352)
01/15/2023 08:49:05 - INFO - __main__ -   test: [batch: 159/391 ] | Loss: 0.748 | Acc: 80.366% (16459/20480)/ 95.283% (19514/20480)
01/15/2023 08:49:08 - INFO - __main__ -   test: [batch: 160/391 ] | Loss: 0.748 | Acc: 80.372% (16563/20608)/ 95.264% (19632/20608)
01/15/2023 08:49:10 - INFO - __main__ -   test: [batch: 161/391 ] | Loss: 0.758 | Acc: 80.131% (16616/20736)/ 95.153% (19731/20736)
01/15/2023 08:49:12 - INFO - __main__ -   test: [batch: 162/391 ] | Loss: 0.764 | Acc: 80.033% (16698/20864)/ 95.073% (19836/20864)
01/15/2023 08:49:14 - INFO - __main__ -   test: [batch: 163/391 ] | Loss: 0.766 | Acc: 79.954% (16784/20992)/ 95.060% (19955/20992)
01/15/2023 08:49:16 - INFO - __main__ -   test: [batch: 164/391 ] | Loss: 0.766 | Acc: 79.934% (16882/21120)/ 95.076% (20080/21120)
01/15/2023 08:49:18 - INFO - __main__ -   test: [batch: 165/391 ] | Loss: 0.768 | Acc: 79.871% (16971/21248)/ 95.077% (20202/21248)
01/15/2023 08:49:20 - INFO - __main__ -   test: [batch: 166/391 ] | Loss: 0.768 | Acc: 79.893% (17078/21376)/ 95.069% (20322/21376)
01/15/2023 08:49:23 - INFO - __main__ -   test: [batch: 167/391 ] | Loss: 0.770 | Acc: 79.846% (17170/21504)/ 95.043% (20438/21504)
01/15/2023 08:49:25 - INFO - __main__ -   test: [batch: 168/391 ] | Loss: 0.770 | Acc: 79.845% (17272/21632)/ 95.040% (20559/21632)
01/15/2023 08:49:27 - INFO - __main__ -   test: [batch: 169/391 ] | Loss: 0.772 | Acc: 79.798% (17364/21760)/ 94.991% (20670/21760)
01/15/2023 08:49:29 - INFO - __main__ -   test: [batch: 170/391 ] | Loss: 0.777 | Acc: 79.701% (17445/21888)/ 94.956% (20784/21888)
01/15/2023 08:49:31 - INFO - __main__ -   test: [batch: 171/391 ] | Loss: 0.780 | Acc: 79.660% (17538/22016)/ 94.926% (20899/22016)
01/15/2023 08:49:33 - INFO - __main__ -   test: [batch: 172/391 ] | Loss: 0.781 | Acc: 79.620% (17631/22144)/ 94.924% (21020/22144)
01/15/2023 08:49:35 - INFO - __main__ -   test: [batch: 173/391 ] | Loss: 0.783 | Acc: 79.571% (17722/22272)/ 94.895% (21135/22272)
01/15/2023 08:49:37 - INFO - __main__ -   test: [batch: 174/391 ] | Loss: 0.787 | Acc: 79.473% (17802/22400)/ 94.853% (21247/22400)
01/15/2023 08:49:40 - INFO - __main__ -   test: [batch: 175/391 ] | Loss: 0.786 | Acc: 79.532% (17917/22528)/ 94.864% (21371/22528)
01/15/2023 08:49:42 - INFO - __main__ -   test: [batch: 176/391 ] | Loss: 0.786 | Acc: 79.524% (18017/22656)/ 94.840% (21487/22656)
01/15/2023 08:49:44 - INFO - __main__ -   test: [batch: 177/391 ] | Loss: 0.790 | Acc: 79.446% (18101/22784)/ 94.803% (21600/22784)
01/15/2023 08:49:46 - INFO - __main__ -   test: [batch: 178/391 ] | Loss: 0.791 | Acc: 79.447% (18203/22912)/ 94.789% (21718/22912)
01/15/2023 08:49:48 - INFO - __main__ -   test: [batch: 179/391 ] | Loss: 0.795 | Acc: 79.388% (18291/23040)/ 94.740% (21828/23040)
01/15/2023 08:49:50 - INFO - __main__ -   test: [batch: 180/391 ] | Loss: 0.801 | Acc: 79.260% (18363/23168)/ 94.700% (21940/23168)
01/15/2023 08:49:52 - INFO - __main__ -   test: [batch: 181/391 ] | Loss: 0.807 | Acc: 79.142% (18437/23296)/ 94.639% (22047/23296)
01/15/2023 08:49:54 - INFO - __main__ -   test: [batch: 182/391 ] | Loss: 0.806 | Acc: 79.167% (18544/23424)/ 94.647% (22170/23424)
01/15/2023 08:49:57 - INFO - __main__ -   test: [batch: 183/391 ] | Loss: 0.812 | Acc: 79.046% (18617/23552)/ 94.548% (22268/23552)
01/15/2023 08:49:59 - INFO - __main__ -   test: [batch: 184/391 ] | Loss: 0.812 | Acc: 79.075% (18725/23680)/ 94.544% (22388/23680)
01/15/2023 08:50:01 - INFO - __main__ -   test: [batch: 185/391 ] | Loss: 0.812 | Acc: 79.091% (18830/23808)/ 94.540% (22508/23808)
01/15/2023 08:50:03 - INFO - __main__ -   test: [batch: 186/391 ] | Loss: 0.814 | Acc: 79.044% (18920/23936)/ 94.498% (22619/23936)
01/15/2023 08:50:05 - INFO - __main__ -   test: [batch: 187/391 ] | Loss: 0.819 | Acc: 78.931% (18994/24064)/ 94.477% (22735/24064)
01/15/2023 08:50:07 - INFO - __main__ -   test: [batch: 188/391 ] | Loss: 0.822 | Acc: 78.815% (19067/24192)/ 94.440% (22847/24192)
01/15/2023 08:50:09 - INFO - __main__ -   test: [batch: 189/391 ] | Loss: 0.824 | Acc: 78.750% (19152/24320)/ 94.433% (22966/24320)
01/15/2023 08:50:11 - INFO - __main__ -   test: [batch: 190/391 ] | Loss: 0.829 | Acc: 78.657% (19230/24448)/ 94.404% (23080/24448)
01/15/2023 08:50:13 - INFO - __main__ -   test: [batch: 191/391 ] | Loss: 0.831 | Acc: 78.621% (19322/24576)/ 94.364% (23191/24576)
01/15/2023 08:50:15 - INFO - __main__ -   test: [batch: 192/391 ] | Loss: 0.838 | Acc: 78.497% (19392/24704)/ 94.268% (23288/24704)
01/15/2023 08:50:17 - INFO - __main__ -   test: [batch: 193/391 ] | Loss: 0.838 | Acc: 78.508% (19495/24832)/ 94.261% (23407/24832)
01/15/2023 08:50:19 - INFO - __main__ -   test: [batch: 194/391 ] | Loss: 0.842 | Acc: 78.433% (19577/24960)/ 94.231% (23520/24960)
01/15/2023 08:50:22 - INFO - __main__ -   test: [batch: 195/391 ] | Loss: 0.847 | Acc: 78.328% (19651/25088)/ 94.161% (23623/25088)
01/15/2023 08:50:24 - INFO - __main__ -   test: [batch: 196/391 ] | Loss: 0.851 | Acc: 78.236% (19728/25216)/ 94.115% (23732/25216)
01/15/2023 08:50:26 - INFO - __main__ -   test: [batch: 197/391 ] | Loss: 0.855 | Acc: 78.153% (19807/25344)/ 94.097% (23848/25344)
01/15/2023 08:50:28 - INFO - __main__ -   test: [batch: 198/391 ] | Loss: 0.857 | Acc: 78.101% (19894/25472)/ 94.064% (23960/25472)
01/15/2023 08:50:30 - INFO - __main__ -   test: [batch: 199/391 ] | Loss: 0.857 | Acc: 78.070% (19986/25600)/ 94.074% (24083/25600)
01/15/2023 08:50:32 - INFO - __main__ -   test: [batch: 200/391 ] | Loss: 0.857 | Acc: 78.043% (20079/25728)/ 94.080% (24205/25728)
01/15/2023 08:50:34 - INFO - __main__ -   test: [batch: 201/391 ] | Loss: 0.861 | Acc: 77.943% (20153/25856)/ 94.044% (24316/25856)
01/15/2023 08:50:36 - INFO - __main__ -   test: [batch: 202/391 ] | Loss: 0.861 | Acc: 77.933% (20250/25984)/ 94.042% (24436/25984)
01/15/2023 08:50:38 - INFO - __main__ -   test: [batch: 203/391 ] | Loss: 0.862 | Acc: 77.914% (20345/26112)/ 94.026% (24552/26112)
01/15/2023 08:50:41 - INFO - __main__ -   test: [batch: 204/391 ] | Loss: 0.866 | Acc: 77.828% (20422/26240)/ 93.998% (24665/26240)
01/15/2023 08:50:43 - INFO - __main__ -   test: [batch: 205/391 ] | Loss: 0.868 | Acc: 77.750% (20501/26368)/ 93.970% (24778/26368)
01/15/2023 08:50:45 - INFO - __main__ -   test: [batch: 206/391 ] | Loss: 0.870 | Acc: 77.717% (20592/26496)/ 93.969% (24898/26496)
01/15/2023 08:50:47 - INFO - __main__ -   test: [batch: 207/391 ] | Loss: 0.874 | Acc: 77.640% (20671/26624)/ 93.938% (25010/26624)
01/15/2023 08:50:49 - INFO - __main__ -   test: [batch: 208/391 ] | Loss: 0.875 | Acc: 77.598% (20759/26752)/ 93.896% (25119/26752)
01/15/2023 08:50:51 - INFO - __main__ -   test: [batch: 209/391 ] | Loss: 0.875 | Acc: 77.604% (20860/26880)/ 93.917% (25245/26880)
01/15/2023 08:50:53 - INFO - __main__ -   test: [batch: 210/391 ] | Loss: 0.877 | Acc: 77.570% (20950/27008)/ 93.898% (25360/27008)
01/15/2023 08:50:55 - INFO - __main__ -   test: [batch: 211/391 ] | Loss: 0.879 | Acc: 77.513% (21034/27136)/ 93.872% (25473/27136)
01/15/2023 08:50:57 - INFO - __main__ -   test: [batch: 212/391 ] | Loss: 0.881 | Acc: 77.443% (21114/27264)/ 93.856% (25589/27264)
01/15/2023 08:51:00 - INFO - __main__ -   test: [batch: 213/391 ] | Loss: 0.881 | Acc: 77.453% (21216/27392)/ 93.863% (25711/27392)
01/15/2023 08:51:02 - INFO - __main__ -   test: [batch: 214/391 ] | Loss: 0.881 | Acc: 77.445% (21313/27520)/ 93.870% (25833/27520)
01/15/2023 08:51:04 - INFO - __main__ -   test: [batch: 215/391 ] | Loss: 0.882 | Acc: 77.431% (21408/27648)/ 93.848% (25947/27648)
01/15/2023 08:51:06 - INFO - __main__ -   test: [batch: 216/391 ] | Loss: 0.880 | Acc: 77.481% (21521/27776)/ 93.862% (26071/27776)
01/15/2023 08:51:08 - INFO - __main__ -   test: [batch: 217/391 ] | Loss: 0.885 | Acc: 77.419% (21603/27904)/ 93.797% (26173/27904)
01/15/2023 08:51:10 - INFO - __main__ -   test: [batch: 218/391 ] | Loss: 0.888 | Acc: 77.354% (21684/28032)/ 93.764% (26284/28032)
01/15/2023 08:51:12 - INFO - __main__ -   test: [batch: 219/391 ] | Loss: 0.886 | Acc: 77.393% (21794/28160)/ 93.778% (26408/28160)
01/15/2023 08:51:14 - INFO - __main__ -   test: [batch: 220/391 ] | Loss: 0.884 | Acc: 77.432% (21904/28288)/ 93.785% (26530/28288)
01/15/2023 08:51:16 - INFO - __main__ -   test: [batch: 221/391 ] | Loss: 0.886 | Acc: 77.404% (21995/28416)/ 93.764% (26644/28416)
01/15/2023 08:51:18 - INFO - __main__ -   test: [batch: 222/391 ] | Loss: 0.884 | Acc: 77.456% (22109/28544)/ 93.782% (26769/28544)
01/15/2023 08:51:21 - INFO - __main__ -   test: [batch: 223/391 ] | Loss: 0.885 | Acc: 77.448% (22206/28672)/ 93.771% (26886/28672)
01/15/2023 08:51:23 - INFO - __main__ -   test: [batch: 224/391 ] | Loss: 0.884 | Acc: 77.469% (22311/28800)/ 93.771% (27006/28800)
01/15/2023 08:51:25 - INFO - __main__ -   test: [batch: 225/391 ] | Loss: 0.884 | Acc: 77.468% (22410/28928)/ 93.774% (27127/28928)
01/15/2023 08:51:27 - INFO - __main__ -   test: [batch: 226/391 ] | Loss: 0.884 | Acc: 77.440% (22501/29056)/ 93.781% (27249/29056)
01/15/2023 08:51:29 - INFO - __main__ -   test: [batch: 227/391 ] | Loss: 0.886 | Acc: 77.436% (22599/29184)/ 93.777% (27368/29184)
01/15/2023 08:51:31 - INFO - __main__ -   test: [batch: 228/391 ] | Loss: 0.890 | Acc: 77.347% (22672/29312)/ 93.712% (27469/29312)
01/15/2023 08:51:34 - INFO - __main__ -   test: [batch: 229/391 ] | Loss: 0.894 | Acc: 77.269% (22748/29440)/ 93.662% (27574/29440)
01/15/2023 08:51:35 - INFO - __main__ -   test: [batch: 230/391 ] | Loss: 0.897 | Acc: 77.195% (22825/29568)/ 93.625% (27683/29568)
01/15/2023 08:51:38 - INFO - __main__ -   test: [batch: 231/391 ] | Loss: 0.897 | Acc: 77.172% (22917/29696)/ 93.612% (27799/29696)
01/15/2023 08:51:40 - INFO - __main__ -   test: [batch: 232/391 ] | Loss: 0.896 | Acc: 77.200% (23024/29824)/ 93.626% (27923/29824)
01/15/2023 08:51:42 - INFO - __main__ -   test: [batch: 233/391 ] | Loss: 0.898 | Acc: 77.160% (23111/29952)/ 93.603% (28036/29952)
01/15/2023 08:51:44 - INFO - __main__ -   test: [batch: 234/391 ] | Loss: 0.904 | Acc: 77.058% (23179/30080)/ 93.534% (28135/30080)
01/15/2023 08:51:46 - INFO - __main__ -   test: [batch: 235/391 ] | Loss: 0.904 | Acc: 77.052% (23276/30208)/ 93.532% (28254/30208)
01/15/2023 08:51:48 - INFO - __main__ -   test: [batch: 236/391 ] | Loss: 0.903 | Acc: 77.103% (23390/30336)/ 93.539% (28376/30336)
01/15/2023 08:51:50 - INFO - __main__ -   test: [batch: 237/391 ] | Loss: 0.903 | Acc: 77.098% (23487/30464)/ 93.510% (28487/30464)
01/15/2023 08:51:52 - INFO - __main__ -   test: [batch: 238/391 ] | Loss: 0.902 | Acc: 77.141% (23599/30592)/ 93.521% (28610/30592)
01/15/2023 08:51:54 - INFO - __main__ -   test: [batch: 239/391 ] | Loss: 0.901 | Acc: 77.171% (23707/30720)/ 93.525% (28731/30720)
01/15/2023 08:51:56 - INFO - __main__ -   test: [batch: 240/391 ] | Loss: 0.901 | Acc: 77.182% (23809/30848)/ 93.517% (28848/30848)
01/15/2023 08:51:59 - INFO - __main__ -   test: [batch: 241/391 ] | Loss: 0.904 | Acc: 77.102% (23883/30976)/ 93.482% (28957/30976)
01/15/2023 08:52:01 - INFO - __main__ -   test: [batch: 242/391 ] | Loss: 0.907 | Acc: 77.003% (23951/31104)/ 93.464% (29071/31104)
01/15/2023 08:52:03 - INFO - __main__ -   test: [batch: 243/391 ] | Loss: 0.912 | Acc: 76.892% (24015/31232)/ 93.388% (29167/31232)
01/15/2023 08:52:05 - INFO - __main__ -   test: [batch: 244/391 ] | Loss: 0.913 | Acc: 76.894% (24114/31360)/ 93.380% (29284/31360)
01/15/2023 08:52:07 - INFO - __main__ -   test: [batch: 245/391 ] | Loss: 0.912 | Acc: 76.902% (24215/31488)/ 93.372% (29401/31488)
01/15/2023 08:52:10 - INFO - __main__ -   test: [batch: 246/391 ] | Loss: 0.913 | Acc: 76.885% (24308/31616)/ 93.355% (29515/31616)
01/15/2023 08:52:12 - INFO - __main__ -   test: [batch: 247/391 ] | Loss: 0.918 | Acc: 76.796% (24378/31744)/ 93.284% (29612/31744)
01/15/2023 08:52:14 - INFO - __main__ -   test: [batch: 248/391 ] | Loss: 0.919 | Acc: 76.763% (24466/31872)/ 93.279% (29730/31872)
01/15/2023 08:52:16 - INFO - __main__ -   test: [batch: 249/391 ] | Loss: 0.921 | Acc: 76.609% (24515/32000)/ 93.275% (29848/32000)
01/15/2023 08:52:18 - INFO - __main__ -   test: [batch: 250/391 ] | Loss: 0.920 | Acc: 76.643% (24624/32128)/ 93.286% (29971/32128)
01/15/2023 08:52:20 - INFO - __main__ -   test: [batch: 251/391 ] | Loss: 0.922 | Acc: 76.615% (24713/32256)/ 93.257% (30081/32256)
01/15/2023 08:52:22 - INFO - __main__ -   test: [batch: 252/391 ] | Loss: 0.921 | Acc: 76.643% (24820/32384)/ 93.253% (30199/32384)
01/15/2023 08:52:24 - INFO - __main__ -   test: [batch: 253/391 ] | Loss: 0.923 | Acc: 76.612% (24908/32512)/ 93.236% (30313/32512)
01/15/2023 08:52:26 - INFO - __main__ -   test: [batch: 254/391 ] | Loss: 0.927 | Acc: 76.529% (24979/32640)/ 93.192% (30418/32640)
01/15/2023 08:52:29 - INFO - __main__ -   test: [batch: 255/391 ] | Loss: 0.929 | Acc: 76.511% (25071/32768)/ 93.179% (30533/32768)
01/15/2023 08:52:31 - INFO - __main__ -   test: [batch: 256/391 ] | Loss: 0.934 | Acc: 76.383% (25127/32896)/ 93.133% (30637/32896)
01/15/2023 08:52:33 - INFO - __main__ -   test: [batch: 257/391 ] | Loss: 0.935 | Acc: 76.369% (25220/33024)/ 93.129% (30755/33024)
01/15/2023 08:52:35 - INFO - __main__ -   test: [batch: 258/391 ] | Loss: 0.936 | Acc: 76.360% (25315/33152)/ 93.114% (30869/33152)
01/15/2023 08:52:37 - INFO - __main__ -   test: [batch: 259/391 ] | Loss: 0.939 | Acc: 76.244% (25374/33280)/ 93.113% (30988/33280)
01/15/2023 08:52:40 - INFO - __main__ -   test: [batch: 260/391 ] | Loss: 0.940 | Acc: 76.224% (25465/33408)/ 93.112% (31107/33408)
01/15/2023 08:52:42 - INFO - __main__ -   test: [batch: 261/391 ] | Loss: 0.938 | Acc: 76.261% (25575/33536)/ 93.136% (31234/33536)
01/15/2023 08:52:44 - INFO - __main__ -   test: [batch: 262/391 ] | Loss: 0.937 | Acc: 76.286% (25681/33664)/ 93.147% (31357/33664)
01/15/2023 08:52:46 - INFO - __main__ -   test: [batch: 263/391 ] | Loss: 0.939 | Acc: 76.222% (25757/33792)/ 93.123% (31468/33792)
01/15/2023 08:52:48 - INFO - __main__ -   test: [batch: 264/391 ] | Loss: 0.944 | Acc: 76.156% (25832/33920)/ 93.060% (31566/33920)
01/15/2023 08:52:51 - INFO - __main__ -   test: [batch: 265/391 ] | Loss: 0.943 | Acc: 76.189% (25941/34048)/ 93.054% (31683/34048)
01/15/2023 08:52:53 - INFO - __main__ -   test: [batch: 266/391 ] | Loss: 0.946 | Acc: 76.124% (26016/34176)/ 93.039% (31797/34176)
01/15/2023 08:52:55 - INFO - __main__ -   test: [batch: 267/391 ] | Loss: 0.945 | Acc: 76.157% (26125/34304)/ 93.039% (31916/34304)
01/15/2023 08:52:57 - INFO - __main__ -   test: [batch: 268/391 ] | Loss: 0.944 | Acc: 76.173% (26228/34432)/ 93.038% (32035/34432)
01/15/2023 08:52:59 - INFO - __main__ -   test: [batch: 269/391 ] | Loss: 0.946 | Acc: 76.120% (26307/34560)/ 93.024% (32149/34560)
01/15/2023 08:53:01 - INFO - __main__ -   test: [batch: 270/391 ] | Loss: 0.949 | Acc: 76.072% (26388/34688)/ 93.000% (32260/34688)
01/15/2023 08:53:03 - INFO - __main__ -   test: [batch: 271/391 ] | Loss: 0.949 | Acc: 76.083% (26489/34816)/ 92.992% (32376/34816)
01/15/2023 08:53:05 - INFO - __main__ -   test: [batch: 272/391 ] | Loss: 0.950 | Acc: 76.056% (26577/34944)/ 92.992% (32495/34944)
01/15/2023 08:53:07 - INFO - __main__ -   test: [batch: 273/391 ] | Loss: 0.950 | Acc: 76.072% (26680/35072)/ 92.969% (32606/35072)
01/15/2023 08:53:09 - INFO - __main__ -   test: [batch: 274/391 ] | Loss: 0.950 | Acc: 76.065% (26775/35200)/ 92.972% (32726/35200)
01/15/2023 08:53:11 - INFO - __main__ -   test: [batch: 275/391 ] | Loss: 0.952 | Acc: 76.030% (26860/35328)/ 92.963% (32842/35328)
01/15/2023 08:53:13 - INFO - __main__ -   test: [batch: 276/391 ] | Loss: 0.953 | Acc: 75.990% (26943/35456)/ 92.949% (32956/35456)
01/15/2023 08:53:16 - INFO - __main__ -   test: [batch: 277/391 ] | Loss: 0.954 | Acc: 75.958% (27029/35584)/ 92.929% (33068/35584)
01/15/2023 08:53:18 - INFO - __main__ -   test: [batch: 278/391 ] | Loss: 0.953 | Acc: 75.974% (27132/35712)/ 92.930% (33187/35712)
01/15/2023 08:53:20 - INFO - __main__ -   test: [batch: 279/391 ] | Loss: 0.953 | Acc: 75.996% (27237/35840)/ 92.924% (33304/35840)
01/15/2023 08:53:22 - INFO - __main__ -   test: [batch: 280/391 ] | Loss: 0.954 | Acc: 75.970% (27325/35968)/ 92.916% (33420/35968)
01/15/2023 08:53:24 - INFO - __main__ -   test: [batch: 281/391 ] | Loss: 0.955 | Acc: 75.975% (27424/36096)/ 92.905% (33535/36096)
01/15/2023 08:53:26 - INFO - __main__ -   test: [batch: 282/391 ] | Loss: 0.954 | Acc: 76.013% (27535/36224)/ 92.905% (33654/36224)
01/15/2023 08:53:28 - INFO - __main__ -   test: [batch: 283/391 ] | Loss: 0.954 | Acc: 76.001% (27628/36352)/ 92.903% (33772/36352)
01/15/2023 08:53:30 - INFO - __main__ -   test: [batch: 284/391 ] | Loss: 0.958 | Acc: 75.954% (27708/36480)/ 92.854% (33873/36480)
01/15/2023 08:53:33 - INFO - __main__ -   test: [batch: 285/391 ] | Loss: 0.960 | Acc: 75.921% (27793/36608)/ 92.808% (33975/36608)
01/15/2023 08:53:35 - INFO - __main__ -   test: [batch: 286/391 ] | Loss: 0.962 | Acc: 75.879% (27875/36736)/ 92.789% (34087/36736)
01/15/2023 08:53:37 - INFO - __main__ -   test: [batch: 287/391 ] | Loss: 0.962 | Acc: 75.871% (27969/36864)/ 92.779% (34202/36864)
01/15/2023 08:53:39 - INFO - __main__ -   test: [batch: 288/391 ] | Loss: 0.961 | Acc: 75.884% (28071/36992)/ 92.782% (34322/36992)
01/15/2023 08:53:41 - INFO - __main__ -   test: [batch: 289/391 ] | Loss: 0.963 | Acc: 75.851% (28156/37120)/ 92.753% (34430/37120)
01/15/2023 08:53:43 - INFO - __main__ -   test: [batch: 290/391 ] | Loss: 0.965 | Acc: 75.765% (28221/37248)/ 92.754% (34549/37248)
01/15/2023 08:53:46 - INFO - __main__ -   test: [batch: 291/391 ] | Loss: 0.965 | Acc: 75.773% (28321/37376)/ 92.755% (34668/37376)
01/15/2023 08:53:48 - INFO - __main__ -   test: [batch: 292/391 ] | Loss: 0.967 | Acc: 75.717% (28397/37504)/ 92.742% (34782/37504)
01/15/2023 08:53:50 - INFO - __main__ -   test: [batch: 293/391 ] | Loss: 0.967 | Acc: 75.699% (28487/37632)/ 92.740% (34900/37632)
01/15/2023 08:53:52 - INFO - __main__ -   test: [batch: 294/391 ] | Loss: 0.969 | Acc: 75.675% (28575/37760)/ 92.725% (35013/37760)
01/15/2023 08:53:54 - INFO - __main__ -   test: [batch: 295/391 ] | Loss: 0.968 | Acc: 75.699% (28681/37888)/ 92.721% (35130/37888)
01/15/2023 08:53:56 - INFO - __main__ -   test: [batch: 296/391 ] | Loss: 0.968 | Acc: 75.692% (28775/38016)/ 92.716% (35247/38016)
01/15/2023 08:53:59 - INFO - __main__ -   test: [batch: 297/391 ] | Loss: 0.970 | Acc: 75.671% (28864/38144)/ 92.691% (35356/38144)
01/15/2023 08:54:00 - INFO - __main__ -   test: [batch: 298/391 ] | Loss: 0.972 | Acc: 75.632% (28946/38272)/ 92.668% (35466/38272)
01/15/2023 08:54:02 - INFO - __main__ -   test: [batch: 299/391 ] | Loss: 0.973 | Acc: 75.615% (29036/38400)/ 92.638% (35573/38400)
01/15/2023 08:54:05 - INFO - __main__ -   test: [batch: 300/391 ] | Loss: 0.974 | Acc: 75.631% (29139/38528)/ 92.637% (35691/38528)
01/15/2023 08:54:07 - INFO - __main__ -   test: [batch: 301/391 ] | Loss: 0.975 | Acc: 75.623% (29233/38656)/ 92.630% (35807/38656)
01/15/2023 08:54:09 - INFO - __main__ -   test: [batch: 302/391 ] | Loss: 0.976 | Acc: 75.585% (29315/38784)/ 92.616% (35920/38784)
01/15/2023 08:54:11 - INFO - __main__ -   test: [batch: 303/391 ] | Loss: 0.977 | Acc: 75.563% (29403/38912)/ 92.606% (36035/38912)
01/15/2023 08:54:13 - INFO - __main__ -   test: [batch: 304/391 ] | Loss: 0.977 | Acc: 75.574% (29504/39040)/ 92.610% (36155/39040)
01/15/2023 08:54:16 - INFO - __main__ -   test: [batch: 305/391 ] | Loss: 0.977 | Acc: 75.539% (29587/39168)/ 92.599% (36269/39168)
01/15/2023 08:54:18 - INFO - __main__ -   test: [batch: 306/391 ] | Loss: 0.979 | Acc: 75.527% (29679/39296)/ 92.574% (36378/39296)
01/15/2023 08:54:20 - INFO - __main__ -   test: [batch: 307/391 ] | Loss: 0.980 | Acc: 75.507% (29768/39424)/ 92.558% (36490/39424)
01/15/2023 08:54:22 - INFO - __main__ -   test: [batch: 308/391 ] | Loss: 0.980 | Acc: 75.496% (29860/39552)/ 92.547% (36604/39552)
01/15/2023 08:54:24 - INFO - __main__ -   test: [batch: 309/391 ] | Loss: 0.981 | Acc: 75.486% (29953/39680)/ 92.528% (36715/39680)
01/15/2023 08:54:27 - INFO - __main__ -   test: [batch: 310/391 ] | Loss: 0.982 | Acc: 75.477% (30046/39808)/ 92.512% (36827/39808)
01/15/2023 08:54:29 - INFO - __main__ -   test: [batch: 311/391 ] | Loss: 0.984 | Acc: 75.448% (30131/39936)/ 92.498% (36940/39936)
01/15/2023 08:54:31 - INFO - __main__ -   test: [batch: 312/391 ] | Loss: 0.985 | Acc: 75.442% (30225/40064)/ 92.480% (37051/40064)
01/15/2023 08:54:33 - INFO - __main__ -   test: [batch: 313/391 ] | Loss: 0.982 | Acc: 75.493% (30342/40192)/ 92.503% (37179/40192)
01/15/2023 08:54:35 - INFO - __main__ -   test: [batch: 314/391 ] | Loss: 0.983 | Acc: 75.479% (30433/40320)/ 92.498% (37295/40320)
01/15/2023 08:54:37 - INFO - __main__ -   test: [batch: 315/391 ] | Loss: 0.984 | Acc: 75.462% (30523/40448)/ 92.482% (37407/40448)
01/15/2023 08:54:39 - INFO - __main__ -   test: [batch: 316/391 ] | Loss: 0.987 | Acc: 75.367% (30581/40576)/ 92.456% (37515/40576)
01/15/2023 08:54:41 - INFO - __main__ -   test: [batch: 317/391 ] | Loss: 0.989 | Acc: 75.317% (30657/40704)/ 92.423% (37620/40704)
01/15/2023 08:54:43 - INFO - __main__ -   test: [batch: 318/391 ] | Loss: 0.988 | Acc: 75.345% (30765/40832)/ 92.442% (37746/40832)
01/15/2023 08:54:45 - INFO - __main__ -   test: [batch: 319/391 ] | Loss: 0.990 | Acc: 75.291% (30839/40960)/ 92.410% (37851/40960)
01/15/2023 08:54:48 - INFO - __main__ -   test: [batch: 320/391 ] | Loss: 0.989 | Acc: 75.326% (30950/41088)/ 92.419% (37973/41088)
01/15/2023 08:54:50 - INFO - __main__ -   test: [batch: 321/391 ] | Loss: 0.989 | Acc: 75.342% (31053/41216)/ 92.416% (38090/41216)
01/15/2023 08:54:52 - INFO - __main__ -   test: [batch: 322/391 ] | Loss: 0.990 | Acc: 75.298% (31131/41344)/ 92.403% (38203/41344)
01/15/2023 08:54:54 - INFO - __main__ -   test: [batch: 323/391 ] | Loss: 0.993 | Acc: 75.263% (31213/41472)/ 92.376% (38310/41472)
01/15/2023 08:54:56 - INFO - __main__ -   test: [batch: 324/391 ] | Loss: 0.993 | Acc: 75.262% (31309/41600)/ 92.370% (38426/41600)
01/15/2023 08:54:58 - INFO - __main__ -   test: [batch: 325/391 ] | Loss: 0.993 | Acc: 75.273% (31410/41728)/ 92.372% (38545/41728)
01/15/2023 08:55:00 - INFO - __main__ -   test: [batch: 326/391 ] | Loss: 0.996 | Acc: 75.189% (31471/41856)/ 92.326% (38644/41856)
01/15/2023 08:55:03 - INFO - __main__ -   test: [batch: 327/391 ] | Loss: 0.999 | Acc: 75.102% (31531/41984)/ 92.288% (38746/41984)
01/15/2023 08:55:05 - INFO - __main__ -   test: [batch: 328/391 ] | Loss: 1.001 | Acc: 75.055% (31607/42112)/ 92.256% (38851/42112)
01/15/2023 08:55:07 - INFO - __main__ -   test: [batch: 329/391 ] | Loss: 1.001 | Acc: 75.066% (31708/42240)/ 92.266% (38973/42240)
01/15/2023 08:55:09 - INFO - __main__ -   test: [batch: 330/391 ] | Loss: 1.003 | Acc: 75.031% (31789/42368)/ 92.244% (39082/42368)
01/15/2023 08:55:11 - INFO - __main__ -   test: [batch: 331/391 ] | Loss: 1.003 | Acc: 75.000% (31872/42496)/ 92.258% (39206/42496)
01/15/2023 08:55:13 - INFO - __main__ -   test: [batch: 332/391 ] | Loss: 1.004 | Acc: 74.981% (31960/42624)/ 92.256% (39323/42624)
01/15/2023 08:55:16 - INFO - __main__ -   test: [batch: 333/391 ] | Loss: 1.002 | Acc: 75.012% (32069/42752)/ 92.265% (39445/42752)
01/15/2023 08:55:18 - INFO - __main__ -   test: [batch: 334/391 ] | Loss: 1.004 | Acc: 74.981% (32152/42880)/ 92.253% (39558/42880)
01/15/2023 08:55:20 - INFO - __main__ -   test: [batch: 335/391 ] | Loss: 1.005 | Acc: 74.956% (32237/43008)/ 92.232% (39667/43008)
01/15/2023 08:55:22 - INFO - __main__ -   test: [batch: 336/391 ] | Loss: 1.007 | Acc: 74.917% (32316/43136)/ 92.218% (39779/43136)
01/15/2023 08:55:24 - INFO - __main__ -   test: [batch: 337/391 ] | Loss: 1.007 | Acc: 74.914% (32411/43264)/ 92.208% (39893/43264)
01/15/2023 08:55:26 - INFO - __main__ -   test: [batch: 338/391 ] | Loss: 1.007 | Acc: 74.912% (32506/43392)/ 92.217% (40015/43392)
01/15/2023 08:55:28 - INFO - __main__ -   test: [batch: 339/391 ] | Loss: 1.009 | Acc: 74.871% (32584/43520)/ 92.183% (40118/43520)
01/15/2023 08:55:31 - INFO - __main__ -   test: [batch: 340/391 ] | Loss: 1.009 | Acc: 74.874% (32681/43648)/ 92.194% (40241/43648)
01/15/2023 08:55:33 - INFO - __main__ -   test: [batch: 341/391 ] | Loss: 1.007 | Acc: 74.913% (32794/43776)/ 92.213% (40367/43776)
01/15/2023 08:55:35 - INFO - __main__ -   test: [batch: 342/391 ] | Loss: 1.008 | Acc: 74.861% (32867/43904)/ 92.201% (40480/43904)
01/15/2023 08:55:37 - INFO - __main__ -   test: [batch: 343/391 ] | Loss: 1.008 | Acc: 74.864% (32964/44032)/ 92.203% (40599/44032)
01/15/2023 08:55:39 - INFO - __main__ -   test: [batch: 344/391 ] | Loss: 1.009 | Acc: 74.860% (33058/44160)/ 92.194% (40713/44160)
01/15/2023 08:55:41 - INFO - __main__ -   test: [batch: 345/391 ] | Loss: 1.012 | Acc: 74.795% (33125/44288)/ 92.165% (40818/44288)
01/15/2023 08:55:43 - INFO - __main__ -   test: [batch: 346/391 ] | Loss: 1.013 | Acc: 74.779% (33214/44416)/ 92.158% (40933/44416)
01/15/2023 08:55:46 - INFO - __main__ -   test: [batch: 347/391 ] | Loss: 1.013 | Acc: 74.787% (33313/44544)/ 92.161% (41052/44544)
01/15/2023 08:55:48 - INFO - __main__ -   test: [batch: 348/391 ] | Loss: 1.015 | Acc: 74.765% (33399/44672)/ 92.143% (41162/44672)
01/15/2023 08:55:50 - INFO - __main__ -   test: [batch: 349/391 ] | Loss: 1.014 | Acc: 74.768% (33496/44800)/ 92.150% (41283/44800)
01/15/2023 08:55:52 - INFO - __main__ -   test: [batch: 350/391 ] | Loss: 1.014 | Acc: 74.766% (33591/44928)/ 92.145% (41399/44928)
01/15/2023 08:55:54 - INFO - __main__ -   test: [batch: 351/391 ] | Loss: 1.016 | Acc: 74.714% (33663/45056)/ 92.125% (41508/45056)
01/15/2023 08:55:56 - INFO - __main__ -   test: [batch: 352/391 ] | Loss: 1.017 | Acc: 74.712% (33758/45184)/ 92.117% (41622/45184)
01/15/2023 08:55:59 - INFO - __main__ -   test: [batch: 353/391 ] | Loss: 1.019 | Acc: 74.665% (33832/45312)/ 92.079% (41723/45312)
01/15/2023 08:56:01 - INFO - __main__ -   test: [batch: 354/391 ] | Loss: 1.022 | Acc: 74.606% (33901/45440)/ 92.060% (41832/45440)
01/15/2023 08:56:03 - INFO - __main__ -   test: [batch: 355/391 ] | Loss: 1.024 | Acc: 74.539% (33966/45568)/ 92.045% (41943/45568)
01/15/2023 08:56:05 - INFO - __main__ -   test: [batch: 356/391 ] | Loss: 1.025 | Acc: 74.536% (34060/45696)/ 92.050% (42063/45696)
01/15/2023 08:56:07 - INFO - __main__ -   test: [batch: 357/391 ] | Loss: 1.023 | Acc: 74.572% (34172/45824)/ 92.063% (42187/45824)
01/15/2023 08:56:09 - INFO - __main__ -   test: [batch: 358/391 ] | Loss: 1.023 | Acc: 74.595% (34278/45952)/ 92.068% (42307/45952)
01/15/2023 08:56:12 - INFO - __main__ -   test: [batch: 359/391 ] | Loss: 1.023 | Acc: 74.596% (34374/46080)/ 92.059% (42421/46080)
01/15/2023 08:56:14 - INFO - __main__ -   test: [batch: 360/391 ] | Loss: 1.024 | Acc: 74.563% (34454/46208)/ 92.058% (42538/46208)
01/15/2023 08:56:16 - INFO - __main__ -   test: [batch: 361/391 ] | Loss: 1.025 | Acc: 74.564% (34550/46336)/ 92.060% (42657/46336)
01/15/2023 08:56:18 - INFO - __main__ -   test: [batch: 362/391 ] | Loss: 1.024 | Acc: 74.559% (34643/46464)/ 92.071% (42780/46464)
01/15/2023 08:56:20 - INFO - __main__ -   test: [batch: 363/391 ] | Loss: 1.025 | Acc: 74.547% (34733/46592)/ 92.065% (42895/46592)
01/15/2023 08:56:22 - INFO - __main__ -   test: [batch: 364/391 ] | Loss: 1.024 | Acc: 74.572% (34840/46720)/ 92.076% (43018/46720)
01/15/2023 08:56:24 - INFO - __main__ -   test: [batch: 365/391 ] | Loss: 1.024 | Acc: 74.575% (34937/46848)/ 92.079% (43137/46848)
01/15/2023 08:56:26 - INFO - __main__ -   test: [batch: 366/391 ] | Loss: 1.022 | Acc: 74.608% (35048/46976)/ 92.094% (43262/46976)
01/15/2023 08:56:28 - INFO - __main__ -   test: [batch: 367/391 ] | Loss: 1.021 | Acc: 74.622% (35150/47104)/ 92.105% (43385/47104)
01/15/2023 08:56:30 - INFO - __main__ -   test: [batch: 368/391 ] | Loss: 1.021 | Acc: 74.617% (35243/47232)/ 92.111% (43506/47232)
01/15/2023 08:56:32 - INFO - __main__ -   test: [batch: 369/391 ] | Loss: 1.020 | Acc: 74.637% (35348/47360)/ 92.122% (43629/47360)
01/15/2023 08:56:35 - INFO - __main__ -   test: [batch: 370/391 ] | Loss: 1.021 | Acc: 74.610% (35431/47488)/ 92.126% (43749/47488)
01/15/2023 08:56:37 - INFO - __main__ -   test: [batch: 371/391 ] | Loss: 1.021 | Acc: 74.620% (35531/47616)/ 92.124% (43866/47616)
01/15/2023 08:56:39 - INFO - __main__ -   test: [batch: 372/391 ] | Loss: 1.019 | Acc: 74.669% (35650/47744)/ 92.144% (43993/47744)
01/15/2023 08:56:41 - INFO - __main__ -   test: [batch: 373/391 ] | Loss: 1.018 | Acc: 74.705% (35763/47872)/ 92.156% (44117/47872)
01/15/2023 08:56:43 - INFO - __main__ -   test: [batch: 374/391 ] | Loss: 1.017 | Acc: 74.731% (35871/48000)/ 92.158% (44236/48000)
01/15/2023 08:56:45 - INFO - __main__ -   test: [batch: 375/391 ] | Loss: 1.020 | Acc: 74.661% (35933/48128)/ 92.125% (44338/48128)
01/15/2023 08:56:47 - INFO - __main__ -   test: [batch: 376/391 ] | Loss: 1.021 | Acc: 74.648% (36022/48256)/ 92.107% (44447/48256)
01/15/2023 08:56:49 - INFO - __main__ -   test: [batch: 377/391 ] | Loss: 1.021 | Acc: 74.642% (36115/48384)/ 92.097% (44560/48384)
01/15/2023 08:56:51 - INFO - __main__ -   test: [batch: 378/391 ] | Loss: 1.024 | Acc: 74.573% (36177/48512)/ 92.056% (44658/48512)
01/15/2023 08:56:53 - INFO - __main__ -   test: [batch: 379/391 ] | Loss: 1.024 | Acc: 74.562% (36267/48640)/ 92.070% (44783/48640)
01/15/2023 08:56:56 - INFO - __main__ -   test: [batch: 380/391 ] | Loss: 1.024 | Acc: 74.569% (36366/48768)/ 92.085% (44908/48768)
01/15/2023 08:56:58 - INFO - __main__ -   test: [batch: 381/391 ] | Loss: 1.025 | Acc: 74.521% (36438/48896)/ 92.083% (45025/48896)
01/15/2023 08:57:00 - INFO - __main__ -   test: [batch: 382/391 ] | Loss: 1.027 | Acc: 74.492% (36519/49024)/ 92.071% (45137/49024)
01/15/2023 08:57:02 - INFO - __main__ -   test: [batch: 383/391 ] | Loss: 1.027 | Acc: 74.497% (36617/49152)/ 92.065% (45252/49152)
01/15/2023 08:57:04 - INFO - __main__ -   test: [batch: 384/391 ] | Loss: 1.025 | Acc: 74.535% (36731/49280)/ 92.080% (45377/49280)
01/15/2023 08:57:06 - INFO - __main__ -   test: [batch: 385/391 ] | Loss: 1.024 | Acc: 74.549% (36833/49408)/ 92.090% (45500/49408)
01/15/2023 08:57:08 - INFO - __main__ -   test: [batch: 386/391 ] | Loss: 1.022 | Acc: 74.588% (36948/49536)/ 92.107% (45626/49536)
01/15/2023 08:57:10 - INFO - __main__ -   test: [batch: 387/391 ] | Loss: 1.020 | Acc: 74.632% (37065/49664)/ 92.117% (45749/49664)
01/15/2023 08:57:12 - INFO - __main__ -   test: [batch: 388/391 ] | Loss: 1.019 | Acc: 74.683% (37186/49792)/ 92.131% (45874/49792)
01/15/2023 08:57:15 - INFO - __main__ -   test: [batch: 389/391 ] | Loss: 1.018 | Acc: 74.685% (37283/49920)/ 92.139% (45996/49920)
01/15/2023 08:57:17 - INFO - __main__ -   test: [batch: 390/391 ] | Loss: 1.020 | Acc: 74.638% (37319/50000)/ 92.130% (46065/50000)
01/15/2023 08:57:17 - INFO - __main__ -   Final accuracy: 74.638
01/15/2023 08:57:17 - INFO - __main__ -   {'milestones': Counter({2: 1, 3: 1, 4: 1}), 'gamma': 0.1, 'base_lrs': [0.00025], 'last_epoch': 1, '_step_count': 2, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.00025]}
01/15/2023 08:57:17 - INFO - __main__ -   
Epoch: 1
01/15/2023 08:57:19 - INFO - __main__ -   test: [epoch: 1 | batch: 0/10010 ] | Loss: 0.675 | Acc: 82.031% (105/128)
01/15/2023 09:00:50 - INFO - __main__ -   test: [epoch: 1 | batch: 100/10010 ] | Loss: 0.910 | Acc: 77.700% (10045/12928)
01/15/2023 09:04:23 - INFO - __main__ -   test: [epoch: 1 | batch: 200/10010 ] | Loss: 0.910 | Acc: 77.484% (19935/25728)
01/15/2023 09:07:54 - INFO - __main__ -   test: [epoch: 1 | batch: 300/10010 ] | Loss: 0.924 | Acc: 77.250% (29763/38528)
01/15/2023 09:11:25 - INFO - __main__ -   test: [epoch: 1 | batch: 400/10010 ] | Loss: 0.935 | Acc: 76.944% (39494/51328)
01/15/2023 09:14:57 - INFO - __main__ -   test: [epoch: 1 | batch: 500/10010 ] | Loss: 0.936 | Acc: 76.857% (49287/64128)
01/15/2023 09:18:31 - INFO - __main__ -   test: [epoch: 1 | batch: 600/10010 ] | Loss: 0.939 | Acc: 76.816% (59093/76928)
01/15/2023 09:22:03 - INFO - __main__ -   test: [epoch: 1 | batch: 700/10010 ] | Loss: 0.939 | Acc: 76.785% (68898/89728)
01/15/2023 09:25:35 - INFO - __main__ -   test: [epoch: 1 | batch: 800/10010 ] | Loss: 0.938 | Acc: 76.796% (78737/102528)
01/15/2023 09:29:05 - INFO - __main__ -   test: [epoch: 1 | batch: 900/10010 ] | Loss: 0.938 | Acc: 76.797% (88568/115328)
01/15/2023 09:32:36 - INFO - __main__ -   test: [epoch: 1 | batch: 1000/10010 ] | Loss: 0.938 | Acc: 76.758% (98349/128128)
01/15/2023 09:36:08 - INFO - __main__ -   test: [epoch: 1 | batch: 1100/10010 ] | Loss: 0.941 | Acc: 76.721% (108122/140928)
01/15/2023 09:39:41 - INFO - __main__ -   test: [epoch: 1 | batch: 1200/10010 ] | Loss: 0.941 | Acc: 76.719% (117939/153728)
01/15/2023 09:43:14 - INFO - __main__ -   test: [epoch: 1 | batch: 1300/10010 ] | Loss: 0.938 | Acc: 76.788% (127873/166528)
01/15/2023 09:46:45 - INFO - __main__ -   test: [epoch: 1 | batch: 1400/10010 ] | Loss: 0.939 | Acc: 76.776% (137681/179328)
01/15/2023 09:50:16 - INFO - __main__ -   test: [epoch: 1 | batch: 1500/10010 ] | Loss: 0.938 | Acc: 76.764% (147486/192128)
01/15/2023 09:53:47 - INFO - __main__ -   test: [epoch: 1 | batch: 1600/10010 ] | Loss: 0.941 | Acc: 76.734% (157250/204928)
01/15/2023 09:57:18 - INFO - __main__ -   test: [epoch: 1 | batch: 1700/10010 ] | Loss: 0.940 | Acc: 76.750% (167106/217728)
01/15/2023 10:00:48 - INFO - __main__ -   test: [epoch: 1 | batch: 1800/10010 ] | Loss: 0.940 | Acc: 76.758% (176948/230528)
01/15/2023 10:04:20 - INFO - __main__ -   test: [epoch: 1 | batch: 1900/10010 ] | Loss: 0.939 | Acc: 76.767% (186795/243328)
01/15/2023 10:07:52 - INFO - __main__ -   test: [epoch: 1 | batch: 2000/10010 ] | Loss: 0.938 | Acc: 76.790% (196681/256128)
01/15/2023 10:11:23 - INFO - __main__ -   test: [epoch: 1 | batch: 2100/10010 ] | Loss: 0.938 | Acc: 76.786% (206499/268928)
01/15/2023 10:14:53 - INFO - __main__ -   test: [epoch: 1 | batch: 2200/10010 ] | Loss: 0.940 | Acc: 76.758% (216249/281728)
01/15/2023 10:18:25 - INFO - __main__ -   test: [epoch: 1 | batch: 2300/10010 ] | Loss: 0.940 | Acc: 76.757% (226072/294528)
01/15/2023 10:21:57 - INFO - __main__ -   test: [epoch: 1 | batch: 2400/10010 ] | Loss: 0.940 | Acc: 76.790% (235998/307328)
01/15/2023 10:25:27 - INFO - __main__ -   test: [epoch: 1 | batch: 2500/10010 ] | Loss: 0.941 | Acc: 76.766% (245750/320128)
01/15/2023 10:28:58 - INFO - __main__ -   test: [epoch: 1 | batch: 2600/10010 ] | Loss: 0.942 | Acc: 76.751% (255526/332928)
01/15/2023 10:32:31 - INFO - __main__ -   test: [epoch: 1 | batch: 2700/10010 ] | Loss: 0.942 | Acc: 76.754% (265361/345728)
01/15/2023 10:36:00 - INFO - __main__ -   test: [epoch: 1 | batch: 2800/10010 ] | Loss: 0.941 | Acc: 76.753% (275182/358528)
01/15/2023 10:39:31 - INFO - __main__ -   test: [epoch: 1 | batch: 2900/10010 ] | Loss: 0.942 | Acc: 76.755% (285013/371328)
01/15/2023 10:43:00 - INFO - __main__ -   test: [epoch: 1 | batch: 3000/10010 ] | Loss: 0.941 | Acc: 76.769% (294891/384128)
01/15/2023 10:46:32 - INFO - __main__ -   test: [epoch: 1 | batch: 3100/10010 ] | Loss: 0.941 | Acc: 76.769% (304717/396928)
01/15/2023 10:50:03 - INFO - __main__ -   test: [epoch: 1 | batch: 3200/10010 ] | Loss: 0.941 | Acc: 76.752% (314475/409728)
01/15/2023 10:53:34 - INFO - __main__ -   test: [epoch: 1 | batch: 3300/10010 ] | Loss: 0.942 | Acc: 76.736% (324230/422528)
01/15/2023 10:57:06 - INFO - __main__ -   test: [epoch: 1 | batch: 3400/10010 ] | Loss: 0.942 | Acc: 76.746% (334097/435328)
01/15/2023 11:00:37 - INFO - __main__ -   test: [epoch: 1 | batch: 3500/10010 ] | Loss: 0.942 | Acc: 76.741% (343897/448128)
01/15/2023 11:04:08 - INFO - __main__ -   test: [epoch: 1 | batch: 3600/10010 ] | Loss: 0.942 | Acc: 76.742% (353724/460928)
01/15/2023 11:07:39 - INFO - __main__ -   test: [epoch: 1 | batch: 3700/10010 ] | Loss: 0.943 | Acc: 76.724% (363465/473728)
01/15/2023 11:11:11 - INFO - __main__ -   test: [epoch: 1 | batch: 3800/10010 ] | Loss: 0.943 | Acc: 76.716% (373244/486528)
01/15/2023 11:14:42 - INFO - __main__ -   test: [epoch: 1 | batch: 3900/10010 ] | Loss: 0.942 | Acc: 76.717% (383069/499328)
01/15/2023 11:18:13 - INFO - __main__ -   test: [epoch: 1 | batch: 4000/10010 ] | Loss: 0.943 | Acc: 76.718% (392894/512128)
01/15/2023 11:21:45 - INFO - __main__ -   test: [epoch: 1 | batch: 4100/10010 ] | Loss: 0.942 | Acc: 76.726% (402758/524928)
01/15/2023 11:25:15 - INFO - __main__ -   test: [epoch: 1 | batch: 4200/10010 ] | Loss: 0.942 | Acc: 76.728% (412586/537728)
01/15/2023 11:28:48 - INFO - __main__ -   test: [epoch: 1 | batch: 4300/10010 ] | Loss: 0.942 | Acc: 76.736% (422452/550528)
01/15/2023 11:32:20 - INFO - __main__ -   test: [epoch: 1 | batch: 4400/10010 ] | Loss: 0.942 | Acc: 76.735% (432268/563328)
01/15/2023 11:35:50 - INFO - __main__ -   test: [epoch: 1 | batch: 4500/10010 ] | Loss: 0.942 | Acc: 76.723% (442025/576128)
01/15/2023 11:39:22 - INFO - __main__ -   test: [epoch: 1 | batch: 4600/10010 ] | Loss: 0.942 | Acc: 76.720% (451826/588928)
01/15/2023 11:42:53 - INFO - __main__ -   test: [epoch: 1 | batch: 4700/10010 ] | Loss: 0.941 | Acc: 76.732% (461715/601728)
01/15/2023 11:46:25 - INFO - __main__ -   test: [epoch: 1 | batch: 4800/10010 ] | Loss: 0.941 | Acc: 76.741% (471592/614528)
01/15/2023 11:50:00 - INFO - __main__ -   test: [epoch: 1 | batch: 4900/10010 ] | Loss: 0.941 | Acc: 76.739% (481403/627328)
01/15/2023 11:53:31 - INFO - __main__ -   test: [epoch: 1 | batch: 5000/10010 ] | Loss: 0.941 | Acc: 76.750% (491297/640128)
01/15/2023 11:57:03 - INFO - __main__ -   test: [epoch: 1 | batch: 5100/10010 ] | Loss: 0.941 | Acc: 76.748% (501111/652928)
01/15/2023 12:00:33 - INFO - __main__ -   test: [epoch: 1 | batch: 5200/10010 ] | Loss: 0.941 | Acc: 76.739% (510876/665728)
01/15/2023 12:04:05 - INFO - __main__ -   test: [epoch: 1 | batch: 5300/10010 ] | Loss: 0.941 | Acc: 76.741% (520707/678528)
01/15/2023 12:07:36 - INFO - __main__ -   test: [epoch: 1 | batch: 5400/10010 ] | Loss: 0.941 | Acc: 76.737% (530507/691328)
01/15/2023 12:11:07 - INFO - __main__ -   test: [epoch: 1 | batch: 5500/10010 ] | Loss: 0.941 | Acc: 76.734% (540305/704128)
01/15/2023 12:14:39 - INFO - __main__ -   test: [epoch: 1 | batch: 5600/10010 ] | Loss: 0.941 | Acc: 76.747% (550218/716928)
01/15/2023 12:18:11 - INFO - __main__ -   test: [epoch: 1 | batch: 5700/10010 ] | Loss: 0.941 | Acc: 76.748% (560054/729728)
01/15/2023 12:21:42 - INFO - __main__ -   test: [epoch: 1 | batch: 5800/10010 ] | Loss: 0.940 | Acc: 76.757% (569940/742528)
01/15/2023 12:25:14 - INFO - __main__ -   test: [epoch: 1 | batch: 5900/10010 ] | Loss: 0.940 | Acc: 76.755% (579751/755328)
01/15/2023 12:28:46 - INFO - __main__ -   test: [epoch: 1 | batch: 6000/10010 ] | Loss: 0.940 | Acc: 76.758% (589596/768128)
01/15/2023 12:32:18 - INFO - __main__ -   test: [epoch: 1 | batch: 6100/10010 ] | Loss: 0.940 | Acc: 76.747% (599335/780928)
01/15/2023 12:35:49 - INFO - __main__ -   test: [epoch: 1 | batch: 6200/10010 ] | Loss: 0.940 | Acc: 76.753% (609214/793728)
01/15/2023 12:39:21 - INFO - __main__ -   test: [epoch: 1 | batch: 6300/10010 ] | Loss: 0.940 | Acc: 76.751% (619015/806528)
01/15/2023 12:42:53 - INFO - __main__ -   test: [epoch: 1 | batch: 6400/10010 ] | Loss: 0.941 | Acc: 76.730% (628670/819328)
01/15/2023 12:46:25 - INFO - __main__ -   test: [epoch: 1 | batch: 6500/10010 ] | Loss: 0.940 | Acc: 76.738% (638562/832128)
01/15/2023 12:49:56 - INFO - __main__ -   test: [epoch: 1 | batch: 6600/10010 ] | Loss: 0.940 | Acc: 76.745% (648444/844928)
01/15/2023 12:53:27 - INFO - __main__ -   test: [epoch: 1 | batch: 6700/10010 ] | Loss: 0.940 | Acc: 76.742% (658239/857728)
01/15/2023 12:56:59 - INFO - __main__ -   test: [epoch: 1 | batch: 6800/10010 ] | Loss: 0.941 | Acc: 76.736% (668012/870528)
01/15/2023 13:00:30 - INFO - __main__ -   test: [epoch: 1 | batch: 6900/10010 ] | Loss: 0.940 | Acc: 76.743% (677890/883328)
01/15/2023 13:04:03 - INFO - __main__ -   test: [epoch: 1 | batch: 7000/10010 ] | Loss: 0.941 | Acc: 76.737% (687659/896128)
01/15/2023 13:07:35 - INFO - __main__ -   test: [epoch: 1 | batch: 7100/10010 ] | Loss: 0.940 | Acc: 76.743% (697535/908928)
01/15/2023 13:11:09 - INFO - __main__ -   test: [epoch: 1 | batch: 7200/10010 ] | Loss: 0.940 | Acc: 76.745% (707380/921728)
01/15/2023 13:14:40 - INFO - __main__ -   test: [epoch: 1 | batch: 7300/10010 ] | Loss: 0.940 | Acc: 76.744% (717194/934528)
01/15/2023 13:18:12 - INFO - __main__ -   test: [epoch: 1 | batch: 7400/10010 ] | Loss: 0.941 | Acc: 76.733% (726916/947328)
01/15/2023 13:21:43 - INFO - __main__ -   test: [epoch: 1 | batch: 7500/10010 ] | Loss: 0.941 | Acc: 76.733% (736736/960128)
01/15/2023 13:25:15 - INFO - __main__ -   test: [epoch: 1 | batch: 7600/10010 ] | Loss: 0.941 | Acc: 76.729% (746520/972928)
01/15/2023 13:28:47 - INFO - __main__ -   test: [epoch: 1 | batch: 7700/10010 ] | Loss: 0.941 | Acc: 76.729% (756339/985728)
01/15/2023 13:32:20 - INFO - __main__ -   test: [epoch: 1 | batch: 7800/10010 ] | Loss: 0.941 | Acc: 76.724% (766113/998528)
01/15/2023 13:35:53 - INFO - __main__ -   test: [epoch: 1 | batch: 7900/10010 ] | Loss: 0.941 | Acc: 76.730% (775993/1011328)
01/15/2023 13:39:25 - INFO - __main__ -   test: [epoch: 1 | batch: 8000/10010 ] | Loss: 0.941 | Acc: 76.736% (785872/1024128)
01/15/2023 13:42:58 - INFO - __main__ -   test: [epoch: 1 | batch: 8100/10010 ] | Loss: 0.941 | Acc: 76.745% (795792/1036928)
01/15/2023 13:46:31 - INFO - __main__ -   test: [epoch: 1 | batch: 8200/10010 ] | Loss: 0.941 | Acc: 76.741% (805572/1049728)
01/15/2023 13:50:02 - INFO - __main__ -   test: [epoch: 1 | batch: 8300/10010 ] | Loss: 0.941 | Acc: 76.736% (815343/1062528)
01/15/2023 13:53:33 - INFO - __main__ -   test: [epoch: 1 | batch: 8400/10010 ] | Loss: 0.941 | Acc: 76.739% (825191/1075328)
01/15/2023 13:57:06 - INFO - __main__ -   test: [epoch: 1 | batch: 8500/10010 ] | Loss: 0.941 | Acc: 76.743% (835058/1088128)
01/15/2023 14:00:37 - INFO - __main__ -   test: [epoch: 1 | batch: 8600/10010 ] | Loss: 0.941 | Acc: 76.741% (844868/1100928)
01/15/2023 14:04:07 - INFO - __main__ -   test: [epoch: 1 | batch: 8700/10010 ] | Loss: 0.941 | Acc: 76.752% (854804/1113728)
01/15/2023 14:07:41 - INFO - __main__ -   test: [epoch: 1 | batch: 8800/10010 ] | Loss: 0.941 | Acc: 76.756% (864682/1126528)
01/15/2023 14:11:13 - INFO - __main__ -   test: [epoch: 1 | batch: 8900/10010 ] | Loss: 0.941 | Acc: 76.762% (874569/1139328)
01/15/2023 14:14:45 - INFO - __main__ -   test: [epoch: 1 | batch: 9000/10010 ] | Loss: 0.941 | Acc: 76.764% (884418/1152128)
01/15/2023 14:18:15 - INFO - __main__ -   test: [epoch: 1 | batch: 9100/10010 ] | Loss: 0.940 | Acc: 76.770% (894311/1164928)
01/15/2023 14:21:47 - INFO - __main__ -   test: [epoch: 1 | batch: 9200/10010 ] | Loss: 0.940 | Acc: 76.772% (904167/1177728)
01/15/2023 14:25:18 - INFO - __main__ -   test: [epoch: 1 | batch: 9300/10010 ] | Loss: 0.940 | Acc: 76.778% (914059/1190528)
01/15/2023 14:28:49 - INFO - __main__ -   test: [epoch: 1 | batch: 9400/10010 ] | Loss: 0.940 | Acc: 76.773% (923831/1203328)
01/15/2023 14:32:20 - INFO - __main__ -   test: [epoch: 1 | batch: 9500/10010 ] | Loss: 0.940 | Acc: 76.773% (933657/1216128)
01/15/2023 14:35:52 - INFO - __main__ -   test: [epoch: 1 | batch: 9600/10010 ] | Loss: 0.940 | Acc: 76.770% (943450/1228928)
01/15/2023 14:39:23 - INFO - __main__ -   test: [epoch: 1 | batch: 9700/10010 ] | Loss: 0.940 | Acc: 76.770% (953274/1241728)
01/15/2023 14:42:56 - INFO - __main__ -   test: [epoch: 1 | batch: 9800/10010 ] | Loss: 0.940 | Acc: 76.765% (963037/1254528)
01/15/2023 14:46:29 - INFO - __main__ -   test: [epoch: 1 | batch: 9900/10010 ] | Loss: 0.941 | Acc: 76.766% (972872/1267328)
01/15/2023 14:50:03 - INFO - __main__ -   test: [epoch: 1 | batch: 10000/10010 ] | Loss: 0.941 | Acc: 76.763% (982669/1280128)
01/15/2023 14:50:22 - INFO - __main__ -   Saving Checkpoint
01/15/2023 14:50:24 - INFO - __main__ -   test: [batch: 0/391 ] | Loss: 0.518 | Acc: 88.281% (113/128)/ 95.312% (122/128)
01/15/2023 14:50:26 - INFO - __main__ -   test: [batch: 1/391 ] | Loss: 0.543 | Acc: 85.547% (219/256)/ 96.484% (247/256)
01/15/2023 14:50:28 - INFO - __main__ -   test: [batch: 2/391 ] | Loss: 0.737 | Acc: 79.948% (307/384)/ 94.271% (362/384)
01/15/2023 14:50:30 - INFO - __main__ -   test: [batch: 3/391 ] | Loss: 0.673 | Acc: 82.617% (423/512)/ 94.727% (485/512)
01/15/2023 14:50:32 - INFO - __main__ -   test: [batch: 4/391 ] | Loss: 0.580 | Acc: 85.000% (544/640)/ 95.625% (612/640)
01/15/2023 14:50:35 - INFO - __main__ -   test: [batch: 5/391 ] | Loss: 0.514 | Acc: 86.198% (662/768)/ 96.354% (740/768)
01/15/2023 14:50:37 - INFO - __main__ -   test: [batch: 6/391 ] | Loss: 0.500 | Acc: 86.830% (778/896)/ 96.317% (863/896)
01/15/2023 14:50:39 - INFO - __main__ -   test: [batch: 7/391 ] | Loss: 0.478 | Acc: 87.793% (899/1024)/ 96.484% (988/1024)
01/15/2023 14:50:41 - INFO - __main__ -   test: [batch: 8/391 ] | Loss: 0.486 | Acc: 87.760% (1011/1152)/ 96.528% (1112/1152)
01/15/2023 14:50:43 - INFO - __main__ -   test: [batch: 9/391 ] | Loss: 0.461 | Acc: 88.516% (1133/1280)/ 96.719% (1238/1280)
01/15/2023 14:50:45 - INFO - __main__ -   test: [batch: 10/391 ] | Loss: 0.505 | Acc: 87.216% (1228/1408)/ 96.804% (1363/1408)
01/15/2023 14:50:47 - INFO - __main__ -   test: [batch: 11/391 ] | Loss: 0.507 | Acc: 87.500% (1344/1536)/ 96.680% (1485/1536)
01/15/2023 14:50:49 - INFO - __main__ -   test: [batch: 12/391 ] | Loss: 0.559 | Acc: 86.238% (1435/1664)/ 96.214% (1601/1664)
01/15/2023 14:50:51 - INFO - __main__ -   test: [batch: 13/391 ] | Loss: 0.621 | Acc: 84.654% (1517/1792)/ 95.368% (1709/1792)
01/15/2023 14:50:53 - INFO - __main__ -   test: [batch: 14/391 ] | Loss: 0.637 | Acc: 83.854% (1610/1920)/ 95.573% (1835/1920)
01/15/2023 14:50:55 - INFO - __main__ -   test: [batch: 15/391 ] | Loss: 0.642 | Acc: 83.252% (1705/2048)/ 95.703% (1960/2048)
01/15/2023 14:50:58 - INFO - __main__ -   test: [batch: 16/391 ] | Loss: 0.650 | Acc: 83.180% (1810/2176)/ 95.588% (2080/2176)
01/15/2023 14:51:00 - INFO - __main__ -   test: [batch: 17/391 ] | Loss: 0.676 | Acc: 82.682% (1905/2304)/ 95.182% (2193/2304)
01/15/2023 14:51:02 - INFO - __main__ -   test: [batch: 18/391 ] | Loss: 0.699 | Acc: 82.196% (1999/2432)/ 94.984% (2310/2432)
01/15/2023 14:51:04 - INFO - __main__ -   test: [batch: 19/391 ] | Loss: 0.709 | Acc: 81.875% (2096/2560)/ 94.805% (2427/2560)
01/15/2023 14:51:06 - INFO - __main__ -   test: [batch: 20/391 ] | Loss: 0.704 | Acc: 82.031% (2205/2688)/ 94.754% (2547/2688)
01/15/2023 14:51:08 - INFO - __main__ -   test: [batch: 21/391 ] | Loss: 0.742 | Acc: 81.037% (2282/2816)/ 94.602% (2664/2816)
01/15/2023 14:51:11 - INFO - __main__ -   test: [batch: 22/391 ] | Loss: 0.740 | Acc: 80.808% (2379/2944)/ 94.599% (2785/2944)
01/15/2023 14:51:13 - INFO - __main__ -   test: [batch: 23/391 ] | Loss: 0.782 | Acc: 79.948% (2456/3072)/ 94.271% (2896/3072)
01/15/2023 14:51:15 - INFO - __main__ -   test: [batch: 24/391 ] | Loss: 0.799 | Acc: 79.562% (2546/3200)/ 94.156% (3013/3200)
01/15/2023 14:51:17 - INFO - __main__ -   test: [batch: 25/391 ] | Loss: 0.828 | Acc: 78.786% (2622/3328)/ 93.840% (3123/3328)
01/15/2023 14:51:19 - INFO - __main__ -   test: [batch: 26/391 ] | Loss: 0.846 | Acc: 78.038% (2697/3456)/ 93.750% (3240/3456)
01/15/2023 14:51:21 - INFO - __main__ -   test: [batch: 27/391 ] | Loss: 0.827 | Acc: 78.544% (2815/3584)/ 93.806% (3362/3584)
01/15/2023 14:51:23 - INFO - __main__ -   test: [batch: 28/391 ] | Loss: 0.833 | Acc: 77.990% (2895/3712)/ 93.939% (3487/3712)
01/15/2023 14:51:26 - INFO - __main__ -   test: [batch: 29/391 ] | Loss: 0.825 | Acc: 78.125% (3000/3840)/ 94.089% (3613/3840)
01/15/2023 14:51:28 - INFO - __main__ -   test: [batch: 30/391 ] | Loss: 0.837 | Acc: 78.024% (3096/3968)/ 93.952% (3728/3968)
01/15/2023 14:51:30 - INFO - __main__ -   test: [batch: 31/391 ] | Loss: 0.831 | Acc: 78.247% (3205/4096)/ 94.067% (3853/4096)
01/15/2023 14:51:32 - INFO - __main__ -   test: [batch: 32/391 ] | Loss: 0.817 | Acc: 78.575% (3319/4224)/ 94.223% (3980/4224)
01/15/2023 14:51:34 - INFO - __main__ -   test: [batch: 33/391 ] | Loss: 0.811 | Acc: 78.745% (3427/4352)/ 94.278% (4103/4352)
01/15/2023 14:51:36 - INFO - __main__ -   test: [batch: 34/391 ] | Loss: 0.796 | Acc: 79.174% (3547/4480)/ 94.353% (4227/4480)
01/15/2023 14:51:38 - INFO - __main__ -   test: [batch: 35/391 ] | Loss: 0.782 | Acc: 79.557% (3666/4608)/ 94.401% (4350/4608)
01/15/2023 14:51:41 - INFO - __main__ -   test: [batch: 36/391 ] | Loss: 0.766 | Acc: 80.004% (3789/4736)/ 94.531% (4477/4736)
01/15/2023 14:51:42 - INFO - __main__ -   test: [batch: 37/391 ] | Loss: 0.756 | Acc: 80.325% (3907/4864)/ 94.613% (4602/4864)
01/15/2023 14:51:44 - INFO - __main__ -   test: [batch: 38/391 ] | Loss: 0.748 | Acc: 80.529% (4020/4992)/ 94.692% (4727/4992)
01/15/2023 14:51:47 - INFO - __main__ -   test: [batch: 39/391 ] | Loss: 0.742 | Acc: 80.723% (4133/5120)/ 94.727% (4850/5120)
01/15/2023 14:51:49 - INFO - __main__ -   test: [batch: 40/391 ] | Loss: 0.740 | Acc: 80.850% (4243/5248)/ 94.665% (4968/5248)
01/15/2023 14:51:51 - INFO - __main__ -   test: [batch: 41/391 ] | Loss: 0.742 | Acc: 81.027% (4356/5376)/ 94.587% (5085/5376)
01/15/2023 14:51:53 - INFO - __main__ -   test: [batch: 42/391 ] | Loss: 0.742 | Acc: 81.050% (4461/5504)/ 94.658% (5210/5504)
01/15/2023 14:51:55 - INFO - __main__ -   test: [batch: 43/391 ] | Loss: 0.741 | Acc: 81.055% (4565/5632)/ 94.638% (5330/5632)
01/15/2023 14:51:57 - INFO - __main__ -   test: [batch: 44/391 ] | Loss: 0.745 | Acc: 81.042% (4668/5760)/ 94.566% (5447/5760)
01/15/2023 14:52:00 - INFO - __main__ -   test: [batch: 45/391 ] | Loss: 0.742 | Acc: 81.165% (4779/5888)/ 94.616% (5571/5888)
01/15/2023 14:52:02 - INFO - __main__ -   test: [batch: 46/391 ] | Loss: 0.745 | Acc: 81.084% (4878/6016)/ 94.681% (5696/6016)
01/15/2023 14:52:04 - INFO - __main__ -   test: [batch: 47/391 ] | Loss: 0.749 | Acc: 80.908% (4971/6144)/ 94.727% (5820/6144)
01/15/2023 14:52:06 - INFO - __main__ -   test: [batch: 48/391 ] | Loss: 0.753 | Acc: 80.788% (5067/6272)/ 94.707% (5940/6272)
01/15/2023 14:52:08 - INFO - __main__ -   test: [batch: 49/391 ] | Loss: 0.758 | Acc: 80.766% (5169/6400)/ 94.688% (6060/6400)
01/15/2023 14:52:10 - INFO - __main__ -   test: [batch: 50/391 ] | Loss: 0.749 | Acc: 80.974% (5286/6528)/ 94.761% (6186/6528)
01/15/2023 14:52:12 - INFO - __main__ -   test: [batch: 51/391 ] | Loss: 0.740 | Acc: 81.190% (5404/6656)/ 94.847% (6313/6656)
01/15/2023 14:52:14 - INFO - __main__ -   test: [batch: 52/391 ] | Loss: 0.736 | Acc: 81.265% (5513/6784)/ 94.885% (6437/6784)
01/15/2023 14:52:16 - INFO - __main__ -   test: [batch: 53/391 ] | Loss: 0.727 | Acc: 81.496% (5633/6912)/ 94.965% (6564/6912)
01/15/2023 14:52:19 - INFO - __main__ -   test: [batch: 54/391 ] | Loss: 0.720 | Acc: 81.676% (5750/7040)/ 95.000% (6688/7040)
01/15/2023 14:52:21 - INFO - __main__ -   test: [batch: 55/391 ] | Loss: 0.713 | Acc: 81.808% (5864/7168)/ 95.047% (6813/7168)
01/15/2023 14:52:23 - INFO - __main__ -   test: [batch: 56/391 ] | Loss: 0.704 | Acc: 82.072% (5988/7296)/ 95.107% (6939/7296)
01/15/2023 14:52:25 - INFO - __main__ -   test: [batch: 57/391 ] | Loss: 0.697 | Acc: 82.260% (6107/7424)/ 95.151% (7064/7424)
01/15/2023 14:52:27 - INFO - __main__ -   test: [batch: 58/391 ] | Loss: 0.693 | Acc: 82.349% (6219/7552)/ 95.140% (7185/7552)
01/15/2023 14:52:29 - INFO - __main__ -   test: [batch: 59/391 ] | Loss: 0.695 | Acc: 82.266% (6318/7680)/ 95.130% (7306/7680)
01/15/2023 14:52:31 - INFO - __main__ -   test: [batch: 60/391 ] | Loss: 0.698 | Acc: 82.198% (6418/7808)/ 95.108% (7426/7808)
01/15/2023 14:52:33 - INFO - __main__ -   test: [batch: 61/391 ] | Loss: 0.697 | Acc: 82.182% (6522/7936)/ 95.123% (7549/7936)
01/15/2023 14:52:35 - INFO - __main__ -   test: [batch: 62/391 ] | Loss: 0.696 | Acc: 82.155% (6625/8064)/ 95.176% (7675/8064)
01/15/2023 14:52:37 - INFO - __main__ -   test: [batch: 63/391 ] | Loss: 0.703 | Acc: 82.043% (6721/8192)/ 95.117% (7792/8192)
01/15/2023 14:52:40 - INFO - __main__ -   test: [batch: 64/391 ] | Loss: 0.711 | Acc: 81.827% (6808/8320)/ 95.072% (7910/8320)
01/15/2023 14:52:42 - INFO - __main__ -   test: [batch: 65/391 ] | Loss: 0.723 | Acc: 81.368% (6874/8448)/ 95.005% (8026/8448)
01/15/2023 14:52:44 - INFO - __main__ -   test: [batch: 66/391 ] | Loss: 0.729 | Acc: 81.367% (6978/8576)/ 94.974% (8145/8576)
01/15/2023 14:52:46 - INFO - __main__ -   test: [batch: 67/391 ] | Loss: 0.731 | Acc: 81.330% (7079/8704)/ 94.956% (8265/8704)
01/15/2023 14:52:48 - INFO - __main__ -   test: [batch: 68/391 ] | Loss: 0.732 | Acc: 81.250% (7176/8832)/ 94.973% (8388/8832)
01/15/2023 14:52:50 - INFO - __main__ -   test: [batch: 69/391 ] | Loss: 0.728 | Acc: 81.362% (7290/8960)/ 95.022% (8514/8960)
01/15/2023 14:52:52 - INFO - __main__ -   test: [batch: 70/391 ] | Loss: 0.728 | Acc: 81.272% (7386/9088)/ 95.015% (8635/9088)
01/15/2023 14:52:54 - INFO - __main__ -   test: [batch: 71/391 ] | Loss: 0.726 | Acc: 81.326% (7495/9216)/ 95.009% (8756/9216)
01/15/2023 14:52:56 - INFO - __main__ -   test: [batch: 72/391 ] | Loss: 0.729 | Acc: 81.218% (7589/9344)/ 95.002% (8877/9344)
01/15/2023 14:52:59 - INFO - __main__ -   test: [batch: 73/391 ] | Loss: 0.732 | Acc: 81.092% (7681/9472)/ 94.996% (8998/9472)
01/15/2023 14:53:01 - INFO - __main__ -   test: [batch: 74/391 ] | Loss: 0.733 | Acc: 81.094% (7785/9600)/ 94.979% (9118/9600)
01/15/2023 14:53:03 - INFO - __main__ -   test: [batch: 75/391 ] | Loss: 0.739 | Acc: 80.900% (7870/9728)/ 94.963% (9238/9728)
01/15/2023 14:53:05 - INFO - __main__ -   test: [batch: 76/391 ] | Loss: 0.738 | Acc: 80.925% (7976/9856)/ 94.978% (9361/9856)
01/15/2023 14:53:07 - INFO - __main__ -   test: [batch: 77/391 ] | Loss: 0.739 | Acc: 80.859% (8073/9984)/ 95.012% (9486/9984)
01/15/2023 14:53:09 - INFO - __main__ -   test: [batch: 78/391 ] | Loss: 0.738 | Acc: 80.835% (8174/10112)/ 95.055% (9612/10112)
01/15/2023 14:53:11 - INFO - __main__ -   test: [batch: 79/391 ] | Loss: 0.736 | Acc: 80.820% (8276/10240)/ 95.088% (9737/10240)
01/15/2023 14:53:13 - INFO - __main__ -   test: [batch: 80/391 ] | Loss: 0.736 | Acc: 80.787% (8376/10368)/ 95.091% (9859/10368)
01/15/2023 14:53:16 - INFO - __main__ -   test: [batch: 81/391 ] | Loss: 0.736 | Acc: 80.802% (8481/10496)/ 95.112% (9983/10496)
01/15/2023 14:53:18 - INFO - __main__ -   test: [batch: 82/391 ] | Loss: 0.735 | Acc: 80.817% (8586/10624)/ 95.115% (10105/10624)
01/15/2023 14:53:20 - INFO - __main__ -   test: [batch: 83/391 ] | Loss: 0.735 | Acc: 80.850% (8693/10752)/ 95.099% (10225/10752)
01/15/2023 14:53:22 - INFO - __main__ -   test: [batch: 84/391 ] | Loss: 0.730 | Acc: 80.965% (8809/10880)/ 95.138% (10351/10880)
01/15/2023 14:53:24 - INFO - __main__ -   test: [batch: 85/391 ] | Loss: 0.729 | Acc: 80.950% (8911/11008)/ 95.185% (10478/11008)
01/15/2023 14:53:26 - INFO - __main__ -   test: [batch: 86/391 ] | Loss: 0.733 | Acc: 80.864% (9005/11136)/ 95.133% (10594/11136)
01/15/2023 14:53:28 - INFO - __main__ -   test: [batch: 87/391 ] | Loss: 0.732 | Acc: 80.922% (9115/11264)/ 95.135% (10716/11264)
01/15/2023 14:53:31 - INFO - __main__ -   test: [batch: 88/391 ] | Loss: 0.739 | Acc: 80.837% (9209/11392)/ 95.084% (10832/11392)
01/15/2023 14:53:33 - INFO - __main__ -   test: [batch: 89/391 ] | Loss: 0.737 | Acc: 80.868% (9316/11520)/ 95.095% (10955/11520)
01/15/2023 14:53:34 - INFO - __main__ -   test: [batch: 90/391 ] | Loss: 0.739 | Acc: 80.786% (9410/11648)/ 95.106% (11078/11648)
01/15/2023 14:53:37 - INFO - __main__ -   test: [batch: 91/391 ] | Loss: 0.737 | Acc: 80.842% (9520/11776)/ 95.126% (11202/11776)
01/15/2023 14:53:39 - INFO - __main__ -   test: [batch: 92/391 ] | Loss: 0.738 | Acc: 80.796% (9618/11904)/ 95.102% (11321/11904)
01/15/2023 14:53:41 - INFO - __main__ -   test: [batch: 93/391 ] | Loss: 0.741 | Acc: 80.593% (9697/12032)/ 95.146% (11448/12032)
01/15/2023 14:53:43 - INFO - __main__ -   test: [batch: 94/391 ] | Loss: 0.744 | Acc: 80.452% (9783/12160)/ 95.156% (11571/12160)
01/15/2023 14:53:46 - INFO - __main__ -   test: [batch: 95/391 ] | Loss: 0.742 | Acc: 80.509% (9893/12288)/ 95.182% (11696/12288)
01/15/2023 14:53:48 - INFO - __main__ -   test: [batch: 96/391 ] | Loss: 0.746 | Acc: 80.372% (9979/12416)/ 95.192% (11819/12416)
01/15/2023 14:53:50 - INFO - __main__ -   test: [batch: 97/391 ] | Loss: 0.748 | Acc: 80.214% (10062/12544)/ 95.209% (11943/12544)
01/15/2023 14:53:52 - INFO - __main__ -   test: [batch: 98/391 ] | Loss: 0.744 | Acc: 80.319% (10178/12672)/ 95.241% (12069/12672)
01/15/2023 14:53:54 - INFO - __main__ -   test: [batch: 99/391 ] | Loss: 0.738 | Acc: 80.477% (10301/12800)/ 95.289% (12197/12800)
01/15/2023 14:53:56 - INFO - __main__ -   test: [batch: 100/391 ] | Loss: 0.737 | Acc: 80.531% (10411/12928)/ 95.312% (12322/12928)
01/15/2023 14:53:59 - INFO - __main__ -   test: [batch: 101/391 ] | Loss: 0.733 | Acc: 80.645% (10529/13056)/ 95.343% (12448/13056)
01/15/2023 14:54:01 - INFO - __main__ -   test: [batch: 102/391 ] | Loss: 0.729 | Acc: 80.742% (10645/13184)/ 95.373% (12574/13184)
01/15/2023 14:54:03 - INFO - __main__ -   test: [batch: 103/391 ] | Loss: 0.731 | Acc: 80.589% (10728/13312)/ 95.380% (12697/13312)
01/15/2023 14:54:05 - INFO - __main__ -   test: [batch: 104/391 ] | Loss: 0.731 | Acc: 80.506% (10820/13440)/ 95.372% (12818/13440)
01/15/2023 14:54:07 - INFO - __main__ -   test: [batch: 105/391 ] | Loss: 0.732 | Acc: 80.506% (10923/13568)/ 95.379% (12941/13568)
01/15/2023 14:54:09 - INFO - __main__ -   test: [batch: 106/391 ] | Loss: 0.740 | Acc: 80.403% (11012/13696)/ 95.320% (13055/13696)
01/15/2023 14:54:12 - INFO - __main__ -   test: [batch: 107/391 ] | Loss: 0.736 | Acc: 80.534% (11133/13824)/ 95.356% (13182/13824)
01/15/2023 14:54:14 - INFO - __main__ -   test: [batch: 108/391 ] | Loss: 0.739 | Acc: 80.404% (11218/13952)/ 95.363% (13305/13952)
01/15/2023 14:54:16 - INFO - __main__ -   test: [batch: 109/391 ] | Loss: 0.739 | Acc: 80.398% (11320/14080)/ 95.369% (13428/14080)
01/15/2023 14:54:18 - INFO - __main__ -   test: [batch: 110/391 ] | Loss: 0.741 | Acc: 80.208% (11396/14208)/ 95.390% (13553/14208)
01/15/2023 14:54:20 - INFO - __main__ -   test: [batch: 111/391 ] | Loss: 0.745 | Acc: 80.183% (11495/14336)/ 95.354% (13670/14336)
01/15/2023 14:54:22 - INFO - __main__ -   test: [batch: 112/391 ] | Loss: 0.745 | Acc: 80.220% (11603/14464)/ 95.375% (13795/14464)
01/15/2023 14:54:24 - INFO - __main__ -   test: [batch: 113/391 ] | Loss: 0.743 | Acc: 80.263% (11712/14592)/ 95.388% (13919/14592)
01/15/2023 14:54:26 - INFO - __main__ -   test: [batch: 114/391 ] | Loss: 0.740 | Acc: 80.367% (11830/14720)/ 95.414% (14045/14720)
01/15/2023 14:54:28 - INFO - __main__ -   test: [batch: 115/391 ] | Loss: 0.737 | Acc: 80.469% (11948/14848)/ 95.440% (14171/14848)
01/15/2023 14:54:30 - INFO - __main__ -   test: [batch: 116/391 ] | Loss: 0.740 | Acc: 80.435% (12046/14976)/ 95.413% (14289/14976)
01/15/2023 14:54:33 - INFO - __main__ -   test: [batch: 117/391 ] | Loss: 0.739 | Acc: 80.469% (12154/15104)/ 95.432% (14414/15104)
01/15/2023 14:54:35 - INFO - __main__ -   test: [batch: 118/391 ] | Loss: 0.742 | Acc: 80.324% (12235/15232)/ 95.444% (14538/15232)
01/15/2023 14:54:37 - INFO - __main__ -   test: [batch: 119/391 ] | Loss: 0.740 | Acc: 80.371% (12345/15360)/ 95.469% (14664/15360)
01/15/2023 14:54:39 - INFO - __main__ -   test: [batch: 120/391 ] | Loss: 0.739 | Acc: 80.436% (12458/15488)/ 95.480% (14788/15488)
01/15/2023 14:54:41 - INFO - __main__ -   test: [batch: 121/391 ] | Loss: 0.744 | Acc: 80.309% (12541/15616)/ 95.447% (14905/15616)
01/15/2023 14:54:43 - INFO - __main__ -   test: [batch: 122/391 ] | Loss: 0.746 | Acc: 80.253% (12635/15744)/ 95.427% (15024/15744)
01/15/2023 14:54:45 - INFO - __main__ -   test: [batch: 123/391 ] | Loss: 0.747 | Acc: 80.280% (12742/15872)/ 95.420% (15145/15872)
01/15/2023 14:54:47 - INFO - __main__ -   test: [batch: 124/391 ] | Loss: 0.746 | Acc: 80.300% (12848/16000)/ 95.425% (15268/16000)
01/15/2023 14:54:49 - INFO - __main__ -   test: [batch: 125/391 ] | Loss: 0.742 | Acc: 80.413% (12969/16128)/ 95.455% (15395/16128)
01/15/2023 14:54:52 - INFO - __main__ -   test: [batch: 126/391 ] | Loss: 0.739 | Acc: 80.506% (13087/16256)/ 95.460% (15518/16256)
01/15/2023 14:54:54 - INFO - __main__ -   test: [batch: 127/391 ] | Loss: 0.737 | Acc: 80.573% (13201/16384)/ 95.471% (15642/16384)
01/15/2023 14:54:56 - INFO - __main__ -   test: [batch: 128/391 ] | Loss: 0.738 | Acc: 80.572% (13304/16512)/ 95.452% (15761/16512)
01/15/2023 14:54:58 - INFO - __main__ -   test: [batch: 129/391 ] | Loss: 0.736 | Acc: 80.613% (13414/16640)/ 95.481% (15888/16640)
01/15/2023 14:55:00 - INFO - __main__ -   test: [batch: 130/391 ] | Loss: 0.732 | Acc: 80.725% (13536/16768)/ 95.503% (16014/16768)
01/15/2023 14:55:02 - INFO - __main__ -   test: [batch: 131/391 ] | Loss: 0.732 | Acc: 80.765% (13646/16896)/ 95.520% (16139/16896)
01/15/2023 14:55:04 - INFO - __main__ -   test: [batch: 132/391 ] | Loss: 0.730 | Acc: 80.827% (13760/17024)/ 95.536% (16264/17024)
01/15/2023 14:55:06 - INFO - __main__ -   test: [batch: 133/391 ] | Loss: 0.732 | Acc: 80.784% (13856/17152)/ 95.522% (16384/17152)
01/15/2023 14:55:08 - INFO - __main__ -   test: [batch: 134/391 ] | Loss: 0.730 | Acc: 80.816% (13965/17280)/ 95.538% (16509/17280)
01/15/2023 14:55:11 - INFO - __main__ -   test: [batch: 135/391 ] | Loss: 0.730 | Acc: 80.808% (14067/17408)/ 95.537% (16631/17408)
01/15/2023 14:55:13 - INFO - __main__ -   test: [batch: 136/391 ] | Loss: 0.728 | Acc: 80.811% (14171/17536)/ 95.558% (16757/17536)
01/15/2023 14:55:15 - INFO - __main__ -   test: [batch: 137/391 ] | Loss: 0.726 | Acc: 80.854% (14282/17664)/ 95.590% (16885/17664)
01/15/2023 14:55:17 - INFO - __main__ -   test: [batch: 138/391 ] | Loss: 0.725 | Acc: 80.902% (14394/17792)/ 95.605% (17010/17792)
01/15/2023 14:55:19 - INFO - __main__ -   test: [batch: 139/391 ] | Loss: 0.730 | Acc: 80.765% (14473/17920)/ 95.592% (17130/17920)
01/15/2023 14:55:21 - INFO - __main__ -   test: [batch: 140/391 ] | Loss: 0.732 | Acc: 80.685% (14562/18048)/ 95.590% (17252/18048)
01/15/2023 14:55:23 - INFO - __main__ -   test: [batch: 141/391 ] | Loss: 0.730 | Acc: 80.727% (14673/18176)/ 95.604% (17377/18176)
01/15/2023 14:55:25 - INFO - __main__ -   test: [batch: 142/391 ] | Loss: 0.728 | Acc: 80.813% (14792/18304)/ 95.613% (17501/18304)
01/15/2023 14:55:27 - INFO - __main__ -   test: [batch: 143/391 ] | Loss: 0.728 | Acc: 80.843% (14901/18432)/ 95.595% (17620/18432)
01/15/2023 14:55:30 - INFO - __main__ -   test: [batch: 144/391 ] | Loss: 0.730 | Acc: 80.814% (14999/18560)/ 95.560% (17736/18560)
01/15/2023 14:55:32 - INFO - __main__ -   test: [batch: 145/391 ] | Loss: 0.733 | Acc: 80.790% (15098/18688)/ 95.537% (17854/18688)
01/15/2023 14:55:34 - INFO - __main__ -   test: [batch: 146/391 ] | Loss: 0.734 | Acc: 80.756% (15195/18816)/ 95.514% (17972/18816)
01/15/2023 14:55:36 - INFO - __main__ -   test: [batch: 147/391 ] | Loss: 0.734 | Acc: 80.759% (15299/18944)/ 95.503% (18092/18944)
01/15/2023 14:55:38 - INFO - __main__ -   test: [batch: 148/391 ] | Loss: 0.736 | Acc: 80.694% (15390/19072)/ 95.501% (18214/19072)
01/15/2023 14:55:40 - INFO - __main__ -   test: [batch: 149/391 ] | Loss: 0.740 | Acc: 80.620% (15479/19200)/ 95.458% (18328/19200)
01/15/2023 14:55:42 - INFO - __main__ -   test: [batch: 150/391 ] | Loss: 0.740 | Acc: 80.562% (15571/19328)/ 95.463% (18451/19328)
01/15/2023 14:55:44 - INFO - __main__ -   test: [batch: 151/391 ] | Loss: 0.739 | Acc: 80.608% (15683/19456)/ 95.456% (18572/19456)
01/15/2023 14:55:46 - INFO - __main__ -   test: [batch: 152/391 ] | Loss: 0.741 | Acc: 80.596% (15784/19584)/ 95.435% (18690/19584)
01/15/2023 14:55:49 - INFO - __main__ -   test: [batch: 153/391 ] | Loss: 0.739 | Acc: 80.656% (15899/19712)/ 95.439% (18813/19712)
01/15/2023 14:55:51 - INFO - __main__ -   test: [batch: 154/391 ] | Loss: 0.740 | Acc: 80.635% (15998/19840)/ 95.408% (18929/19840)
01/15/2023 14:55:53 - INFO - __main__ -   test: [batch: 155/391 ] | Loss: 0.741 | Acc: 80.639% (16102/19968)/ 95.398% (19049/19968)
01/15/2023 14:55:55 - INFO - __main__ -   test: [batch: 156/391 ] | Loss: 0.743 | Acc: 80.563% (16190/20096)/ 95.377% (19167/20096)
01/15/2023 14:55:57 - INFO - __main__ -   test: [batch: 157/391 ] | Loss: 0.746 | Acc: 80.523% (16285/20224)/ 95.352% (19284/20224)
01/15/2023 14:55:59 - INFO - __main__ -   test: [batch: 158/391 ] | Loss: 0.747 | Acc: 80.518% (16387/20352)/ 95.327% (19401/20352)
01/15/2023 14:56:02 - INFO - __main__ -   test: [batch: 159/391 ] | Loss: 0.748 | Acc: 80.488% (16484/20480)/ 95.332% (19524/20480)
01/15/2023 14:56:04 - INFO - __main__ -   test: [batch: 160/391 ] | Loss: 0.748 | Acc: 80.488% (16587/20608)/ 95.317% (19643/20608)
01/15/2023 14:56:06 - INFO - __main__ -   test: [batch: 161/391 ] | Loss: 0.758 | Acc: 80.242% (16639/20736)/ 95.216% (19744/20736)
01/15/2023 14:56:08 - INFO - __main__ -   test: [batch: 162/391 ] | Loss: 0.764 | Acc: 80.148% (16722/20864)/ 95.130% (19848/20864)
01/15/2023 14:56:10 - INFO - __main__ -   test: [batch: 163/391 ] | Loss: 0.766 | Acc: 80.073% (16809/20992)/ 95.108% (19965/20992)
01/15/2023 14:56:12 - INFO - __main__ -   test: [batch: 164/391 ] | Loss: 0.766 | Acc: 80.052% (16907/21120)/ 95.123% (20090/21120)
01/15/2023 14:56:14 - INFO - __main__ -   test: [batch: 165/391 ] | Loss: 0.768 | Acc: 79.979% (16994/21248)/ 95.120% (20211/21248)
01/15/2023 14:56:16 - INFO - __main__ -   test: [batch: 166/391 ] | Loss: 0.768 | Acc: 80.001% (17101/21376)/ 95.111% (20331/21376)
01/15/2023 14:56:19 - INFO - __main__ -   test: [batch: 167/391 ] | Loss: 0.771 | Acc: 79.957% (17194/21504)/ 95.075% (20445/21504)
01/15/2023 14:56:21 - INFO - __main__ -   test: [batch: 168/391 ] | Loss: 0.770 | Acc: 79.956% (17296/21632)/ 95.072% (20566/21632)
01/15/2023 14:56:23 - INFO - __main__ -   test: [batch: 169/391 ] | Loss: 0.772 | Acc: 79.899% (17386/21760)/ 95.023% (20677/21760)
01/15/2023 14:56:25 - INFO - __main__ -   test: [batch: 170/391 ] | Loss: 0.777 | Acc: 79.793% (17465/21888)/ 94.984% (20790/21888)
01/15/2023 14:56:27 - INFO - __main__ -   test: [batch: 171/391 ] | Loss: 0.780 | Acc: 79.747% (17557/22016)/ 94.949% (20904/22016)
01/15/2023 14:56:29 - INFO - __main__ -   test: [batch: 172/391 ] | Loss: 0.781 | Acc: 79.701% (17649/22144)/ 94.947% (21025/22144)
01/15/2023 14:56:31 - INFO - __main__ -   test: [batch: 173/391 ] | Loss: 0.784 | Acc: 79.656% (17741/22272)/ 94.917% (21140/22272)
01/15/2023 14:56:33 - INFO - __main__ -   test: [batch: 174/391 ] | Loss: 0.788 | Acc: 79.558% (17821/22400)/ 94.875% (21252/22400)
01/15/2023 14:56:35 - INFO - __main__ -   test: [batch: 175/391 ] | Loss: 0.786 | Acc: 79.621% (17937/22528)/ 94.886% (21376/22528)
01/15/2023 14:56:37 - INFO - __main__ -   test: [batch: 176/391 ] | Loss: 0.787 | Acc: 79.612% (18037/22656)/ 94.853% (21490/22656)
01/15/2023 14:56:39 - INFO - __main__ -   test: [batch: 177/391 ] | Loss: 0.791 | Acc: 79.543% (18123/22784)/ 94.817% (21603/22784)
01/15/2023 14:56:41 - INFO - __main__ -   test: [batch: 178/391 ] | Loss: 0.792 | Acc: 79.548% (18226/22912)/ 94.802% (21721/22912)
01/15/2023 14:56:44 - INFO - __main__ -   test: [batch: 179/391 ] | Loss: 0.796 | Acc: 79.479% (18312/23040)/ 94.748% (21830/23040)
01/15/2023 14:56:46 - INFO - __main__ -   test: [batch: 180/391 ] | Loss: 0.802 | Acc: 79.347% (18383/23168)/ 94.708% (21942/23168)
01/15/2023 14:56:48 - INFO - __main__ -   test: [batch: 181/391 ] | Loss: 0.808 | Acc: 79.224% (18456/23296)/ 94.647% (22049/23296)
01/15/2023 14:56:50 - INFO - __main__ -   test: [batch: 182/391 ] | Loss: 0.807 | Acc: 79.248% (18563/23424)/ 94.651% (22171/23424)
01/15/2023 14:56:52 - INFO - __main__ -   test: [batch: 183/391 ] | Loss: 0.814 | Acc: 79.127% (18636/23552)/ 94.535% (22265/23552)
01/15/2023 14:56:54 - INFO - __main__ -   test: [batch: 184/391 ] | Loss: 0.813 | Acc: 79.147% (18742/23680)/ 94.527% (22384/23680)
01/15/2023 14:56:56 - INFO - __main__ -   test: [batch: 185/391 ] | Loss: 0.813 | Acc: 79.158% (18846/23808)/ 94.523% (22504/23808)
01/15/2023 14:56:58 - INFO - __main__ -   test: [batch: 186/391 ] | Loss: 0.816 | Acc: 79.115% (18937/23936)/ 94.485% (22616/23936)
01/15/2023 14:57:00 - INFO - __main__ -   test: [batch: 187/391 ] | Loss: 0.820 | Acc: 78.998% (19010/24064)/ 94.465% (22732/24064)
01/15/2023 14:57:02 - INFO - __main__ -   test: [batch: 188/391 ] | Loss: 0.824 | Acc: 78.881% (19083/24192)/ 94.440% (22847/24192)
01/15/2023 14:57:05 - INFO - __main__ -   test: [batch: 189/391 ] | Loss: 0.825 | Acc: 78.816% (19168/24320)/ 94.445% (22969/24320)
01/15/2023 14:57:07 - INFO - __main__ -   test: [batch: 190/391 ] | Loss: 0.830 | Acc: 78.726% (19247/24448)/ 94.417% (23083/24448)
01/15/2023 14:57:09 - INFO - __main__ -   test: [batch: 191/391 ] | Loss: 0.833 | Acc: 78.691% (19339/24576)/ 94.377% (23194/24576)
01/15/2023 14:57:11 - INFO - __main__ -   test: [batch: 192/391 ] | Loss: 0.840 | Acc: 78.570% (19410/24704)/ 94.280% (23291/24704)
01/15/2023 14:57:13 - INFO - __main__ -   test: [batch: 193/391 ] | Loss: 0.840 | Acc: 78.576% (19512/24832)/ 94.278% (23411/24832)
01/15/2023 14:57:15 - INFO - __main__ -   test: [batch: 194/391 ] | Loss: 0.843 | Acc: 78.510% (19596/24960)/ 94.251% (23525/24960)
01/15/2023 14:57:17 - INFO - __main__ -   test: [batch: 195/391 ] | Loss: 0.849 | Acc: 78.412% (19672/25088)/ 94.184% (23629/25088)
01/15/2023 14:57:19 - INFO - __main__ -   test: [batch: 196/391 ] | Loss: 0.852 | Acc: 78.311% (19747/25216)/ 94.139% (23738/25216)
01/15/2023 14:57:22 - INFO - __main__ -   test: [batch: 197/391 ] | Loss: 0.857 | Acc: 78.224% (19825/25344)/ 94.117% (23853/25344)
01/15/2023 14:57:24 - INFO - __main__ -   test: [batch: 198/391 ] | Loss: 0.859 | Acc: 78.180% (19914/25472)/ 94.080% (23964/25472)
01/15/2023 14:57:25 - INFO - __main__ -   test: [batch: 199/391 ] | Loss: 0.859 | Acc: 78.152% (20007/25600)/ 94.090% (24087/25600)
01/15/2023 14:57:28 - INFO - __main__ -   test: [batch: 200/391 ] | Loss: 0.859 | Acc: 78.133% (20102/25728)/ 94.092% (24208/25728)
01/15/2023 14:57:30 - INFO - __main__ -   test: [batch: 201/391 ] | Loss: 0.862 | Acc: 78.036% (20177/25856)/ 94.059% (24320/25856)
01/15/2023 14:57:32 - INFO - __main__ -   test: [batch: 202/391 ] | Loss: 0.863 | Acc: 78.025% (20274/25984)/ 94.058% (24440/25984)
01/15/2023 14:57:34 - INFO - __main__ -   test: [batch: 203/391 ] | Loss: 0.864 | Acc: 78.018% (20372/26112)/ 94.049% (24558/26112)
01/15/2023 14:57:36 - INFO - __main__ -   test: [batch: 204/391 ] | Loss: 0.867 | Acc: 77.934% (20450/26240)/ 94.021% (24671/26240)
01/15/2023 14:57:38 - INFO - __main__ -   test: [batch: 205/391 ] | Loss: 0.870 | Acc: 77.860% (20530/26368)/ 93.997% (24785/26368)
01/15/2023 14:57:40 - INFO - __main__ -   test: [batch: 206/391 ] | Loss: 0.872 | Acc: 77.827% (20621/26496)/ 93.995% (24905/26496)
01/15/2023 14:57:42 - INFO - __main__ -   test: [batch: 207/391 ] | Loss: 0.875 | Acc: 77.749% (20700/26624)/ 93.964% (25017/26624)
01/15/2023 14:57:44 - INFO - __main__ -   test: [batch: 208/391 ] | Loss: 0.877 | Acc: 77.710% (20789/26752)/ 93.926% (25127/26752)
01/15/2023 14:57:46 - INFO - __main__ -   test: [batch: 209/391 ] | Loss: 0.876 | Acc: 77.708% (20888/26880)/ 93.947% (25253/26880)
01/15/2023 14:57:49 - INFO - __main__ -   test: [batch: 210/391 ] | Loss: 0.878 | Acc: 77.673% (20978/27008)/ 93.928% (25368/27008)
01/15/2023 14:57:51 - INFO - __main__ -   test: [batch: 211/391 ] | Loss: 0.880 | Acc: 77.613% (21061/27136)/ 93.897% (25480/27136)
01/15/2023 14:57:53 - INFO - __main__ -   test: [batch: 212/391 ] | Loss: 0.882 | Acc: 77.549% (21143/27264)/ 93.886% (25597/27264)
01/15/2023 14:57:55 - INFO - __main__ -   test: [batch: 213/391 ] | Loss: 0.882 | Acc: 77.552% (21243/27392)/ 93.896% (25720/27392)
01/15/2023 14:57:57 - INFO - __main__ -   test: [batch: 214/391 ] | Loss: 0.882 | Acc: 77.540% (21339/27520)/ 93.903% (25842/27520)
01/15/2023 14:57:59 - INFO - __main__ -   test: [batch: 215/391 ] | Loss: 0.883 | Acc: 77.528% (21435/27648)/ 93.880% (25956/27648)
01/15/2023 14:58:01 - INFO - __main__ -   test: [batch: 216/391 ] | Loss: 0.881 | Acc: 77.578% (21548/27776)/ 93.894% (26080/27776)
01/15/2023 14:58:04 - INFO - __main__ -   test: [batch: 217/391 ] | Loss: 0.886 | Acc: 77.516% (21630/27904)/ 93.829% (26182/27904)
01/15/2023 14:58:06 - INFO - __main__ -   test: [batch: 218/391 ] | Loss: 0.889 | Acc: 77.454% (21712/28032)/ 93.796% (26293/28032)
01/15/2023 14:58:08 - INFO - __main__ -   test: [batch: 219/391 ] | Loss: 0.888 | Acc: 77.493% (21822/28160)/ 93.810% (26417/28160)
01/15/2023 14:58:10 - INFO - __main__ -   test: [batch: 220/391 ] | Loss: 0.886 | Acc: 77.535% (21933/28288)/ 93.817% (26539/28288)
01/15/2023 14:58:12 - INFO - __main__ -   test: [batch: 221/391 ] | Loss: 0.888 | Acc: 77.502% (22023/28416)/ 93.796% (26653/28416)
01/15/2023 14:58:14 - INFO - __main__ -   test: [batch: 222/391 ] | Loss: 0.886 | Acc: 77.554% (22137/28544)/ 93.806% (26776/28544)
01/15/2023 14:58:16 - INFO - __main__ -   test: [batch: 223/391 ] | Loss: 0.886 | Acc: 77.550% (22235/28672)/ 93.792% (26892/28672)
01/15/2023 14:58:18 - INFO - __main__ -   test: [batch: 224/391 ] | Loss: 0.886 | Acc: 77.566% (22339/28800)/ 93.788% (27011/28800)
01/15/2023 14:58:20 - INFO - __main__ -   test: [batch: 225/391 ] | Loss: 0.885 | Acc: 77.551% (22434/28928)/ 93.795% (27133/28928)
01/15/2023 14:58:23 - INFO - __main__ -   test: [batch: 226/391 ] | Loss: 0.886 | Acc: 77.519% (22524/29056)/ 93.798% (27254/29056)
01/15/2023 14:58:25 - INFO - __main__ -   test: [batch: 227/391 ] | Loss: 0.887 | Acc: 77.522% (22624/29184)/ 93.795% (27373/29184)
01/15/2023 14:58:27 - INFO - __main__ -   test: [batch: 228/391 ] | Loss: 0.892 | Acc: 77.432% (22697/29312)/ 93.730% (27474/29312)
01/15/2023 14:58:29 - INFO - __main__ -   test: [batch: 229/391 ] | Loss: 0.895 | Acc: 77.361% (22775/29440)/ 93.675% (27578/29440)
01/15/2023 14:58:31 - INFO - __main__ -   test: [batch: 230/391 ] | Loss: 0.899 | Acc: 77.283% (22851/29568)/ 93.635% (27686/29568)
01/15/2023 14:58:33 - INFO - __main__ -   test: [batch: 231/391 ] | Loss: 0.899 | Acc: 77.263% (22944/29696)/ 93.622% (27802/29696)
01/15/2023 14:58:35 - INFO - __main__ -   test: [batch: 232/391 ] | Loss: 0.898 | Acc: 77.297% (23053/29824)/ 93.636% (27926/29824)
01/15/2023 14:58:37 - INFO - __main__ -   test: [batch: 233/391 ] | Loss: 0.900 | Acc: 77.264% (23142/29952)/ 93.613% (28039/29952)
01/15/2023 14:58:39 - INFO - __main__ -   test: [batch: 234/391 ] | Loss: 0.906 | Acc: 77.151% (23207/30080)/ 93.544% (28138/30080)
01/15/2023 14:58:42 - INFO - __main__ -   test: [batch: 235/391 ] | Loss: 0.906 | Acc: 77.148% (23305/30208)/ 93.538% (28256/30208)
01/15/2023 14:58:44 - INFO - __main__ -   test: [batch: 236/391 ] | Loss: 0.905 | Acc: 77.195% (23418/30336)/ 93.549% (28379/30336)
01/15/2023 14:58:46 - INFO - __main__ -   test: [batch: 237/391 ] | Loss: 0.905 | Acc: 77.189% (23515/30464)/ 93.520% (28490/30464)
01/15/2023 14:58:48 - INFO - __main__ -   test: [batch: 238/391 ] | Loss: 0.904 | Acc: 77.226% (23625/30592)/ 93.531% (28613/30592)
01/15/2023 14:58:50 - INFO - __main__ -   test: [batch: 239/391 ] | Loss: 0.903 | Acc: 77.256% (23733/30720)/ 93.535% (28734/30720)
01/15/2023 14:58:53 - INFO - __main__ -   test: [batch: 240/391 ] | Loss: 0.904 | Acc: 77.269% (23836/30848)/ 93.530% (28852/30848)
01/15/2023 14:58:55 - INFO - __main__ -   test: [batch: 241/391 ] | Loss: 0.907 | Acc: 77.189% (23910/30976)/ 93.501% (28963/30976)
01/15/2023 14:58:57 - INFO - __main__ -   test: [batch: 242/391 ] | Loss: 0.909 | Acc: 77.093% (23979/31104)/ 93.483% (29077/31104)
01/15/2023 14:58:59 - INFO - __main__ -   test: [batch: 243/391 ] | Loss: 0.915 | Acc: 76.976% (24041/31232)/ 93.414% (29175/31232)
01/15/2023 14:59:01 - INFO - __main__ -   test: [batch: 244/391 ] | Loss: 0.915 | Acc: 76.977% (24140/31360)/ 93.402% (29291/31360)
01/15/2023 14:59:03 - INFO - __main__ -   test: [batch: 245/391 ] | Loss: 0.914 | Acc: 76.985% (24241/31488)/ 93.394% (29408/31488)
01/15/2023 14:59:05 - INFO - __main__ -   test: [batch: 246/391 ] | Loss: 0.915 | Acc: 76.977% (24337/31616)/ 93.386% (29525/31616)
01/15/2023 14:59:07 - INFO - __main__ -   test: [batch: 247/391 ] | Loss: 0.920 | Acc: 76.890% (24408/31744)/ 93.322% (29624/31744)
01/15/2023 14:59:09 - INFO - __main__ -   test: [batch: 248/391 ] | Loss: 0.921 | Acc: 76.851% (24494/31872)/ 93.320% (29743/31872)
01/15/2023 14:59:11 - INFO - __main__ -   test: [batch: 249/391 ] | Loss: 0.923 | Acc: 76.684% (24539/32000)/ 93.312% (29860/32000)
01/15/2023 14:59:14 - INFO - __main__ -   test: [batch: 250/391 ] | Loss: 0.922 | Acc: 76.712% (24646/32128)/ 93.320% (29982/32128)
01/15/2023 14:59:16 - INFO - __main__ -   test: [batch: 251/391 ] | Loss: 0.924 | Acc: 76.687% (24736/32256)/ 93.291% (30092/32256)
01/15/2023 14:59:18 - INFO - __main__ -   test: [batch: 252/391 ] | Loss: 0.923 | Acc: 76.714% (24843/32384)/ 93.287% (30210/32384)
01/15/2023 14:59:20 - INFO - __main__ -   test: [batch: 253/391 ] | Loss: 0.925 | Acc: 76.679% (24930/32512)/ 93.270% (30324/32512)
01/15/2023 14:59:22 - INFO - __main__ -   test: [batch: 254/391 ] | Loss: 0.929 | Acc: 76.596% (25001/32640)/ 93.223% (30428/32640)
01/15/2023 14:59:24 - INFO - __main__ -   test: [batch: 255/391 ] | Loss: 0.931 | Acc: 76.575% (25092/32768)/ 93.216% (30545/32768)
01/15/2023 14:59:26 - INFO - __main__ -   test: [batch: 256/391 ] | Loss: 0.936 | Acc: 76.447% (25148/32896)/ 93.169% (30649/32896)
01/15/2023 14:59:28 - INFO - __main__ -   test: [batch: 257/391 ] | Loss: 0.936 | Acc: 76.435% (25242/33024)/ 93.166% (30767/33024)
01/15/2023 14:59:30 - INFO - __main__ -   test: [batch: 258/391 ] | Loss: 0.937 | Acc: 76.427% (25337/33152)/ 93.153% (30882/33152)
01/15/2023 14:59:33 - INFO - __main__ -   test: [batch: 259/391 ] | Loss: 0.941 | Acc: 76.310% (25396/33280)/ 93.152% (31001/33280)
01/15/2023 14:59:35 - INFO - __main__ -   test: [batch: 260/391 ] | Loss: 0.941 | Acc: 76.281% (25484/33408)/ 93.157% (31122/33408)
01/15/2023 14:59:37 - INFO - __main__ -   test: [batch: 261/391 ] | Loss: 0.939 | Acc: 76.321% (25595/33536)/ 93.180% (31249/33536)
01/15/2023 14:59:39 - INFO - __main__ -   test: [batch: 262/391 ] | Loss: 0.939 | Acc: 76.343% (25700/33664)/ 93.192% (31372/33664)
01/15/2023 14:59:41 - INFO - __main__ -   test: [batch: 263/391 ] | Loss: 0.941 | Acc: 76.272% (25774/33792)/ 93.170% (31484/33792)
01/15/2023 14:59:43 - INFO - __main__ -   test: [batch: 264/391 ] | Loss: 0.946 | Acc: 76.206% (25849/33920)/ 93.104% (31581/33920)
01/15/2023 14:59:45 - INFO - __main__ -   test: [batch: 265/391 ] | Loss: 0.945 | Acc: 76.239% (25958/34048)/ 93.101% (31699/34048)
01/15/2023 14:59:47 - INFO - __main__ -   test: [batch: 266/391 ] | Loss: 0.948 | Acc: 76.167% (26031/34176)/ 93.086% (31813/34176)
01/15/2023 14:59:49 - INFO - __main__ -   test: [batch: 267/391 ] | Loss: 0.947 | Acc: 76.201% (26140/34304)/ 93.085% (31932/34304)
01/15/2023 14:59:52 - INFO - __main__ -   test: [batch: 268/391 ] | Loss: 0.947 | Acc: 76.214% (26242/34432)/ 93.085% (32051/34432)
01/15/2023 14:59:54 - INFO - __main__ -   test: [batch: 269/391 ] | Loss: 0.948 | Acc: 76.163% (26322/34560)/ 93.067% (32164/34560)
01/15/2023 14:59:56 - INFO - __main__ -   test: [batch: 270/391 ] | Loss: 0.951 | Acc: 76.116% (26403/34688)/ 93.044% (32275/34688)
01/15/2023 14:59:58 - INFO - __main__ -   test: [batch: 271/391 ] | Loss: 0.951 | Acc: 76.126% (26504/34816)/ 93.041% (32393/34816)
01/15/2023 15:00:00 - INFO - __main__ -   test: [batch: 272/391 ] | Loss: 0.952 | Acc: 76.099% (26592/34944)/ 93.040% (32512/34944)
01/15/2023 15:00:02 - INFO - __main__ -   test: [batch: 273/391 ] | Loss: 0.953 | Acc: 76.112% (26694/35072)/ 93.017% (32623/35072)
01/15/2023 15:00:04 - INFO - __main__ -   test: [batch: 274/391 ] | Loss: 0.952 | Acc: 76.111% (26791/35200)/ 93.020% (32743/35200)
01/15/2023 15:00:06 - INFO - __main__ -   test: [batch: 275/391 ] | Loss: 0.954 | Acc: 76.073% (26875/35328)/ 93.008% (32858/35328)
01/15/2023 15:00:08 - INFO - __main__ -   test: [batch: 276/391 ] | Loss: 0.955 | Acc: 76.032% (26958/35456)/ 93.000% (32974/35456)
01/15/2023 15:00:11 - INFO - __main__ -   test: [batch: 277/391 ] | Loss: 0.956 | Acc: 76.003% (27045/35584)/ 92.986% (33088/35584)
01/15/2023 15:00:13 - INFO - __main__ -   test: [batch: 278/391 ] | Loss: 0.956 | Acc: 76.014% (27146/35712)/ 92.986% (33207/35712)
01/15/2023 15:00:15 - INFO - __main__ -   test: [batch: 279/391 ] | Loss: 0.956 | Acc: 76.038% (27252/35840)/ 92.980% (33324/35840)
01/15/2023 15:00:17 - INFO - __main__ -   test: [batch: 280/391 ] | Loss: 0.957 | Acc: 76.004% (27337/35968)/ 92.974% (33441/35968)
01/15/2023 15:00:19 - INFO - __main__ -   test: [batch: 281/391 ] | Loss: 0.957 | Acc: 76.003% (27434/36096)/ 92.966% (33557/36096)
01/15/2023 15:00:21 - INFO - __main__ -   test: [batch: 282/391 ] | Loss: 0.956 | Acc: 76.041% (27545/36224)/ 92.966% (33676/36224)
01/15/2023 15:00:24 - INFO - __main__ -   test: [batch: 283/391 ] | Loss: 0.957 | Acc: 76.021% (27635/36352)/ 92.960% (33793/36352)
01/15/2023 15:00:26 - INFO - __main__ -   test: [batch: 284/391 ] | Loss: 0.960 | Acc: 75.973% (27715/36480)/ 92.911% (33894/36480)
01/15/2023 15:00:28 - INFO - __main__ -   test: [batch: 285/391 ] | Loss: 0.963 | Acc: 75.934% (27798/36608)/ 92.859% (33994/36608)
01/15/2023 15:00:30 - INFO - __main__ -   test: [batch: 286/391 ] | Loss: 0.964 | Acc: 75.901% (27883/36736)/ 92.844% (34107/36736)
01/15/2023 15:00:32 - INFO - __main__ -   test: [batch: 287/391 ] | Loss: 0.965 | Acc: 75.895% (27978/36864)/ 92.830% (34221/36864)
01/15/2023 15:00:34 - INFO - __main__ -   test: [batch: 288/391 ] | Loss: 0.964 | Acc: 75.914% (28082/36992)/ 92.834% (34341/36992)
01/15/2023 15:00:36 - INFO - __main__ -   test: [batch: 289/391 ] | Loss: 0.966 | Acc: 75.881% (28167/37120)/ 92.804% (34449/37120)
01/15/2023 15:00:39 - INFO - __main__ -   test: [batch: 290/391 ] | Loss: 0.967 | Acc: 75.808% (28237/37248)/ 92.805% (34568/37248)
01/15/2023 15:00:41 - INFO - __main__ -   test: [batch: 291/391 ] | Loss: 0.967 | Acc: 75.813% (28336/37376)/ 92.803% (34686/37376)
01/15/2023 15:00:43 - INFO - __main__ -   test: [batch: 292/391 ] | Loss: 0.969 | Acc: 75.752% (28410/37504)/ 92.787% (34799/37504)
01/15/2023 15:00:45 - INFO - __main__ -   test: [batch: 293/391 ] | Loss: 0.970 | Acc: 75.728% (28498/37632)/ 92.785% (34917/37632)
01/15/2023 15:00:47 - INFO - __main__ -   test: [batch: 294/391 ] | Loss: 0.971 | Acc: 75.704% (28586/37760)/ 92.767% (35029/37760)
01/15/2023 15:00:49 - INFO - __main__ -   test: [batch: 295/391 ] | Loss: 0.970 | Acc: 75.728% (28692/37888)/ 92.766% (35147/37888)
01/15/2023 15:00:51 - INFO - __main__ -   test: [batch: 296/391 ] | Loss: 0.971 | Acc: 75.721% (28786/38016)/ 92.761% (35264/38016)
01/15/2023 15:00:54 - INFO - __main__ -   test: [batch: 297/391 ] | Loss: 0.973 | Acc: 75.695% (28873/38144)/ 92.733% (35372/38144)
01/15/2023 15:00:56 - INFO - __main__ -   test: [batch: 298/391 ] | Loss: 0.975 | Acc: 75.648% (28952/38272)/ 92.707% (35481/38272)
01/15/2023 15:00:58 - INFO - __main__ -   test: [batch: 299/391 ] | Loss: 0.976 | Acc: 75.635% (29044/38400)/ 92.680% (35589/38400)
01/15/2023 15:01:00 - INFO - __main__ -   test: [batch: 300/391 ] | Loss: 0.976 | Acc: 75.651% (29147/38528)/ 92.678% (35707/38528)
01/15/2023 15:01:02 - INFO - __main__ -   test: [batch: 301/391 ] | Loss: 0.977 | Acc: 75.644% (29241/38656)/ 92.666% (35821/38656)
01/15/2023 15:01:04 - INFO - __main__ -   test: [batch: 302/391 ] | Loss: 0.979 | Acc: 75.606% (29323/38784)/ 92.641% (35930/38784)
01/15/2023 15:01:06 - INFO - __main__ -   test: [batch: 303/391 ] | Loss: 0.980 | Acc: 75.578% (29409/38912)/ 92.635% (36046/38912)
01/15/2023 15:01:08 - INFO - __main__ -   test: [batch: 304/391 ] | Loss: 0.979 | Acc: 75.592% (29511/39040)/ 92.636% (36165/39040)
01/15/2023 15:01:10 - INFO - __main__ -   test: [batch: 305/391 ] | Loss: 0.980 | Acc: 75.551% (29592/39168)/ 92.627% (36280/39168)
01/15/2023 15:01:13 - INFO - __main__ -   test: [batch: 306/391 ] | Loss: 0.981 | Acc: 75.542% (29685/39296)/ 92.600% (36388/39296)
01/15/2023 15:01:15 - INFO - __main__ -   test: [batch: 307/391 ] | Loss: 0.982 | Acc: 75.525% (29775/39424)/ 92.586% (36501/39424)
01/15/2023 15:01:17 - INFO - __main__ -   test: [batch: 308/391 ] | Loss: 0.982 | Acc: 75.511% (29866/39552)/ 92.574% (36615/39552)
01/15/2023 15:01:19 - INFO - __main__ -   test: [batch: 309/391 ] | Loss: 0.984 | Acc: 75.502% (29959/39680)/ 92.553% (36725/39680)
01/15/2023 15:01:21 - INFO - __main__ -   test: [batch: 310/391 ] | Loss: 0.984 | Acc: 75.492% (30052/39808)/ 92.534% (36836/39808)
01/15/2023 15:01:23 - INFO - __main__ -   test: [batch: 311/391 ] | Loss: 0.986 | Acc: 75.473% (30141/39936)/ 92.516% (36947/39936)
01/15/2023 15:01:25 - INFO - __main__ -   test: [batch: 312/391 ] | Loss: 0.987 | Acc: 75.462% (30233/40064)/ 92.497% (37058/40064)
01/15/2023 15:01:27 - INFO - __main__ -   test: [batch: 313/391 ] | Loss: 0.985 | Acc: 75.515% (30351/40192)/ 92.521% (37186/40192)
01/15/2023 15:01:29 - INFO - __main__ -   test: [batch: 314/391 ] | Loss: 0.985 | Acc: 75.506% (30444/40320)/ 92.510% (37300/40320)
01/15/2023 15:01:31 - INFO - __main__ -   test: [batch: 315/391 ] | Loss: 0.986 | Acc: 75.494% (30536/40448)/ 92.497% (37413/40448)
01/15/2023 15:01:33 - INFO - __main__ -   test: [batch: 316/391 ] | Loss: 0.989 | Acc: 75.402% (30595/40576)/ 92.471% (37521/40576)
01/15/2023 15:01:36 - INFO - __main__ -   test: [batch: 317/391 ] | Loss: 0.991 | Acc: 75.361% (30675/40704)/ 92.441% (37627/40704)
01/15/2023 15:01:38 - INFO - __main__ -   test: [batch: 318/391 ] | Loss: 0.989 | Acc: 75.392% (30784/40832)/ 92.459% (37753/40832)
01/15/2023 15:01:40 - INFO - __main__ -   test: [batch: 319/391 ] | Loss: 0.992 | Acc: 75.337% (30858/40960)/ 92.429% (37859/40960)
01/15/2023 15:01:42 - INFO - __main__ -   test: [batch: 320/391 ] | Loss: 0.991 | Acc: 75.375% (30970/41088)/ 92.438% (37981/41088)
01/15/2023 15:01:44 - INFO - __main__ -   test: [batch: 321/391 ] | Loss: 0.990 | Acc: 75.395% (31075/41216)/ 92.430% (38096/41216)
01/15/2023 15:01:47 - INFO - __main__ -   test: [batch: 322/391 ] | Loss: 0.992 | Acc: 75.353% (31154/41344)/ 92.417% (38209/41344)
01/15/2023 15:01:49 - INFO - __main__ -   test: [batch: 323/391 ] | Loss: 0.994 | Acc: 75.326% (31239/41472)/ 92.390% (38316/41472)
01/15/2023 15:01:51 - INFO - __main__ -   test: [batch: 324/391 ] | Loss: 0.995 | Acc: 75.327% (31336/41600)/ 92.385% (38432/41600)
01/15/2023 15:01:53 - INFO - __main__ -   test: [batch: 325/391 ] | Loss: 0.994 | Acc: 75.336% (31436/41728)/ 92.389% (38552/41728)
01/15/2023 15:01:55 - INFO - __main__ -   test: [batch: 326/391 ] | Loss: 0.998 | Acc: 75.253% (31498/41856)/ 92.338% (38649/41856)
01/15/2023 15:01:57 - INFO - __main__ -   test: [batch: 327/391 ] | Loss: 1.001 | Acc: 75.164% (31557/41984)/ 92.299% (38751/41984)
01/15/2023 15:01:59 - INFO - __main__ -   test: [batch: 328/391 ] | Loss: 1.003 | Acc: 75.116% (31633/42112)/ 92.263% (38854/42112)
01/15/2023 15:02:02 - INFO - __main__ -   test: [batch: 329/391 ] | Loss: 1.003 | Acc: 75.121% (31731/42240)/ 92.270% (38975/42240)
01/15/2023 15:02:04 - INFO - __main__ -   test: [batch: 330/391 ] | Loss: 1.005 | Acc: 75.085% (31812/42368)/ 92.244% (39082/42368)
01/15/2023 15:02:06 - INFO - __main__ -   test: [batch: 331/391 ] | Loss: 1.005 | Acc: 75.054% (31895/42496)/ 92.260% (39207/42496)
01/15/2023 15:02:08 - INFO - __main__ -   test: [batch: 332/391 ] | Loss: 1.006 | Acc: 75.023% (31978/42624)/ 92.258% (39324/42624)
01/15/2023 15:02:10 - INFO - __main__ -   test: [batch: 333/391 ] | Loss: 1.005 | Acc: 75.056% (32088/42752)/ 92.265% (39445/42752)
01/15/2023 15:02:12 - INFO - __main__ -   test: [batch: 334/391 ] | Loss: 1.006 | Acc: 75.023% (32170/42880)/ 92.255% (39559/42880)
01/15/2023 15:02:14 - INFO - __main__ -   test: [batch: 335/391 ] | Loss: 1.007 | Acc: 74.998% (32255/43008)/ 92.234% (39668/43008)
01/15/2023 15:02:16 - INFO - __main__ -   test: [batch: 336/391 ] | Loss: 1.009 | Acc: 74.956% (32333/43136)/ 92.215% (39778/43136)
01/15/2023 15:02:18 - INFO - __main__ -   test: [batch: 337/391 ] | Loss: 1.009 | Acc: 74.947% (32425/43264)/ 92.206% (39892/43264)
01/15/2023 15:02:21 - INFO - __main__ -   test: [batch: 338/391 ] | Loss: 1.009 | Acc: 74.938% (32517/43392)/ 92.213% (40013/43392)
01/15/2023 15:02:23 - INFO - __main__ -   test: [batch: 339/391 ] | Loss: 1.012 | Acc: 74.903% (32598/43520)/ 92.183% (40118/43520)
01/15/2023 15:02:25 - INFO - __main__ -   test: [batch: 340/391 ] | Loss: 1.011 | Acc: 74.906% (32695/43648)/ 92.194% (40241/43648)
01/15/2023 15:02:27 - INFO - __main__ -   test: [batch: 341/391 ] | Loss: 1.010 | Acc: 74.945% (32808/43776)/ 92.213% (40367/43776)
01/15/2023 15:02:29 - INFO - __main__ -   test: [batch: 342/391 ] | Loss: 1.010 | Acc: 74.888% (32879/43904)/ 92.199% (40479/43904)
01/15/2023 15:02:31 - INFO - __main__ -   test: [batch: 343/391 ] | Loss: 1.010 | Acc: 74.891% (32976/44032)/ 92.201% (40598/44032)
01/15/2023 15:02:33 - INFO - __main__ -   test: [batch: 344/391 ] | Loss: 1.011 | Acc: 74.889% (33071/44160)/ 92.192% (40712/44160)
01/15/2023 15:02:35 - INFO - __main__ -   test: [batch: 345/391 ] | Loss: 1.014 | Acc: 74.815% (33134/44288)/ 92.156% (40814/44288)
01/15/2023 15:02:38 - INFO - __main__ -   test: [batch: 346/391 ] | Loss: 1.016 | Acc: 74.797% (33222/44416)/ 92.147% (40928/44416)
01/15/2023 15:02:40 - INFO - __main__ -   test: [batch: 347/391 ] | Loss: 1.015 | Acc: 74.807% (33322/44544)/ 92.149% (41047/44544)
01/15/2023 15:02:42 - INFO - __main__ -   test: [batch: 348/391 ] | Loss: 1.017 | Acc: 74.785% (33408/44672)/ 92.134% (41158/44672)
01/15/2023 15:02:44 - INFO - __main__ -   test: [batch: 349/391 ] | Loss: 1.016 | Acc: 74.783% (33503/44800)/ 92.143% (41280/44800)
01/15/2023 15:02:46 - INFO - __main__ -   test: [batch: 350/391 ] | Loss: 1.016 | Acc: 74.791% (33602/44928)/ 92.139% (41396/44928)
01/15/2023 15:02:48 - INFO - __main__ -   test: [batch: 351/391 ] | Loss: 1.018 | Acc: 74.736% (33673/45056)/ 92.123% (41507/45056)
01/15/2023 15:02:50 - INFO - __main__ -   test: [batch: 352/391 ] | Loss: 1.019 | Acc: 74.730% (33766/45184)/ 92.117% (41622/45184)
01/15/2023 15:02:52 - INFO - __main__ -   test: [batch: 353/391 ] | Loss: 1.022 | Acc: 74.680% (33839/45312)/ 92.077% (41722/45312)
01/15/2023 15:02:54 - INFO - __main__ -   test: [batch: 354/391 ] | Loss: 1.024 | Acc: 74.615% (33905/45440)/ 92.060% (41832/45440)
01/15/2023 15:02:56 - INFO - __main__ -   test: [batch: 355/391 ] | Loss: 1.027 | Acc: 74.552% (33972/45568)/ 92.049% (41945/45568)
01/15/2023 15:02:58 - INFO - __main__ -   test: [batch: 356/391 ] | Loss: 1.027 | Acc: 74.549% (34066/45696)/ 92.056% (42066/45696)
01/15/2023 15:03:01 - INFO - __main__ -   test: [batch: 357/391 ] | Loss: 1.025 | Acc: 74.585% (34178/45824)/ 92.070% (42190/45824)
01/15/2023 15:03:03 - INFO - __main__ -   test: [batch: 358/391 ] | Loss: 1.025 | Acc: 74.613% (34286/45952)/ 92.074% (42310/45952)
01/15/2023 15:03:05 - INFO - __main__ -   test: [batch: 359/391 ] | Loss: 1.025 | Acc: 74.616% (34383/46080)/ 92.066% (42424/46080)
01/15/2023 15:03:07 - INFO - __main__ -   test: [batch: 360/391 ] | Loss: 1.027 | Acc: 74.578% (34461/46208)/ 92.064% (42541/46208)
01/15/2023 15:03:09 - INFO - __main__ -   test: [batch: 361/391 ] | Loss: 1.027 | Acc: 74.575% (34555/46336)/ 92.067% (42660/46336)
01/15/2023 15:03:11 - INFO - __main__ -   test: [batch: 362/391 ] | Loss: 1.026 | Acc: 74.572% (34649/46464)/ 92.078% (42783/46464)
01/15/2023 15:03:13 - INFO - __main__ -   test: [batch: 363/391 ] | Loss: 1.027 | Acc: 74.556% (34737/46592)/ 92.067% (42896/46592)
01/15/2023 15:03:16 - INFO - __main__ -   test: [batch: 364/391 ] | Loss: 1.026 | Acc: 74.580% (34844/46720)/ 92.080% (43020/46720)
01/15/2023 15:03:18 - INFO - __main__ -   test: [batch: 365/391 ] | Loss: 1.026 | Acc: 74.586% (34942/46848)/ 92.081% (43138/46848)
01/15/2023 15:03:20 - INFO - __main__ -   test: [batch: 366/391 ] | Loss: 1.024 | Acc: 74.621% (35054/46976)/ 92.098% (43264/46976)
01/15/2023 15:03:22 - INFO - __main__ -   test: [batch: 367/391 ] | Loss: 1.023 | Acc: 74.633% (35155/47104)/ 92.107% (43386/47104)
01/15/2023 15:03:24 - INFO - __main__ -   test: [batch: 368/391 ] | Loss: 1.023 | Acc: 74.625% (35247/47232)/ 92.113% (43507/47232)
01/15/2023 15:03:26 - INFO - __main__ -   test: [batch: 369/391 ] | Loss: 1.022 | Acc: 74.643% (35351/47360)/ 92.124% (43630/47360)
01/15/2023 15:03:28 - INFO - __main__ -   test: [batch: 370/391 ] | Loss: 1.023 | Acc: 74.617% (35434/47488)/ 92.129% (43750/47488)
01/15/2023 15:03:30 - INFO - __main__ -   test: [batch: 371/391 ] | Loss: 1.022 | Acc: 74.626% (35534/47616)/ 92.127% (43867/47616)
01/15/2023 15:03:33 - INFO - __main__ -   test: [batch: 372/391 ] | Loss: 1.021 | Acc: 74.671% (35651/47744)/ 92.146% (43994/47744)
01/15/2023 15:03:35 - INFO - __main__ -   test: [batch: 373/391 ] | Loss: 1.019 | Acc: 74.705% (35763/47872)/ 92.158% (44118/47872)
01/15/2023 15:03:37 - INFO - __main__ -   test: [batch: 374/391 ] | Loss: 1.019 | Acc: 74.731% (35871/48000)/ 92.158% (44236/48000)
01/15/2023 15:03:39 - INFO - __main__ -   test: [batch: 375/391 ] | Loss: 1.021 | Acc: 74.661% (35933/48128)/ 92.129% (44340/48128)
01/15/2023 15:03:41 - INFO - __main__ -   test: [batch: 376/391 ] | Loss: 1.022 | Acc: 74.650% (36023/48256)/ 92.115% (44451/48256)
01/15/2023 15:03:43 - INFO - __main__ -   test: [batch: 377/391 ] | Loss: 1.023 | Acc: 74.645% (36116/48384)/ 92.105% (44564/48384)
01/15/2023 15:03:45 - INFO - __main__ -   test: [batch: 378/391 ] | Loss: 1.026 | Acc: 74.573% (36177/48512)/ 92.062% (44661/48512)
01/15/2023 15:03:47 - INFO - __main__ -   test: [batch: 379/391 ] | Loss: 1.026 | Acc: 74.562% (36267/48640)/ 92.076% (44786/48640)
01/15/2023 15:03:50 - INFO - __main__ -   test: [batch: 380/391 ] | Loss: 1.026 | Acc: 74.569% (36366/48768)/ 92.091% (44911/48768)
01/15/2023 15:03:52 - INFO - __main__ -   test: [batch: 381/391 ] | Loss: 1.027 | Acc: 74.519% (36437/48896)/ 92.085% (45026/48896)
01/15/2023 15:03:54 - INFO - __main__ -   test: [batch: 382/391 ] | Loss: 1.029 | Acc: 74.486% (36516/49024)/ 92.073% (45138/49024)
01/15/2023 15:03:56 - INFO - __main__ -   test: [batch: 383/391 ] | Loss: 1.029 | Acc: 74.489% (36613/49152)/ 92.067% (45253/49152)
01/15/2023 15:03:58 - INFO - __main__ -   test: [batch: 384/391 ] | Loss: 1.027 | Acc: 74.529% (36728/49280)/ 92.082% (45378/49280)
01/15/2023 15:04:00 - INFO - __main__ -   test: [batch: 385/391 ] | Loss: 1.026 | Acc: 74.543% (36830/49408)/ 92.094% (45502/49408)
01/15/2023 15:04:02 - INFO - __main__ -   test: [batch: 386/391 ] | Loss: 1.024 | Acc: 74.580% (36944/49536)/ 92.111% (45628/49536)
01/15/2023 15:04:04 - INFO - __main__ -   test: [batch: 387/391 ] | Loss: 1.022 | Acc: 74.623% (37061/49664)/ 92.121% (45751/49664)
01/15/2023 15:04:07 - INFO - __main__ -   test: [batch: 388/391 ] | Loss: 1.020 | Acc: 74.673% (37181/49792)/ 92.135% (45876/49792)
01/15/2023 15:04:09 - INFO - __main__ -   test: [batch: 389/391 ] | Loss: 1.020 | Acc: 74.675% (37278/49920)/ 92.145% (45999/49920)
01/15/2023 15:04:11 - INFO - __main__ -   test: [batch: 390/391 ] | Loss: 1.022 | Acc: 74.628% (37314/50000)/ 92.134% (46067/50000)
01/15/2023 15:04:11 - INFO - __main__ -   Final accuracy: 74.628
01/15/2023 15:04:11 - INFO - __main__ -   {'milestones': Counter({2: 1, 3: 1, 4: 1}), 'gamma': 0.1, 'base_lrs': [0.00025], 'last_epoch': 2, '_step_count': 3, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [2.5e-05]}
01/15/2023 15:04:11 - INFO - __main__ -   
Epoch: 2
01/15/2023 15:04:13 - INFO - __main__ -   test: [epoch: 2 | batch: 0/10010 ] | Loss: 1.089 | Acc: 76.562% (98/128)
01/15/2023 15:07:44 - INFO - __main__ -   test: [epoch: 2 | batch: 100/10010 ] | Loss: 0.915 | Acc: 77.800% (10058/12928)
01/15/2023 15:11:14 - INFO - __main__ -   test: [epoch: 2 | batch: 200/10010 ] | Loss: 0.919 | Acc: 77.208% (19864/25728)
01/15/2023 15:14:46 - INFO - __main__ -   test: [epoch: 2 | batch: 300/10010 ] | Loss: 0.924 | Acc: 77.209% (29747/38528)
01/15/2023 15:18:16 - INFO - __main__ -   test: [epoch: 2 | batch: 400/10010 ] | Loss: 0.930 | Acc: 77.065% (39556/51328)
01/15/2023 15:21:48 - INFO - __main__ -   test: [epoch: 2 | batch: 500/10010 ] | Loss: 0.936 | Acc: 76.949% (49346/64128)
01/15/2023 15:25:21 - INFO - __main__ -   test: [epoch: 2 | batch: 600/10010 ] | Loss: 0.938 | Acc: 76.893% (59152/76928)
01/15/2023 15:28:51 - INFO - __main__ -   test: [epoch: 2 | batch: 700/10010 ] | Loss: 0.939 | Acc: 76.837% (68944/89728)
01/15/2023 15:32:22 - INFO - __main__ -   test: [epoch: 2 | batch: 800/10010 ] | Loss: 0.941 | Acc: 76.781% (78722/102528)
01/15/2023 15:35:55 - INFO - __main__ -   test: [epoch: 2 | batch: 900/10010 ] | Loss: 0.939 | Acc: 76.820% (88595/115328)
01/15/2023 15:39:25 - INFO - __main__ -   test: [epoch: 2 | batch: 1000/10010 ] | Loss: 0.940 | Acc: 76.801% (98404/128128)
01/15/2023 15:42:58 - INFO - __main__ -   test: [epoch: 2 | batch: 1100/10010 ] | Loss: 0.942 | Acc: 76.768% (108187/140928)
01/15/2023 15:46:28 - INFO - __main__ -   test: [epoch: 2 | batch: 1200/10010 ] | Loss: 0.943 | Acc: 76.743% (117975/153728)
01/15/2023 15:49:58 - INFO - __main__ -   test: [epoch: 2 | batch: 1300/10010 ] | Loss: 0.942 | Acc: 76.752% (127814/166528)
01/15/2023 15:53:29 - INFO - __main__ -   test: [epoch: 2 | batch: 1400/10010 ] | Loss: 0.941 | Acc: 76.767% (137664/179328)
01/15/2023 15:57:01 - INFO - __main__ -   test: [epoch: 2 | batch: 1500/10010 ] | Loss: 0.942 | Acc: 76.741% (147441/192128)
01/15/2023 16:00:33 - INFO - __main__ -   test: [epoch: 2 | batch: 1600/10010 ] | Loss: 0.943 | Acc: 76.711% (157202/204928)
01/15/2023 16:04:04 - INFO - __main__ -   test: [epoch: 2 | batch: 1700/10010 ] | Loss: 0.943 | Acc: 76.717% (167034/217728)
01/15/2023 16:07:37 - INFO - __main__ -   test: [epoch: 2 | batch: 1800/10010 ] | Loss: 0.942 | Acc: 76.731% (176887/230528)
01/15/2023 16:11:07 - INFO - __main__ -   test: [epoch: 2 | batch: 1900/10010 ] | Loss: 0.941 | Acc: 76.754% (186763/243328)
01/15/2023 16:14:37 - INFO - __main__ -   test: [epoch: 2 | batch: 2000/10010 ] | Loss: 0.940 | Acc: 76.773% (196636/256128)
01/15/2023 16:18:08 - INFO - __main__ -   test: [epoch: 2 | batch: 2100/10010 ] | Loss: 0.939 | Acc: 76.770% (206455/268928)
01/15/2023 16:21:40 - INFO - __main__ -   test: [epoch: 2 | batch: 2200/10010 ] | Loss: 0.941 | Acc: 76.753% (216235/281728)
01/15/2023 16:25:11 - INFO - __main__ -   test: [epoch: 2 | batch: 2300/10010 ] | Loss: 0.942 | Acc: 76.720% (225961/294528)
01/15/2023 16:28:44 - INFO - __main__ -   test: [epoch: 2 | batch: 2400/10010 ] | Loss: 0.942 | Acc: 76.721% (235784/307328)
01/15/2023 16:32:14 - INFO - __main__ -   test: [epoch: 2 | batch: 2500/10010 ] | Loss: 0.942 | Acc: 76.721% (245604/320128)
01/15/2023 16:35:45 - INFO - __main__ -   test: [epoch: 2 | batch: 2600/10010 ] | Loss: 0.942 | Acc: 76.725% (255440/332928)
01/15/2023 16:39:16 - INFO - __main__ -   test: [epoch: 2 | batch: 2700/10010 ] | Loss: 0.941 | Acc: 76.751% (265350/345728)
01/15/2023 16:42:48 - INFO - __main__ -   test: [epoch: 2 | batch: 2800/10010 ] | Loss: 0.941 | Acc: 76.747% (275158/358528)
01/15/2023 16:46:20 - INFO - __main__ -   test: [epoch: 2 | batch: 2900/10010 ] | Loss: 0.941 | Acc: 76.744% (284973/371328)
01/15/2023 16:49:54 - INFO - __main__ -   test: [epoch: 2 | batch: 3000/10010 ] | Loss: 0.941 | Acc: 76.742% (294787/384128)
01/15/2023 16:53:27 - INFO - __main__ -   test: [epoch: 2 | batch: 3100/10010 ] | Loss: 0.941 | Acc: 76.761% (304686/396928)
01/15/2023 16:56:58 - INFO - __main__ -   test: [epoch: 2 | batch: 3200/10010 ] | Loss: 0.941 | Acc: 76.756% (314492/409728)
01/15/2023 17:00:30 - INFO - __main__ -   test: [epoch: 2 | batch: 3300/10010 ] | Loss: 0.941 | Acc: 76.742% (324257/422528)
01/15/2023 17:04:02 - INFO - __main__ -   test: [epoch: 2 | batch: 3400/10010 ] | Loss: 0.941 | Acc: 76.752% (334121/435328)
01/15/2023 17:07:31 - INFO - __main__ -   test: [epoch: 2 | batch: 3500/10010 ] | Loss: 0.941 | Acc: 76.758% (343972/448128)
01/15/2023 17:11:02 - INFO - __main__ -   test: [epoch: 2 | batch: 3600/10010 ] | Loss: 0.940 | Acc: 76.771% (353858/460928)
01/15/2023 17:14:34 - INFO - __main__ -   test: [epoch: 2 | batch: 3700/10010 ] | Loss: 0.941 | Acc: 76.760% (363634/473728)
01/15/2023 17:18:06 - INFO - __main__ -   test: [epoch: 2 | batch: 3800/10010 ] | Loss: 0.940 | Acc: 76.787% (373588/486528)
01/15/2023 17:21:39 - INFO - __main__ -   test: [epoch: 2 | batch: 3900/10010 ] | Loss: 0.940 | Acc: 76.781% (383391/499328)
01/15/2023 17:25:10 - INFO - __main__ -   test: [epoch: 2 | batch: 4000/10010 ] | Loss: 0.941 | Acc: 76.764% (393130/512128)
01/15/2023 17:28:41 - INFO - __main__ -   test: [epoch: 2 | batch: 4100/10010 ] | Loss: 0.941 | Acc: 76.767% (402970/524928)
01/15/2023 17:32:14 - INFO - __main__ -   test: [epoch: 2 | batch: 4200/10010 ] | Loss: 0.940 | Acc: 76.768% (412804/537728)
01/15/2023 17:35:46 - INFO - __main__ -   test: [epoch: 2 | batch: 4300/10010 ] | Loss: 0.940 | Acc: 76.772% (422650/550528)
01/15/2023 17:39:17 - INFO - __main__ -   test: [epoch: 2 | batch: 4400/10010 ] | Loss: 0.940 | Acc: 76.783% (432541/563328)
01/15/2023 17:42:48 - INFO - __main__ -   test: [epoch: 2 | batch: 4500/10010 ] | Loss: 0.940 | Acc: 76.779% (442344/576128)
01/15/2023 17:46:19 - INFO - __main__ -   test: [epoch: 2 | batch: 4600/10010 ] | Loss: 0.941 | Acc: 76.771% (452124/588928)
01/15/2023 17:49:50 - INFO - __main__ -   test: [epoch: 2 | batch: 4700/10010 ] | Loss: 0.941 | Acc: 76.767% (461931/601728)
01/15/2023 17:53:21 - INFO - __main__ -   test: [epoch: 2 | batch: 4800/10010 ] | Loss: 0.940 | Acc: 76.772% (471785/614528)
01/15/2023 17:56:52 - INFO - __main__ -   test: [epoch: 2 | batch: 4900/10010 ] | Loss: 0.941 | Acc: 76.774% (481622/627328)
01/15/2023 18:00:24 - INFO - __main__ -   test: [epoch: 2 | batch: 5000/10010 ] | Loss: 0.940 | Acc: 76.783% (491509/640128)
01/15/2023 18:03:57 - INFO - __main__ -   test: [epoch: 2 | batch: 5100/10010 ] | Loss: 0.941 | Acc: 76.770% (501250/652928)
01/15/2023 18:07:29 - INFO - __main__ -   test: [epoch: 2 | batch: 5200/10010 ] | Loss: 0.941 | Acc: 76.762% (511024/665728)
01/15/2023 18:10:58 - INFO - __main__ -   test: [epoch: 2 | batch: 5300/10010 ] | Loss: 0.941 | Acc: 76.757% (520817/678528)
01/15/2023 18:14:29 - INFO - __main__ -   test: [epoch: 2 | batch: 5400/10010 ] | Loss: 0.941 | Acc: 76.753% (530612/691328)
01/15/2023 18:18:00 - INFO - __main__ -   test: [epoch: 2 | batch: 5500/10010 ] | Loss: 0.941 | Acc: 76.756% (540458/704128)
01/15/2023 18:21:32 - INFO - __main__ -   test: [epoch: 2 | batch: 5600/10010 ] | Loss: 0.941 | Acc: 76.762% (550326/716928)
01/15/2023 18:25:03 - INFO - __main__ -   test: [epoch: 2 | batch: 5700/10010 ] | Loss: 0.941 | Acc: 76.761% (560147/729728)
01/15/2023 18:28:34 - INFO - __main__ -   test: [epoch: 2 | batch: 5800/10010 ] | Loss: 0.941 | Acc: 76.762% (569981/742528)
01/15/2023 18:32:04 - INFO - __main__ -   test: [epoch: 2 | batch: 5900/10010 ] | Loss: 0.941 | Acc: 76.770% (579868/755328)
01/15/2023 18:35:36 - INFO - __main__ -   test: [epoch: 2 | batch: 6000/10010 ] | Loss: 0.941 | Acc: 76.769% (589683/768128)
01/15/2023 18:39:07 - INFO - __main__ -   test: [epoch: 2 | batch: 6100/10010 ] | Loss: 0.941 | Acc: 76.759% (599435/780928)
01/15/2023 18:42:37 - INFO - __main__ -   test: [epoch: 2 | batch: 6200/10010 ] | Loss: 0.941 | Acc: 76.762% (609278/793728)
01/15/2023 18:46:08 - INFO - __main__ -   test: [epoch: 2 | batch: 6300/10010 ] | Loss: 0.941 | Acc: 76.762% (619104/806528)
01/15/2023 18:49:41 - INFO - __main__ -   test: [epoch: 2 | batch: 6400/10010 ] | Loss: 0.942 | Acc: 76.753% (628862/819328)
01/15/2023 18:53:13 - INFO - __main__ -   test: [epoch: 2 | batch: 6500/10010 ] | Loss: 0.941 | Acc: 76.762% (638760/832128)
01/15/2023 18:56:45 - INFO - __main__ -   test: [epoch: 2 | batch: 6600/10010 ] | Loss: 0.941 | Acc: 76.758% (648551/844928)
01/15/2023 19:00:17 - INFO - __main__ -   test: [epoch: 2 | batch: 6700/10010 ] | Loss: 0.942 | Acc: 76.752% (658322/857728)
01/15/2023 19:03:49 - INFO - __main__ -   test: [epoch: 2 | batch: 6800/10010 ] | Loss: 0.941 | Acc: 76.750% (668127/870528)
01/15/2023 19:07:21 - INFO - __main__ -   test: [epoch: 2 | batch: 6900/10010 ] | Loss: 0.941 | Acc: 76.750% (677957/883328)
01/15/2023 19:10:52 - INFO - __main__ -   test: [epoch: 2 | batch: 7000/10010 ] | Loss: 0.942 | Acc: 76.742% (687711/896128)
01/15/2023 19:14:23 - INFO - __main__ -   test: [epoch: 2 | batch: 7100/10010 ] | Loss: 0.941 | Acc: 76.750% (697600/908928)
01/15/2023 19:17:55 - INFO - __main__ -   test: [epoch: 2 | batch: 7200/10010 ] | Loss: 0.941 | Acc: 76.752% (707445/921728)
01/15/2023 19:21:28 - INFO - __main__ -   test: [epoch: 2 | batch: 7300/10010 ] | Loss: 0.941 | Acc: 76.750% (717253/934528)
01/15/2023 19:24:59 - INFO - __main__ -   test: [epoch: 2 | batch: 7400/10010 ] | Loss: 0.941 | Acc: 76.753% (727102/947328)
01/15/2023 19:28:31 - INFO - __main__ -   test: [epoch: 2 | batch: 7500/10010 ] | Loss: 0.941 | Acc: 76.754% (736941/960128)
01/15/2023 19:32:02 - INFO - __main__ -   test: [epoch: 2 | batch: 7600/10010 ] | Loss: 0.941 | Acc: 76.760% (746824/972928)
01/15/2023 19:35:33 - INFO - __main__ -   test: [epoch: 2 | batch: 7700/10010 ] | Loss: 0.941 | Acc: 76.756% (756609/985728)
01/15/2023 19:39:04 - INFO - __main__ -   test: [epoch: 2 | batch: 7800/10010 ] | Loss: 0.941 | Acc: 76.750% (766366/998528)
01/15/2023 19:42:37 - INFO - __main__ -   test: [epoch: 2 | batch: 7900/10010 ] | Loss: 0.941 | Acc: 76.753% (776220/1011328)
01/15/2023 19:46:09 - INFO - __main__ -   test: [epoch: 2 | batch: 8000/10010 ] | Loss: 0.941 | Acc: 76.748% (786002/1024128)
01/15/2023 19:49:39 - INFO - __main__ -   test: [epoch: 2 | batch: 8100/10010 ] | Loss: 0.941 | Acc: 76.752% (795867/1036928)
01/15/2023 19:53:12 - INFO - __main__ -   test: [epoch: 2 | batch: 8200/10010 ] | Loss: 0.941 | Acc: 76.753% (805694/1049728)
01/15/2023 19:56:43 - INFO - __main__ -   test: [epoch: 2 | batch: 8300/10010 ] | Loss: 0.941 | Acc: 76.748% (815474/1062528)
01/15/2023 20:00:14 - INFO - __main__ -   test: [epoch: 2 | batch: 8400/10010 ] | Loss: 0.941 | Acc: 76.749% (825300/1075328)
01/15/2023 20:03:46 - INFO - __main__ -   test: [epoch: 2 | batch: 8500/10010 ] | Loss: 0.941 | Acc: 76.753% (835176/1088128)
01/15/2023 20:07:17 - INFO - __main__ -   test: [epoch: 2 | batch: 8600/10010 ] | Loss: 0.941 | Acc: 76.757% (845037/1100928)
01/15/2023 20:10:48 - INFO - __main__ -   test: [epoch: 2 | batch: 8700/10010 ] | Loss: 0.941 | Acc: 76.758% (854873/1113728)
01/15/2023 20:14:17 - INFO - __main__ -   test: [epoch: 2 | batch: 8800/10010 ] | Loss: 0.941 | Acc: 76.764% (864766/1126528)
01/15/2023 20:17:48 - INFO - __main__ -   test: [epoch: 2 | batch: 8900/10010 ] | Loss: 0.940 | Acc: 76.765% (874608/1139328)
01/15/2023 20:21:18 - INFO - __main__ -   test: [epoch: 2 | batch: 9000/10010 ] | Loss: 0.941 | Acc: 76.759% (884365/1152128)
01/15/2023 20:24:50 - INFO - __main__ -   test: [epoch: 2 | batch: 9100/10010 ] | Loss: 0.941 | Acc: 76.757% (894158/1164928)
01/15/2023 20:28:22 - INFO - __main__ -   test: [epoch: 2 | batch: 9200/10010 ] | Loss: 0.941 | Acc: 76.762% (904047/1177728)
01/15/2023 20:31:53 - INFO - __main__ -   test: [epoch: 2 | batch: 9300/10010 ] | Loss: 0.941 | Acc: 76.757% (913812/1190528)
01/15/2023 20:35:22 - INFO - __main__ -   test: [epoch: 2 | batch: 9400/10010 ] | Loss: 0.941 | Acc: 76.760% (923679/1203328)
01/15/2023 20:38:55 - INFO - __main__ -   test: [epoch: 2 | batch: 9500/10010 ] | Loss: 0.941 | Acc: 76.756% (933457/1216128)
01/15/2023 20:42:28 - INFO - __main__ -   test: [epoch: 2 | batch: 9600/10010 ] | Loss: 0.941 | Acc: 76.756% (943279/1228928)
01/15/2023 20:45:59 - INFO - __main__ -   test: [epoch: 2 | batch: 9700/10010 ] | Loss: 0.941 | Acc: 76.753% (953069/1241728)
01/15/2023 20:49:31 - INFO - __main__ -   test: [epoch: 2 | batch: 9800/10010 ] | Loss: 0.941 | Acc: 76.750% (962847/1254528)
01/15/2023 20:52:55 - INFO - __main__ -   test: [epoch: 2 | batch: 9900/10010 ] | Loss: 0.941 | Acc: 76.751% (972685/1267328)
01/15/2023 20:55:29 - INFO - __main__ -   test: [epoch: 2 | batch: 10000/10010 ] | Loss: 0.941 | Acc: 76.747% (982460/1280128)
01/15/2023 20:55:43 - INFO - __main__ -   Saving Checkpoint
01/15/2023 20:55:45 - INFO - __main__ -   test: [batch: 0/391 ] | Loss: 0.548 | Acc: 88.281% (113/128)/ 95.312% (122/128)
01/15/2023 20:55:46 - INFO - __main__ -   test: [batch: 1/391 ] | Loss: 0.559 | Acc: 85.938% (220/256)/ 96.484% (247/256)
01/15/2023 20:55:48 - INFO - __main__ -   test: [batch: 2/391 ] | Loss: 0.745 | Acc: 79.948% (307/384)/ 94.792% (364/384)
01/15/2023 20:55:50 - INFO - __main__ -   test: [batch: 3/391 ] | Loss: 0.686 | Acc: 82.422% (422/512)/ 95.117% (487/512)
01/15/2023 20:55:51 - INFO - __main__ -   test: [batch: 4/391 ] | Loss: 0.590 | Acc: 84.844% (543/640)/ 95.938% (614/640)
01/15/2023 20:55:53 - INFO - __main__ -   test: [batch: 5/391 ] | Loss: 0.524 | Acc: 86.068% (661/768)/ 96.615% (742/768)
01/15/2023 20:55:54 - INFO - __main__ -   test: [batch: 6/391 ] | Loss: 0.508 | Acc: 86.719% (777/896)/ 96.540% (865/896)
01/15/2023 20:55:56 - INFO - __main__ -   test: [batch: 7/391 ] | Loss: 0.484 | Acc: 87.793% (899/1024)/ 96.680% (990/1024)
01/15/2023 20:55:57 - INFO - __main__ -   test: [batch: 8/391 ] | Loss: 0.494 | Acc: 87.760% (1011/1152)/ 96.701% (1114/1152)
01/15/2023 20:55:59 - INFO - __main__ -   test: [batch: 9/391 ] | Loss: 0.469 | Acc: 88.438% (1132/1280)/ 96.875% (1240/1280)
01/15/2023 20:56:01 - INFO - __main__ -   test: [batch: 10/391 ] | Loss: 0.512 | Acc: 87.145% (1227/1408)/ 96.875% (1364/1408)
01/15/2023 20:56:02 - INFO - __main__ -   test: [batch: 11/391 ] | Loss: 0.514 | Acc: 87.305% (1341/1536)/ 96.745% (1486/1536)
01/15/2023 20:56:04 - INFO - __main__ -   test: [batch: 12/391 ] | Loss: 0.563 | Acc: 86.118% (1433/1664)/ 96.274% (1602/1664)
01/15/2023 20:56:05 - INFO - __main__ -   test: [batch: 13/391 ] | Loss: 0.619 | Acc: 84.654% (1517/1792)/ 95.480% (1711/1792)
01/15/2023 20:56:07 - INFO - __main__ -   test: [batch: 14/391 ] | Loss: 0.635 | Acc: 83.906% (1611/1920)/ 95.677% (1837/1920)
01/15/2023 20:56:09 - INFO - __main__ -   test: [batch: 15/391 ] | Loss: 0.641 | Acc: 83.252% (1705/2048)/ 95.703% (1960/2048)
01/15/2023 20:56:10 - INFO - __main__ -   test: [batch: 16/391 ] | Loss: 0.646 | Acc: 83.180% (1810/2176)/ 95.634% (2081/2176)
01/15/2023 20:56:12 - INFO - __main__ -   test: [batch: 17/391 ] | Loss: 0.670 | Acc: 82.726% (1906/2304)/ 95.356% (2197/2304)
01/15/2023 20:56:13 - INFO - __main__ -   test: [batch: 18/391 ] | Loss: 0.694 | Acc: 82.237% (2000/2432)/ 95.189% (2315/2432)
01/15/2023 20:56:15 - INFO - __main__ -   test: [batch: 19/391 ] | Loss: 0.702 | Acc: 81.953% (2098/2560)/ 95.000% (2432/2560)
01/15/2023 20:56:17 - INFO - __main__ -   test: [batch: 20/391 ] | Loss: 0.698 | Acc: 82.068% (2206/2688)/ 94.978% (2553/2688)
01/15/2023 20:56:18 - INFO - __main__ -   test: [batch: 21/391 ] | Loss: 0.736 | Acc: 81.072% (2283/2816)/ 94.815% (2670/2816)
01/15/2023 20:56:20 - INFO - __main__ -   test: [batch: 22/391 ] | Loss: 0.735 | Acc: 80.842% (2380/2944)/ 94.803% (2791/2944)
01/15/2023 20:56:21 - INFO - __main__ -   test: [batch: 23/391 ] | Loss: 0.777 | Acc: 79.980% (2457/3072)/ 94.499% (2903/3072)
01/15/2023 20:56:23 - INFO - __main__ -   test: [batch: 24/391 ] | Loss: 0.795 | Acc: 79.500% (2544/3200)/ 94.375% (3020/3200)
01/15/2023 20:56:24 - INFO - __main__ -   test: [batch: 25/391 ] | Loss: 0.822 | Acc: 78.816% (2623/3328)/ 94.081% (3131/3328)
01/15/2023 20:56:26 - INFO - __main__ -   test: [batch: 26/391 ] | Loss: 0.839 | Acc: 78.009% (2696/3456)/ 94.068% (3251/3456)
01/15/2023 20:56:27 - INFO - __main__ -   test: [batch: 27/391 ] | Loss: 0.820 | Acc: 78.516% (2814/3584)/ 94.113% (3373/3584)
01/15/2023 20:56:29 - INFO - __main__ -   test: [batch: 28/391 ] | Loss: 0.827 | Acc: 78.017% (2896/3712)/ 94.235% (3498/3712)
01/15/2023 20:56:31 - INFO - __main__ -   test: [batch: 29/391 ] | Loss: 0.819 | Acc: 78.151% (3001/3840)/ 94.375% (3624/3840)
01/15/2023 20:56:32 - INFO - __main__ -   test: [batch: 30/391 ] | Loss: 0.830 | Acc: 78.049% (3097/3968)/ 94.229% (3739/3968)
01/15/2023 20:56:34 - INFO - __main__ -   test: [batch: 31/391 ] | Loss: 0.824 | Acc: 78.296% (3207/4096)/ 94.312% (3863/4096)
01/15/2023 20:56:35 - INFO - __main__ -   test: [batch: 32/391 ] | Loss: 0.810 | Acc: 78.622% (3321/4224)/ 94.460% (3990/4224)
01/15/2023 20:56:36 - INFO - __main__ -   test: [batch: 33/391 ] | Loss: 0.803 | Acc: 78.814% (3430/4352)/ 94.508% (4113/4352)
01/15/2023 20:56:38 - INFO - __main__ -   test: [batch: 34/391 ] | Loss: 0.789 | Acc: 79.241% (3550/4480)/ 94.576% (4237/4480)
01/15/2023 20:56:39 - INFO - __main__ -   test: [batch: 35/391 ] | Loss: 0.776 | Acc: 79.601% (3668/4608)/ 94.596% (4359/4608)
01/15/2023 20:56:41 - INFO - __main__ -   test: [batch: 36/391 ] | Loss: 0.761 | Acc: 80.068% (3792/4736)/ 94.721% (4486/4736)
01/15/2023 20:56:43 - INFO - __main__ -   test: [batch: 37/391 ] | Loss: 0.751 | Acc: 80.366% (3909/4864)/ 94.799% (4611/4864)
01/15/2023 20:56:44 - INFO - __main__ -   test: [batch: 38/391 ] | Loss: 0.743 | Acc: 80.529% (4020/4992)/ 94.852% (4735/4992)
01/15/2023 20:56:46 - INFO - __main__ -   test: [batch: 39/391 ] | Loss: 0.737 | Acc: 80.684% (4131/5120)/ 94.902% (4859/5120)
01/15/2023 20:56:47 - INFO - __main__ -   test: [batch: 40/391 ] | Loss: 0.735 | Acc: 80.812% (4241/5248)/ 94.817% (4976/5248)
01/15/2023 20:56:49 - INFO - __main__ -   test: [batch: 41/391 ] | Loss: 0.738 | Acc: 80.971% (4353/5376)/ 94.736% (5093/5376)
01/15/2023 20:56:50 - INFO - __main__ -   test: [batch: 42/391 ] | Loss: 0.738 | Acc: 81.014% (4459/5504)/ 94.804% (5218/5504)
01/15/2023 20:56:52 - INFO - __main__ -   test: [batch: 43/391 ] | Loss: 0.737 | Acc: 81.037% (4564/5632)/ 94.780% (5338/5632)
01/15/2023 20:56:53 - INFO - __main__ -   test: [batch: 44/391 ] | Loss: 0.742 | Acc: 81.042% (4668/5760)/ 94.705% (5455/5760)
01/15/2023 20:56:55 - INFO - __main__ -   test: [batch: 45/391 ] | Loss: 0.739 | Acc: 81.182% (4780/5888)/ 94.752% (5579/5888)
01/15/2023 20:56:57 - INFO - __main__ -   test: [batch: 46/391 ] | Loss: 0.742 | Acc: 81.150% (4882/6016)/ 94.797% (5703/6016)
01/15/2023 20:56:58 - INFO - __main__ -   test: [batch: 47/391 ] | Loss: 0.745 | Acc: 80.990% (4976/6144)/ 94.840% (5827/6144)
01/15/2023 20:57:00 - INFO - __main__ -   test: [batch: 48/391 ] | Loss: 0.750 | Acc: 80.851% (5071/6272)/ 94.818% (5947/6272)
01/15/2023 20:57:01 - INFO - __main__ -   test: [batch: 49/391 ] | Loss: 0.755 | Acc: 80.844% (5174/6400)/ 94.766% (6065/6400)
01/15/2023 20:57:03 - INFO - __main__ -   test: [batch: 50/391 ] | Loss: 0.746 | Acc: 81.051% (5291/6528)/ 94.853% (6192/6528)
01/15/2023 20:57:04 - INFO - __main__ -   test: [batch: 51/391 ] | Loss: 0.738 | Acc: 81.265% (5409/6656)/ 94.937% (6319/6656)
01/15/2023 20:57:06 - INFO - __main__ -   test: [batch: 52/391 ] | Loss: 0.735 | Acc: 81.338% (5518/6784)/ 94.973% (6443/6784)
01/15/2023 20:57:08 - INFO - __main__ -   test: [batch: 53/391 ] | Loss: 0.725 | Acc: 81.568% (5638/6912)/ 95.052% (6570/6912)
01/15/2023 20:57:09 - INFO - __main__ -   test: [batch: 54/391 ] | Loss: 0.718 | Acc: 81.733% (5754/7040)/ 95.085% (6694/7040)
01/15/2023 20:57:11 - INFO - __main__ -   test: [batch: 55/391 ] | Loss: 0.712 | Acc: 81.864% (5868/7168)/ 95.131% (6819/7168)
01/15/2023 20:57:12 - INFO - __main__ -   test: [batch: 56/391 ] | Loss: 0.703 | Acc: 82.100% (5990/7296)/ 95.175% (6944/7296)
01/15/2023 20:57:14 - INFO - __main__ -   test: [batch: 57/391 ] | Loss: 0.696 | Acc: 82.301% (6110/7424)/ 95.218% (7069/7424)
01/15/2023 20:57:16 - INFO - __main__ -   test: [batch: 58/391 ] | Loss: 0.692 | Acc: 82.389% (6222/7552)/ 95.207% (7190/7552)
01/15/2023 20:57:17 - INFO - __main__ -   test: [batch: 59/391 ] | Loss: 0.694 | Acc: 82.292% (6320/7680)/ 95.208% (7312/7680)
01/15/2023 20:57:19 - INFO - __main__ -   test: [batch: 60/391 ] | Loss: 0.697 | Acc: 82.223% (6420/7808)/ 95.197% (7433/7808)
01/15/2023 20:57:21 - INFO - __main__ -   test: [batch: 61/391 ] | Loss: 0.696 | Acc: 82.208% (6524/7936)/ 95.237% (7558/7936)
01/15/2023 20:57:22 - INFO - __main__ -   test: [batch: 62/391 ] | Loss: 0.694 | Acc: 82.180% (6627/8064)/ 95.288% (7684/8064)
01/15/2023 20:57:24 - INFO - __main__ -   test: [batch: 63/391 ] | Loss: 0.701 | Acc: 82.056% (6722/8192)/ 95.215% (7800/8192)
01/15/2023 20:57:25 - INFO - __main__ -   test: [batch: 64/391 ] | Loss: 0.710 | Acc: 81.839% (6809/8320)/ 95.156% (7917/8320)
01/15/2023 20:57:26 - INFO - __main__ -   test: [batch: 65/391 ] | Loss: 0.722 | Acc: 81.404% (6877/8448)/ 95.076% (8032/8448)
01/15/2023 20:57:28 - INFO - __main__ -   test: [batch: 66/391 ] | Loss: 0.727 | Acc: 81.413% (6982/8576)/ 95.044% (8151/8576)
01/15/2023 20:57:29 - INFO - __main__ -   test: [batch: 67/391 ] | Loss: 0.730 | Acc: 81.353% (7081/8704)/ 95.037% (8272/8704)
01/15/2023 20:57:31 - INFO - __main__ -   test: [batch: 68/391 ] | Loss: 0.731 | Acc: 81.284% (7179/8832)/ 95.063% (8396/8832)
01/15/2023 20:57:32 - INFO - __main__ -   test: [batch: 69/391 ] | Loss: 0.726 | Acc: 81.373% (7291/8960)/ 95.112% (8522/8960)
01/15/2023 20:57:34 - INFO - __main__ -   test: [batch: 70/391 ] | Loss: 0.727 | Acc: 81.316% (7390/9088)/ 95.125% (8645/9088)
01/15/2023 20:57:36 - INFO - __main__ -   test: [batch: 71/391 ] | Loss: 0.725 | Acc: 81.380% (7500/9216)/ 95.117% (8766/9216)
01/15/2023 20:57:37 - INFO - __main__ -   test: [batch: 72/391 ] | Loss: 0.727 | Acc: 81.271% (7594/9344)/ 95.109% (8887/9344)
01/15/2023 20:57:39 - INFO - __main__ -   test: [batch: 73/391 ] | Loss: 0.731 | Acc: 81.176% (7689/9472)/ 95.101% (9008/9472)
01/15/2023 20:57:40 - INFO - __main__ -   test: [batch: 74/391 ] | Loss: 0.732 | Acc: 81.177% (7793/9600)/ 95.083% (9128/9600)
01/15/2023 20:57:42 - INFO - __main__ -   test: [batch: 75/391 ] | Loss: 0.738 | Acc: 80.972% (7877/9728)/ 95.076% (9249/9728)
01/15/2023 20:57:44 - INFO - __main__ -   test: [batch: 76/391 ] | Loss: 0.737 | Acc: 81.006% (7984/9856)/ 95.099% (9373/9856)
01/15/2023 20:57:45 - INFO - __main__ -   test: [batch: 77/391 ] | Loss: 0.738 | Acc: 80.919% (8079/9984)/ 95.122% (9497/9984)
01/15/2023 20:57:47 - INFO - __main__ -   test: [batch: 78/391 ] | Loss: 0.737 | Acc: 80.864% (8177/10112)/ 95.164% (9623/10112)
01/15/2023 20:57:49 - INFO - __main__ -   test: [batch: 79/391 ] | Loss: 0.736 | Acc: 80.830% (8277/10240)/ 95.186% (9747/10240)
01/15/2023 20:57:50 - INFO - __main__ -   test: [batch: 80/391 ] | Loss: 0.736 | Acc: 80.787% (8376/10368)/ 95.177% (9868/10368)
01/15/2023 20:57:52 - INFO - __main__ -   test: [batch: 81/391 ] | Loss: 0.736 | Acc: 80.783% (8479/10496)/ 95.189% (9991/10496)
01/15/2023 20:57:53 - INFO - __main__ -   test: [batch: 82/391 ] | Loss: 0.735 | Acc: 80.808% (8585/10624)/ 95.190% (10113/10624)
01/15/2023 20:57:55 - INFO - __main__ -   test: [batch: 83/391 ] | Loss: 0.735 | Acc: 80.859% (8694/10752)/ 95.164% (10232/10752)
01/15/2023 20:57:56 - INFO - __main__ -   test: [batch: 84/391 ] | Loss: 0.731 | Acc: 80.956% (8808/10880)/ 95.202% (10358/10880)
01/15/2023 20:57:58 - INFO - __main__ -   test: [batch: 85/391 ] | Loss: 0.729 | Acc: 80.950% (8911/11008)/ 95.249% (10485/11008)
01/15/2023 20:58:00 - INFO - __main__ -   test: [batch: 86/391 ] | Loss: 0.733 | Acc: 80.864% (9005/11136)/ 95.205% (10602/11136)
01/15/2023 20:58:01 - INFO - __main__ -   test: [batch: 87/391 ] | Loss: 0.732 | Acc: 80.904% (9113/11264)/ 95.197% (10723/11264)
01/15/2023 20:58:02 - INFO - __main__ -   test: [batch: 88/391 ] | Loss: 0.739 | Acc: 80.846% (9210/11392)/ 95.137% (10838/11392)
01/15/2023 20:58:04 - INFO - __main__ -   test: [batch: 89/391 ] | Loss: 0.737 | Acc: 80.894% (9319/11520)/ 95.148% (10961/11520)
01/15/2023 20:58:05 - INFO - __main__ -   test: [batch: 90/391 ] | Loss: 0.738 | Acc: 80.795% (9411/11648)/ 95.158% (11084/11648)
01/15/2023 20:58:07 - INFO - __main__ -   test: [batch: 91/391 ] | Loss: 0.737 | Acc: 80.834% (9519/11776)/ 95.168% (11207/11776)
01/15/2023 20:58:08 - INFO - __main__ -   test: [batch: 92/391 ] | Loss: 0.738 | Acc: 80.796% (9618/11904)/ 95.144% (11326/11904)
01/15/2023 20:58:10 - INFO - __main__ -   test: [batch: 93/391 ] | Loss: 0.741 | Acc: 80.602% (9698/12032)/ 95.188% (11453/12032)
01/15/2023 20:58:12 - INFO - __main__ -   test: [batch: 94/391 ] | Loss: 0.744 | Acc: 80.477% (9786/12160)/ 95.197% (11576/12160)
01/15/2023 20:58:13 - INFO - __main__ -   test: [batch: 95/391 ] | Loss: 0.742 | Acc: 80.550% (9898/12288)/ 95.207% (11699/12288)
01/15/2023 20:58:15 - INFO - __main__ -   test: [batch: 96/391 ] | Loss: 0.746 | Acc: 80.428% (9986/12416)/ 95.208% (11821/12416)
01/15/2023 20:58:16 - INFO - __main__ -   test: [batch: 97/391 ] | Loss: 0.748 | Acc: 80.269% (10069/12544)/ 95.225% (11945/12544)
01/15/2023 20:58:18 - INFO - __main__ -   test: [batch: 98/391 ] | Loss: 0.744 | Acc: 80.366% (10184/12672)/ 95.257% (12071/12672)
01/15/2023 20:58:19 - INFO - __main__ -   test: [batch: 99/391 ] | Loss: 0.738 | Acc: 80.523% (10307/12800)/ 95.305% (12199/12800)
01/15/2023 20:58:21 - INFO - __main__ -   test: [batch: 100/391 ] | Loss: 0.737 | Acc: 80.577% (10417/12928)/ 95.320% (12323/12928)
01/15/2023 20:58:23 - INFO - __main__ -   test: [batch: 101/391 ] | Loss: 0.732 | Acc: 80.699% (10536/13056)/ 95.351% (12449/13056)
01/15/2023 20:58:24 - INFO - __main__ -   test: [batch: 102/391 ] | Loss: 0.729 | Acc: 80.787% (10651/13184)/ 95.381% (12575/13184)
01/15/2023 20:58:26 - INFO - __main__ -   test: [batch: 103/391 ] | Loss: 0.730 | Acc: 80.642% (10735/13312)/ 95.395% (12699/13312)
01/15/2023 20:58:27 - INFO - __main__ -   test: [batch: 104/391 ] | Loss: 0.730 | Acc: 80.565% (10828/13440)/ 95.394% (12821/13440)
01/15/2023 20:58:29 - INFO - __main__ -   test: [batch: 105/391 ] | Loss: 0.731 | Acc: 80.565% (10931/13568)/ 95.401% (12944/13568)
01/15/2023 20:58:30 - INFO - __main__ -   test: [batch: 106/391 ] | Loss: 0.739 | Acc: 80.476% (11022/13696)/ 95.342% (13058/13696)
01/15/2023 20:58:32 - INFO - __main__ -   test: [batch: 107/391 ] | Loss: 0.735 | Acc: 80.606% (11143/13824)/ 95.378% (13185/13824)
01/15/2023 20:58:33 - INFO - __main__ -   test: [batch: 108/391 ] | Loss: 0.738 | Acc: 80.454% (11225/13952)/ 95.384% (13308/13952)
01/15/2023 20:58:35 - INFO - __main__ -   test: [batch: 109/391 ] | Loss: 0.739 | Acc: 80.462% (11329/14080)/ 95.391% (13431/14080)
01/15/2023 20:58:36 - INFO - __main__ -   test: [batch: 110/391 ] | Loss: 0.741 | Acc: 80.286% (11407/14208)/ 95.411% (13556/14208)
01/15/2023 20:58:38 - INFO - __main__ -   test: [batch: 111/391 ] | Loss: 0.744 | Acc: 80.266% (11507/14336)/ 95.375% (13673/14336)
01/15/2023 20:58:40 - INFO - __main__ -   test: [batch: 112/391 ] | Loss: 0.744 | Acc: 80.289% (11613/14464)/ 95.395% (13798/14464)
01/15/2023 20:58:41 - INFO - __main__ -   test: [batch: 113/391 ] | Loss: 0.743 | Acc: 80.332% (11722/14592)/ 95.408% (13922/14592)
01/15/2023 20:58:43 - INFO - __main__ -   test: [batch: 114/391 ] | Loss: 0.739 | Acc: 80.435% (11840/14720)/ 95.435% (14048/14720)
01/15/2023 20:58:44 - INFO - __main__ -   test: [batch: 115/391 ] | Loss: 0.736 | Acc: 80.523% (11956/14848)/ 95.454% (14173/14848)
01/15/2023 20:58:46 - INFO - __main__ -   test: [batch: 116/391 ] | Loss: 0.740 | Acc: 80.495% (12055/14976)/ 95.433% (14292/14976)
01/15/2023 20:58:47 - INFO - __main__ -   test: [batch: 117/391 ] | Loss: 0.738 | Acc: 80.528% (12163/15104)/ 95.452% (14417/15104)
01/15/2023 20:58:49 - INFO - __main__ -   test: [batch: 118/391 ] | Loss: 0.741 | Acc: 80.397% (12246/15232)/ 95.463% (14541/15232)
01/15/2023 20:58:50 - INFO - __main__ -   test: [batch: 119/391 ] | Loss: 0.739 | Acc: 80.449% (12357/15360)/ 95.482% (14666/15360)
01/15/2023 20:58:52 - INFO - __main__ -   test: [batch: 120/391 ] | Loss: 0.738 | Acc: 80.507% (12469/15488)/ 95.500% (14791/15488)
01/15/2023 20:58:53 - INFO - __main__ -   test: [batch: 121/391 ] | Loss: 0.743 | Acc: 80.379% (12552/15616)/ 95.466% (14908/15616)
01/15/2023 20:58:55 - INFO - __main__ -   test: [batch: 122/391 ] | Loss: 0.745 | Acc: 80.335% (12648/15744)/ 95.446% (15027/15744)
01/15/2023 20:58:56 - INFO - __main__ -   test: [batch: 123/391 ] | Loss: 0.746 | Acc: 80.355% (12754/15872)/ 95.426% (15146/15872)
01/15/2023 20:58:58 - INFO - __main__ -   test: [batch: 124/391 ] | Loss: 0.745 | Acc: 80.381% (12861/16000)/ 95.431% (15269/16000)
01/15/2023 20:58:59 - INFO - __main__ -   test: [batch: 125/391 ] | Loss: 0.741 | Acc: 80.494% (12982/16128)/ 95.461% (15396/16128)
01/15/2023 20:59:01 - INFO - __main__ -   test: [batch: 126/391 ] | Loss: 0.738 | Acc: 80.586% (13100/16256)/ 95.466% (15519/16256)
01/15/2023 20:59:02 - INFO - __main__ -   test: [batch: 127/391 ] | Loss: 0.736 | Acc: 80.658% (13215/16384)/ 95.477% (15643/16384)
01/15/2023 20:59:04 - INFO - __main__ -   test: [batch: 128/391 ] | Loss: 0.737 | Acc: 80.656% (13318/16512)/ 95.452% (15761/16512)
01/15/2023 20:59:05 - INFO - __main__ -   test: [batch: 129/391 ] | Loss: 0.735 | Acc: 80.697% (13428/16640)/ 95.481% (15888/16640)
01/15/2023 20:59:07 - INFO - __main__ -   test: [batch: 130/391 ] | Loss: 0.731 | Acc: 80.809% (13550/16768)/ 95.503% (16014/16768)
01/15/2023 20:59:08 - INFO - __main__ -   test: [batch: 131/391 ] | Loss: 0.731 | Acc: 80.848% (13660/16896)/ 95.520% (16139/16896)
01/15/2023 20:59:10 - INFO - __main__ -   test: [batch: 132/391 ] | Loss: 0.729 | Acc: 80.903% (13773/17024)/ 95.536% (16264/17024)
01/15/2023 20:59:11 - INFO - __main__ -   test: [batch: 133/391 ] | Loss: 0.730 | Acc: 80.854% (13868/17152)/ 95.522% (16384/17152)
01/15/2023 20:59:13 - INFO - __main__ -   test: [batch: 134/391 ] | Loss: 0.729 | Acc: 80.880% (13976/17280)/ 95.538% (16509/17280)
01/15/2023 20:59:15 - INFO - __main__ -   test: [batch: 135/391 ] | Loss: 0.729 | Acc: 80.865% (14077/17408)/ 95.542% (16632/17408)
01/15/2023 20:59:16 - INFO - __main__ -   test: [batch: 136/391 ] | Loss: 0.727 | Acc: 80.845% (14177/17536)/ 95.558% (16757/17536)
01/15/2023 20:59:18 - INFO - __main__ -   test: [batch: 137/391 ] | Loss: 0.725 | Acc: 80.888% (14288/17664)/ 95.590% (16885/17664)
01/15/2023 20:59:19 - INFO - __main__ -   test: [batch: 138/391 ] | Loss: 0.724 | Acc: 80.952% (14403/17792)/ 95.599% (17009/17792)
01/15/2023 20:59:21 - INFO - __main__ -   test: [batch: 139/391 ] | Loss: 0.728 | Acc: 80.826% (14484/17920)/ 95.586% (17129/17920)
01/15/2023 20:59:22 - INFO - __main__ -   test: [batch: 140/391 ] | Loss: 0.730 | Acc: 80.740% (14572/18048)/ 95.584% (17251/18048)
01/15/2023 20:59:24 - INFO - __main__ -   test: [batch: 141/391 ] | Loss: 0.728 | Acc: 80.788% (14684/18176)/ 95.599% (17376/18176)
01/15/2023 20:59:25 - INFO - __main__ -   test: [batch: 142/391 ] | Loss: 0.726 | Acc: 80.873% (14803/18304)/ 95.608% (17500/18304)
01/15/2023 20:59:26 - INFO - __main__ -   test: [batch: 143/391 ] | Loss: 0.726 | Acc: 80.897% (14911/18432)/ 95.589% (17619/18432)
01/15/2023 20:59:28 - INFO - __main__ -   test: [batch: 144/391 ] | Loss: 0.728 | Acc: 80.867% (15009/18560)/ 95.555% (17735/18560)
01/15/2023 20:59:29 - INFO - __main__ -   test: [batch: 145/391 ] | Loss: 0.731 | Acc: 80.849% (15109/18688)/ 95.532% (17853/18688)
01/15/2023 20:59:31 - INFO - __main__ -   test: [batch: 146/391 ] | Loss: 0.733 | Acc: 80.820% (15207/18816)/ 95.509% (17971/18816)
01/15/2023 20:59:32 - INFO - __main__ -   test: [batch: 147/391 ] | Loss: 0.733 | Acc: 80.817% (15310/18944)/ 95.492% (18090/18944)
01/15/2023 20:59:34 - INFO - __main__ -   test: [batch: 148/391 ] | Loss: 0.734 | Acc: 80.736% (15398/19072)/ 95.496% (18213/19072)
01/15/2023 20:59:36 - INFO - __main__ -   test: [batch: 149/391 ] | Loss: 0.738 | Acc: 80.661% (15487/19200)/ 95.458% (18328/19200)
01/15/2023 20:59:37 - INFO - __main__ -   test: [batch: 150/391 ] | Loss: 0.738 | Acc: 80.603% (15579/19328)/ 95.473% (18453/19328)
01/15/2023 20:59:39 - INFO - __main__ -   test: [batch: 151/391 ] | Loss: 0.738 | Acc: 80.638% (15689/19456)/ 95.467% (18574/19456)
01/15/2023 20:59:40 - INFO - __main__ -   test: [batch: 152/391 ] | Loss: 0.740 | Acc: 80.627% (15790/19584)/ 95.450% (18693/19584)
01/15/2023 20:59:41 - INFO - __main__ -   test: [batch: 153/391 ] | Loss: 0.738 | Acc: 80.682% (15904/19712)/ 95.455% (18816/19712)
01/15/2023 20:59:43 - INFO - __main__ -   test: [batch: 154/391 ] | Loss: 0.739 | Acc: 80.655% (16002/19840)/ 95.413% (18930/19840)
01/15/2023 20:59:44 - INFO - __main__ -   test: [batch: 155/391 ] | Loss: 0.740 | Acc: 80.659% (16106/19968)/ 95.403% (19050/19968)
01/15/2023 20:59:46 - INFO - __main__ -   test: [batch: 156/391 ] | Loss: 0.742 | Acc: 80.578% (16193/20096)/ 95.382% (19168/20096)
01/15/2023 20:59:48 - INFO - __main__ -   test: [batch: 157/391 ] | Loss: 0.745 | Acc: 80.533% (16287/20224)/ 95.367% (19287/20224)
01/15/2023 20:59:49 - INFO - __main__ -   test: [batch: 158/391 ] | Loss: 0.746 | Acc: 80.508% (16385/20352)/ 95.337% (19403/20352)
01/15/2023 20:59:51 - INFO - __main__ -   test: [batch: 159/391 ] | Loss: 0.747 | Acc: 80.488% (16484/20480)/ 95.342% (19526/20480)
01/15/2023 20:59:52 - INFO - __main__ -   test: [batch: 160/391 ] | Loss: 0.747 | Acc: 80.474% (16584/20608)/ 95.322% (19644/20608)
01/15/2023 20:59:54 - INFO - __main__ -   test: [batch: 161/391 ] | Loss: 0.758 | Acc: 80.218% (16634/20736)/ 95.216% (19744/20736)
01/15/2023 20:59:55 - INFO - __main__ -   test: [batch: 162/391 ] | Loss: 0.763 | Acc: 80.114% (16715/20864)/ 95.140% (19850/20864)
01/15/2023 20:59:57 - INFO - __main__ -   test: [batch: 163/391 ] | Loss: 0.766 | Acc: 80.040% (16802/20992)/ 95.122% (19968/20992)
01/15/2023 20:59:58 - INFO - __main__ -   test: [batch: 164/391 ] | Loss: 0.765 | Acc: 80.024% (16901/21120)/ 95.137% (20093/21120)
01/15/2023 21:00:00 - INFO - __main__ -   test: [batch: 165/391 ] | Loss: 0.767 | Acc: 79.946% (16987/21248)/ 95.138% (20215/21248)
01/15/2023 21:00:01 - INFO - __main__ -   test: [batch: 166/391 ] | Loss: 0.767 | Acc: 79.973% (17095/21376)/ 95.130% (20335/21376)
01/15/2023 21:00:03 - INFO - __main__ -   test: [batch: 167/391 ] | Loss: 0.770 | Acc: 79.925% (17187/21504)/ 95.099% (20450/21504)
01/15/2023 21:00:05 - INFO - __main__ -   test: [batch: 168/391 ] | Loss: 0.769 | Acc: 79.923% (17289/21632)/ 95.095% (20571/21632)
01/15/2023 21:00:06 - INFO - __main__ -   test: [batch: 169/391 ] | Loss: 0.772 | Acc: 79.876% (17381/21760)/ 95.046% (20682/21760)
01/15/2023 21:00:07 - INFO - __main__ -   test: [batch: 170/391 ] | Loss: 0.776 | Acc: 79.779% (17462/21888)/ 95.011% (20796/21888)
01/15/2023 21:00:09 - INFO - __main__ -   test: [batch: 171/391 ] | Loss: 0.779 | Acc: 79.733% (17554/22016)/ 94.976% (20910/22016)
01/15/2023 21:00:10 - INFO - __main__ -   test: [batch: 172/391 ] | Loss: 0.780 | Acc: 79.692% (17647/22144)/ 94.974% (21031/22144)
01/15/2023 21:00:12 - INFO - __main__ -   test: [batch: 173/391 ] | Loss: 0.783 | Acc: 79.643% (17738/22272)/ 94.949% (21147/22272)
01/15/2023 21:00:13 - INFO - __main__ -   test: [batch: 174/391 ] | Loss: 0.787 | Acc: 79.549% (17819/22400)/ 94.911% (21260/22400)
01/15/2023 21:00:15 - INFO - __main__ -   test: [batch: 175/391 ] | Loss: 0.785 | Acc: 79.608% (17934/22528)/ 94.922% (21384/22528)
01/15/2023 21:00:16 - INFO - __main__ -   test: [batch: 176/391 ] | Loss: 0.786 | Acc: 79.599% (18034/22656)/ 94.893% (21499/22656)
01/15/2023 21:00:18 - INFO - __main__ -   test: [batch: 177/391 ] | Loss: 0.790 | Acc: 79.525% (18119/22784)/ 94.856% (21612/22784)
01/15/2023 21:00:19 - INFO - __main__ -   test: [batch: 178/391 ] | Loss: 0.791 | Acc: 79.526% (18221/22912)/ 94.841% (21730/22912)
01/15/2023 21:00:21 - INFO - __main__ -   test: [batch: 179/391 ] | Loss: 0.794 | Acc: 79.457% (18307/23040)/ 94.783% (21838/23040)
01/15/2023 21:00:22 - INFO - __main__ -   test: [batch: 180/391 ] | Loss: 0.800 | Acc: 79.329% (18379/23168)/ 94.743% (21950/23168)
01/15/2023 21:00:24 - INFO - __main__ -   test: [batch: 181/391 ] | Loss: 0.806 | Acc: 79.215% (18454/23296)/ 94.690% (22059/23296)
01/15/2023 21:00:26 - INFO - __main__ -   test: [batch: 182/391 ] | Loss: 0.806 | Acc: 79.231% (18559/23424)/ 94.693% (22181/23424)
01/15/2023 21:00:27 - INFO - __main__ -   test: [batch: 183/391 ] | Loss: 0.812 | Acc: 79.110% (18632/23552)/ 94.586% (22277/23552)
01/15/2023 21:00:29 - INFO - __main__ -   test: [batch: 184/391 ] | Loss: 0.812 | Acc: 79.130% (18738/23680)/ 94.578% (22396/23680)
01/15/2023 21:00:30 - INFO - __main__ -   test: [batch: 185/391 ] | Loss: 0.811 | Acc: 79.150% (18844/23808)/ 94.573% (22516/23808)
01/15/2023 21:00:32 - INFO - __main__ -   test: [batch: 186/391 ] | Loss: 0.814 | Acc: 79.111% (18936/23936)/ 94.531% (22627/23936)
01/15/2023 21:00:33 - INFO - __main__ -   test: [batch: 187/391 ] | Loss: 0.819 | Acc: 79.002% (19011/24064)/ 94.510% (22743/24064)
01/15/2023 21:00:35 - INFO - __main__ -   test: [batch: 188/391 ] | Loss: 0.822 | Acc: 78.890% (19085/24192)/ 94.486% (22858/24192)
01/15/2023 21:00:37 - INFO - __main__ -   test: [batch: 189/391 ] | Loss: 0.824 | Acc: 78.824% (19170/24320)/ 94.486% (22979/24320)
01/15/2023 21:00:38 - INFO - __main__ -   test: [batch: 190/391 ] | Loss: 0.829 | Acc: 78.730% (19248/24448)/ 94.458% (23093/24448)
01/15/2023 21:00:40 - INFO - __main__ -   test: [batch: 191/391 ] | Loss: 0.831 | Acc: 78.695% (19340/24576)/ 94.417% (23204/24576)
01/15/2023 21:00:41 - INFO - __main__ -   test: [batch: 192/391 ] | Loss: 0.838 | Acc: 78.574% (19411/24704)/ 94.317% (23300/24704)
01/15/2023 21:00:43 - INFO - __main__ -   test: [batch: 193/391 ] | Loss: 0.838 | Acc: 78.584% (19514/24832)/ 94.314% (23420/24832)
01/15/2023 21:00:44 - INFO - __main__ -   test: [batch: 194/391 ] | Loss: 0.841 | Acc: 78.518% (19598/24960)/ 94.291% (23535/24960)
01/15/2023 21:00:46 - INFO - __main__ -   test: [batch: 195/391 ] | Loss: 0.847 | Acc: 78.424% (19675/25088)/ 94.220% (23638/25088)
01/15/2023 21:00:47 - INFO - __main__ -   test: [batch: 196/391 ] | Loss: 0.850 | Acc: 78.327% (19751/25216)/ 94.174% (23747/25216)
01/15/2023 21:00:49 - INFO - __main__ -   test: [batch: 197/391 ] | Loss: 0.855 | Acc: 78.239% (19829/25344)/ 94.156% (23863/25344)
01/15/2023 21:00:50 - INFO - __main__ -   test: [batch: 198/391 ] | Loss: 0.857 | Acc: 78.184% (19915/25472)/ 94.119% (23974/25472)
01/15/2023 21:00:52 - INFO - __main__ -   test: [batch: 199/391 ] | Loss: 0.856 | Acc: 78.152% (20007/25600)/ 94.129% (24097/25600)
01/15/2023 21:00:53 - INFO - __main__ -   test: [batch: 200/391 ] | Loss: 0.856 | Acc: 78.133% (20102/25728)/ 94.131% (24218/25728)
01/15/2023 21:00:55 - INFO - __main__ -   test: [batch: 201/391 ] | Loss: 0.860 | Acc: 78.036% (20177/25856)/ 94.098% (24330/25856)
01/15/2023 21:00:56 - INFO - __main__ -   test: [batch: 202/391 ] | Loss: 0.861 | Acc: 78.029% (20275/25984)/ 94.089% (24448/25984)
01/15/2023 21:00:58 - INFO - __main__ -   test: [batch: 203/391 ] | Loss: 0.862 | Acc: 78.010% (20370/26112)/ 94.079% (24566/26112)
01/15/2023 21:01:00 - INFO - __main__ -   test: [batch: 204/391 ] | Loss: 0.865 | Acc: 77.927% (20448/26240)/ 94.047% (24678/26240)
01/15/2023 21:01:01 - INFO - __main__ -   test: [batch: 205/391 ] | Loss: 0.868 | Acc: 77.844% (20526/26368)/ 94.023% (24792/26368)
01/15/2023 21:01:03 - INFO - __main__ -   test: [batch: 206/391 ] | Loss: 0.869 | Acc: 77.808% (20616/26496)/ 94.022% (24912/26496)
01/15/2023 21:01:04 - INFO - __main__ -   test: [batch: 207/391 ] | Loss: 0.873 | Acc: 77.734% (20696/26624)/ 93.987% (25023/26624)
01/15/2023 21:01:06 - INFO - __main__ -   test: [batch: 208/391 ] | Loss: 0.875 | Acc: 77.691% (20784/26752)/ 93.941% (25131/26752)
01/15/2023 21:01:08 - INFO - __main__ -   test: [batch: 209/391 ] | Loss: 0.874 | Acc: 77.686% (20882/26880)/ 93.962% (25257/26880)
01/15/2023 21:01:09 - INFO - __main__ -   test: [batch: 210/391 ] | Loss: 0.876 | Acc: 77.655% (20973/27008)/ 93.943% (25372/27008)
01/15/2023 21:01:11 - INFO - __main__ -   test: [batch: 211/391 ] | Loss: 0.878 | Acc: 77.594% (21056/27136)/ 93.916% (25485/27136)
01/15/2023 21:01:12 - INFO - __main__ -   test: [batch: 212/391 ] | Loss: 0.880 | Acc: 77.520% (21135/27264)/ 93.904% (25602/27264)
01/15/2023 21:01:14 - INFO - __main__ -   test: [batch: 213/391 ] | Loss: 0.880 | Acc: 77.519% (21234/27392)/ 93.911% (25724/27392)
01/15/2023 21:01:15 - INFO - __main__ -   test: [batch: 214/391 ] | Loss: 0.880 | Acc: 77.507% (21330/27520)/ 93.917% (25846/27520)
01/15/2023 21:01:17 - INFO - __main__ -   test: [batch: 215/391 ] | Loss: 0.882 | Acc: 77.488% (21424/27648)/ 93.895% (25960/27648)
01/15/2023 21:01:19 - INFO - __main__ -   test: [batch: 216/391 ] | Loss: 0.879 | Acc: 77.538% (21537/27776)/ 93.912% (26085/27776)
01/15/2023 21:01:20 - INFO - __main__ -   test: [batch: 217/391 ] | Loss: 0.884 | Acc: 77.473% (21618/27904)/ 93.843% (26186/27904)
01/15/2023 21:01:22 - INFO - __main__ -   test: [batch: 218/391 ] | Loss: 0.887 | Acc: 77.415% (21701/28032)/ 93.811% (26297/28032)
01/15/2023 21:01:23 - INFO - __main__ -   test: [batch: 219/391 ] | Loss: 0.886 | Acc: 77.450% (21810/28160)/ 93.821% (26420/28160)
01/15/2023 21:01:25 - INFO - __main__ -   test: [batch: 220/391 ] | Loss: 0.884 | Acc: 77.485% (21919/28288)/ 93.828% (26542/28288)
01/15/2023 21:01:27 - INFO - __main__ -   test: [batch: 221/391 ] | Loss: 0.886 | Acc: 77.453% (22009/28416)/ 93.806% (26656/28416)
01/15/2023 21:01:28 - INFO - __main__ -   test: [batch: 222/391 ] | Loss: 0.884 | Acc: 77.508% (22124/28544)/ 93.820% (26780/28544)
01/15/2023 21:01:30 - INFO - __main__ -   test: [batch: 223/391 ] | Loss: 0.885 | Acc: 77.504% (22222/28672)/ 93.806% (26896/28672)
01/15/2023 21:01:31 - INFO - __main__ -   test: [batch: 224/391 ] | Loss: 0.884 | Acc: 77.524% (22327/28800)/ 93.806% (27016/28800)
01/15/2023 21:01:33 - INFO - __main__ -   test: [batch: 225/391 ] | Loss: 0.884 | Acc: 77.520% (22425/28928)/ 93.809% (27137/28928)
01/15/2023 21:01:34 - INFO - __main__ -   test: [batch: 226/391 ] | Loss: 0.884 | Acc: 77.492% (22516/29056)/ 93.809% (27257/29056)
01/15/2023 21:01:36 - INFO - __main__ -   test: [batch: 227/391 ] | Loss: 0.885 | Acc: 77.495% (22616/29184)/ 93.805% (27376/29184)
01/15/2023 21:01:38 - INFO - __main__ -   test: [batch: 228/391 ] | Loss: 0.890 | Acc: 77.409% (22690/29312)/ 93.743% (27478/29312)
01/15/2023 21:01:39 - INFO - __main__ -   test: [batch: 229/391 ] | Loss: 0.893 | Acc: 77.337% (22768/29440)/ 93.692% (27583/29440)
01/15/2023 21:01:41 - INFO - __main__ -   test: [batch: 230/391 ] | Loss: 0.897 | Acc: 77.263% (22845/29568)/ 93.652% (27691/29568)
01/15/2023 21:01:42 - INFO - __main__ -   test: [batch: 231/391 ] | Loss: 0.897 | Acc: 77.249% (22940/29696)/ 93.639% (27807/29696)
01/15/2023 21:01:44 - INFO - __main__ -   test: [batch: 232/391 ] | Loss: 0.896 | Acc: 77.277% (23047/29824)/ 93.653% (27931/29824)
01/15/2023 21:01:45 - INFO - __main__ -   test: [batch: 233/391 ] | Loss: 0.898 | Acc: 77.237% (23134/29952)/ 93.630% (28044/29952)
01/15/2023 21:01:47 - INFO - __main__ -   test: [batch: 234/391 ] | Loss: 0.904 | Acc: 77.131% (23201/30080)/ 93.554% (28141/30080)
01/15/2023 21:01:48 - INFO - __main__ -   test: [batch: 235/391 ] | Loss: 0.904 | Acc: 77.129% (23299/30208)/ 93.551% (28260/30208)
01/15/2023 21:01:50 - INFO - __main__ -   test: [batch: 236/391 ] | Loss: 0.903 | Acc: 77.176% (23412/30336)/ 93.562% (28383/30336)
01/15/2023 21:01:52 - INFO - __main__ -   test: [batch: 237/391 ] | Loss: 0.903 | Acc: 77.176% (23511/30464)/ 93.540% (28496/30464)
01/15/2023 21:01:53 - INFO - __main__ -   test: [batch: 238/391 ] | Loss: 0.902 | Acc: 77.220% (23623/30592)/ 93.554% (28620/30592)
01/15/2023 21:01:55 - INFO - __main__ -   test: [batch: 239/391 ] | Loss: 0.901 | Acc: 77.253% (23732/30720)/ 93.555% (28740/30720)
01/15/2023 21:01:56 - INFO - __main__ -   test: [batch: 240/391 ] | Loss: 0.901 | Acc: 77.263% (23834/30848)/ 93.549% (28858/30848)
01/15/2023 21:01:58 - INFO - __main__ -   test: [batch: 241/391 ] | Loss: 0.904 | Acc: 77.182% (23908/30976)/ 93.508% (28965/30976)
01/15/2023 21:01:59 - INFO - __main__ -   test: [batch: 242/391 ] | Loss: 0.907 | Acc: 77.083% (23976/31104)/ 93.493% (29080/31104)
01/15/2023 21:02:01 - INFO - __main__ -   test: [batch: 243/391 ] | Loss: 0.912 | Acc: 76.972% (24040/31232)/ 93.420% (29177/31232)
01/15/2023 21:02:02 - INFO - __main__ -   test: [batch: 244/391 ] | Loss: 0.912 | Acc: 76.974% (24139/31360)/ 93.415% (29295/31360)
01/15/2023 21:02:04 - INFO - __main__ -   test: [batch: 245/391 ] | Loss: 0.912 | Acc: 76.982% (24240/31488)/ 93.407% (29412/31488)
01/15/2023 21:02:05 - INFO - __main__ -   test: [batch: 246/391 ] | Loss: 0.913 | Acc: 76.967% (24334/31616)/ 93.393% (29527/31616)
01/15/2023 21:02:07 - INFO - __main__ -   test: [batch: 247/391 ] | Loss: 0.918 | Acc: 76.884% (24406/31744)/ 93.325% (29625/31744)
01/15/2023 21:02:09 - INFO - __main__ -   test: [batch: 248/391 ] | Loss: 0.919 | Acc: 76.848% (24493/31872)/ 93.320% (29743/31872)
01/15/2023 21:02:10 - INFO - __main__ -   test: [batch: 249/391 ] | Loss: 0.921 | Acc: 76.688% (24540/32000)/ 93.316% (29861/32000)
01/15/2023 21:02:12 - INFO - __main__ -   test: [batch: 250/391 ] | Loss: 0.920 | Acc: 76.718% (24648/32128)/ 93.327% (29984/32128)
01/15/2023 21:02:13 - INFO - __main__ -   test: [batch: 251/391 ] | Loss: 0.922 | Acc: 76.693% (24738/32256)/ 93.297% (30094/32256)
01/15/2023 21:02:15 - INFO - __main__ -   test: [batch: 252/391 ] | Loss: 0.921 | Acc: 76.720% (24845/32384)/ 93.290% (30211/32384)
01/15/2023 21:02:16 - INFO - __main__ -   test: [batch: 253/391 ] | Loss: 0.923 | Acc: 76.692% (24934/32512)/ 93.273% (30325/32512)
01/15/2023 21:02:17 - INFO - __main__ -   test: [batch: 254/391 ] | Loss: 0.927 | Acc: 76.612% (25006/32640)/ 93.226% (30429/32640)
01/15/2023 21:02:19 - INFO - __main__ -   test: [batch: 255/391 ] | Loss: 0.929 | Acc: 76.590% (25097/32768)/ 93.216% (30545/32768)
01/15/2023 21:02:20 - INFO - __main__ -   test: [batch: 256/391 ] | Loss: 0.934 | Acc: 76.462% (25153/32896)/ 93.172% (30650/32896)
01/15/2023 21:02:22 - INFO - __main__ -   test: [batch: 257/391 ] | Loss: 0.934 | Acc: 76.450% (25247/33024)/ 93.172% (30769/33024)
01/15/2023 21:02:24 - INFO - __main__ -   test: [batch: 258/391 ] | Loss: 0.935 | Acc: 76.442% (25342/33152)/ 93.156% (30883/33152)
01/15/2023 21:02:25 - INFO - __main__ -   test: [batch: 259/391 ] | Loss: 0.939 | Acc: 76.334% (25404/33280)/ 93.152% (31001/33280)
01/15/2023 21:02:27 - INFO - __main__ -   test: [batch: 260/391 ] | Loss: 0.939 | Acc: 76.314% (25495/33408)/ 93.151% (31120/33408)
01/15/2023 21:02:28 - INFO - __main__ -   test: [batch: 261/391 ] | Loss: 0.937 | Acc: 76.351% (25605/33536)/ 93.174% (31247/33536)
01/15/2023 21:02:30 - INFO - __main__ -   test: [batch: 262/391 ] | Loss: 0.936 | Acc: 76.369% (25709/33664)/ 93.186% (31370/33664)
01/15/2023 21:02:32 - INFO - __main__ -   test: [batch: 263/391 ] | Loss: 0.939 | Acc: 76.302% (25784/33792)/ 93.161% (31481/33792)
01/15/2023 21:02:33 - INFO - __main__ -   test: [batch: 264/391 ] | Loss: 0.944 | Acc: 76.232% (25858/33920)/ 93.096% (31578/33920)
01/15/2023 21:02:35 - INFO - __main__ -   test: [batch: 265/391 ] | Loss: 0.943 | Acc: 76.266% (25967/34048)/ 93.095% (31697/34048)
01/15/2023 21:02:36 - INFO - __main__ -   test: [batch: 266/391 ] | Loss: 0.945 | Acc: 76.197% (26041/34176)/ 93.080% (31811/34176)
01/15/2023 21:02:38 - INFO - __main__ -   test: [batch: 267/391 ] | Loss: 0.945 | Acc: 76.230% (26150/34304)/ 93.082% (31931/34304)
01/15/2023 21:02:40 - INFO - __main__ -   test: [batch: 268/391 ] | Loss: 0.944 | Acc: 76.240% (26251/34432)/ 93.079% (32049/34432)
01/15/2023 21:02:41 - INFO - __main__ -   test: [batch: 269/391 ] | Loss: 0.946 | Acc: 76.189% (26331/34560)/ 93.061% (32162/34560)
01/15/2023 21:02:43 - INFO - __main__ -   test: [batch: 270/391 ] | Loss: 0.949 | Acc: 76.142% (26412/34688)/ 93.038% (32273/34688)
01/15/2023 21:02:44 - INFO - __main__ -   test: [batch: 271/391 ] | Loss: 0.949 | Acc: 76.155% (26514/34816)/ 93.032% (32390/34816)
01/15/2023 21:02:46 - INFO - __main__ -   test: [batch: 272/391 ] | Loss: 0.950 | Acc: 76.125% (26601/34944)/ 93.029% (32508/34944)
01/15/2023 21:02:47 - INFO - __main__ -   test: [batch: 273/391 ] | Loss: 0.950 | Acc: 76.141% (26704/35072)/ 93.006% (32619/35072)
01/15/2023 21:02:48 - INFO - __main__ -   test: [batch: 274/391 ] | Loss: 0.950 | Acc: 76.142% (26802/35200)/ 93.011% (32740/35200)
01/15/2023 21:02:50 - INFO - __main__ -   test: [batch: 275/391 ] | Loss: 0.952 | Acc: 76.110% (26888/35328)/ 93.000% (32855/35328)
01/15/2023 21:02:51 - INFO - __main__ -   test: [batch: 276/391 ] | Loss: 0.953 | Acc: 76.060% (26968/35456)/ 92.991% (32971/35456)
01/15/2023 21:02:53 - INFO - __main__ -   test: [batch: 277/391 ] | Loss: 0.954 | Acc: 76.034% (27056/35584)/ 92.969% (33082/35584)
01/15/2023 21:02:54 - INFO - __main__ -   test: [batch: 278/391 ] | Loss: 0.953 | Acc: 76.044% (27157/35712)/ 92.969% (33201/35712)
01/15/2023 21:02:56 - INFO - __main__ -   test: [batch: 279/391 ] | Loss: 0.954 | Acc: 76.069% (27263/35840)/ 92.963% (33318/35840)
01/15/2023 21:02:57 - INFO - __main__ -   test: [batch: 280/391 ] | Loss: 0.954 | Acc: 76.040% (27350/35968)/ 92.952% (33433/35968)
01/15/2023 21:02:59 - INFO - __main__ -   test: [batch: 281/391 ] | Loss: 0.955 | Acc: 76.039% (27447/36096)/ 92.944% (33549/36096)
01/15/2023 21:03:01 - INFO - __main__ -   test: [batch: 282/391 ] | Loss: 0.954 | Acc: 76.074% (27557/36224)/ 92.944% (33668/36224)
01/15/2023 21:03:02 - INFO - __main__ -   test: [batch: 283/391 ] | Loss: 0.954 | Acc: 76.056% (27648/36352)/ 92.938% (33785/36352)
01/15/2023 21:03:03 - INFO - __main__ -   test: [batch: 284/391 ] | Loss: 0.958 | Acc: 76.009% (27728/36480)/ 92.889% (33886/36480)
01/15/2023 21:03:05 - INFO - __main__ -   test: [batch: 285/391 ] | Loss: 0.961 | Acc: 75.975% (27813/36608)/ 92.840% (33987/36608)
01/15/2023 21:03:06 - INFO - __main__ -   test: [batch: 286/391 ] | Loss: 0.962 | Acc: 75.942% (27898/36736)/ 92.824% (34100/36736)
01/15/2023 21:03:08 - INFO - __main__ -   test: [batch: 287/391 ] | Loss: 0.963 | Acc: 75.933% (27992/36864)/ 92.809% (34213/36864)
01/15/2023 21:03:09 - INFO - __main__ -   test: [batch: 288/391 ] | Loss: 0.962 | Acc: 75.954% (28097/36992)/ 92.809% (34332/36992)
01/15/2023 21:03:11 - INFO - __main__ -   test: [batch: 289/391 ] | Loss: 0.964 | Acc: 75.924% (28183/37120)/ 92.780% (34440/37120)
01/15/2023 21:03:12 - INFO - __main__ -   test: [batch: 290/391 ] | Loss: 0.965 | Acc: 75.840% (28249/37248)/ 92.778% (34558/37248)
01/15/2023 21:03:14 - INFO - __main__ -   test: [batch: 291/391 ] | Loss: 0.965 | Acc: 75.845% (28348/37376)/ 92.779% (34677/37376)
01/15/2023 21:03:15 - INFO - __main__ -   test: [batch: 292/391 ] | Loss: 0.967 | Acc: 75.784% (28422/37504)/ 92.758% (34788/37504)
01/15/2023 21:03:17 - INFO - __main__ -   test: [batch: 293/391 ] | Loss: 0.968 | Acc: 75.752% (28507/37632)/ 92.756% (34906/37632)
01/15/2023 21:03:18 - INFO - __main__ -   test: [batch: 294/391 ] | Loss: 0.969 | Acc: 75.734% (28597/37760)/ 92.738% (35018/37760)
01/15/2023 21:03:20 - INFO - __main__ -   test: [batch: 295/391 ] | Loss: 0.968 | Acc: 75.755% (28702/37888)/ 92.736% (35136/37888)
01/15/2023 21:03:22 - INFO - __main__ -   test: [batch: 296/391 ] | Loss: 0.969 | Acc: 75.744% (28795/38016)/ 92.729% (35252/38016)
01/15/2023 21:03:23 - INFO - __main__ -   test: [batch: 297/391 ] | Loss: 0.971 | Acc: 75.718% (28882/38144)/ 92.707% (35362/38144)
01/15/2023 21:03:24 - INFO - __main__ -   test: [batch: 298/391 ] | Loss: 0.973 | Acc: 75.677% (28963/38272)/ 92.681% (35471/38272)
01/15/2023 21:03:26 - INFO - __main__ -   test: [batch: 299/391 ] | Loss: 0.974 | Acc: 75.667% (29056/38400)/ 92.659% (35581/38400)
01/15/2023 21:03:27 - INFO - __main__ -   test: [batch: 300/391 ] | Loss: 0.974 | Acc: 75.680% (29158/38528)/ 92.660% (35700/38528)
01/15/2023 21:03:27 - INFO - __main__ -   test: [batch: 301/391 ] | Loss: 0.975 | Acc: 75.670% (29251/38656)/ 92.651% (35815/38656)
01/15/2023 21:03:28 - INFO - __main__ -   test: [batch: 302/391 ] | Loss: 0.977 | Acc: 75.629% (29332/38784)/ 92.634% (35927/38784)
01/15/2023 21:03:28 - INFO - __main__ -   test: [batch: 303/391 ] | Loss: 0.978 | Acc: 75.596% (29416/38912)/ 92.624% (36042/38912)
01/15/2023 21:03:29 - INFO - __main__ -   test: [batch: 304/391 ] | Loss: 0.977 | Acc: 75.607% (29517/39040)/ 92.626% (36161/39040)
01/15/2023 21:03:30 - INFO - __main__ -   test: [batch: 305/391 ] | Loss: 0.978 | Acc: 75.569% (29599/39168)/ 92.619% (36277/39168)
01/15/2023 21:03:31 - INFO - __main__ -   test: [batch: 306/391 ] | Loss: 0.979 | Acc: 75.560% (29692/39296)/ 92.597% (36387/39296)
01/15/2023 21:03:31 - INFO - __main__ -   test: [batch: 307/391 ] | Loss: 0.980 | Acc: 75.548% (29784/39424)/ 92.586% (36501/39424)
01/15/2023 21:03:32 - INFO - __main__ -   test: [batch: 308/391 ] | Loss: 0.981 | Acc: 75.536% (29876/39552)/ 92.577% (36616/39552)
01/15/2023 21:03:32 - INFO - __main__ -   test: [batch: 309/391 ] | Loss: 0.982 | Acc: 75.524% (29968/39680)/ 92.555% (36726/39680)
01/15/2023 21:03:33 - INFO - __main__ -   test: [batch: 310/391 ] | Loss: 0.982 | Acc: 75.515% (30061/39808)/ 92.539% (36838/39808)
01/15/2023 21:03:34 - INFO - __main__ -   test: [batch: 311/391 ] | Loss: 0.984 | Acc: 75.491% (30148/39936)/ 92.521% (36949/39936)
01/15/2023 21:03:34 - INFO - __main__ -   test: [batch: 312/391 ] | Loss: 0.985 | Acc: 75.477% (30239/40064)/ 92.502% (37060/40064)
01/15/2023 21:03:35 - INFO - __main__ -   test: [batch: 313/391 ] | Loss: 0.983 | Acc: 75.525% (30355/40192)/ 92.526% (37188/40192)
01/15/2023 21:03:36 - INFO - __main__ -   test: [batch: 314/391 ] | Loss: 0.984 | Acc: 75.508% (30445/40320)/ 92.517% (37303/40320)
01/15/2023 21:03:36 - INFO - __main__ -   test: [batch: 315/391 ] | Loss: 0.984 | Acc: 75.492% (30535/40448)/ 92.504% (37416/40448)
01/15/2023 21:03:37 - INFO - __main__ -   test: [batch: 316/391 ] | Loss: 0.987 | Acc: 75.404% (30596/40576)/ 92.476% (37523/40576)
01/15/2023 21:03:37 - INFO - __main__ -   test: [batch: 317/391 ] | Loss: 0.989 | Acc: 75.364% (30676/40704)/ 92.445% (37629/40704)
01/15/2023 21:03:38 - INFO - __main__ -   test: [batch: 318/391 ] | Loss: 0.988 | Acc: 75.397% (30786/40832)/ 92.464% (37755/40832)
01/15/2023 21:03:38 - INFO - __main__ -   test: [batch: 319/391 ] | Loss: 0.990 | Acc: 75.342% (30860/40960)/ 92.432% (37860/40960)
01/15/2023 21:03:39 - INFO - __main__ -   test: [batch: 320/391 ] | Loss: 0.989 | Acc: 75.377% (30971/41088)/ 92.441% (37982/41088)
01/15/2023 21:03:40 - INFO - __main__ -   test: [batch: 321/391 ] | Loss: 0.989 | Acc: 75.393% (31074/41216)/ 92.435% (38098/41216)
01/15/2023 21:03:41 - INFO - __main__ -   test: [batch: 322/391 ] | Loss: 0.991 | Acc: 75.353% (31154/41344)/ 92.425% (38212/41344)
01/15/2023 21:03:41 - INFO - __main__ -   test: [batch: 323/391 ] | Loss: 0.993 | Acc: 75.323% (31238/41472)/ 92.402% (38321/41472)
01/15/2023 21:03:42 - INFO - __main__ -   test: [batch: 324/391 ] | Loss: 0.993 | Acc: 75.325% (31335/41600)/ 92.397% (38437/41600)
01/15/2023 21:03:42 - INFO - __main__ -   test: [batch: 325/391 ] | Loss: 0.993 | Acc: 75.326% (31432/41728)/ 92.401% (38557/41728)
01/15/2023 21:03:43 - INFO - __main__ -   test: [batch: 326/391 ] | Loss: 0.996 | Acc: 75.244% (31494/41856)/ 92.357% (38657/41856)
01/15/2023 21:03:44 - INFO - __main__ -   test: [batch: 327/391 ] | Loss: 1.000 | Acc: 75.155% (31553/41984)/ 92.311% (38756/41984)
01/15/2023 21:03:44 - INFO - __main__ -   test: [batch: 328/391 ] | Loss: 1.002 | Acc: 75.107% (31629/42112)/ 92.278% (38860/42112)
01/15/2023 21:03:45 - INFO - __main__ -   test: [batch: 329/391 ] | Loss: 1.002 | Acc: 75.104% (31724/42240)/ 92.282% (38980/42240)
01/15/2023 21:03:45 - INFO - __main__ -   test: [batch: 330/391 ] | Loss: 1.003 | Acc: 75.068% (31805/42368)/ 92.258% (39088/42368)
01/15/2023 21:03:46 - INFO - __main__ -   test: [batch: 331/391 ] | Loss: 1.003 | Acc: 75.042% (31890/42496)/ 92.275% (39213/42496)
01/15/2023 21:03:47 - INFO - __main__ -   test: [batch: 332/391 ] | Loss: 1.004 | Acc: 75.023% (31978/42624)/ 92.270% (39329/42624)
01/15/2023 21:03:47 - INFO - __main__ -   test: [batch: 333/391 ] | Loss: 1.003 | Acc: 75.056% (32088/42752)/ 92.279% (39451/42752)
01/15/2023 21:03:48 - INFO - __main__ -   test: [batch: 334/391 ] | Loss: 1.004 | Acc: 75.028% (32172/42880)/ 92.269% (39565/42880)
01/15/2023 21:03:49 - INFO - __main__ -   test: [batch: 335/391 ] | Loss: 1.005 | Acc: 75.005% (32258/43008)/ 92.248% (39674/43008)
01/15/2023 21:03:49 - INFO - __main__ -   test: [batch: 336/391 ] | Loss: 1.007 | Acc: 74.961% (32335/43136)/ 92.229% (39784/43136)
01/15/2023 21:03:50 - INFO - __main__ -   test: [batch: 337/391 ] | Loss: 1.007 | Acc: 74.958% (32430/43264)/ 92.220% (39898/43264)
01/15/2023 21:03:51 - INFO - __main__ -   test: [batch: 338/391 ] | Loss: 1.007 | Acc: 74.954% (32524/43392)/ 92.224% (40018/43392)
01/15/2023 21:03:51 - INFO - __main__ -   test: [batch: 339/391 ] | Loss: 1.010 | Acc: 74.917% (32604/43520)/ 92.190% (40121/43520)
01/15/2023 21:03:52 - INFO - __main__ -   test: [batch: 340/391 ] | Loss: 1.009 | Acc: 74.915% (32699/43648)/ 92.201% (40244/43648)
01/15/2023 21:03:53 - INFO - __main__ -   test: [batch: 341/391 ] | Loss: 1.008 | Acc: 74.957% (32813/43776)/ 92.219% (40370/43776)
01/15/2023 21:03:53 - INFO - __main__ -   test: [batch: 342/391 ] | Loss: 1.008 | Acc: 74.907% (32887/43904)/ 92.208% (40483/43904)
01/15/2023 21:03:54 - INFO - __main__ -   test: [batch: 343/391 ] | Loss: 1.008 | Acc: 74.911% (32985/44032)/ 92.210% (40602/44032)
01/15/2023 21:03:55 - INFO - __main__ -   test: [batch: 344/391 ] | Loss: 1.009 | Acc: 74.912% (33081/44160)/ 92.199% (40715/44160)
01/15/2023 21:03:55 - INFO - __main__ -   test: [batch: 345/391 ] | Loss: 1.012 | Acc: 74.842% (33146/44288)/ 92.167% (40819/44288)
01/15/2023 21:03:56 - INFO - __main__ -   test: [batch: 346/391 ] | Loss: 1.014 | Acc: 74.827% (33235/44416)/ 92.158% (40933/44416)
01/15/2023 21:03:57 - INFO - __main__ -   test: [batch: 347/391 ] | Loss: 1.014 | Acc: 74.838% (33336/44544)/ 92.161% (41052/44544)
01/15/2023 21:03:57 - INFO - __main__ -   test: [batch: 348/391 ] | Loss: 1.015 | Acc: 74.816% (33422/44672)/ 92.138% (41160/44672)
01/15/2023 21:03:58 - INFO - __main__ -   test: [batch: 349/391 ] | Loss: 1.014 | Acc: 74.817% (33518/44800)/ 92.145% (41281/44800)
01/15/2023 21:03:59 - INFO - __main__ -   test: [batch: 350/391 ] | Loss: 1.014 | Acc: 74.824% (33617/44928)/ 92.141% (41397/44928)
01/15/2023 21:03:59 - INFO - __main__ -   test: [batch: 351/391 ] | Loss: 1.017 | Acc: 74.771% (33689/45056)/ 92.121% (41506/45056)
01/15/2023 21:04:00 - INFO - __main__ -   test: [batch: 352/391 ] | Loss: 1.017 | Acc: 74.765% (33782/45184)/ 92.114% (41621/45184)
01/15/2023 21:04:00 - INFO - __main__ -   test: [batch: 353/391 ] | Loss: 1.020 | Acc: 74.718% (33856/45312)/ 92.077% (41722/45312)
01/15/2023 21:04:01 - INFO - __main__ -   test: [batch: 354/391 ] | Loss: 1.022 | Acc: 74.650% (33921/45440)/ 92.060% (41832/45440)
01/15/2023 21:04:02 - INFO - __main__ -   test: [batch: 355/391 ] | Loss: 1.025 | Acc: 74.583% (33986/45568)/ 92.047% (41944/45568)
01/15/2023 21:04:03 - INFO - __main__ -   test: [batch: 356/391 ] | Loss: 1.025 | Acc: 74.580% (34080/45696)/ 92.054% (42065/45696)
01/15/2023 21:04:03 - INFO - __main__ -   test: [batch: 357/391 ] | Loss: 1.023 | Acc: 74.618% (34193/45824)/ 92.067% (42189/45824)
01/15/2023 21:04:04 - INFO - __main__ -   test: [batch: 358/391 ] | Loss: 1.023 | Acc: 74.639% (34298/45952)/ 92.070% (42308/45952)
01/15/2023 21:04:04 - INFO - __main__ -   test: [batch: 359/391 ] | Loss: 1.023 | Acc: 74.642% (34395/46080)/ 92.064% (42423/46080)
01/15/2023 21:04:05 - INFO - __main__ -   test: [batch: 360/391 ] | Loss: 1.025 | Acc: 74.606% (34474/46208)/ 92.058% (42538/46208)
01/15/2023 21:04:06 - INFO - __main__ -   test: [batch: 361/391 ] | Loss: 1.025 | Acc: 74.601% (34567/46336)/ 92.060% (42657/46336)
01/15/2023 21:04:06 - INFO - __main__ -   test: [batch: 362/391 ] | Loss: 1.024 | Acc: 74.598% (34661/46464)/ 92.071% (42780/46464)
01/15/2023 21:04:07 - INFO - __main__ -   test: [batch: 363/391 ] | Loss: 1.025 | Acc: 74.588% (34752/46592)/ 92.061% (42893/46592)
01/15/2023 21:04:08 - INFO - __main__ -   test: [batch: 364/391 ] | Loss: 1.024 | Acc: 74.610% (34858/46720)/ 92.074% (43017/46720)
01/15/2023 21:04:08 - INFO - __main__ -   test: [batch: 365/391 ] | Loss: 1.024 | Acc: 74.616% (34956/46848)/ 92.072% (43134/46848)
01/15/2023 21:04:09 - INFO - __main__ -   test: [batch: 366/391 ] | Loss: 1.022 | Acc: 74.653% (35069/46976)/ 92.090% (43260/46976)
01/15/2023 21:04:09 - INFO - __main__ -   test: [batch: 367/391 ] | Loss: 1.021 | Acc: 74.662% (35169/47104)/ 92.098% (43382/47104)
01/15/2023 21:04:10 - INFO - __main__ -   test: [batch: 368/391 ] | Loss: 1.022 | Acc: 74.657% (35262/47232)/ 92.103% (43502/47232)
01/15/2023 21:04:11 - INFO - __main__ -   test: [batch: 369/391 ] | Loss: 1.021 | Acc: 74.677% (35367/47360)/ 92.111% (43624/47360)
01/15/2023 21:04:11 - INFO - __main__ -   test: [batch: 370/391 ] | Loss: 1.021 | Acc: 74.655% (35452/47488)/ 92.116% (43744/47488)
01/15/2023 21:04:12 - INFO - __main__ -   test: [batch: 371/391 ] | Loss: 1.021 | Acc: 74.660% (35550/47616)/ 92.116% (43862/47616)
01/15/2023 21:04:13 - INFO - __main__ -   test: [batch: 372/391 ] | Loss: 1.019 | Acc: 74.707% (35668/47744)/ 92.135% (43989/47744)
01/15/2023 21:04:13 - INFO - __main__ -   test: [batch: 373/391 ] | Loss: 1.018 | Acc: 74.743% (35781/47872)/ 92.148% (44113/47872)
01/15/2023 21:04:14 - INFO - __main__ -   test: [batch: 374/391 ] | Loss: 1.017 | Acc: 74.771% (35890/48000)/ 92.152% (44233/48000)
01/15/2023 21:04:15 - INFO - __main__ -   test: [batch: 375/391 ] | Loss: 1.020 | Acc: 74.699% (35951/48128)/ 92.121% (44336/48128)
01/15/2023 21:04:15 - INFO - __main__ -   test: [batch: 376/391 ] | Loss: 1.021 | Acc: 74.687% (36041/48256)/ 92.107% (44447/48256)
01/15/2023 21:04:16 - INFO - __main__ -   test: [batch: 377/391 ] | Loss: 1.021 | Acc: 74.680% (36133/48384)/ 92.097% (44560/48384)
01/15/2023 21:04:17 - INFO - __main__ -   test: [batch: 378/391 ] | Loss: 1.025 | Acc: 74.608% (36194/48512)/ 92.056% (44658/48512)
01/15/2023 21:04:17 - INFO - __main__ -   test: [batch: 379/391 ] | Loss: 1.025 | Acc: 74.597% (36284/48640)/ 92.070% (44783/48640)
01/15/2023 21:04:18 - INFO - __main__ -   test: [batch: 380/391 ] | Loss: 1.024 | Acc: 74.604% (36383/48768)/ 92.083% (44907/48768)
01/15/2023 21:04:19 - INFO - __main__ -   test: [batch: 381/391 ] | Loss: 1.026 | Acc: 74.556% (36455/48896)/ 92.079% (45023/48896)
01/15/2023 21:04:19 - INFO - __main__ -   test: [batch: 382/391 ] | Loss: 1.027 | Acc: 74.527% (36536/49024)/ 92.069% (45136/49024)
01/15/2023 21:04:20 - INFO - __main__ -   test: [batch: 383/391 ] | Loss: 1.027 | Acc: 74.530% (36633/49152)/ 92.063% (45251/49152)
01/15/2023 21:04:20 - INFO - __main__ -   test: [batch: 384/391 ] | Loss: 1.026 | Acc: 74.568% (36747/49280)/ 92.078% (45376/49280)
01/15/2023 21:04:21 - INFO - __main__ -   test: [batch: 385/391 ] | Loss: 1.024 | Acc: 74.577% (36847/49408)/ 92.088% (45499/49408)
01/15/2023 21:04:22 - INFO - __main__ -   test: [batch: 386/391 ] | Loss: 1.022 | Acc: 74.616% (36962/49536)/ 92.105% (45625/49536)
01/15/2023 21:04:22 - INFO - __main__ -   test: [batch: 387/391 ] | Loss: 1.021 | Acc: 74.660% (37079/49664)/ 92.115% (45748/49664)
01/15/2023 21:04:23 - INFO - __main__ -   test: [batch: 388/391 ] | Loss: 1.019 | Acc: 74.711% (37200/49792)/ 92.129% (45873/49792)
01/15/2023 21:04:23 - INFO - __main__ -   test: [batch: 389/391 ] | Loss: 1.018 | Acc: 74.714% (37297/49920)/ 92.139% (45996/49920)
01/15/2023 21:04:24 - INFO - __main__ -   test: [batch: 390/391 ] | Loss: 1.020 | Acc: 74.670% (37335/50000)/ 92.128% (46064/50000)
01/15/2023 21:04:24 - INFO - __main__ -   Final accuracy: 74.670
