/home/jjc/miniconda3/envs/ant_quant/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
Namespace(a_low=75, a_up=150, abit=8, batch_size=128, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='csd_eb3', epoch=3, layer_4bit_l=None, layer_8bit_l='25,47,46,35,29,38,46,5,8,15', layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='resnet50', percent=100, ptq=False, resume=False, search=False, tag='', train=True, w_low=75, w_up=150, wbit=8)
01/13/2023 15:44:40 - INFO - __main__ -   output/resnet50_imagenet/int_W8A8_32860/gpu_0
01/13/2023 15:44:40 - INFO - __main__ -   Namespace(a_low=75, a_up=150, abit=8, batch_size=128, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='csd_eb3', epoch=3, layer_4bit_l=None, layer_8bit_l='25,47,46,35,29,38,46,5,8,15', layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='resnet50', percent=100, ptq=False, resume=False, search=False, tag='', train=True, w_low=75, w_up=150, wbit=8)
01/13/2023 15:44:40 - INFO - __main__ -   ==> Preparing data..
01/13/2023 15:44:43 - INFO - __main__ -   ==> Setting quantizer..
Namespace(a_low=75, a_up=150, abit=8, batch_size=128, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='csd_eb3', epoch=3, layer_4bit_l=None, layer_8bit_l='25,47,46,35,29,38,46,5,8,15', layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='resnet50', percent=100, ptq=False, resume=False, search=False, tag='', train=True, w_low=75, w_up=150, wbit=8)
01/13/2023 15:44:43 - INFO - __main__ -   Namespace(a_low=75, a_up=150, abit=8, batch_size=128, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='csd_eb3', epoch=3, layer_4bit_l=None, layer_8bit_l='25,47,46,35,29,38,46,5,8,15', layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='resnet50', percent=100, ptq=False, resume=False, search=False, tag='', train=True, w_low=75, w_up=150, wbit=8)
01/13/2023 15:44:43 - INFO - __main__ -   ==> Building model..
ResNet(
  (conv1): Conv2dQuantizer(
    (quant_weight): TensorQuantizer()
    (quant_input): TensorQuantizer()
  )
  (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (1): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (1): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): LinearQuantizer(
    (quant_weight): TensorQuantizer()
    (quant_input): TensorQuantizer()
  )
)
01/13/2023 15:44:47 - INFO - __main__ -   {'milestones': Counter({2: 1, 3: 1, 4: 1}), 'gamma': 0.1, 'base_lrs': [0.00025], 'last_epoch': 0, '_step_count': 1, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.00025]}
01/13/2023 15:44:47 - INFO - __main__ -   
Epoch: 0
Layer quant EB csd_eb3
int	8-bit 	 conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.3.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.3.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.3.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.3.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.3.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.3.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.4.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.4.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.4.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.5.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.5.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.5.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 fc.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 fc.quant_input,
set init to 1
------------- 8-bit EB2 Re-SET -------------
10
layer1.1.conv1.quant_weight 5
layer1.1.conv1.quant_input 5
layer1.2.conv1.quant_weight 8
layer1.2.conv1.quant_input 8
layer2.1.conv1.quant_weight 15
layer2.1.conv1.quant_input 15
layer3.0.conv2.quant_weight 25
layer3.0.conv2.quant_input 25
layer3.1.conv2.quant_weight 29
layer3.1.conv2.quant_input 29
layer3.3.conv2.quant_weight 35
layer3.3.conv2.quant_input 35
layer3.4.conv2.quant_weight 38
layer3.4.conv2.quant_input 38
layer4.0.downsample.0.quant_weight 46
layer4.0.downsample.0.quant_input 46
layer4.1.conv1.quant_weight 47
layer4.1.conv1.quant_input 47
------------- 8-bit EB2 Re-SET -------------
Layer quant EB csd_eb3
int	8-bit 	 conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.3.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.3.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.3.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.3.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.3.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.3.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.4.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.4.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.4.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.5.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.5.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.5.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 fc.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 fc.quant_input,
set init to 1
01/13/2023 15:45:30 - INFO - __main__ -   test: [epoch: 0 | batch: 0/10010 ] | Loss: 0.672 | Acc: 82.031% (105/128)
01/13/2023 15:49:19 - INFO - __main__ -   test: [epoch: 0 | batch: 100/10010 ] | Loss: 0.841 | Acc: 79.316% (10254/12928)
01/13/2023 15:53:40 - INFO - __main__ -   test: [epoch: 0 | batch: 200/10010 ] | Loss: 0.825 | Acc: 79.392% (20426/25728)
01/13/2023 15:58:00 - INFO - __main__ -   test: [epoch: 0 | batch: 300/10010 ] | Loss: 0.839 | Acc: 79.078% (30467/38528)
01/13/2023 16:02:20 - INFO - __main__ -   test: [epoch: 0 | batch: 400/10010 ] | Loss: 0.850 | Acc: 78.793% (40443/51328)
01/13/2023 16:06:39 - INFO - __main__ -   test: [epoch: 0 | batch: 500/10010 ] | Loss: 0.851 | Acc: 78.671% (50450/64128)
01/13/2023 16:11:02 - INFO - __main__ -   test: [epoch: 0 | batch: 600/10010 ] | Loss: 0.852 | Acc: 78.638% (60495/76928)
01/13/2023 16:15:23 - INFO - __main__ -   test: [epoch: 0 | batch: 700/10010 ] | Loss: 0.853 | Acc: 78.655% (70576/89728)
01/13/2023 16:19:43 - INFO - __main__ -   test: [epoch: 0 | batch: 800/10010 ] | Loss: 0.854 | Acc: 78.617% (80604/102528)
01/13/2023 16:24:04 - INFO - __main__ -   test: [epoch: 0 | batch: 900/10010 ] | Loss: 0.855 | Acc: 78.600% (90648/115328)
01/13/2023 16:28:24 - INFO - __main__ -   test: [epoch: 0 | batch: 1000/10010 ] | Loss: 0.856 | Acc: 78.584% (100688/128128)
01/13/2023 16:32:43 - INFO - __main__ -   test: [epoch: 0 | batch: 1100/10010 ] | Loss: 0.858 | Acc: 78.588% (110752/140928)
01/13/2023 16:37:04 - INFO - __main__ -   test: [epoch: 0 | batch: 1200/10010 ] | Loss: 0.859 | Acc: 78.563% (120774/153728)
01/13/2023 16:41:23 - INFO - __main__ -   test: [epoch: 0 | batch: 1300/10010 ] | Loss: 0.858 | Acc: 78.616% (130917/166528)
01/13/2023 16:45:44 - INFO - __main__ -   test: [epoch: 0 | batch: 1400/10010 ] | Loss: 0.858 | Acc: 78.631% (141007/179328)
01/13/2023 16:50:03 - INFO - __main__ -   test: [epoch: 0 | batch: 1500/10010 ] | Loss: 0.856 | Acc: 78.685% (151176/192128)
01/13/2023 16:54:21 - INFO - __main__ -   test: [epoch: 0 | batch: 1600/10010 ] | Loss: 0.857 | Acc: 78.673% (161223/204928)
01/13/2023 16:58:44 - INFO - __main__ -   test: [epoch: 0 | batch: 1700/10010 ] | Loss: 0.857 | Acc: 78.666% (171278/217728)
01/13/2023 17:03:03 - INFO - __main__ -   test: [epoch: 0 | batch: 1800/10010 ] | Loss: 0.856 | Acc: 78.711% (181451/230528)
01/13/2023 17:07:25 - INFO - __main__ -   test: [epoch: 0 | batch: 1900/10010 ] | Loss: 0.854 | Acc: 78.717% (191541/243328)
01/13/2023 17:11:46 - INFO - __main__ -   test: [epoch: 0 | batch: 2000/10010 ] | Loss: 0.854 | Acc: 78.731% (201652/256128)
01/13/2023 17:16:06 - INFO - __main__ -   test: [epoch: 0 | batch: 2100/10010 ] | Loss: 0.855 | Acc: 78.715% (211686/268928)
01/13/2023 17:20:27 - INFO - __main__ -   test: [epoch: 0 | batch: 2200/10010 ] | Loss: 0.855 | Acc: 78.703% (221729/281728)
01/13/2023 17:24:46 - INFO - __main__ -   test: [epoch: 0 | batch: 2300/10010 ] | Loss: 0.855 | Acc: 78.694% (231776/294528)
01/13/2023 17:29:05 - INFO - __main__ -   test: [epoch: 0 | batch: 2400/10010 ] | Loss: 0.856 | Acc: 78.700% (241868/307328)
01/13/2023 17:33:28 - INFO - __main__ -   test: [epoch: 0 | batch: 2500/10010 ] | Loss: 0.856 | Acc: 78.702% (251948/320128)
01/13/2023 17:37:49 - INFO - __main__ -   test: [epoch: 0 | batch: 2600/10010 ] | Loss: 0.856 | Acc: 78.699% (262012/332928)
01/13/2023 17:42:09 - INFO - __main__ -   test: [epoch: 0 | batch: 2700/10010 ] | Loss: 0.855 | Acc: 78.702% (272095/345728)
01/13/2023 17:46:29 - INFO - __main__ -   test: [epoch: 0 | batch: 2800/10010 ] | Loss: 0.856 | Acc: 78.690% (282126/358528)
01/13/2023 17:50:49 - INFO - __main__ -   test: [epoch: 0 | batch: 2900/10010 ] | Loss: 0.856 | Acc: 78.687% (292187/371328)
01/13/2023 17:55:10 - INFO - __main__ -   test: [epoch: 0 | batch: 3000/10010 ] | Loss: 0.855 | Acc: 78.683% (302244/384128)
01/13/2023 17:59:31 - INFO - __main__ -   test: [epoch: 0 | batch: 3100/10010 ] | Loss: 0.856 | Acc: 78.682% (312312/396928)
01/13/2023 18:03:51 - INFO - __main__ -   test: [epoch: 0 | batch: 3200/10010 ] | Loss: 0.856 | Acc: 78.678% (322365/409728)
01/13/2023 18:08:10 - INFO - __main__ -   test: [epoch: 0 | batch: 3300/10010 ] | Loss: 0.856 | Acc: 78.668% (332396/422528)
01/13/2023 18:12:30 - INFO - __main__ -   test: [epoch: 0 | batch: 3400/10010 ] | Loss: 0.856 | Acc: 78.679% (342513/435328)
01/13/2023 18:16:51 - INFO - __main__ -   test: [epoch: 0 | batch: 3500/10010 ] | Loss: 0.856 | Acc: 78.686% (352615/448128)
01/13/2023 18:21:12 - INFO - __main__ -   test: [epoch: 0 | batch: 3600/10010 ] | Loss: 0.856 | Acc: 78.681% (362663/460928)
01/13/2023 18:25:32 - INFO - __main__ -   test: [epoch: 0 | batch: 3700/10010 ] | Loss: 0.856 | Acc: 78.670% (372682/473728)
01/13/2023 18:29:52 - INFO - __main__ -   test: [epoch: 0 | batch: 3800/10010 ] | Loss: 0.856 | Acc: 78.671% (382757/486528)
01/13/2023 18:34:14 - INFO - __main__ -   test: [epoch: 0 | batch: 3900/10010 ] | Loss: 0.857 | Acc: 78.655% (392748/499328)
01/13/2023 18:38:34 - INFO - __main__ -   test: [epoch: 0 | batch: 4000/10010 ] | Loss: 0.857 | Acc: 78.651% (402794/512128)
01/13/2023 18:42:55 - INFO - __main__ -   test: [epoch: 0 | batch: 4100/10010 ] | Loss: 0.857 | Acc: 78.656% (412885/524928)
01/13/2023 18:47:17 - INFO - __main__ -   test: [epoch: 0 | batch: 4200/10010 ] | Loss: 0.857 | Acc: 78.652% (422936/537728)
01/13/2023 18:51:38 - INFO - __main__ -   test: [epoch: 0 | batch: 4300/10010 ] | Loss: 0.857 | Acc: 78.653% (433008/550528)
01/13/2023 18:55:58 - INFO - __main__ -   test: [epoch: 0 | batch: 4400/10010 ] | Loss: 0.857 | Acc: 78.659% (443110/563328)
01/13/2023 19:00:18 - INFO - __main__ -   test: [epoch: 0 | batch: 4500/10010 ] | Loss: 0.857 | Acc: 78.652% (453135/576128)
01/13/2023 19:04:36 - INFO - __main__ -   test: [epoch: 0 | batch: 4600/10010 ] | Loss: 0.857 | Acc: 78.661% (463258/588928)
01/13/2023 19:08:57 - INFO - __main__ -   test: [epoch: 0 | batch: 4700/10010 ] | Loss: 0.857 | Acc: 78.661% (473328/601728)
01/13/2023 19:13:20 - INFO - __main__ -   test: [epoch: 0 | batch: 4800/10010 ] | Loss: 0.857 | Acc: 78.661% (483395/614528)
01/13/2023 19:17:40 - INFO - __main__ -   test: [epoch: 0 | batch: 4900/10010 ] | Loss: 0.857 | Acc: 78.664% (493479/627328)
01/13/2023 19:22:01 - INFO - __main__ -   test: [epoch: 0 | batch: 5000/10010 ] | Loss: 0.856 | Acc: 78.676% (503628/640128)
01/13/2023 19:26:20 - INFO - __main__ -   test: [epoch: 0 | batch: 5100/10010 ] | Loss: 0.856 | Acc: 78.680% (513723/652928)
01/13/2023 19:30:40 - INFO - __main__ -   test: [epoch: 0 | batch: 5200/10010 ] | Loss: 0.856 | Acc: 78.678% (523782/665728)
01/13/2023 19:34:58 - INFO - __main__ -   test: [epoch: 0 | batch: 5300/10010 ] | Loss: 0.857 | Acc: 78.682% (533879/678528)
01/13/2023 19:39:18 - INFO - __main__ -   test: [epoch: 0 | batch: 5400/10010 ] | Loss: 0.857 | Acc: 78.671% (543878/691328)
01/13/2023 19:43:37 - INFO - __main__ -   test: [epoch: 0 | batch: 5500/10010 ] | Loss: 0.857 | Acc: 78.661% (553872/704128)
01/13/2023 19:47:58 - INFO - __main__ -   test: [epoch: 0 | batch: 5600/10010 ] | Loss: 0.857 | Acc: 78.668% (563990/716928)
01/13/2023 19:52:19 - INFO - __main__ -   test: [epoch: 0 | batch: 5700/10010 ] | Loss: 0.857 | Acc: 78.670% (574075/729728)
01/13/2023 19:56:40 - INFO - __main__ -   test: [epoch: 0 | batch: 5800/10010 ] | Loss: 0.856 | Acc: 78.669% (584137/742528)
01/13/2023 20:01:00 - INFO - __main__ -   test: [epoch: 0 | batch: 5900/10010 ] | Loss: 0.856 | Acc: 78.674% (594243/755328)
01/13/2023 20:05:20 - INFO - __main__ -   test: [epoch: 0 | batch: 6000/10010 ] | Loss: 0.857 | Acc: 78.670% (604287/768128)
01/13/2023 20:09:41 - INFO - __main__ -   test: [epoch: 0 | batch: 6100/10010 ] | Loss: 0.857 | Acc: 78.670% (614357/780928)
01/13/2023 20:14:01 - INFO - __main__ -   test: [epoch: 0 | batch: 6200/10010 ] | Loss: 0.857 | Acc: 78.662% (624363/793728)
01/13/2023 20:18:21 - INFO - __main__ -   test: [epoch: 0 | batch: 6300/10010 ] | Loss: 0.857 | Acc: 78.661% (634426/806528)
01/13/2023 20:22:43 - INFO - __main__ -   test: [epoch: 0 | batch: 6400/10010 ] | Loss: 0.858 | Acc: 78.659% (644473/819328)
01/13/2023 20:27:04 - INFO - __main__ -   test: [epoch: 0 | batch: 6500/10010 ] | Loss: 0.858 | Acc: 78.658% (654533/832128)
01/13/2023 20:31:25 - INFO - __main__ -   test: [epoch: 0 | batch: 6600/10010 ] | Loss: 0.858 | Acc: 78.656% (664590/844928)
01/13/2023 20:35:46 - INFO - __main__ -   test: [epoch: 0 | batch: 6700/10010 ] | Loss: 0.857 | Acc: 78.660% (674690/857728)
01/13/2023 20:40:08 - INFO - __main__ -   test: [epoch: 0 | batch: 6800/10010 ] | Loss: 0.858 | Acc: 78.655% (684718/870528)
01/13/2023 20:44:29 - INFO - __main__ -   test: [epoch: 0 | batch: 6900/10010 ] | Loss: 0.857 | Acc: 78.660% (694827/883328)
01/13/2023 20:48:51 - INFO - __main__ -   test: [epoch: 0 | batch: 7000/10010 ] | Loss: 0.857 | Acc: 78.657% (704867/896128)
01/13/2023 20:53:12 - INFO - __main__ -   test: [epoch: 0 | batch: 7100/10010 ] | Loss: 0.857 | Acc: 78.666% (715021/908928)
01/13/2023 20:57:34 - INFO - __main__ -   test: [epoch: 0 | batch: 7200/10010 ] | Loss: 0.857 | Acc: 78.675% (725165/921728)
01/13/2023 21:01:54 - INFO - __main__ -   test: [epoch: 0 | batch: 7300/10010 ] | Loss: 0.857 | Acc: 78.665% (735151/934528)
01/13/2023 21:06:15 - INFO - __main__ -   test: [epoch: 0 | batch: 7400/10010 ] | Loss: 0.857 | Acc: 78.661% (745181/947328)
01/13/2023 21:10:36 - INFO - __main__ -   test: [epoch: 0 | batch: 7500/10010 ] | Loss: 0.857 | Acc: 78.664% (755278/960128)
01/13/2023 21:14:56 - INFO - __main__ -   test: [epoch: 0 | batch: 7600/10010 ] | Loss: 0.857 | Acc: 78.666% (765363/972928)
01/13/2023 21:19:17 - INFO - __main__ -   test: [epoch: 0 | batch: 7700/10010 ] | Loss: 0.858 | Acc: 78.655% (775320/985728)
01/13/2023 21:23:40 - INFO - __main__ -   test: [epoch: 0 | batch: 7800/10010 ] | Loss: 0.858 | Acc: 78.655% (785392/998528)
01/13/2023 21:28:00 - INFO - __main__ -   test: [epoch: 0 | batch: 7900/10010 ] | Loss: 0.858 | Acc: 78.658% (795490/1011328)
01/13/2023 21:32:21 - INFO - __main__ -   test: [epoch: 0 | batch: 8000/10010 ] | Loss: 0.858 | Acc: 78.654% (805521/1024128)
01/13/2023 21:36:41 - INFO - __main__ -   test: [epoch: 0 | batch: 8100/10010 ] | Loss: 0.858 | Acc: 78.659% (815634/1036928)
01/13/2023 21:41:02 - INFO - __main__ -   test: [epoch: 0 | batch: 8200/10010 ] | Loss: 0.858 | Acc: 78.653% (825646/1049728)
01/13/2023 21:45:24 - INFO - __main__ -   test: [epoch: 0 | batch: 8300/10010 ] | Loss: 0.858 | Acc: 78.654% (835724/1062528)
01/13/2023 21:49:47 - INFO - __main__ -   test: [epoch: 0 | batch: 8400/10010 ] | Loss: 0.858 | Acc: 78.656% (845810/1075328)
01/13/2023 21:54:08 - INFO - __main__ -   test: [epoch: 0 | batch: 8500/10010 ] | Loss: 0.857 | Acc: 78.660% (855923/1088128)
01/13/2023 21:58:28 - INFO - __main__ -   test: [epoch: 0 | batch: 8600/10010 ] | Loss: 0.858 | Acc: 78.654% (865923/1100928)
01/13/2023 22:02:49 - INFO - __main__ -   test: [epoch: 0 | batch: 8700/10010 ] | Loss: 0.858 | Acc: 78.653% (875980/1113728)
01/13/2023 22:07:09 - INFO - __main__ -   test: [epoch: 0 | batch: 8800/10010 ] | Loss: 0.858 | Acc: 78.653% (886048/1126528)
01/13/2023 22:11:31 - INFO - __main__ -   test: [epoch: 0 | batch: 8900/10010 ] | Loss: 0.858 | Acc: 78.658% (896167/1139328)
01/13/2023 22:15:51 - INFO - __main__ -   test: [epoch: 0 | batch: 9000/10010 ] | Loss: 0.857 | Acc: 78.661% (906272/1152128)
01/13/2023 22:20:12 - INFO - __main__ -   test: [epoch: 0 | batch: 9100/10010 ] | Loss: 0.857 | Acc: 78.659% (916325/1164928)
01/13/2023 22:24:33 - INFO - __main__ -   test: [epoch: 0 | batch: 9200/10010 ] | Loss: 0.857 | Acc: 78.660% (926402/1177728)
01/13/2023 22:28:53 - INFO - __main__ -   test: [epoch: 0 | batch: 9300/10010 ] | Loss: 0.857 | Acc: 78.664% (936514/1190528)
01/13/2023 22:33:15 - INFO - __main__ -   test: [epoch: 0 | batch: 9400/10010 ] | Loss: 0.857 | Acc: 78.663% (946574/1203328)
01/13/2023 22:37:36 - INFO - __main__ -   test: [epoch: 0 | batch: 9500/10010 ] | Loss: 0.857 | Acc: 78.667% (956696/1216128)
01/13/2023 22:41:57 - INFO - __main__ -   test: [epoch: 0 | batch: 9600/10010 ] | Loss: 0.858 | Acc: 78.667% (966756/1228928)
01/13/2023 22:46:16 - INFO - __main__ -   test: [epoch: 0 | batch: 9700/10010 ] | Loss: 0.858 | Acc: 78.664% (976796/1241728)
01/13/2023 22:50:37 - INFO - __main__ -   test: [epoch: 0 | batch: 9800/10010 ] | Loss: 0.858 | Acc: 78.660% (986814/1254528)
01/13/2023 22:55:01 - INFO - __main__ -   test: [epoch: 0 | batch: 9900/10010 ] | Loss: 0.858 | Acc: 78.664% (996931/1267328)
01/13/2023 22:59:22 - INFO - __main__ -   test: [epoch: 0 | batch: 10000/10010 ] | Loss: 0.858 | Acc: 78.661% (1006960/1280128)
01/13/2023 22:59:46 - INFO - __main__ -   Saving Checkpoint
01/13/2023 22:59:49 - INFO - __main__ -   test: [batch: 0/391 ] | Loss: 0.458 | Acc: 86.719% (111/128)/ 97.656% (125/128)
01/13/2023 22:59:51 - INFO - __main__ -   test: [batch: 1/391 ] | Loss: 0.470 | Acc: 86.328% (221/256)/ 98.047% (251/256)
01/13/2023 22:59:54 - INFO - __main__ -   test: [batch: 2/391 ] | Loss: 0.618 | Acc: 82.552% (317/384)/ 95.833% (368/384)
01/13/2023 22:59:57 - INFO - __main__ -   test: [batch: 3/391 ] | Loss: 0.571 | Acc: 84.375% (432/512)/ 96.484% (494/512)
01/13/2023 22:59:59 - INFO - __main__ -   test: [batch: 4/391 ] | Loss: 0.487 | Acc: 86.719% (555/640)/ 97.188% (622/640)
01/13/2023 23:00:02 - INFO - __main__ -   test: [batch: 5/391 ] | Loss: 0.431 | Acc: 87.891% (675/768)/ 97.656% (750/768)
01/13/2023 23:00:05 - INFO - __main__ -   test: [batch: 6/391 ] | Loss: 0.425 | Acc: 88.058% (789/896)/ 97.545% (874/896)
01/13/2023 23:00:07 - INFO - __main__ -   test: [batch: 7/391 ] | Loss: 0.407 | Acc: 89.062% (912/1024)/ 97.656% (1000/1024)
01/13/2023 23:00:10 - INFO - __main__ -   test: [batch: 8/391 ] | Loss: 0.424 | Acc: 89.149% (1027/1152)/ 97.569% (1124/1152)
01/13/2023 23:00:13 - INFO - __main__ -   test: [batch: 9/391 ] | Loss: 0.404 | Acc: 89.688% (1148/1280)/ 97.656% (1250/1280)
01/13/2023 23:00:15 - INFO - __main__ -   test: [batch: 10/391 ] | Loss: 0.454 | Acc: 88.423% (1245/1408)/ 97.514% (1373/1408)
01/13/2023 23:00:18 - INFO - __main__ -   test: [batch: 11/391 ] | Loss: 0.457 | Acc: 88.607% (1361/1536)/ 97.331% (1495/1536)
01/13/2023 23:00:21 - INFO - __main__ -   test: [batch: 12/391 ] | Loss: 0.506 | Acc: 87.560% (1457/1664)/ 97.055% (1615/1664)
01/13/2023 23:00:23 - INFO - __main__ -   test: [batch: 13/391 ] | Loss: 0.555 | Acc: 86.049% (1542/1792)/ 96.540% (1730/1792)
01/13/2023 23:00:26 - INFO - __main__ -   test: [batch: 14/391 ] | Loss: 0.575 | Acc: 85.260% (1637/1920)/ 96.615% (1855/1920)
01/13/2023 23:00:28 - INFO - __main__ -   test: [batch: 15/391 ] | Loss: 0.583 | Acc: 84.961% (1740/2048)/ 96.729% (1981/2048)
01/13/2023 23:00:31 - INFO - __main__ -   test: [batch: 16/391 ] | Loss: 0.593 | Acc: 84.743% (1844/2176)/ 96.553% (2101/2176)
01/13/2023 23:00:33 - INFO - __main__ -   test: [batch: 17/391 ] | Loss: 0.620 | Acc: 84.288% (1942/2304)/ 96.050% (2213/2304)
01/13/2023 23:00:36 - INFO - __main__ -   test: [batch: 18/391 ] | Loss: 0.642 | Acc: 83.717% (2036/2432)/ 95.888% (2332/2432)
01/13/2023 23:00:38 - INFO - __main__ -   test: [batch: 19/391 ] | Loss: 0.648 | Acc: 83.477% (2137/2560)/ 95.859% (2454/2560)
01/13/2023 23:00:41 - INFO - __main__ -   test: [batch: 20/391 ] | Loss: 0.646 | Acc: 83.557% (2246/2688)/ 95.759% (2574/2688)
01/13/2023 23:00:44 - INFO - __main__ -   test: [batch: 21/391 ] | Loss: 0.678 | Acc: 82.919% (2335/2816)/ 95.597% (2692/2816)
01/13/2023 23:00:46 - INFO - __main__ -   test: [batch: 22/391 ] | Loss: 0.678 | Acc: 82.779% (2437/2944)/ 95.618% (2815/2944)
01/13/2023 23:00:49 - INFO - __main__ -   test: [batch: 23/391 ] | Loss: 0.722 | Acc: 81.836% (2514/3072)/ 95.345% (2929/3072)
01/13/2023 23:00:52 - INFO - __main__ -   test: [batch: 24/391 ] | Loss: 0.740 | Acc: 81.406% (2605/3200)/ 95.188% (3046/3200)
01/13/2023 23:00:54 - INFO - __main__ -   test: [batch: 25/391 ] | Loss: 0.765 | Acc: 80.769% (2688/3328)/ 94.922% (3159/3328)
01/13/2023 23:00:57 - INFO - __main__ -   test: [batch: 26/391 ] | Loss: 0.778 | Acc: 80.093% (2768/3456)/ 94.907% (3280/3456)
01/13/2023 23:00:59 - INFO - __main__ -   test: [batch: 27/391 ] | Loss: 0.760 | Acc: 80.608% (2889/3584)/ 94.950% (3403/3584)
01/13/2023 23:01:02 - INFO - __main__ -   test: [batch: 28/391 ] | Loss: 0.769 | Acc: 80.092% (2973/3712)/ 95.016% (3527/3712)
01/13/2023 23:01:05 - INFO - __main__ -   test: [batch: 29/391 ] | Loss: 0.762 | Acc: 80.234% (3081/3840)/ 95.130% (3653/3840)
01/13/2023 23:01:07 - INFO - __main__ -   test: [batch: 30/391 ] | Loss: 0.776 | Acc: 80.166% (3181/3968)/ 95.010% (3770/3968)
01/13/2023 23:01:10 - INFO - __main__ -   test: [batch: 31/391 ] | Loss: 0.772 | Acc: 80.371% (3292/4096)/ 95.093% (3895/4096)
01/13/2023 23:01:12 - INFO - __main__ -   test: [batch: 32/391 ] | Loss: 0.757 | Acc: 80.658% (3407/4224)/ 95.194% (4021/4224)
01/13/2023 23:01:15 - INFO - __main__ -   test: [batch: 33/391 ] | Loss: 0.751 | Acc: 80.859% (3519/4352)/ 95.244% (4145/4352)
01/13/2023 23:01:18 - INFO - __main__ -   test: [batch: 34/391 ] | Loss: 0.737 | Acc: 81.228% (3639/4480)/ 95.312% (4270/4480)
01/13/2023 23:01:20 - INFO - __main__ -   test: [batch: 35/391 ] | Loss: 0.724 | Acc: 81.597% (3760/4608)/ 95.312% (4392/4608)
01/13/2023 23:01:23 - INFO - __main__ -   test: [batch: 36/391 ] | Loss: 0.709 | Acc: 82.031% (3885/4736)/ 95.418% (4519/4736)
01/13/2023 23:01:25 - INFO - __main__ -   test: [batch: 37/391 ] | Loss: 0.699 | Acc: 82.340% (4005/4864)/ 95.477% (4644/4864)
01/13/2023 23:01:28 - INFO - __main__ -   test: [batch: 38/391 ] | Loss: 0.692 | Acc: 82.472% (4117/4992)/ 95.573% (4771/4992)
01/13/2023 23:01:31 - INFO - __main__ -   test: [batch: 39/391 ] | Loss: 0.687 | Acc: 82.520% (4225/5120)/ 95.605% (4895/5120)
01/13/2023 23:01:33 - INFO - __main__ -   test: [batch: 40/391 ] | Loss: 0.685 | Acc: 82.641% (4337/5248)/ 95.503% (5012/5248)
01/13/2023 23:01:36 - INFO - __main__ -   test: [batch: 41/391 ] | Loss: 0.689 | Acc: 82.682% (4445/5376)/ 95.424% (5130/5376)
01/13/2023 23:01:38 - INFO - __main__ -   test: [batch: 42/391 ] | Loss: 0.689 | Acc: 82.613% (4547/5504)/ 95.476% (5255/5504)
01/13/2023 23:01:41 - INFO - __main__ -   test: [batch: 43/391 ] | Loss: 0.686 | Acc: 82.688% (4657/5632)/ 95.455% (5376/5632)
01/13/2023 23:01:44 - INFO - __main__ -   test: [batch: 44/391 ] | Loss: 0.690 | Acc: 82.691% (4763/5760)/ 95.365% (5493/5760)
01/13/2023 23:01:46 - INFO - __main__ -   test: [batch: 45/391 ] | Loss: 0.687 | Acc: 82.846% (4878/5888)/ 95.363% (5615/5888)
01/13/2023 23:01:49 - INFO - __main__ -   test: [batch: 46/391 ] | Loss: 0.690 | Acc: 82.812% (4982/6016)/ 95.379% (5738/6016)
01/13/2023 23:01:52 - INFO - __main__ -   test: [batch: 47/391 ] | Loss: 0.691 | Acc: 82.764% (5085/6144)/ 95.443% (5864/6144)
01/13/2023 23:01:54 - INFO - __main__ -   test: [batch: 48/391 ] | Loss: 0.697 | Acc: 82.573% (5179/6272)/ 95.424% (5985/6272)
01/13/2023 23:01:57 - INFO - __main__ -   test: [batch: 49/391 ] | Loss: 0.701 | Acc: 82.609% (5287/6400)/ 95.344% (6102/6400)
01/13/2023 23:01:59 - INFO - __main__ -   test: [batch: 50/391 ] | Loss: 0.692 | Acc: 82.812% (5406/6528)/ 95.404% (6228/6528)
01/13/2023 23:02:02 - INFO - __main__ -   test: [batch: 51/391 ] | Loss: 0.684 | Acc: 83.023% (5526/6656)/ 95.493% (6356/6656)
01/13/2023 23:02:05 - INFO - __main__ -   test: [batch: 52/391 ] | Loss: 0.681 | Acc: 83.107% (5638/6784)/ 95.519% (6480/6784)
01/13/2023 23:02:07 - INFO - __main__ -   test: [batch: 53/391 ] | Loss: 0.672 | Acc: 83.319% (5759/6912)/ 95.587% (6607/6912)
01/13/2023 23:02:10 - INFO - __main__ -   test: [batch: 54/391 ] | Loss: 0.665 | Acc: 83.452% (5875/7040)/ 95.625% (6732/7040)
01/13/2023 23:02:13 - INFO - __main__ -   test: [batch: 55/391 ] | Loss: 0.659 | Acc: 83.594% (5992/7168)/ 95.661% (6857/7168)
01/13/2023 23:02:15 - INFO - __main__ -   test: [batch: 56/391 ] | Loss: 0.651 | Acc: 83.799% (6114/7296)/ 95.710% (6983/7296)
01/13/2023 23:02:18 - INFO - __main__ -   test: [batch: 57/391 ] | Loss: 0.644 | Acc: 83.998% (6236/7424)/ 95.757% (7109/7424)
01/13/2023 23:02:21 - INFO - __main__ -   test: [batch: 58/391 ] | Loss: 0.642 | Acc: 84.057% (6348/7552)/ 95.749% (7231/7552)
01/13/2023 23:02:24 - INFO - __main__ -   test: [batch: 59/391 ] | Loss: 0.644 | Acc: 83.971% (6449/7680)/ 95.768% (7355/7680)
01/13/2023 23:02:26 - INFO - __main__ -   test: [batch: 60/391 ] | Loss: 0.649 | Acc: 83.863% (6548/7808)/ 95.735% (7475/7808)
01/13/2023 23:02:29 - INFO - __main__ -   test: [batch: 61/391 ] | Loss: 0.649 | Acc: 83.871% (6656/7936)/ 95.754% (7599/7936)
01/13/2023 23:02:31 - INFO - __main__ -   test: [batch: 62/391 ] | Loss: 0.648 | Acc: 83.829% (6760/8064)/ 95.771% (7723/8064)
01/13/2023 23:02:34 - INFO - __main__ -   test: [batch: 63/391 ] | Loss: 0.657 | Acc: 83.691% (6856/8192)/ 95.703% (7840/8192)
01/13/2023 23:02:36 - INFO - __main__ -   test: [batch: 64/391 ] | Loss: 0.665 | Acc: 83.486% (6946/8320)/ 95.637% (7957/8320)
01/13/2023 23:02:39 - INFO - __main__ -   test: [batch: 65/391 ] | Loss: 0.677 | Acc: 82.990% (7011/8448)/ 95.561% (8073/8448)
01/13/2023 23:02:41 - INFO - __main__ -   test: [batch: 66/391 ] | Loss: 0.683 | Acc: 82.976% (7116/8576)/ 95.522% (8192/8576)
01/13/2023 23:02:44 - INFO - __main__ -   test: [batch: 67/391 ] | Loss: 0.685 | Acc: 82.904% (7216/8704)/ 95.531% (8315/8704)
01/13/2023 23:02:47 - INFO - __main__ -   test: [batch: 68/391 ] | Loss: 0.686 | Acc: 82.858% (7318/8832)/ 95.550% (8439/8832)
01/13/2023 23:02:49 - INFO - __main__ -   test: [batch: 69/391 ] | Loss: 0.681 | Acc: 82.935% (7431/8960)/ 95.592% (8565/8960)
01/13/2023 23:02:52 - INFO - __main__ -   test: [batch: 70/391 ] | Loss: 0.683 | Acc: 82.868% (7531/9088)/ 95.599% (8688/9088)
01/13/2023 23:02:54 - INFO - __main__ -   test: [batch: 71/391 ] | Loss: 0.681 | Acc: 82.899% (7640/9216)/ 95.605% (8811/9216)
01/13/2023 23:02:57 - INFO - __main__ -   test: [batch: 72/391 ] | Loss: 0.683 | Acc: 82.791% (7736/9344)/ 95.623% (8935/9344)
01/13/2023 23:02:59 - INFO - __main__ -   test: [batch: 73/391 ] | Loss: 0.687 | Acc: 82.665% (7830/9472)/ 95.629% (9058/9472)
01/13/2023 23:03:02 - INFO - __main__ -   test: [batch: 74/391 ] | Loss: 0.688 | Acc: 82.646% (7934/9600)/ 95.615% (9179/9600)
01/13/2023 23:03:05 - INFO - __main__ -   test: [batch: 75/391 ] | Loss: 0.693 | Acc: 82.463% (8022/9728)/ 95.590% (9299/9728)
01/13/2023 23:03:07 - INFO - __main__ -   test: [batch: 76/391 ] | Loss: 0.693 | Acc: 82.447% (8126/9856)/ 95.586% (9421/9856)
01/13/2023 23:03:10 - INFO - __main__ -   test: [batch: 77/391 ] | Loss: 0.693 | Acc: 82.412% (8228/9984)/ 95.613% (9546/9984)
01/13/2023 23:03:12 - INFO - __main__ -   test: [batch: 78/391 ] | Loss: 0.693 | Acc: 82.318% (8324/10112)/ 95.659% (9673/10112)
01/13/2023 23:03:15 - INFO - __main__ -   test: [batch: 79/391 ] | Loss: 0.692 | Acc: 82.314% (8429/10240)/ 95.674% (9797/10240)
01/13/2023 23:03:17 - INFO - __main__ -   test: [batch: 80/391 ] | Loss: 0.691 | Acc: 82.301% (8533/10368)/ 95.679% (9920/10368)
01/13/2023 23:03:20 - INFO - __main__ -   test: [batch: 81/391 ] | Loss: 0.690 | Acc: 82.308% (8639/10496)/ 95.703% (10045/10496)
01/13/2023 23:03:23 - INFO - __main__ -   test: [batch: 82/391 ] | Loss: 0.690 | Acc: 82.314% (8745/10624)/ 95.698% (10167/10624)
01/13/2023 23:03:25 - INFO - __main__ -   test: [batch: 83/391 ] | Loss: 0.690 | Acc: 82.347% (8854/10752)/ 95.685% (10288/10752)
01/13/2023 23:03:28 - INFO - __main__ -   test: [batch: 84/391 ] | Loss: 0.686 | Acc: 82.463% (8972/10880)/ 95.717% (10414/10880)
01/13/2023 23:03:31 - INFO - __main__ -   test: [batch: 85/391 ] | Loss: 0.684 | Acc: 82.467% (9078/11008)/ 95.758% (10541/11008)
01/13/2023 23:03:33 - INFO - __main__ -   test: [batch: 86/391 ] | Loss: 0.688 | Acc: 82.390% (9175/11136)/ 95.726% (10660/11136)
01/13/2023 23:03:36 - INFO - __main__ -   test: [batch: 87/391 ] | Loss: 0.686 | Acc: 82.431% (9285/11264)/ 95.739% (10784/11264)
01/13/2023 23:03:39 - INFO - __main__ -   test: [batch: 88/391 ] | Loss: 0.693 | Acc: 82.382% (9385/11392)/ 95.672% (10899/11392)
01/13/2023 23:03:41 - INFO - __main__ -   test: [batch: 89/391 ] | Loss: 0.691 | Acc: 82.422% (9495/11520)/ 95.686% (11023/11520)
01/13/2023 23:03:44 - INFO - __main__ -   test: [batch: 90/391 ] | Loss: 0.692 | Acc: 82.315% (9588/11648)/ 95.690% (11146/11648)
01/13/2023 23:03:47 - INFO - __main__ -   test: [batch: 91/391 ] | Loss: 0.691 | Acc: 82.354% (9698/11776)/ 95.703% (11270/11776)
01/13/2023 23:03:49 - INFO - __main__ -   test: [batch: 92/391 ] | Loss: 0.692 | Acc: 82.317% (9799/11904)/ 95.691% (11391/11904)
01/13/2023 23:03:52 - INFO - __main__ -   test: [batch: 93/391 ] | Loss: 0.696 | Acc: 82.106% (9879/12032)/ 95.720% (11517/12032)
01/13/2023 23:03:54 - INFO - __main__ -   test: [batch: 94/391 ] | Loss: 0.698 | Acc: 81.982% (9969/12160)/ 95.732% (11641/12160)
01/13/2023 23:03:57 - INFO - __main__ -   test: [batch: 95/391 ] | Loss: 0.696 | Acc: 82.048% (10082/12288)/ 95.736% (11764/12288)
01/13/2023 23:03:59 - INFO - __main__ -   test: [batch: 96/391 ] | Loss: 0.699 | Acc: 81.959% (10176/12416)/ 95.747% (11888/12416)
01/13/2023 23:04:02 - INFO - __main__ -   test: [batch: 97/391 ] | Loss: 0.701 | Acc: 81.776% (10258/12544)/ 95.767% (12013/12544)
01/13/2023 23:04:05 - INFO - __main__ -   test: [batch: 98/391 ] | Loss: 0.697 | Acc: 81.873% (10375/12672)/ 95.794% (12139/12672)
01/13/2023 23:04:07 - INFO - __main__ -   test: [batch: 99/391 ] | Loss: 0.691 | Acc: 82.016% (10498/12800)/ 95.836% (12267/12800)
01/13/2023 23:04:10 - INFO - __main__ -   test: [batch: 100/391 ] | Loss: 0.690 | Acc: 82.062% (10609/12928)/ 95.854% (12392/12928)
01/13/2023 23:04:13 - INFO - __main__ -   test: [batch: 101/391 ] | Loss: 0.686 | Acc: 82.161% (10727/13056)/ 95.887% (12519/13056)
01/13/2023 23:04:15 - INFO - __main__ -   test: [batch: 102/391 ] | Loss: 0.683 | Acc: 82.297% (10850/13184)/ 95.912% (12645/13184)
01/13/2023 23:04:18 - INFO - __main__ -   test: [batch: 103/391 ] | Loss: 0.684 | Acc: 82.129% (10933/13312)/ 95.921% (12769/13312)
01/13/2023 23:04:21 - INFO - __main__ -   test: [batch: 104/391 ] | Loss: 0.684 | Acc: 82.076% (11031/13440)/ 95.915% (12891/13440)
01/13/2023 23:04:23 - INFO - __main__ -   test: [batch: 105/391 ] | Loss: 0.685 | Acc: 82.061% (11134/13568)/ 95.909% (13013/13568)
01/13/2023 23:04:26 - INFO - __main__ -   test: [batch: 106/391 ] | Loss: 0.693 | Acc: 81.951% (11224/13696)/ 95.838% (13126/13696)
01/13/2023 23:04:28 - INFO - __main__ -   test: [batch: 107/391 ] | Loss: 0.690 | Acc: 82.075% (11346/13824)/ 95.870% (13253/13824)
01/13/2023 23:04:31 - INFO - __main__ -   test: [batch: 108/391 ] | Loss: 0.693 | Acc: 81.938% (11432/13952)/ 95.879% (13377/13952)
01/13/2023 23:04:34 - INFO - __main__ -   test: [batch: 109/391 ] | Loss: 0.694 | Acc: 81.918% (11534/14080)/ 95.874% (13499/14080)
01/13/2023 23:04:36 - INFO - __main__ -   test: [batch: 110/391 ] | Loss: 0.697 | Acc: 81.736% (11613/14208)/ 95.883% (13623/14208)
01/13/2023 23:04:39 - INFO - __main__ -   test: [batch: 111/391 ] | Loss: 0.700 | Acc: 81.696% (11712/14336)/ 95.850% (13741/14336)
01/13/2023 23:04:42 - INFO - __main__ -   test: [batch: 112/391 ] | Loss: 0.700 | Acc: 81.713% (11819/14464)/ 95.866% (13866/14464)
01/13/2023 23:04:44 - INFO - __main__ -   test: [batch: 113/391 ] | Loss: 0.699 | Acc: 81.757% (11930/14592)/ 95.874% (13990/14592)
01/13/2023 23:04:47 - INFO - __main__ -   test: [batch: 114/391 ] | Loss: 0.696 | Acc: 81.848% (12048/14720)/ 95.897% (14116/14720)
01/13/2023 23:04:50 - INFO - __main__ -   test: [batch: 115/391 ] | Loss: 0.693 | Acc: 81.937% (12166/14848)/ 95.919% (14242/14848)
01/13/2023 23:04:52 - INFO - __main__ -   test: [batch: 116/391 ] | Loss: 0.696 | Acc: 81.904% (12266/14976)/ 95.887% (14360/14976)
01/13/2023 23:04:55 - INFO - __main__ -   test: [batch: 117/391 ] | Loss: 0.695 | Acc: 81.919% (12373/15104)/ 95.895% (14484/15104)
01/13/2023 23:04:58 - INFO - __main__ -   test: [batch: 118/391 ] | Loss: 0.698 | Acc: 81.808% (12461/15232)/ 95.910% (14609/15232)
01/13/2023 23:05:00 - INFO - __main__ -   test: [batch: 119/391 ] | Loss: 0.696 | Acc: 81.855% (12573/15360)/ 95.931% (14735/15360)
01/13/2023 23:05:03 - INFO - __main__ -   test: [batch: 120/391 ] | Loss: 0.695 | Acc: 81.896% (12684/15488)/ 95.945% (14860/15488)
01/13/2023 23:05:06 - INFO - __main__ -   test: [batch: 121/391 ] | Loss: 0.700 | Acc: 81.749% (12766/15616)/ 95.908% (14977/15616)
01/13/2023 23:05:09 - INFO - __main__ -   test: [batch: 122/391 ] | Loss: 0.702 | Acc: 81.695% (12862/15744)/ 95.878% (15095/15744)
01/13/2023 23:05:11 - INFO - __main__ -   test: [batch: 123/391 ] | Loss: 0.703 | Acc: 81.710% (12969/15872)/ 95.867% (15216/15872)
01/13/2023 23:05:14 - INFO - __main__ -   test: [batch: 124/391 ] | Loss: 0.702 | Acc: 81.719% (13075/16000)/ 95.881% (15341/16000)
01/13/2023 23:05:16 - INFO - __main__ -   test: [batch: 125/391 ] | Loss: 0.698 | Acc: 81.814% (13195/16128)/ 95.908% (15468/16128)
01/13/2023 23:05:19 - INFO - __main__ -   test: [batch: 126/391 ] | Loss: 0.694 | Acc: 81.908% (13315/16256)/ 95.922% (15593/16256)
01/13/2023 23:05:21 - INFO - __main__ -   test: [batch: 127/391 ] | Loss: 0.692 | Acc: 81.989% (13433/16384)/ 95.929% (15717/16384)
01/13/2023 23:05:24 - INFO - __main__ -   test: [batch: 128/391 ] | Loss: 0.693 | Acc: 81.977% (13536/16512)/ 95.900% (15835/16512)
01/13/2023 23:05:26 - INFO - __main__ -   test: [batch: 129/391 ] | Loss: 0.692 | Acc: 81.989% (13643/16640)/ 95.907% (15959/16640)
01/13/2023 23:05:29 - INFO - __main__ -   test: [batch: 130/391 ] | Loss: 0.689 | Acc: 82.091% (13765/16768)/ 95.927% (16085/16768)
01/13/2023 23:05:32 - INFO - __main__ -   test: [batch: 131/391 ] | Loss: 0.689 | Acc: 82.138% (13878/16896)/ 95.934% (16209/16896)
01/13/2023 23:05:35 - INFO - __main__ -   test: [batch: 132/391 ] | Loss: 0.686 | Acc: 82.213% (13996/17024)/ 95.947% (16334/17024)
01/13/2023 23:05:37 - INFO - __main__ -   test: [batch: 133/391 ] | Loss: 0.687 | Acc: 82.160% (14092/17152)/ 95.931% (16454/17152)
01/13/2023 23:05:40 - INFO - __main__ -   test: [batch: 134/391 ] | Loss: 0.685 | Acc: 82.188% (14202/17280)/ 95.955% (16581/17280)
01/13/2023 23:05:42 - INFO - __main__ -   test: [batch: 135/391 ] | Loss: 0.686 | Acc: 82.181% (14306/17408)/ 95.950% (16703/17408)
01/13/2023 23:05:45 - INFO - __main__ -   test: [batch: 136/391 ] | Loss: 0.684 | Acc: 82.145% (14405/17536)/ 95.974% (16830/17536)
01/13/2023 23:05:47 - INFO - __main__ -   test: [batch: 137/391 ] | Loss: 0.682 | Acc: 82.184% (14517/17664)/ 96.003% (16958/17664)
01/13/2023 23:05:50 - INFO - __main__ -   test: [batch: 138/391 ] | Loss: 0.681 | Acc: 82.200% (14625/17792)/ 96.009% (17082/17792)
01/13/2023 23:05:52 - INFO - __main__ -   test: [batch: 139/391 ] | Loss: 0.687 | Acc: 82.042% (14702/17920)/ 95.999% (17203/17920)
01/13/2023 23:05:55 - INFO - __main__ -   test: [batch: 140/391 ] | Loss: 0.689 | Acc: 81.981% (14796/18048)/ 95.994% (17325/18048)
01/13/2023 23:05:57 - INFO - __main__ -   test: [batch: 141/391 ] | Loss: 0.688 | Acc: 82.020% (14908/18176)/ 96.006% (17450/18176)
01/13/2023 23:06:00 - INFO - __main__ -   test: [batch: 142/391 ] | Loss: 0.685 | Acc: 82.102% (15028/18304)/ 96.017% (17575/18304)
01/13/2023 23:06:02 - INFO - __main__ -   test: [batch: 143/391 ] | Loss: 0.686 | Acc: 82.118% (15136/18432)/ 96.002% (17695/18432)
01/13/2023 23:06:05 - INFO - __main__ -   test: [batch: 144/391 ] | Loss: 0.688 | Acc: 82.096% (15237/18560)/ 95.981% (17814/18560)
01/13/2023 23:06:07 - INFO - __main__ -   test: [batch: 145/391 ] | Loss: 0.691 | Acc: 82.074% (15338/18688)/ 95.955% (17932/18688)
01/13/2023 23:06:10 - INFO - __main__ -   test: [batch: 146/391 ] | Loss: 0.692 | Acc: 82.058% (15440/18816)/ 95.940% (18052/18816)
01/13/2023 23:06:13 - INFO - __main__ -   test: [batch: 147/391 ] | Loss: 0.692 | Acc: 82.058% (15545/18944)/ 95.914% (18170/18944)
01/13/2023 23:06:15 - INFO - __main__ -   test: [batch: 148/391 ] | Loss: 0.694 | Acc: 81.989% (15637/19072)/ 95.915% (18293/19072)
01/13/2023 23:06:18 - INFO - __main__ -   test: [batch: 149/391 ] | Loss: 0.698 | Acc: 81.885% (15722/19200)/ 95.885% (18410/19200)
01/13/2023 23:06:21 - INFO - __main__ -   test: [batch: 150/391 ] | Loss: 0.698 | Acc: 81.819% (15814/19328)/ 95.892% (18534/19328)
01/13/2023 23:06:24 - INFO - __main__ -   test: [batch: 151/391 ] | Loss: 0.698 | Acc: 81.851% (15925/19456)/ 95.893% (18657/19456)
01/13/2023 23:06:26 - INFO - __main__ -   test: [batch: 152/391 ] | Loss: 0.699 | Acc: 81.837% (16027/19584)/ 95.884% (18778/19584)
01/13/2023 23:06:29 - INFO - __main__ -   test: [batch: 153/391 ] | Loss: 0.697 | Acc: 81.899% (16144/19712)/ 95.886% (18901/19712)
01/13/2023 23:06:31 - INFO - __main__ -   test: [batch: 154/391 ] | Loss: 0.697 | Acc: 81.885% (16246/19840)/ 95.857% (19018/19840)
01/13/2023 23:06:34 - INFO - __main__ -   test: [batch: 155/391 ] | Loss: 0.698 | Acc: 81.901% (16354/19968)/ 95.848% (19139/19968)
01/13/2023 23:06:37 - INFO - __main__ -   test: [batch: 156/391 ] | Loss: 0.700 | Acc: 81.817% (16442/20096)/ 95.830% (19258/20096)
01/13/2023 23:06:39 - INFO - __main__ -   test: [batch: 157/391 ] | Loss: 0.701 | Acc: 81.804% (16544/20224)/ 95.807% (19376/20224)
01/13/2023 23:06:42 - INFO - __main__ -   test: [batch: 158/391 ] | Loss: 0.702 | Acc: 81.761% (16640/20352)/ 95.779% (19493/20352)
01/13/2023 23:06:45 - INFO - __main__ -   test: [batch: 159/391 ] | Loss: 0.704 | Acc: 81.733% (16739/20480)/ 95.771% (19614/20480)
01/13/2023 23:06:47 - INFO - __main__ -   test: [batch: 160/391 ] | Loss: 0.704 | Acc: 81.711% (16839/20608)/ 95.754% (19733/20608)
01/13/2023 23:06:50 - INFO - __main__ -   test: [batch: 161/391 ] | Loss: 0.714 | Acc: 81.501% (16900/20736)/ 95.665% (19837/20736)
01/13/2023 23:06:52 - INFO - __main__ -   test: [batch: 162/391 ] | Loss: 0.719 | Acc: 81.403% (16984/20864)/ 95.595% (19945/20864)
01/13/2023 23:06:55 - INFO - __main__ -   test: [batch: 163/391 ] | Loss: 0.722 | Acc: 81.345% (17076/20992)/ 95.579% (20064/20992)
01/13/2023 23:06:57 - INFO - __main__ -   test: [batch: 164/391 ] | Loss: 0.722 | Acc: 81.335% (17178/21120)/ 95.597% (20190/21120)
01/13/2023 23:07:00 - INFO - __main__ -   test: [batch: 165/391 ] | Loss: 0.724 | Acc: 81.259% (17266/21248)/ 95.595% (20312/21248)
01/13/2023 23:07:03 - INFO - __main__ -   test: [batch: 166/391 ] | Loss: 0.723 | Acc: 81.269% (17372/21376)/ 95.579% (20431/21376)
01/13/2023 23:07:05 - INFO - __main__ -   test: [batch: 167/391 ] | Loss: 0.725 | Acc: 81.203% (17462/21504)/ 95.564% (20550/21504)
01/13/2023 23:07:08 - INFO - __main__ -   test: [batch: 168/391 ] | Loss: 0.725 | Acc: 81.208% (17567/21632)/ 95.562% (20672/21632)
01/13/2023 23:07:10 - INFO - __main__ -   test: [batch: 169/391 ] | Loss: 0.727 | Acc: 81.176% (17664/21760)/ 95.524% (20786/21760)
01/13/2023 23:07:13 - INFO - __main__ -   test: [batch: 170/391 ] | Loss: 0.732 | Acc: 81.067% (17744/21888)/ 95.482% (20899/21888)
01/13/2023 23:07:16 - INFO - __main__ -   test: [batch: 171/391 ] | Loss: 0.735 | Acc: 81.014% (17836/22016)/ 95.453% (21015/22016)
01/13/2023 23:07:18 - INFO - __main__ -   test: [batch: 172/391 ] | Loss: 0.736 | Acc: 80.970% (17930/22144)/ 95.452% (21137/22144)
01/13/2023 23:07:21 - INFO - __main__ -   test: [batch: 173/391 ] | Loss: 0.738 | Acc: 80.913% (18021/22272)/ 95.425% (21253/22272)
01/13/2023 23:07:23 - INFO - __main__ -   test: [batch: 174/391 ] | Loss: 0.742 | Acc: 80.826% (18105/22400)/ 95.384% (21366/22400)
01/13/2023 23:07:26 - INFO - __main__ -   test: [batch: 175/391 ] | Loss: 0.741 | Acc: 80.877% (18220/22528)/ 95.388% (21489/22528)
01/13/2023 23:07:29 - INFO - __main__ -   test: [batch: 176/391 ] | Loss: 0.741 | Acc: 80.879% (18324/22656)/ 95.365% (21606/22656)
01/13/2023 23:07:31 - INFO - __main__ -   test: [batch: 177/391 ] | Loss: 0.745 | Acc: 80.807% (18411/22784)/ 95.326% (21719/22784)
01/13/2023 23:07:34 - INFO - __main__ -   test: [batch: 178/391 ] | Loss: 0.746 | Acc: 80.787% (18510/22912)/ 95.304% (21836/22912)
01/13/2023 23:07:37 - INFO - __main__ -   test: [batch: 179/391 ] | Loss: 0.749 | Acc: 80.738% (18602/23040)/ 95.265% (21949/23040)
01/13/2023 23:07:39 - INFO - __main__ -   test: [batch: 180/391 ] | Loss: 0.755 | Acc: 80.611% (18676/23168)/ 95.226% (22062/23168)
01/13/2023 23:07:42 - INFO - __main__ -   test: [batch: 181/391 ] | Loss: 0.762 | Acc: 80.482% (18749/23296)/ 95.167% (22170/23296)
01/13/2023 23:07:44 - INFO - __main__ -   test: [batch: 182/391 ] | Loss: 0.761 | Acc: 80.494% (18855/23424)/ 95.167% (22292/23424)
01/13/2023 23:07:47 - INFO - __main__ -   test: [batch: 183/391 ] | Loss: 0.767 | Acc: 80.363% (18927/23552)/ 95.079% (22393/23552)
01/13/2023 23:07:50 - INFO - __main__ -   test: [batch: 184/391 ] | Loss: 0.766 | Acc: 80.389% (19036/23680)/ 95.068% (22512/23680)
01/13/2023 23:07:52 - INFO - __main__ -   test: [batch: 185/391 ] | Loss: 0.766 | Acc: 80.402% (19142/23808)/ 95.060% (22632/23808)
01/13/2023 23:07:55 - INFO - __main__ -   test: [batch: 186/391 ] | Loss: 0.769 | Acc: 80.364% (19236/23936)/ 95.016% (22743/23936)
01/13/2023 23:07:58 - INFO - __main__ -   test: [batch: 187/391 ] | Loss: 0.774 | Acc: 80.249% (19311/24064)/ 94.993% (22859/24064)
01/13/2023 23:08:00 - INFO - __main__ -   test: [batch: 188/391 ] | Loss: 0.778 | Acc: 80.113% (19381/24192)/ 94.974% (22976/24192)
01/13/2023 23:08:03 - INFO - __main__ -   test: [batch: 189/391 ] | Loss: 0.779 | Acc: 80.049% (19468/24320)/ 94.975% (23098/24320)
01/13/2023 23:08:05 - INFO - __main__ -   test: [batch: 190/391 ] | Loss: 0.784 | Acc: 79.962% (19549/24448)/ 94.953% (23214/24448)
01/13/2023 23:08:08 - INFO - __main__ -   test: [batch: 191/391 ] | Loss: 0.786 | Acc: 79.932% (19644/24576)/ 94.930% (23330/24576)
01/13/2023 23:08:10 - INFO - __main__ -   test: [batch: 192/391 ] | Loss: 0.792 | Acc: 79.813% (19717/24704)/ 94.843% (23430/24704)
01/13/2023 23:08:13 - INFO - __main__ -   test: [batch: 193/391 ] | Loss: 0.792 | Acc: 79.828% (19823/24832)/ 94.841% (23551/24832)
01/13/2023 23:08:16 - INFO - __main__ -   test: [batch: 194/391 ] | Loss: 0.795 | Acc: 79.764% (19909/24960)/ 94.820% (23667/24960)
01/13/2023 23:08:18 - INFO - __main__ -   test: [batch: 195/391 ] | Loss: 0.800 | Acc: 79.695% (19994/25088)/ 94.762% (23774/25088)
01/13/2023 23:08:21 - INFO - __main__ -   test: [batch: 196/391 ] | Loss: 0.804 | Acc: 79.588% (20069/25216)/ 94.714% (23883/25216)
01/13/2023 23:08:24 - INFO - __main__ -   test: [batch: 197/391 ] | Loss: 0.808 | Acc: 79.510% (20151/25344)/ 94.693% (23999/25344)
01/13/2023 23:08:26 - INFO - __main__ -   test: [batch: 198/391 ] | Loss: 0.810 | Acc: 79.476% (20244/25472)/ 94.661% (24112/25472)
01/13/2023 23:08:29 - INFO - __main__ -   test: [batch: 199/391 ] | Loss: 0.810 | Acc: 79.457% (20341/25600)/ 94.668% (24235/25600)
01/13/2023 23:08:31 - INFO - __main__ -   test: [batch: 200/391 ] | Loss: 0.809 | Acc: 79.431% (20436/25728)/ 94.667% (24356/25728)
01/13/2023 23:08:34 - INFO - __main__ -   test: [batch: 201/391 ] | Loss: 0.813 | Acc: 79.347% (20516/25856)/ 94.632% (24468/25856)
01/13/2023 23:08:37 - INFO - __main__ -   test: [batch: 202/391 ] | Loss: 0.814 | Acc: 79.337% (20615/25984)/ 94.620% (24586/25984)
01/13/2023 23:08:39 - INFO - __main__ -   test: [batch: 203/391 ] | Loss: 0.815 | Acc: 79.328% (20714/26112)/ 94.608% (24704/26112)
01/13/2023 23:08:42 - INFO - __main__ -   test: [batch: 204/391 ] | Loss: 0.818 | Acc: 79.242% (20793/26240)/ 94.588% (24820/26240)
01/13/2023 23:08:45 - INFO - __main__ -   test: [batch: 205/391 ] | Loss: 0.820 | Acc: 79.160% (20873/26368)/ 94.558% (24933/26368)
01/13/2023 23:08:47 - INFO - __main__ -   test: [batch: 206/391 ] | Loss: 0.822 | Acc: 79.140% (20969/26496)/ 94.550% (25052/26496)
01/13/2023 23:08:50 - INFO - __main__ -   test: [batch: 207/391 ] | Loss: 0.826 | Acc: 79.056% (21048/26624)/ 94.509% (25162/26624)
01/13/2023 23:08:52 - INFO - __main__ -   test: [batch: 208/391 ] | Loss: 0.827 | Acc: 79.018% (21139/26752)/ 94.486% (25277/26752)
01/13/2023 23:08:55 - INFO - __main__ -   test: [batch: 209/391 ] | Loss: 0.827 | Acc: 79.014% (21239/26880)/ 94.505% (25403/26880)
01/13/2023 23:08:57 - INFO - __main__ -   test: [batch: 210/391 ] | Loss: 0.829 | Acc: 78.995% (21335/27008)/ 94.487% (25519/27008)
01/13/2023 23:09:00 - INFO - __main__ -   test: [batch: 211/391 ] | Loss: 0.831 | Acc: 78.936% (21420/27136)/ 94.465% (25634/27136)
01/13/2023 23:09:03 - INFO - __main__ -   test: [batch: 212/391 ] | Loss: 0.833 | Acc: 78.873% (21504/27264)/ 94.443% (25749/27264)
01/13/2023 23:09:05 - INFO - __main__ -   test: [batch: 213/391 ] | Loss: 0.833 | Acc: 78.877% (21606/27392)/ 94.447% (25871/27392)
01/13/2023 23:09:08 - INFO - __main__ -   test: [batch: 214/391 ] | Loss: 0.833 | Acc: 78.855% (21701/27520)/ 94.448% (25992/27520)
01/13/2023 23:09:10 - INFO - __main__ -   test: [batch: 215/391 ] | Loss: 0.833 | Acc: 78.881% (21809/27648)/ 94.441% (26111/27648)
01/13/2023 23:09:13 - INFO - __main__ -   test: [batch: 216/391 ] | Loss: 0.831 | Acc: 78.921% (21921/27776)/ 94.463% (26238/27776)
01/13/2023 23:09:15 - INFO - __main__ -   test: [batch: 217/391 ] | Loss: 0.836 | Acc: 78.852% (22003/27904)/ 94.399% (26341/27904)
01/13/2023 23:09:18 - INFO - __main__ -   test: [batch: 218/391 ] | Loss: 0.839 | Acc: 78.789% (22086/28032)/ 94.360% (26451/28032)
01/13/2023 23:09:21 - INFO - __main__ -   test: [batch: 219/391 ] | Loss: 0.837 | Acc: 78.825% (22197/28160)/ 94.364% (26573/28160)
01/13/2023 23:09:23 - INFO - __main__ -   test: [batch: 220/391 ] | Loss: 0.836 | Acc: 78.853% (22306/28288)/ 94.379% (26698/28288)
01/13/2023 23:09:26 - INFO - __main__ -   test: [batch: 221/391 ] | Loss: 0.838 | Acc: 78.811% (22395/28416)/ 94.366% (26815/28416)
01/13/2023 23:09:29 - INFO - __main__ -   test: [batch: 222/391 ] | Loss: 0.836 | Acc: 78.871% (22513/28544)/ 94.381% (26940/28544)
01/13/2023 23:09:31 - INFO - __main__ -   test: [batch: 223/391 ] | Loss: 0.836 | Acc: 78.871% (22614/28672)/ 94.371% (27058/28672)
01/13/2023 23:09:34 - INFO - __main__ -   test: [batch: 224/391 ] | Loss: 0.835 | Acc: 78.892% (22721/28800)/ 94.365% (27177/28800)
01/13/2023 23:09:36 - INFO - __main__ -   test: [batch: 225/391 ] | Loss: 0.835 | Acc: 78.892% (22822/28928)/ 94.369% (27299/28928)
01/13/2023 23:09:39 - INFO - __main__ -   test: [batch: 226/391 ] | Loss: 0.835 | Acc: 78.865% (22915/29056)/ 94.376% (27422/29056)
01/13/2023 23:09:41 - INFO - __main__ -   test: [batch: 227/391 ] | Loss: 0.837 | Acc: 78.855% (23013/29184)/ 94.363% (27539/29184)
01/13/2023 23:09:44 - INFO - __main__ -   test: [batch: 228/391 ] | Loss: 0.841 | Acc: 78.787% (23094/29312)/ 94.306% (27643/29312)
01/13/2023 23:09:47 - INFO - __main__ -   test: [batch: 229/391 ] | Loss: 0.844 | Acc: 78.730% (23178/29440)/ 94.260% (27750/29440)
01/13/2023 23:09:49 - INFO - __main__ -   test: [batch: 230/391 ] | Loss: 0.847 | Acc: 78.673% (23262/29568)/ 94.223% (27860/29568)
01/13/2023 23:09:52 - INFO - __main__ -   test: [batch: 231/391 ] | Loss: 0.847 | Acc: 78.664% (23360/29696)/ 94.208% (27976/29696)
01/13/2023 23:09:55 - INFO - __main__ -   test: [batch: 232/391 ] | Loss: 0.846 | Acc: 78.692% (23469/29824)/ 94.223% (28101/29824)
01/13/2023 23:09:58 - INFO - __main__ -   test: [batch: 233/391 ] | Loss: 0.849 | Acc: 78.639% (23554/29952)/ 94.201% (28215/29952)
01/13/2023 23:10:00 - INFO - __main__ -   test: [batch: 234/391 ] | Loss: 0.855 | Acc: 78.524% (23620/30080)/ 94.129% (28314/30080)
01/13/2023 23:10:03 - INFO - __main__ -   test: [batch: 235/391 ] | Loss: 0.855 | Acc: 78.512% (23717/30208)/ 94.127% (28434/30208)
01/13/2023 23:10:05 - INFO - __main__ -   test: [batch: 236/391 ] | Loss: 0.853 | Acc: 78.557% (23831/30336)/ 94.136% (28557/30336)
01/13/2023 23:10:08 - INFO - __main__ -   test: [batch: 237/391 ] | Loss: 0.854 | Acc: 78.562% (23933/30464)/ 94.108% (28669/30464)
01/13/2023 23:10:10 - INFO - __main__ -   test: [batch: 238/391 ] | Loss: 0.853 | Acc: 78.602% (24046/30592)/ 94.116% (28792/30592)
01/13/2023 23:10:13 - INFO - __main__ -   test: [batch: 239/391 ] | Loss: 0.851 | Acc: 78.639% (24158/30720)/ 94.121% (28914/30720)
01/13/2023 23:10:16 - INFO - __main__ -   test: [batch: 240/391 ] | Loss: 0.851 | Acc: 78.653% (24263/30848)/ 94.107% (29030/30848)
01/13/2023 23:10:18 - INFO - __main__ -   test: [batch: 241/391 ] | Loss: 0.854 | Acc: 78.567% (24337/30976)/ 94.070% (29139/30976)
01/13/2023 23:10:21 - INFO - __main__ -   test: [batch: 242/391 ] | Loss: 0.857 | Acc: 78.463% (24405/31104)/ 94.049% (29253/31104)
01/13/2023 23:10:23 - INFO - __main__ -   test: [batch: 243/391 ] | Loss: 0.863 | Acc: 78.333% (24465/31232)/ 93.987% (29354/31232)
01/13/2023 23:10:26 - INFO - __main__ -   test: [batch: 244/391 ] | Loss: 0.863 | Acc: 78.339% (24567/31360)/ 93.980% (29472/31360)
01/13/2023 23:10:29 - INFO - __main__ -   test: [batch: 245/391 ] | Loss: 0.862 | Acc: 78.354% (24672/31488)/ 93.969% (29589/31488)
01/13/2023 23:10:31 - INFO - __main__ -   test: [batch: 246/391 ] | Loss: 0.863 | Acc: 78.337% (24767/31616)/ 93.959% (29706/31616)
01/13/2023 23:10:34 - INFO - __main__ -   test: [batch: 247/391 ] | Loss: 0.869 | Acc: 78.245% (24838/31744)/ 93.885% (29803/31744)
01/13/2023 23:10:37 - INFO - __main__ -   test: [batch: 248/391 ] | Loss: 0.870 | Acc: 78.210% (24927/31872)/ 93.872% (29919/31872)
01/13/2023 23:10:39 - INFO - __main__ -   test: [batch: 249/391 ] | Loss: 0.871 | Acc: 78.091% (24989/32000)/ 93.862% (30036/32000)
01/13/2023 23:10:42 - INFO - __main__ -   test: [batch: 250/391 ] | Loss: 0.870 | Acc: 78.119% (25098/32128)/ 93.881% (30162/32128)
01/13/2023 23:10:45 - INFO - __main__ -   test: [batch: 251/391 ] | Loss: 0.872 | Acc: 78.075% (25184/32256)/ 93.855% (30274/32256)
01/13/2023 23:10:47 - INFO - __main__ -   test: [batch: 252/391 ] | Loss: 0.872 | Acc: 78.106% (25294/32384)/ 93.849% (30392/32384)
01/13/2023 23:10:50 - INFO - __main__ -   test: [batch: 253/391 ] | Loss: 0.873 | Acc: 78.076% (25384/32512)/ 93.833% (30507/32512)
01/13/2023 23:10:53 - INFO - __main__ -   test: [batch: 254/391 ] | Loss: 0.877 | Acc: 78.012% (25463/32640)/ 93.790% (30613/32640)
01/13/2023 23:10:55 - INFO - __main__ -   test: [batch: 255/391 ] | Loss: 0.879 | Acc: 77.979% (25552/32768)/ 93.781% (30730/32768)
01/13/2023 23:10:58 - INFO - __main__ -   test: [batch: 256/391 ] | Loss: 0.884 | Acc: 77.857% (25612/32896)/ 93.753% (30841/32896)
01/13/2023 23:11:01 - INFO - __main__ -   test: [batch: 257/391 ] | Loss: 0.885 | Acc: 77.843% (25707/33024)/ 93.741% (30957/33024)
01/13/2023 23:11:03 - INFO - __main__ -   test: [batch: 258/391 ] | Loss: 0.886 | Acc: 77.835% (25804/33152)/ 93.726% (31072/33152)
01/13/2023 23:11:06 - INFO - __main__ -   test: [batch: 259/391 ] | Loss: 0.890 | Acc: 77.716% (25864/33280)/ 93.711% (31187/33280)
01/13/2023 23:11:09 - INFO - __main__ -   test: [batch: 260/391 ] | Loss: 0.890 | Acc: 77.682% (25952/33408)/ 93.705% (31305/33408)
01/13/2023 23:11:11 - INFO - __main__ -   test: [batch: 261/391 ] | Loss: 0.888 | Acc: 77.728% (26067/33536)/ 93.726% (31432/33536)
01/13/2023 23:11:14 - INFO - __main__ -   test: [batch: 262/391 ] | Loss: 0.887 | Acc: 77.757% (26176/33664)/ 93.735% (31555/33664)
01/13/2023 23:11:16 - INFO - __main__ -   test: [batch: 263/391 ] | Loss: 0.890 | Acc: 77.696% (26255/33792)/ 93.712% (31667/33792)
01/13/2023 23:11:19 - INFO - __main__ -   test: [batch: 264/391 ] | Loss: 0.895 | Acc: 77.636% (26334/33920)/ 93.650% (31766/33920)
01/13/2023 23:11:22 - INFO - __main__ -   test: [batch: 265/391 ] | Loss: 0.894 | Acc: 77.664% (26443/34048)/ 93.653% (31887/34048)
01/13/2023 23:11:24 - INFO - __main__ -   test: [batch: 266/391 ] | Loss: 0.897 | Acc: 77.590% (26517/34176)/ 93.636% (32001/34176)
01/13/2023 23:11:27 - INFO - __main__ -   test: [batch: 267/391 ] | Loss: 0.896 | Acc: 77.621% (26627/34304)/ 93.636% (32121/34304)
01/13/2023 23:11:29 - INFO - __main__ -   test: [batch: 268/391 ] | Loss: 0.895 | Acc: 77.640% (26733/34432)/ 93.634% (32240/34432)
01/13/2023 23:11:32 - INFO - __main__ -   test: [batch: 269/391 ] | Loss: 0.897 | Acc: 77.587% (26814/34560)/ 93.617% (32354/34560)
01/13/2023 23:11:35 - INFO - __main__ -   test: [batch: 270/391 ] | Loss: 0.900 | Acc: 77.525% (26892/34688)/ 93.583% (32462/34688)
01/13/2023 23:11:37 - INFO - __main__ -   test: [batch: 271/391 ] | Loss: 0.900 | Acc: 77.539% (26996/34816)/ 93.578% (32580/34816)
01/13/2023 23:11:40 - INFO - __main__ -   test: [batch: 272/391 ] | Loss: 0.902 | Acc: 77.504% (27083/34944)/ 93.575% (32699/34944)
01/13/2023 23:11:42 - INFO - __main__ -   test: [batch: 273/391 ] | Loss: 0.902 | Acc: 77.512% (27185/35072)/ 93.562% (32814/35072)
01/13/2023 23:11:45 - INFO - __main__ -   test: [batch: 274/391 ] | Loss: 0.902 | Acc: 77.511% (27284/35200)/ 93.565% (32935/35200)
01/13/2023 23:11:48 - INFO - __main__ -   test: [batch: 275/391 ] | Loss: 0.904 | Acc: 77.482% (27373/35328)/ 93.558% (33052/35328)
01/13/2023 23:11:50 - INFO - __main__ -   test: [batch: 276/391 ] | Loss: 0.904 | Acc: 77.445% (27459/35456)/ 93.555% (33171/35456)
01/13/2023 23:11:53 - INFO - __main__ -   test: [batch: 277/391 ] | Loss: 0.905 | Acc: 77.417% (27548/35584)/ 93.548% (33288/35584)
01/13/2023 23:11:56 - INFO - __main__ -   test: [batch: 278/391 ] | Loss: 0.905 | Acc: 77.431% (27652/35712)/ 93.546% (33407/35712)
01/13/2023 23:11:58 - INFO - __main__ -   test: [batch: 279/391 ] | Loss: 0.905 | Acc: 77.441% (27755/35840)/ 93.541% (33525/35840)
01/13/2023 23:12:01 - INFO - __main__ -   test: [batch: 280/391 ] | Loss: 0.906 | Acc: 77.424% (27848/35968)/ 93.533% (33642/35968)
01/13/2023 23:12:04 - INFO - __main__ -   test: [batch: 281/391 ] | Loss: 0.906 | Acc: 77.430% (27949/36096)/ 93.528% (33760/36096)
01/13/2023 23:12:06 - INFO - __main__ -   test: [batch: 282/391 ] | Loss: 0.905 | Acc: 77.462% (28060/36224)/ 93.532% (33881/36224)
01/13/2023 23:12:09 - INFO - __main__ -   test: [batch: 283/391 ] | Loss: 0.906 | Acc: 77.432% (28148/36352)/ 93.527% (33999/36352)
01/13/2023 23:12:12 - INFO - __main__ -   test: [batch: 284/391 ] | Loss: 0.909 | Acc: 77.388% (28231/36480)/ 93.495% (34107/36480)
01/13/2023 23:12:14 - INFO - __main__ -   test: [batch: 285/391 ] | Loss: 0.911 | Acc: 77.344% (28314/36608)/ 93.452% (34211/36608)
01/13/2023 23:12:17 - INFO - __main__ -   test: [batch: 286/391 ] | Loss: 0.913 | Acc: 77.311% (28401/36736)/ 93.440% (34326/36736)
01/13/2023 23:12:20 - INFO - __main__ -   test: [batch: 287/391 ] | Loss: 0.913 | Acc: 77.311% (28500/36864)/ 93.430% (34442/36864)
01/13/2023 23:12:22 - INFO - __main__ -   test: [batch: 288/391 ] | Loss: 0.912 | Acc: 77.328% (28605/36992)/ 93.434% (34563/36992)
01/13/2023 23:12:25 - INFO - __main__ -   test: [batch: 289/391 ] | Loss: 0.914 | Acc: 77.287% (28689/37120)/ 93.400% (34670/37120)
01/13/2023 23:12:28 - INFO - __main__ -   test: [batch: 290/391 ] | Loss: 0.915 | Acc: 77.210% (28759/37248)/ 93.404% (34791/37248)
01/13/2023 23:12:30 - INFO - __main__ -   test: [batch: 291/391 ] | Loss: 0.915 | Acc: 77.223% (28863/37376)/ 93.397% (34908/37376)
01/13/2023 23:12:33 - INFO - __main__ -   test: [batch: 292/391 ] | Loss: 0.917 | Acc: 77.168% (28941/37504)/ 93.379% (35021/37504)
01/13/2023 23:12:36 - INFO - __main__ -   test: [batch: 293/391 ] | Loss: 0.917 | Acc: 77.166% (29039/37632)/ 93.373% (35138/37632)
01/13/2023 23:12:38 - INFO - __main__ -   test: [batch: 294/391 ] | Loss: 0.918 | Acc: 77.140% (29128/37760)/ 93.371% (35257/37760)
01/13/2023 23:12:41 - INFO - __main__ -   test: [batch: 295/391 ] | Loss: 0.917 | Acc: 77.175% (29240/37888)/ 93.373% (35377/37888)
01/13/2023 23:12:43 - INFO - __main__ -   test: [batch: 296/391 ] | Loss: 0.918 | Acc: 77.175% (29339/38016)/ 93.363% (35493/38016)
01/13/2023 23:12:46 - INFO - __main__ -   test: [batch: 297/391 ] | Loss: 0.920 | Acc: 77.158% (29431/38144)/ 93.341% (35604/38144)
01/13/2023 23:12:49 - INFO - __main__ -   test: [batch: 298/391 ] | Loss: 0.922 | Acc: 77.124% (29517/38272)/ 93.314% (35713/38272)
01/13/2023 23:12:51 - INFO - __main__ -   test: [batch: 299/391 ] | Loss: 0.923 | Acc: 77.117% (29613/38400)/ 93.294% (35825/38400)
01/13/2023 23:12:54 - INFO - __main__ -   test: [batch: 300/391 ] | Loss: 0.923 | Acc: 77.121% (29713/38528)/ 93.291% (35943/38528)
01/13/2023 23:12:56 - INFO - __main__ -   test: [batch: 301/391 ] | Loss: 0.924 | Acc: 77.106% (29806/38656)/ 93.277% (36057/38656)
01/13/2023 23:12:59 - INFO - __main__ -   test: [batch: 302/391 ] | Loss: 0.925 | Acc: 77.070% (29891/38784)/ 93.260% (36170/38784)
01/13/2023 23:13:01 - INFO - __main__ -   test: [batch: 303/391 ] | Loss: 0.926 | Acc: 77.048% (29981/38912)/ 93.249% (36285/38912)
01/13/2023 23:13:04 - INFO - __main__ -   test: [batch: 304/391 ] | Loss: 0.926 | Acc: 77.062% (30085/39040)/ 93.253% (36406/39040)
01/13/2023 23:13:07 - INFO - __main__ -   test: [batch: 305/391 ] | Loss: 0.926 | Acc: 77.017% (30166/39168)/ 93.250% (36524/39168)
01/13/2023 23:13:09 - INFO - __main__ -   test: [batch: 306/391 ] | Loss: 0.928 | Acc: 76.998% (30257/39296)/ 93.223% (36633/39296)
01/13/2023 23:13:12 - INFO - __main__ -   test: [batch: 307/391 ] | Loss: 0.928 | Acc: 76.999% (30356/39424)/ 93.207% (36746/39424)
01/13/2023 23:13:15 - INFO - __main__ -   test: [batch: 308/391 ] | Loss: 0.929 | Acc: 76.977% (30446/39552)/ 93.206% (36865/39552)
01/13/2023 23:13:17 - INFO - __main__ -   test: [batch: 309/391 ] | Loss: 0.930 | Acc: 76.973% (30543/39680)/ 93.188% (36977/39680)
01/13/2023 23:13:20 - INFO - __main__ -   test: [batch: 310/391 ] | Loss: 0.930 | Acc: 76.969% (30640/39808)/ 93.175% (37091/39808)
01/13/2023 23:13:23 - INFO - __main__ -   test: [batch: 311/391 ] | Loss: 0.932 | Acc: 76.938% (30726/39936)/ 93.164% (37206/39936)
01/13/2023 23:13:25 - INFO - __main__ -   test: [batch: 312/391 ] | Loss: 0.933 | Acc: 76.922% (30818/40064)/ 93.141% (37316/40064)
01/13/2023 23:13:28 - INFO - __main__ -   test: [batch: 313/391 ] | Loss: 0.931 | Acc: 76.963% (30933/40192)/ 93.160% (37443/40192)
01/13/2023 23:13:31 - INFO - __main__ -   test: [batch: 314/391 ] | Loss: 0.932 | Acc: 76.949% (31026/40320)/ 93.157% (37561/40320)
01/13/2023 23:13:33 - INFO - __main__ -   test: [batch: 315/391 ] | Loss: 0.933 | Acc: 76.928% (31116/40448)/ 93.142% (37674/40448)
01/13/2023 23:13:36 - INFO - __main__ -   test: [batch: 316/391 ] | Loss: 0.935 | Acc: 76.831% (31175/40576)/ 93.119% (37784/40576)
01/13/2023 23:13:38 - INFO - __main__ -   test: [batch: 317/391 ] | Loss: 0.937 | Acc: 76.789% (31256/40704)/ 93.092% (37892/40704)
01/13/2023 23:13:41 - INFO - __main__ -   test: [batch: 318/391 ] | Loss: 0.936 | Acc: 76.820% (31367/40832)/ 93.108% (38018/40832)
01/13/2023 23:13:43 - INFO - __main__ -   test: [batch: 319/391 ] | Loss: 0.939 | Acc: 76.763% (31442/40960)/ 93.079% (38125/40960)
01/13/2023 23:13:46 - INFO - __main__ -   test: [batch: 320/391 ] | Loss: 0.938 | Acc: 76.799% (31555/41088)/ 93.088% (38248/41088)
01/13/2023 23:13:49 - INFO - __main__ -   test: [batch: 321/391 ] | Loss: 0.937 | Acc: 76.817% (31661/41216)/ 93.083% (38365/41216)
01/13/2023 23:13:51 - INFO - __main__ -   test: [batch: 322/391 ] | Loss: 0.939 | Acc: 76.795% (31750/41344)/ 93.078% (38482/41344)
01/13/2023 23:13:54 - INFO - __main__ -   test: [batch: 323/391 ] | Loss: 0.941 | Acc: 76.755% (31832/41472)/ 93.053% (38591/41472)
01/13/2023 23:13:57 - INFO - __main__ -   test: [batch: 324/391 ] | Loss: 0.941 | Acc: 76.748% (31927/41600)/ 93.048% (38708/41600)
01/13/2023 23:13:59 - INFO - __main__ -   test: [batch: 325/391 ] | Loss: 0.941 | Acc: 76.757% (32029/41728)/ 93.043% (38825/41728)
01/13/2023 23:14:02 - INFO - __main__ -   test: [batch: 326/391 ] | Loss: 0.944 | Acc: 76.675% (32093/41856)/ 93.014% (38932/41856)
01/13/2023 23:14:04 - INFO - __main__ -   test: [batch: 327/391 ] | Loss: 0.947 | Acc: 76.596% (32158/41984)/ 92.983% (39038/41984)
01/13/2023 23:14:07 - INFO - __main__ -   test: [batch: 328/391 ] | Loss: 0.949 | Acc: 76.544% (32234/42112)/ 92.959% (39147/42112)
01/13/2023 23:14:10 - INFO - __main__ -   test: [batch: 329/391 ] | Loss: 0.949 | Acc: 76.534% (32328/42240)/ 92.959% (39266/42240)
01/13/2023 23:14:12 - INFO - __main__ -   test: [batch: 330/391 ] | Loss: 0.951 | Acc: 76.492% (32408/42368)/ 92.933% (39374/42368)
01/13/2023 23:14:15 - INFO - __main__ -   test: [batch: 331/391 ] | Loss: 0.951 | Acc: 76.454% (32490/42496)/ 92.941% (39496/42496)
01/13/2023 23:14:17 - INFO - __main__ -   test: [batch: 332/391 ] | Loss: 0.952 | Acc: 76.441% (32582/42624)/ 92.938% (39614/42624)
01/13/2023 23:14:20 - INFO - __main__ -   test: [batch: 333/391 ] | Loss: 0.951 | Acc: 76.469% (32692/42752)/ 92.948% (39737/42752)
01/13/2023 23:14:23 - INFO - __main__ -   test: [batch: 334/391 ] | Loss: 0.952 | Acc: 76.437% (32776/42880)/ 92.929% (39848/42880)
01/13/2023 23:14:25 - INFO - __main__ -   test: [batch: 335/391 ] | Loss: 0.953 | Acc: 76.414% (32864/43008)/ 92.915% (39961/43008)
01/13/2023 23:14:28 - INFO - __main__ -   test: [batch: 336/391 ] | Loss: 0.955 | Acc: 76.368% (32942/43136)/ 92.904% (40075/43136)
01/13/2023 23:14:30 - INFO - __main__ -   test: [batch: 337/391 ] | Loss: 0.955 | Acc: 76.359% (33036/43264)/ 92.902% (40193/43264)
01/13/2023 23:14:33 - INFO - __main__ -   test: [batch: 338/391 ] | Loss: 0.954 | Acc: 76.346% (33128/43392)/ 92.911% (40316/43392)
01/13/2023 23:14:35 - INFO - __main__ -   test: [batch: 339/391 ] | Loss: 0.957 | Acc: 76.314% (33212/43520)/ 92.884% (40423/43520)
01/13/2023 23:14:38 - INFO - __main__ -   test: [batch: 340/391 ] | Loss: 0.957 | Acc: 76.308% (33307/43648)/ 92.893% (40546/43648)
01/13/2023 23:14:41 - INFO - __main__ -   test: [batch: 341/391 ] | Loss: 0.955 | Acc: 76.345% (33421/43776)/ 92.912% (40673/43776)
01/13/2023 23:14:43 - INFO - __main__ -   test: [batch: 342/391 ] | Loss: 0.956 | Acc: 76.291% (33495/43904)/ 92.905% (40789/43904)
01/13/2023 23:14:46 - INFO - __main__ -   test: [batch: 343/391 ] | Loss: 0.956 | Acc: 76.295% (33594/44032)/ 92.907% (40909/44032)
01/13/2023 23:14:48 - INFO - __main__ -   test: [batch: 344/391 ] | Loss: 0.956 | Acc: 76.293% (33691/44160)/ 92.899% (41024/44160)
01/13/2023 23:14:51 - INFO - __main__ -   test: [batch: 345/391 ] | Loss: 0.960 | Acc: 76.231% (33761/44288)/ 92.863% (41127/44288)
01/13/2023 23:14:54 - INFO - __main__ -   test: [batch: 346/391 ] | Loss: 0.961 | Acc: 76.218% (33853/44416)/ 92.854% (41242/44416)
01/13/2023 23:14:56 - INFO - __main__ -   test: [batch: 347/391 ] | Loss: 0.961 | Acc: 76.228% (33955/44544)/ 92.861% (41364/44544)
01/13/2023 23:14:59 - INFO - __main__ -   test: [batch: 348/391 ] | Loss: 0.962 | Acc: 76.204% (34042/44672)/ 92.837% (41472/44672)
01/13/2023 23:15:01 - INFO - __main__ -   test: [batch: 349/391 ] | Loss: 0.962 | Acc: 76.201% (34138/44800)/ 92.844% (41594/44800)
01/13/2023 23:15:04 - INFO - __main__ -   test: [batch: 350/391 ] | Loss: 0.961 | Acc: 76.211% (34240/44928)/ 92.842% (41712/44928)
01/13/2023 23:15:07 - INFO - __main__ -   test: [batch: 351/391 ] | Loss: 0.964 | Acc: 76.156% (34313/45056)/ 92.827% (41824/45056)
01/13/2023 23:15:09 - INFO - __main__ -   test: [batch: 352/391 ] | Loss: 0.964 | Acc: 76.155% (34410/45184)/ 92.825% (41942/45184)
01/13/2023 23:15:12 - INFO - __main__ -   test: [batch: 353/391 ] | Loss: 0.966 | Acc: 76.110% (34487/45312)/ 92.786% (42043/45312)
01/13/2023 23:15:15 - INFO - __main__ -   test: [batch: 354/391 ] | Loss: 0.969 | Acc: 76.050% (34557/45440)/ 92.762% (42151/45440)
01/13/2023 23:15:17 - INFO - __main__ -   test: [batch: 355/391 ] | Loss: 0.971 | Acc: 75.983% (34624/45568)/ 92.754% (42266/45568)
01/13/2023 23:15:20 - INFO - __main__ -   test: [batch: 356/391 ] | Loss: 0.972 | Acc: 75.978% (34719/45696)/ 92.756% (42386/45696)
01/13/2023 23:15:22 - INFO - __main__ -   test: [batch: 357/391 ] | Loss: 0.970 | Acc: 76.013% (34832/45824)/ 92.770% (42511/45824)
01/13/2023 23:15:25 - INFO - __main__ -   test: [batch: 358/391 ] | Loss: 0.969 | Acc: 76.034% (34939/45952)/ 92.773% (42631/45952)
01/13/2023 23:15:28 - INFO - __main__ -   test: [batch: 359/391 ] | Loss: 0.970 | Acc: 76.044% (35041/46080)/ 92.769% (42748/46080)
01/13/2023 23:15:30 - INFO - __main__ -   test: [batch: 360/391 ] | Loss: 0.972 | Acc: 76.013% (35124/46208)/ 92.763% (42864/46208)
01/13/2023 23:15:33 - INFO - __main__ -   test: [batch: 361/391 ] | Loss: 0.972 | Acc: 76.014% (35222/46336)/ 92.770% (42986/46336)
01/13/2023 23:15:36 - INFO - __main__ -   test: [batch: 362/391 ] | Loss: 0.971 | Acc: 76.009% (35317/46464)/ 92.782% (43110/46464)
01/13/2023 23:15:38 - INFO - __main__ -   test: [batch: 363/391 ] | Loss: 0.971 | Acc: 76.002% (35411/46592)/ 92.773% (43225/46592)
01/13/2023 23:15:41 - INFO - __main__ -   test: [batch: 364/391 ] | Loss: 0.970 | Acc: 76.030% (35521/46720)/ 92.783% (43348/46720)
01/13/2023 23:15:43 - INFO - __main__ -   test: [batch: 365/391 ] | Loss: 0.970 | Acc: 76.044% (35625/46848)/ 92.787% (43469/46848)
01/13/2023 23:15:46 - INFO - __main__ -   test: [batch: 366/391 ] | Loss: 0.968 | Acc: 76.079% (35739/46976)/ 92.805% (43596/46976)
01/13/2023 23:15:49 - INFO - __main__ -   test: [batch: 367/391 ] | Loss: 0.967 | Acc: 76.104% (35848/47104)/ 92.818% (43721/47104)
01/13/2023 23:15:51 - INFO - __main__ -   test: [batch: 368/391 ] | Loss: 0.967 | Acc: 76.107% (35947/47232)/ 92.825% (43843/47232)
01/13/2023 23:15:54 - INFO - __main__ -   test: [batch: 369/391 ] | Loss: 0.966 | Acc: 76.125% (36053/47360)/ 92.836% (43967/47360)
01/13/2023 23:15:57 - INFO - __main__ -   test: [batch: 370/391 ] | Loss: 0.966 | Acc: 76.112% (36144/47488)/ 92.836% (44086/47488)
01/13/2023 23:15:59 - INFO - __main__ -   test: [batch: 371/391 ] | Loss: 0.966 | Acc: 76.115% (36243/47616)/ 92.839% (44206/47616)
01/13/2023 23:16:02 - INFO - __main__ -   test: [batch: 372/391 ] | Loss: 0.964 | Acc: 76.160% (36362/47744)/ 92.851% (44331/47744)
01/13/2023 23:16:04 - INFO - __main__ -   test: [batch: 373/391 ] | Loss: 0.963 | Acc: 76.195% (36476/47872)/ 92.858% (44453/47872)
01/13/2023 23:16:07 - INFO - __main__ -   test: [batch: 374/391 ] | Loss: 0.962 | Acc: 76.225% (36588/48000)/ 92.865% (44575/48000)
01/13/2023 23:16:10 - INFO - __main__ -   test: [batch: 375/391 ] | Loss: 0.965 | Acc: 76.155% (36652/48128)/ 92.830% (44677/48128)
01/13/2023 23:16:12 - INFO - __main__ -   test: [batch: 376/391 ] | Loss: 0.965 | Acc: 76.150% (36747/48256)/ 92.817% (44790/48256)
01/13/2023 23:16:15 - INFO - __main__ -   test: [batch: 377/391 ] | Loss: 0.966 | Acc: 76.137% (36838/48384)/ 92.812% (44906/48384)
01/13/2023 23:16:18 - INFO - __main__ -   test: [batch: 378/391 ] | Loss: 0.969 | Acc: 76.060% (36898/48512)/ 92.775% (45007/48512)
01/13/2023 23:16:20 - INFO - __main__ -   test: [batch: 379/391 ] | Loss: 0.969 | Acc: 76.046% (36989/48640)/ 92.782% (45129/48640)
01/13/2023 23:16:23 - INFO - __main__ -   test: [batch: 380/391 ] | Loss: 0.969 | Acc: 76.044% (37085/48768)/ 92.790% (45252/48768)
01/13/2023 23:16:26 - INFO - __main__ -   test: [batch: 381/391 ] | Loss: 0.970 | Acc: 76.000% (37161/48896)/ 92.793% (45372/48896)
01/13/2023 23:16:28 - INFO - __main__ -   test: [batch: 382/391 ] | Loss: 0.972 | Acc: 75.965% (37241/49024)/ 92.777% (45483/49024)
01/13/2023 23:16:31 - INFO - __main__ -   test: [batch: 383/391 ] | Loss: 0.972 | Acc: 75.972% (37342/49152)/ 92.775% (45601/49152)
01/13/2023 23:16:33 - INFO - __main__ -   test: [batch: 384/391 ] | Loss: 0.970 | Acc: 76.015% (37460/49280)/ 92.788% (45726/49280)
01/13/2023 23:16:36 - INFO - __main__ -   test: [batch: 385/391 ] | Loss: 0.969 | Acc: 76.024% (37562/49408)/ 92.797% (45849/49408)
01/13/2023 23:16:38 - INFO - __main__ -   test: [batch: 386/391 ] | Loss: 0.967 | Acc: 76.072% (37683/49536)/ 92.813% (45976/49536)
01/13/2023 23:16:41 - INFO - __main__ -   test: [batch: 387/391 ] | Loss: 0.966 | Acc: 76.111% (37800/49664)/ 92.824% (46100/49664)
01/13/2023 23:16:44 - INFO - __main__ -   test: [batch: 388/391 ] | Loss: 0.964 | Acc: 76.157% (37920/49792)/ 92.836% (46225/49792)
01/13/2023 23:16:46 - INFO - __main__ -   test: [batch: 389/391 ] | Loss: 0.963 | Acc: 76.160% (38019/49920)/ 92.841% (46346/49920)
01/13/2023 23:16:49 - INFO - __main__ -   test: [batch: 390/391 ] | Loss: 0.965 | Acc: 76.114% (38057/50000)/ 92.830% (46415/50000)
01/13/2023 23:16:49 - INFO - __main__ -   Final accuracy: 76.114
01/13/2023 23:16:49 - INFO - __main__ -   {'milestones': Counter({2: 1, 3: 1, 4: 1}), 'gamma': 0.1, 'base_lrs': [0.00025], 'last_epoch': 1, '_step_count': 2, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.00025]}
01/13/2023 23:16:49 - INFO - __main__ -   
Epoch: 1
01/13/2023 23:16:52 - INFO - __main__ -   test: [epoch: 1 | batch: 0/10010 ] | Loss: 0.630 | Acc: 86.719% (111/128)
01/13/2023 23:21:13 - INFO - __main__ -   test: [epoch: 1 | batch: 100/10010 ] | Loss: 0.826 | Acc: 79.672% (10300/12928)
01/13/2023 23:25:34 - INFO - __main__ -   test: [epoch: 1 | batch: 200/10010 ] | Loss: 0.825 | Acc: 79.489% (20451/25728)
01/13/2023 23:29:54 - INFO - __main__ -   test: [epoch: 1 | batch: 300/10010 ] | Loss: 0.839 | Acc: 79.163% (30500/38528)
01/13/2023 23:34:14 - INFO - __main__ -   test: [epoch: 1 | batch: 400/10010 ] | Loss: 0.852 | Acc: 78.924% (40510/51328)
01/13/2023 23:38:35 - INFO - __main__ -   test: [epoch: 1 | batch: 500/10010 ] | Loss: 0.852 | Acc: 78.883% (50586/64128)
01/13/2023 23:42:56 - INFO - __main__ -   test: [epoch: 1 | batch: 600/10010 ] | Loss: 0.855 | Acc: 78.810% (60627/76928)
01/13/2023 23:47:17 - INFO - __main__ -   test: [epoch: 1 | batch: 700/10010 ] | Loss: 0.855 | Acc: 78.776% (70684/89728)
01/13/2023 23:51:38 - INFO - __main__ -   test: [epoch: 1 | batch: 800/10010 ] | Loss: 0.854 | Acc: 78.762% (80753/102528)
01/13/2023 23:55:58 - INFO - __main__ -   test: [epoch: 1 | batch: 900/10010 ] | Loss: 0.855 | Acc: 78.728% (90795/115328)
01/14/2023 00:00:18 - INFO - __main__ -   test: [epoch: 1 | batch: 1000/10010 ] | Loss: 0.855 | Acc: 78.692% (100826/128128)
01/14/2023 00:04:38 - INFO - __main__ -   test: [epoch: 1 | batch: 1100/10010 ] | Loss: 0.857 | Acc: 78.638% (110823/140928)
01/14/2023 00:09:00 - INFO - __main__ -   test: [epoch: 1 | batch: 1200/10010 ] | Loss: 0.857 | Acc: 78.653% (120912/153728)
01/14/2023 00:13:23 - INFO - __main__ -   test: [epoch: 1 | batch: 1300/10010 ] | Loss: 0.854 | Acc: 78.725% (131100/166528)
01/14/2023 00:17:44 - INFO - __main__ -   test: [epoch: 1 | batch: 1400/10010 ] | Loss: 0.854 | Acc: 78.741% (141205/179328)
01/14/2023 00:22:05 - INFO - __main__ -   test: [epoch: 1 | batch: 1500/10010 ] | Loss: 0.854 | Acc: 78.731% (151265/192128)
01/14/2023 00:26:27 - INFO - __main__ -   test: [epoch: 1 | batch: 1600/10010 ] | Loss: 0.856 | Acc: 78.688% (161253/204928)
01/14/2023 00:30:47 - INFO - __main__ -   test: [epoch: 1 | batch: 1700/10010 ] | Loss: 0.856 | Acc: 78.696% (171344/217728)
01/14/2023 00:35:07 - INFO - __main__ -   test: [epoch: 1 | batch: 1800/10010 ] | Loss: 0.856 | Acc: 78.705% (181438/230528)
01/14/2023 00:39:29 - INFO - __main__ -   test: [epoch: 1 | batch: 1900/10010 ] | Loss: 0.855 | Acc: 78.740% (191597/243328)
01/14/2023 00:43:49 - INFO - __main__ -   test: [epoch: 1 | batch: 2000/10010 ] | Loss: 0.854 | Acc: 78.743% (201684/256128)
01/14/2023 00:48:10 - INFO - __main__ -   test: [epoch: 1 | batch: 2100/10010 ] | Loss: 0.854 | Acc: 78.733% (211736/268928)
01/14/2023 00:52:31 - INFO - __main__ -   test: [epoch: 1 | batch: 2200/10010 ] | Loss: 0.855 | Acc: 78.698% (221715/281728)
01/14/2023 00:56:52 - INFO - __main__ -   test: [epoch: 1 | batch: 2300/10010 ] | Loss: 0.856 | Acc: 78.694% (231775/294528)
01/14/2023 01:01:13 - INFO - __main__ -   test: [epoch: 1 | batch: 2400/10010 ] | Loss: 0.855 | Acc: 78.727% (241950/307328)
01/14/2023 01:05:32 - INFO - __main__ -   test: [epoch: 1 | batch: 2500/10010 ] | Loss: 0.856 | Acc: 78.697% (251931/320128)
01/14/2023 01:09:52 - INFO - __main__ -   test: [epoch: 1 | batch: 2600/10010 ] | Loss: 0.856 | Acc: 78.700% (262016/332928)
01/14/2023 01:14:15 - INFO - __main__ -   test: [epoch: 1 | batch: 2700/10010 ] | Loss: 0.856 | Acc: 78.698% (272080/345728)
01/14/2023 01:18:34 - INFO - __main__ -   test: [epoch: 1 | batch: 2800/10010 ] | Loss: 0.856 | Acc: 78.688% (282119/358528)
01/14/2023 01:22:55 - INFO - __main__ -   test: [epoch: 1 | batch: 2900/10010 ] | Loss: 0.857 | Acc: 78.689% (292195/371328)
01/14/2023 01:27:16 - INFO - __main__ -   test: [epoch: 1 | batch: 3000/10010 ] | Loss: 0.856 | Acc: 78.704% (302324/384128)
01/14/2023 01:31:36 - INFO - __main__ -   test: [epoch: 1 | batch: 3100/10010 ] | Loss: 0.856 | Acc: 78.701% (312386/396928)
01/14/2023 01:35:57 - INFO - __main__ -   test: [epoch: 1 | batch: 3200/10010 ] | Loss: 0.857 | Acc: 78.686% (322399/409728)
01/14/2023 01:40:19 - INFO - __main__ -   test: [epoch: 1 | batch: 3300/10010 ] | Loss: 0.857 | Acc: 78.670% (332403/422528)
01/14/2023 01:44:41 - INFO - __main__ -   test: [epoch: 1 | batch: 3400/10010 ] | Loss: 0.857 | Acc: 78.683% (342528/435328)
01/14/2023 01:49:01 - INFO - __main__ -   test: [epoch: 1 | batch: 3500/10010 ] | Loss: 0.857 | Acc: 78.678% (352580/448128)
01/14/2023 01:53:22 - INFO - __main__ -   test: [epoch: 1 | batch: 3600/10010 ] | Loss: 0.857 | Acc: 78.680% (362660/460928)
01/14/2023 01:57:43 - INFO - __main__ -   test: [epoch: 1 | batch: 3700/10010 ] | Loss: 0.858 | Acc: 78.667% (372669/473728)
01/14/2023 02:02:04 - INFO - __main__ -   test: [epoch: 1 | batch: 3800/10010 ] | Loss: 0.858 | Acc: 78.666% (382733/486528)
01/14/2023 02:06:26 - INFO - __main__ -   test: [epoch: 1 | batch: 3900/10010 ] | Loss: 0.857 | Acc: 78.677% (392858/499328)
01/14/2023 02:10:47 - INFO - __main__ -   test: [epoch: 1 | batch: 4000/10010 ] | Loss: 0.858 | Acc: 78.672% (402901/512128)
01/14/2023 02:15:09 - INFO - __main__ -   test: [epoch: 1 | batch: 4100/10010 ] | Loss: 0.857 | Acc: 78.684% (413032/524928)
01/14/2023 02:19:30 - INFO - __main__ -   test: [epoch: 1 | batch: 4200/10010 ] | Loss: 0.857 | Acc: 78.675% (423060/537728)
01/14/2023 02:23:50 - INFO - __main__ -   test: [epoch: 1 | batch: 4300/10010 ] | Loss: 0.857 | Acc: 78.682% (433165/550528)
01/14/2023 02:28:10 - INFO - __main__ -   test: [epoch: 1 | batch: 4400/10010 ] | Loss: 0.857 | Acc: 78.686% (443263/563328)
01/14/2023 02:32:32 - INFO - __main__ -   test: [epoch: 1 | batch: 4500/10010 ] | Loss: 0.857 | Acc: 78.680% (453300/576128)
01/14/2023 02:36:53 - INFO - __main__ -   test: [epoch: 1 | batch: 4600/10010 ] | Loss: 0.857 | Acc: 78.678% (463357/588928)
01/14/2023 02:41:17 - INFO - __main__ -   test: [epoch: 1 | batch: 4700/10010 ] | Loss: 0.857 | Acc: 78.691% (473507/601728)
01/14/2023 02:45:38 - INFO - __main__ -   test: [epoch: 1 | batch: 4800/10010 ] | Loss: 0.856 | Acc: 78.698% (483624/614528)
01/14/2023 02:49:58 - INFO - __main__ -   test: [epoch: 1 | batch: 4900/10010 ] | Loss: 0.856 | Acc: 78.694% (493670/627328)
01/14/2023 02:54:19 - INFO - __main__ -   test: [epoch: 1 | batch: 5000/10010 ] | Loss: 0.856 | Acc: 78.707% (503825/640128)
01/14/2023 02:58:39 - INFO - __main__ -   test: [epoch: 1 | batch: 5100/10010 ] | Loss: 0.856 | Acc: 78.707% (513899/652928)
01/14/2023 03:02:59 - INFO - __main__ -   test: [epoch: 1 | batch: 5200/10010 ] | Loss: 0.856 | Acc: 78.697% (523909/665728)
01/14/2023 03:07:21 - INFO - __main__ -   test: [epoch: 1 | batch: 5300/10010 ] | Loss: 0.857 | Acc: 78.693% (533952/678528)
01/14/2023 03:11:42 - INFO - __main__ -   test: [epoch: 1 | batch: 5400/10010 ] | Loss: 0.857 | Acc: 78.692% (544017/691328)
01/14/2023 03:16:03 - INFO - __main__ -   test: [epoch: 1 | batch: 5500/10010 ] | Loss: 0.857 | Acc: 78.683% (554031/704128)
01/14/2023 03:20:25 - INFO - __main__ -   test: [epoch: 1 | batch: 5600/10010 ] | Loss: 0.856 | Acc: 78.697% (564198/716928)
01/14/2023 03:24:46 - INFO - __main__ -   test: [epoch: 1 | batch: 5700/10010 ] | Loss: 0.856 | Acc: 78.702% (574310/729728)
01/14/2023 03:29:05 - INFO - __main__ -   test: [epoch: 1 | batch: 5800/10010 ] | Loss: 0.856 | Acc: 78.710% (584445/742528)
01/14/2023 03:33:25 - INFO - __main__ -   test: [epoch: 1 | batch: 5900/10010 ] | Loss: 0.856 | Acc: 78.709% (594513/755328)
01/14/2023 03:37:45 - INFO - __main__ -   test: [epoch: 1 | batch: 6000/10010 ] | Loss: 0.856 | Acc: 78.712% (604611/768128)
01/14/2023 03:42:05 - INFO - __main__ -   test: [epoch: 1 | batch: 6100/10010 ] | Loss: 0.856 | Acc: 78.699% (614584/780928)
01/14/2023 03:46:26 - INFO - __main__ -   test: [epoch: 1 | batch: 6200/10010 ] | Loss: 0.856 | Acc: 78.699% (624657/793728)
01/14/2023 03:50:48 - INFO - __main__ -   test: [epoch: 1 | batch: 6300/10010 ] | Loss: 0.856 | Acc: 78.696% (634709/806528)
01/14/2023 03:55:10 - INFO - __main__ -   test: [epoch: 1 | batch: 6400/10010 ] | Loss: 0.856 | Acc: 78.683% (644675/819328)
01/14/2023 03:59:31 - INFO - __main__ -   test: [epoch: 1 | batch: 6500/10010 ] | Loss: 0.856 | Acc: 78.685% (654761/832128)
01/14/2023 04:03:51 - INFO - __main__ -   test: [epoch: 1 | batch: 6600/10010 ] | Loss: 0.856 | Acc: 78.690% (664871/844928)
01/14/2023 04:08:11 - INFO - __main__ -   test: [epoch: 1 | batch: 6700/10010 ] | Loss: 0.856 | Acc: 78.682% (674881/857728)
01/14/2023 04:12:32 - INFO - __main__ -   test: [epoch: 1 | batch: 6800/10010 ] | Loss: 0.856 | Acc: 78.674% (684882/870528)
01/14/2023 04:16:51 - INFO - __main__ -   test: [epoch: 1 | batch: 6900/10010 ] | Loss: 0.856 | Acc: 78.678% (694988/883328)
01/14/2023 04:21:13 - INFO - __main__ -   test: [epoch: 1 | batch: 7000/10010 ] | Loss: 0.856 | Acc: 78.677% (705046/896128)
01/14/2023 04:25:34 - INFO - __main__ -   test: [epoch: 1 | batch: 7100/10010 ] | Loss: 0.856 | Acc: 78.683% (715173/908928)
01/14/2023 04:29:53 - INFO - __main__ -   test: [epoch: 1 | batch: 7200/10010 ] | Loss: 0.856 | Acc: 78.683% (725240/921728)
01/14/2023 04:34:14 - INFO - __main__ -   test: [epoch: 1 | batch: 7300/10010 ] | Loss: 0.856 | Acc: 78.680% (735287/934528)
01/14/2023 04:38:36 - INFO - __main__ -   test: [epoch: 1 | batch: 7400/10010 ] | Loss: 0.856 | Acc: 78.673% (745293/947328)
01/14/2023 04:42:57 - INFO - __main__ -   test: [epoch: 1 | batch: 7500/10010 ] | Loss: 0.856 | Acc: 78.673% (755363/960128)
01/14/2023 04:47:18 - INFO - __main__ -   test: [epoch: 1 | batch: 7600/10010 ] | Loss: 0.857 | Acc: 78.671% (765412/972928)
01/14/2023 04:51:39 - INFO - __main__ -   test: [epoch: 1 | batch: 7700/10010 ] | Loss: 0.857 | Acc: 78.669% (775464/985728)
01/14/2023 04:56:01 - INFO - __main__ -   test: [epoch: 1 | batch: 7800/10010 ] | Loss: 0.857 | Acc: 78.658% (785421/998528)
01/14/2023 05:00:23 - INFO - __main__ -   test: [epoch: 1 | batch: 7900/10010 ] | Loss: 0.857 | Acc: 78.661% (795517/1011328)
01/14/2023 05:04:44 - INFO - __main__ -   test: [epoch: 1 | batch: 8000/10010 ] | Loss: 0.857 | Acc: 78.664% (805618/1024128)
01/14/2023 05:09:05 - INFO - __main__ -   test: [epoch: 1 | batch: 8100/10010 ] | Loss: 0.857 | Acc: 78.675% (815801/1036928)
01/14/2023 05:13:25 - INFO - __main__ -   test: [epoch: 1 | batch: 8200/10010 ] | Loss: 0.857 | Acc: 78.671% (825828/1049728)
01/14/2023 05:17:47 - INFO - __main__ -   test: [epoch: 1 | batch: 8300/10010 ] | Loss: 0.857 | Acc: 78.667% (835860/1062528)
01/14/2023 05:22:08 - INFO - __main__ -   test: [epoch: 1 | batch: 8400/10010 ] | Loss: 0.857 | Acc: 78.667% (845925/1075328)
01/14/2023 05:26:27 - INFO - __main__ -   test: [epoch: 1 | batch: 8500/10010 ] | Loss: 0.857 | Acc: 78.671% (856041/1088128)
01/14/2023 05:30:49 - INFO - __main__ -   test: [epoch: 1 | batch: 8600/10010 ] | Loss: 0.857 | Acc: 78.668% (866080/1100928)
01/14/2023 05:35:09 - INFO - __main__ -   test: [epoch: 1 | batch: 8700/10010 ] | Loss: 0.857 | Acc: 78.676% (876234/1113728)
01/14/2023 05:39:28 - INFO - __main__ -   test: [epoch: 1 | batch: 8800/10010 ] | Loss: 0.857 | Acc: 78.683% (886383/1126528)
01/14/2023 05:43:47 - INFO - __main__ -   test: [epoch: 1 | batch: 8900/10010 ] | Loss: 0.856 | Acc: 78.686% (896489/1139328)
01/14/2023 05:48:08 - INFO - __main__ -   test: [epoch: 1 | batch: 9000/10010 ] | Loss: 0.856 | Acc: 78.688% (906582/1152128)
01/14/2023 05:52:30 - INFO - __main__ -   test: [epoch: 1 | batch: 9100/10010 ] | Loss: 0.856 | Acc: 78.687% (916651/1164928)
01/14/2023 05:56:50 - INFO - __main__ -   test: [epoch: 1 | batch: 9200/10010 ] | Loss: 0.856 | Acc: 78.688% (926736/1177728)
01/14/2023 06:01:12 - INFO - __main__ -   test: [epoch: 1 | batch: 9300/10010 ] | Loss: 0.856 | Acc: 78.695% (936882/1190528)
01/14/2023 06:05:32 - INFO - __main__ -   test: [epoch: 1 | batch: 9400/10010 ] | Loss: 0.856 | Acc: 78.691% (946912/1203328)
01/14/2023 06:09:56 - INFO - __main__ -   test: [epoch: 1 | batch: 9500/10010 ] | Loss: 0.856 | Acc: 78.693% (957011/1216128)
01/14/2023 06:14:25 - INFO - __main__ -   test: [epoch: 1 | batch: 9600/10010 ] | Loss: 0.856 | Acc: 78.691% (967061/1228928)
01/14/2023 06:18:45 - INFO - __main__ -   test: [epoch: 1 | batch: 9700/10010 ] | Loss: 0.856 | Acc: 78.693% (977155/1241728)
01/14/2023 06:23:07 - INFO - __main__ -   test: [epoch: 1 | batch: 9800/10010 ] | Loss: 0.857 | Acc: 78.692% (987210/1254528)
01/14/2023 06:27:30 - INFO - __main__ -   test: [epoch: 1 | batch: 9900/10010 ] | Loss: 0.857 | Acc: 78.693% (997297/1267328)
01/14/2023 06:31:53 - INFO - __main__ -   test: [epoch: 1 | batch: 10000/10010 ] | Loss: 0.857 | Acc: 78.690% (1007336/1280128)
01/14/2023 06:32:17 - INFO - __main__ -   Saving Checkpoint
01/14/2023 06:32:19 - INFO - __main__ -   test: [batch: 0/391 ] | Loss: 0.448 | Acc: 86.719% (111/128)/ 97.656% (125/128)
01/14/2023 06:32:22 - INFO - __main__ -   test: [batch: 1/391 ] | Loss: 0.458 | Acc: 85.938% (220/256)/ 98.047% (251/256)
01/14/2023 06:32:25 - INFO - __main__ -   test: [batch: 2/391 ] | Loss: 0.613 | Acc: 82.552% (317/384)/ 96.094% (369/384)
01/14/2023 06:32:27 - INFO - __main__ -   test: [batch: 3/391 ] | Loss: 0.568 | Acc: 84.375% (432/512)/ 96.680% (495/512)
01/14/2023 06:32:30 - INFO - __main__ -   test: [batch: 4/391 ] | Loss: 0.487 | Acc: 86.719% (555/640)/ 97.188% (622/640)
01/14/2023 06:32:33 - INFO - __main__ -   test: [batch: 5/391 ] | Loss: 0.431 | Acc: 87.891% (675/768)/ 97.656% (750/768)
01/14/2023 06:32:35 - INFO - __main__ -   test: [batch: 6/391 ] | Loss: 0.425 | Acc: 88.058% (789/896)/ 97.545% (874/896)
01/14/2023 06:32:38 - INFO - __main__ -   test: [batch: 7/391 ] | Loss: 0.406 | Acc: 89.062% (912/1024)/ 97.656% (1000/1024)
01/14/2023 06:32:40 - INFO - __main__ -   test: [batch: 8/391 ] | Loss: 0.423 | Acc: 89.149% (1027/1152)/ 97.656% (1125/1152)
01/14/2023 06:32:43 - INFO - __main__ -   test: [batch: 9/391 ] | Loss: 0.402 | Acc: 89.688% (1148/1280)/ 97.734% (1251/1280)
01/14/2023 06:32:46 - INFO - __main__ -   test: [batch: 10/391 ] | Loss: 0.451 | Acc: 88.423% (1245/1408)/ 97.727% (1376/1408)
01/14/2023 06:32:48 - INFO - __main__ -   test: [batch: 11/391 ] | Loss: 0.454 | Acc: 88.607% (1361/1536)/ 97.526% (1498/1536)
01/14/2023 06:32:51 - INFO - __main__ -   test: [batch: 12/391 ] | Loss: 0.504 | Acc: 87.560% (1457/1664)/ 97.175% (1617/1664)
01/14/2023 06:32:53 - INFO - __main__ -   test: [batch: 13/391 ] | Loss: 0.555 | Acc: 85.993% (1541/1792)/ 96.596% (1731/1792)
01/14/2023 06:32:56 - INFO - __main__ -   test: [batch: 14/391 ] | Loss: 0.576 | Acc: 85.156% (1635/1920)/ 96.667% (1856/1920)
01/14/2023 06:32:59 - INFO - __main__ -   test: [batch: 15/391 ] | Loss: 0.584 | Acc: 84.717% (1735/2048)/ 96.777% (1982/2048)
01/14/2023 06:33:01 - INFO - __main__ -   test: [batch: 16/391 ] | Loss: 0.594 | Acc: 84.559% (1840/2176)/ 96.599% (2102/2176)
01/14/2023 06:33:04 - INFO - __main__ -   test: [batch: 17/391 ] | Loss: 0.621 | Acc: 84.158% (1939/2304)/ 96.137% (2215/2304)
01/14/2023 06:33:06 - INFO - __main__ -   test: [batch: 18/391 ] | Loss: 0.643 | Acc: 83.594% (2033/2432)/ 95.970% (2334/2432)
01/14/2023 06:33:09 - INFO - __main__ -   test: [batch: 19/391 ] | Loss: 0.652 | Acc: 83.320% (2133/2560)/ 95.898% (2455/2560)
01/14/2023 06:33:12 - INFO - __main__ -   test: [batch: 20/391 ] | Loss: 0.649 | Acc: 83.408% (2242/2688)/ 95.833% (2576/2688)
01/14/2023 06:33:14 - INFO - __main__ -   test: [batch: 21/391 ] | Loss: 0.680 | Acc: 82.706% (2329/2816)/ 95.632% (2693/2816)
01/14/2023 06:33:17 - INFO - __main__ -   test: [batch: 22/391 ] | Loss: 0.679 | Acc: 82.609% (2432/2944)/ 95.652% (2816/2944)
01/14/2023 06:33:20 - INFO - __main__ -   test: [batch: 23/391 ] | Loss: 0.724 | Acc: 81.771% (2512/3072)/ 95.378% (2930/3072)
01/14/2023 06:33:22 - INFO - __main__ -   test: [batch: 24/391 ] | Loss: 0.743 | Acc: 81.281% (2601/3200)/ 95.188% (3046/3200)
01/14/2023 06:33:25 - INFO - __main__ -   test: [batch: 25/391 ] | Loss: 0.768 | Acc: 80.589% (2682/3328)/ 94.922% (3159/3328)
01/14/2023 06:33:28 - INFO - __main__ -   test: [batch: 26/391 ] | Loss: 0.783 | Acc: 79.861% (2760/3456)/ 94.850% (3278/3456)
01/14/2023 06:33:30 - INFO - __main__ -   test: [batch: 27/391 ] | Loss: 0.765 | Acc: 80.357% (2880/3584)/ 94.894% (3401/3584)
01/14/2023 06:33:33 - INFO - __main__ -   test: [batch: 28/391 ] | Loss: 0.774 | Acc: 79.822% (2963/3712)/ 94.962% (3525/3712)
01/14/2023 06:33:35 - INFO - __main__ -   test: [batch: 29/391 ] | Loss: 0.766 | Acc: 79.974% (3071/3840)/ 95.078% (3651/3840)
01/14/2023 06:33:38 - INFO - __main__ -   test: [batch: 30/391 ] | Loss: 0.781 | Acc: 79.940% (3172/3968)/ 94.960% (3768/3968)
01/14/2023 06:33:40 - INFO - __main__ -   test: [batch: 31/391 ] | Loss: 0.776 | Acc: 80.127% (3282/4096)/ 95.020% (3892/4096)
01/14/2023 06:33:43 - INFO - __main__ -   test: [batch: 32/391 ] | Loss: 0.762 | Acc: 80.421% (3397/4224)/ 95.123% (4018/4224)
01/14/2023 06:33:46 - INFO - __main__ -   test: [batch: 33/391 ] | Loss: 0.755 | Acc: 80.561% (3506/4352)/ 95.175% (4142/4352)
01/14/2023 06:33:49 - INFO - __main__ -   test: [batch: 34/391 ] | Loss: 0.742 | Acc: 80.938% (3626/4480)/ 95.223% (4266/4480)
01/14/2023 06:33:51 - INFO - __main__ -   test: [batch: 35/391 ] | Loss: 0.729 | Acc: 81.315% (3747/4608)/ 95.247% (4389/4608)
01/14/2023 06:33:54 - INFO - __main__ -   test: [batch: 36/391 ] | Loss: 0.714 | Acc: 81.757% (3872/4736)/ 95.355% (4516/4736)
01/14/2023 06:33:56 - INFO - __main__ -   test: [batch: 37/391 ] | Loss: 0.704 | Acc: 82.052% (3991/4864)/ 95.415% (4641/4864)
01/14/2023 06:33:59 - INFO - __main__ -   test: [batch: 38/391 ] | Loss: 0.696 | Acc: 82.232% (4105/4992)/ 95.493% (4767/4992)
01/14/2023 06:34:02 - INFO - __main__ -   test: [batch: 39/391 ] | Loss: 0.691 | Acc: 82.266% (4212/5120)/ 95.527% (4891/5120)
01/14/2023 06:34:04 - INFO - __main__ -   test: [batch: 40/391 ] | Loss: 0.690 | Acc: 82.374% (4323/5248)/ 95.446% (5009/5248)
01/14/2023 06:34:07 - INFO - __main__ -   test: [batch: 41/391 ] | Loss: 0.694 | Acc: 82.422% (4431/5376)/ 95.387% (5128/5376)
01/14/2023 06:34:09 - INFO - __main__ -   test: [batch: 42/391 ] | Loss: 0.693 | Acc: 82.395% (4535/5504)/ 95.422% (5252/5504)
01/14/2023 06:34:12 - INFO - __main__ -   test: [batch: 43/391 ] | Loss: 0.690 | Acc: 82.475% (4645/5632)/ 95.384% (5372/5632)
01/14/2023 06:34:15 - INFO - __main__ -   test: [batch: 44/391 ] | Loss: 0.693 | Acc: 82.500% (4752/5760)/ 95.312% (5490/5760)
01/14/2023 06:34:17 - INFO - __main__ -   test: [batch: 45/391 ] | Loss: 0.689 | Acc: 82.677% (4868/5888)/ 95.312% (5612/5888)
01/14/2023 06:34:20 - INFO - __main__ -   test: [batch: 46/391 ] | Loss: 0.693 | Acc: 82.663% (4973/6016)/ 95.329% (5735/6016)
01/14/2023 06:34:22 - INFO - __main__ -   test: [batch: 47/391 ] | Loss: 0.694 | Acc: 82.601% (5075/6144)/ 95.394% (5861/6144)
01/14/2023 06:34:25 - INFO - __main__ -   test: [batch: 48/391 ] | Loss: 0.700 | Acc: 82.462% (5172/6272)/ 95.360% (5981/6272)
01/14/2023 06:34:28 - INFO - __main__ -   test: [batch: 49/391 ] | Loss: 0.703 | Acc: 82.516% (5281/6400)/ 95.281% (6098/6400)
01/14/2023 06:34:30 - INFO - __main__ -   test: [batch: 50/391 ] | Loss: 0.694 | Acc: 82.721% (5400/6528)/ 95.328% (6223/6528)
01/14/2023 06:34:33 - INFO - __main__ -   test: [batch: 51/391 ] | Loss: 0.686 | Acc: 82.903% (5518/6656)/ 95.418% (6351/6656)
01/14/2023 06:34:35 - INFO - __main__ -   test: [batch: 52/391 ] | Loss: 0.683 | Acc: 82.989% (5630/6784)/ 95.445% (6475/6784)
01/14/2023 06:34:38 - INFO - __main__ -   test: [batch: 53/391 ] | Loss: 0.674 | Acc: 83.203% (5751/6912)/ 95.515% (6602/6912)
01/14/2023 06:34:41 - INFO - __main__ -   test: [batch: 54/391 ] | Loss: 0.667 | Acc: 83.338% (5867/7040)/ 95.568% (6728/7040)
01/14/2023 06:34:43 - INFO - __main__ -   test: [batch: 55/391 ] | Loss: 0.660 | Acc: 83.482% (5984/7168)/ 95.605% (6853/7168)
01/14/2023 06:34:46 - INFO - __main__ -   test: [batch: 56/391 ] | Loss: 0.652 | Acc: 83.703% (6107/7296)/ 95.655% (6979/7296)
01/14/2023 06:34:48 - INFO - __main__ -   test: [batch: 57/391 ] | Loss: 0.645 | Acc: 83.890% (6228/7424)/ 95.703% (7105/7424)
01/14/2023 06:34:51 - INFO - __main__ -   test: [batch: 58/391 ] | Loss: 0.643 | Acc: 83.951% (6340/7552)/ 95.683% (7226/7552)
01/14/2023 06:34:53 - INFO - __main__ -   test: [batch: 59/391 ] | Loss: 0.645 | Acc: 83.880% (6442/7680)/ 95.703% (7350/7680)
01/14/2023 06:34:56 - INFO - __main__ -   test: [batch: 60/391 ] | Loss: 0.650 | Acc: 83.786% (6542/7808)/ 95.684% (7471/7808)
01/14/2023 06:34:59 - INFO - __main__ -   test: [batch: 61/391 ] | Loss: 0.650 | Acc: 83.795% (6650/7936)/ 95.678% (7593/7936)
01/14/2023 06:35:01 - INFO - __main__ -   test: [batch: 62/391 ] | Loss: 0.650 | Acc: 83.767% (6755/8064)/ 95.709% (7718/8064)
01/14/2023 06:35:04 - INFO - __main__ -   test: [batch: 63/391 ] | Loss: 0.658 | Acc: 83.630% (6851/8192)/ 95.654% (7836/8192)
01/14/2023 06:35:07 - INFO - __main__ -   test: [batch: 64/391 ] | Loss: 0.666 | Acc: 83.450% (6943/8320)/ 95.613% (7955/8320)
01/14/2023 06:35:09 - INFO - __main__ -   test: [batch: 65/391 ] | Loss: 0.679 | Acc: 82.955% (7008/8448)/ 95.537% (8071/8448)
01/14/2023 06:35:12 - INFO - __main__ -   test: [batch: 66/391 ] | Loss: 0.684 | Acc: 82.952% (7114/8576)/ 95.499% (8190/8576)
01/14/2023 06:35:14 - INFO - __main__ -   test: [batch: 67/391 ] | Loss: 0.687 | Acc: 82.881% (7214/8704)/ 95.508% (8313/8704)
01/14/2023 06:35:17 - INFO - __main__ -   test: [batch: 68/391 ] | Loss: 0.688 | Acc: 82.835% (7316/8832)/ 95.528% (8437/8832)
01/14/2023 06:35:19 - INFO - __main__ -   test: [batch: 69/391 ] | Loss: 0.683 | Acc: 82.935% (7431/8960)/ 95.558% (8562/8960)
01/14/2023 06:35:22 - INFO - __main__ -   test: [batch: 70/391 ] | Loss: 0.684 | Acc: 82.868% (7531/9088)/ 95.555% (8684/9088)
01/14/2023 06:35:24 - INFO - __main__ -   test: [batch: 71/391 ] | Loss: 0.683 | Acc: 82.910% (7641/9216)/ 95.562% (8807/9216)
01/14/2023 06:35:27 - INFO - __main__ -   test: [batch: 72/391 ] | Loss: 0.685 | Acc: 82.791% (7736/9344)/ 95.580% (8931/9344)
01/14/2023 06:35:30 - INFO - __main__ -   test: [batch: 73/391 ] | Loss: 0.689 | Acc: 82.665% (7830/9472)/ 95.587% (9054/9472)
01/14/2023 06:35:32 - INFO - __main__ -   test: [batch: 74/391 ] | Loss: 0.690 | Acc: 82.635% (7933/9600)/ 95.573% (9175/9600)
01/14/2023 06:35:35 - INFO - __main__ -   test: [batch: 75/391 ] | Loss: 0.695 | Acc: 82.473% (8023/9728)/ 95.559% (9296/9728)
01/14/2023 06:35:37 - INFO - __main__ -   test: [batch: 76/391 ] | Loss: 0.695 | Acc: 82.447% (8126/9856)/ 95.566% (9419/9856)
01/14/2023 06:35:40 - INFO - __main__ -   test: [batch: 77/391 ] | Loss: 0.696 | Acc: 82.402% (8227/9984)/ 95.593% (9544/9984)
01/14/2023 06:35:43 - INFO - __main__ -   test: [batch: 78/391 ] | Loss: 0.695 | Acc: 82.288% (8321/10112)/ 95.639% (9671/10112)
01/14/2023 06:35:45 - INFO - __main__ -   test: [batch: 79/391 ] | Loss: 0.694 | Acc: 82.275% (8425/10240)/ 95.664% (9796/10240)
01/14/2023 06:35:48 - INFO - __main__ -   test: [batch: 80/391 ] | Loss: 0.693 | Acc: 82.263% (8529/10368)/ 95.669% (9919/10368)
01/14/2023 06:35:50 - INFO - __main__ -   test: [batch: 81/391 ] | Loss: 0.692 | Acc: 82.260% (8634/10496)/ 95.694% (10044/10496)
01/14/2023 06:35:53 - INFO - __main__ -   test: [batch: 82/391 ] | Loss: 0.692 | Acc: 82.276% (8741/10624)/ 95.689% (10166/10624)
01/14/2023 06:35:55 - INFO - __main__ -   test: [batch: 83/391 ] | Loss: 0.691 | Acc: 82.301% (8849/10752)/ 95.675% (10287/10752)
01/14/2023 06:35:58 - INFO - __main__ -   test: [batch: 84/391 ] | Loss: 0.688 | Acc: 82.426% (8968/10880)/ 95.708% (10413/10880)
01/14/2023 06:36:00 - INFO - __main__ -   test: [batch: 85/391 ] | Loss: 0.686 | Acc: 82.422% (9073/11008)/ 95.749% (10540/11008)
01/14/2023 06:36:03 - INFO - __main__ -   test: [batch: 86/391 ] | Loss: 0.690 | Acc: 82.337% (9169/11136)/ 95.726% (10660/11136)
01/14/2023 06:36:05 - INFO - __main__ -   test: [batch: 87/391 ] | Loss: 0.688 | Acc: 82.360% (9277/11264)/ 95.730% (10783/11264)
01/14/2023 06:36:08 - INFO - __main__ -   test: [batch: 88/391 ] | Loss: 0.695 | Acc: 82.286% (9374/11392)/ 95.664% (10898/11392)
01/14/2023 06:36:10 - INFO - __main__ -   test: [batch: 89/391 ] | Loss: 0.693 | Acc: 82.344% (9486/11520)/ 95.677% (11022/11520)
01/14/2023 06:36:13 - INFO - __main__ -   test: [batch: 90/391 ] | Loss: 0.694 | Acc: 82.263% (9582/11648)/ 95.673% (11144/11648)
01/14/2023 06:36:16 - INFO - __main__ -   test: [batch: 91/391 ] | Loss: 0.693 | Acc: 82.303% (9692/11776)/ 95.686% (11268/11776)
01/14/2023 06:36:18 - INFO - __main__ -   test: [batch: 92/391 ] | Loss: 0.695 | Acc: 82.283% (9795/11904)/ 95.649% (11386/11904)
01/14/2023 06:36:21 - INFO - __main__ -   test: [batch: 93/391 ] | Loss: 0.698 | Acc: 82.081% (9876/12032)/ 95.678% (11512/12032)
01/14/2023 06:36:24 - INFO - __main__ -   test: [batch: 94/391 ] | Loss: 0.700 | Acc: 81.965% (9967/12160)/ 95.691% (11636/12160)
01/14/2023 06:36:26 - INFO - __main__ -   test: [batch: 95/391 ] | Loss: 0.698 | Acc: 82.023% (10079/12288)/ 95.695% (11759/12288)
01/14/2023 06:36:29 - INFO - __main__ -   test: [batch: 96/391 ] | Loss: 0.701 | Acc: 81.935% (10173/12416)/ 95.707% (11883/12416)
01/14/2023 06:36:31 - INFO - __main__ -   test: [batch: 97/391 ] | Loss: 0.703 | Acc: 81.752% (10255/12544)/ 95.727% (12008/12544)
01/14/2023 06:36:34 - INFO - __main__ -   test: [batch: 98/391 ] | Loss: 0.700 | Acc: 81.858% (10373/12672)/ 95.754% (12134/12672)
01/14/2023 06:36:36 - INFO - __main__ -   test: [batch: 99/391 ] | Loss: 0.694 | Acc: 82.000% (10496/12800)/ 95.797% (12262/12800)
01/14/2023 06:36:39 - INFO - __main__ -   test: [batch: 100/391 ] | Loss: 0.693 | Acc: 82.054% (10608/12928)/ 95.815% (12387/12928)
01/14/2023 06:36:41 - INFO - __main__ -   test: [batch: 101/391 ] | Loss: 0.689 | Acc: 82.154% (10726/13056)/ 95.849% (12514/13056)
01/14/2023 06:36:44 - INFO - __main__ -   test: [batch: 102/391 ] | Loss: 0.685 | Acc: 82.289% (10849/13184)/ 95.874% (12640/13184)
01/14/2023 06:36:47 - INFO - __main__ -   test: [batch: 103/391 ] | Loss: 0.687 | Acc: 82.121% (10932/13312)/ 95.898% (12766/13312)
01/14/2023 06:36:49 - INFO - __main__ -   test: [batch: 104/391 ] | Loss: 0.687 | Acc: 82.054% (11028/13440)/ 95.900% (12889/13440)
01/14/2023 06:36:52 - INFO - __main__ -   test: [batch: 105/391 ] | Loss: 0.687 | Acc: 82.031% (11130/13568)/ 95.895% (13011/13568)
01/14/2023 06:36:54 - INFO - __main__ -   test: [batch: 106/391 ] | Loss: 0.696 | Acc: 81.907% (11218/13696)/ 95.824% (13124/13696)
01/14/2023 06:36:57 - INFO - __main__ -   test: [batch: 107/391 ] | Loss: 0.692 | Acc: 82.031% (11340/13824)/ 95.855% (13251/13824)
01/14/2023 06:37:00 - INFO - __main__ -   test: [batch: 108/391 ] | Loss: 0.695 | Acc: 81.902% (11427/13952)/ 95.864% (13375/13952)
01/14/2023 06:37:02 - INFO - __main__ -   test: [batch: 109/391 ] | Loss: 0.696 | Acc: 81.882% (11529/14080)/ 95.866% (13498/14080)
01/14/2023 06:37:05 - INFO - __main__ -   test: [batch: 110/391 ] | Loss: 0.699 | Acc: 81.700% (11608/14208)/ 95.876% (13622/14208)
01/14/2023 06:37:08 - INFO - __main__ -   test: [batch: 111/391 ] | Loss: 0.702 | Acc: 81.662% (11707/14336)/ 95.843% (13740/14336)
01/14/2023 06:37:10 - INFO - __main__ -   test: [batch: 112/391 ] | Loss: 0.703 | Acc: 81.692% (11816/14464)/ 95.859% (13865/14464)
01/14/2023 06:37:13 - INFO - __main__ -   test: [batch: 113/391 ] | Loss: 0.702 | Acc: 81.737% (11927/14592)/ 95.868% (13989/14592)
01/14/2023 06:37:16 - INFO - __main__ -   test: [batch: 114/391 ] | Loss: 0.698 | Acc: 81.827% (12045/14720)/ 95.890% (14115/14720)
01/14/2023 06:37:19 - INFO - __main__ -   test: [batch: 115/391 ] | Loss: 0.695 | Acc: 81.910% (12162/14848)/ 95.912% (14241/14848)
01/14/2023 06:37:21 - INFO - __main__ -   test: [batch: 116/391 ] | Loss: 0.699 | Acc: 81.858% (12259/14976)/ 95.873% (14358/14976)
01/14/2023 06:37:24 - INFO - __main__ -   test: [batch: 117/391 ] | Loss: 0.697 | Acc: 81.879% (12367/15104)/ 95.882% (14482/15104)
01/14/2023 06:37:26 - INFO - __main__ -   test: [batch: 118/391 ] | Loss: 0.701 | Acc: 81.769% (12455/15232)/ 95.890% (14606/15232)
01/14/2023 06:37:29 - INFO - __main__ -   test: [batch: 119/391 ] | Loss: 0.699 | Acc: 81.816% (12567/15360)/ 95.911% (14732/15360)
01/14/2023 06:37:32 - INFO - __main__ -   test: [batch: 120/391 ] | Loss: 0.697 | Acc: 81.857% (12678/15488)/ 95.919% (14856/15488)
01/14/2023 06:37:34 - INFO - __main__ -   test: [batch: 121/391 ] | Loss: 0.702 | Acc: 81.717% (12761/15616)/ 95.889% (14974/15616)
01/14/2023 06:37:37 - INFO - __main__ -   test: [batch: 122/391 ] | Loss: 0.704 | Acc: 81.663% (12857/15744)/ 95.865% (15093/15744)
01/14/2023 06:37:39 - INFO - __main__ -   test: [batch: 123/391 ] | Loss: 0.705 | Acc: 81.678% (12964/15872)/ 95.861% (15215/15872)
01/14/2023 06:37:42 - INFO - __main__ -   test: [batch: 124/391 ] | Loss: 0.704 | Acc: 81.688% (13070/16000)/ 95.875% (15340/16000)
01/14/2023 06:37:44 - INFO - __main__ -   test: [batch: 125/391 ] | Loss: 0.699 | Acc: 81.789% (13191/16128)/ 95.902% (15467/16128)
01/14/2023 06:37:47 - INFO - __main__ -   test: [batch: 126/391 ] | Loss: 0.696 | Acc: 81.884% (13311/16256)/ 95.915% (15592/16256)
01/14/2023 06:37:50 - INFO - __main__ -   test: [batch: 127/391 ] | Loss: 0.694 | Acc: 81.970% (13430/16384)/ 95.923% (15716/16384)
01/14/2023 06:37:52 - INFO - __main__ -   test: [batch: 128/391 ] | Loss: 0.695 | Acc: 81.959% (13533/16512)/ 95.888% (15833/16512)
01/14/2023 06:37:55 - INFO - __main__ -   test: [batch: 129/391 ] | Loss: 0.693 | Acc: 81.971% (13640/16640)/ 95.895% (15957/16640)
01/14/2023 06:37:57 - INFO - __main__ -   test: [batch: 130/391 ] | Loss: 0.690 | Acc: 82.067% (13761/16768)/ 95.915% (16083/16768)
01/14/2023 06:38:00 - INFO - __main__ -   test: [batch: 131/391 ] | Loss: 0.690 | Acc: 82.114% (13874/16896)/ 95.922% (16207/16896)
01/14/2023 06:38:03 - INFO - __main__ -   test: [batch: 132/391 ] | Loss: 0.687 | Acc: 82.184% (13991/17024)/ 95.935% (16332/17024)
01/14/2023 06:38:06 - INFO - __main__ -   test: [batch: 133/391 ] | Loss: 0.689 | Acc: 82.136% (14088/17152)/ 95.919% (16452/17152)
01/14/2023 06:38:08 - INFO - __main__ -   test: [batch: 134/391 ] | Loss: 0.687 | Acc: 82.164% (14198/17280)/ 95.938% (16578/17280)
01/14/2023 06:38:11 - INFO - __main__ -   test: [batch: 135/391 ] | Loss: 0.687 | Acc: 82.158% (14302/17408)/ 95.933% (16700/17408)
01/14/2023 06:38:13 - INFO - __main__ -   test: [batch: 136/391 ] | Loss: 0.686 | Acc: 82.105% (14398/17536)/ 95.957% (16827/17536)
01/14/2023 06:38:16 - INFO - __main__ -   test: [batch: 137/391 ] | Loss: 0.684 | Acc: 82.150% (14511/17664)/ 95.986% (16955/17664)
01/14/2023 06:38:18 - INFO - __main__ -   test: [batch: 138/391 ] | Loss: 0.683 | Acc: 82.155% (14617/17792)/ 95.987% (17078/17792)
01/14/2023 06:38:21 - INFO - __main__ -   test: [batch: 139/391 ] | Loss: 0.688 | Acc: 82.009% (14696/17920)/ 95.982% (17200/17920)
01/14/2023 06:38:24 - INFO - __main__ -   test: [batch: 140/391 ] | Loss: 0.690 | Acc: 81.937% (14788/18048)/ 95.977% (17322/18048)
01/14/2023 06:38:27 - INFO - __main__ -   test: [batch: 141/391 ] | Loss: 0.689 | Acc: 81.976% (14900/18176)/ 95.989% (17447/18176)
01/14/2023 06:38:29 - INFO - __main__ -   test: [batch: 142/391 ] | Loss: 0.686 | Acc: 82.053% (15019/18304)/ 96.006% (17573/18304)
01/14/2023 06:38:32 - INFO - __main__ -   test: [batch: 143/391 ] | Loss: 0.687 | Acc: 82.064% (15126/18432)/ 95.991% (17693/18432)
01/14/2023 06:38:34 - INFO - __main__ -   test: [batch: 144/391 ] | Loss: 0.689 | Acc: 82.037% (15226/18560)/ 95.964% (17811/18560)
01/14/2023 06:38:37 - INFO - __main__ -   test: [batch: 145/391 ] | Loss: 0.692 | Acc: 82.021% (15328/18688)/ 95.939% (17929/18688)
01/14/2023 06:38:40 - INFO - __main__ -   test: [batch: 146/391 ] | Loss: 0.693 | Acc: 82.005% (15430/18816)/ 95.929% (18050/18816)
01/14/2023 06:38:42 - INFO - __main__ -   test: [batch: 147/391 ] | Loss: 0.693 | Acc: 82.015% (15537/18944)/ 95.909% (18169/18944)
01/14/2023 06:38:45 - INFO - __main__ -   test: [batch: 148/391 ] | Loss: 0.695 | Acc: 81.953% (15630/19072)/ 95.905% (18291/19072)
01/14/2023 06:38:48 - INFO - __main__ -   test: [batch: 149/391 ] | Loss: 0.699 | Acc: 81.865% (15718/19200)/ 95.870% (18407/19200)
01/14/2023 06:38:50 - INFO - __main__ -   test: [batch: 150/391 ] | Loss: 0.699 | Acc: 81.814% (15813/19328)/ 95.876% (18531/19328)
01/14/2023 06:38:53 - INFO - __main__ -   test: [batch: 151/391 ] | Loss: 0.699 | Acc: 81.846% (15924/19456)/ 95.878% (18654/19456)
01/14/2023 06:38:55 - INFO - __main__ -   test: [batch: 152/391 ] | Loss: 0.700 | Acc: 81.827% (16025/19584)/ 95.869% (18775/19584)
01/14/2023 06:38:58 - INFO - __main__ -   test: [batch: 153/391 ] | Loss: 0.698 | Acc: 81.884% (16141/19712)/ 95.871% (18898/19712)
01/14/2023 06:39:00 - INFO - __main__ -   test: [batch: 154/391 ] | Loss: 0.698 | Acc: 81.880% (16245/19840)/ 95.852% (19017/19840)
01/14/2023 06:39:03 - INFO - __main__ -   test: [batch: 155/391 ] | Loss: 0.698 | Acc: 81.891% (16352/19968)/ 95.848% (19139/19968)
01/14/2023 06:39:06 - INFO - __main__ -   test: [batch: 156/391 ] | Loss: 0.700 | Acc: 81.802% (16439/20096)/ 95.830% (19258/20096)
01/14/2023 06:39:08 - INFO - __main__ -   test: [batch: 157/391 ] | Loss: 0.702 | Acc: 81.784% (16540/20224)/ 95.812% (19377/20224)
01/14/2023 06:39:11 - INFO - __main__ -   test: [batch: 158/391 ] | Loss: 0.703 | Acc: 81.746% (16637/20352)/ 95.789% (19495/20352)
01/14/2023 06:39:13 - INFO - __main__ -   test: [batch: 159/391 ] | Loss: 0.705 | Acc: 81.724% (16737/20480)/ 95.781% (19616/20480)
01/14/2023 06:39:16 - INFO - __main__ -   test: [batch: 160/391 ] | Loss: 0.705 | Acc: 81.706% (16838/20608)/ 95.769% (19736/20608)
01/14/2023 06:39:18 - INFO - __main__ -   test: [batch: 161/391 ] | Loss: 0.714 | Acc: 81.472% (16894/20736)/ 95.684% (19841/20736)
01/14/2023 06:39:21 - INFO - __main__ -   test: [batch: 162/391 ] | Loss: 0.720 | Acc: 81.379% (16979/20864)/ 95.605% (19947/20864)
01/14/2023 06:39:23 - INFO - __main__ -   test: [batch: 163/391 ] | Loss: 0.722 | Acc: 81.326% (17072/20992)/ 95.594% (20067/20992)
01/14/2023 06:39:26 - INFO - __main__ -   test: [batch: 164/391 ] | Loss: 0.722 | Acc: 81.316% (17174/21120)/ 95.611% (20193/21120)
01/14/2023 06:39:28 - INFO - __main__ -   test: [batch: 165/391 ] | Loss: 0.724 | Acc: 81.226% (17259/21248)/ 95.604% (20314/21248)
01/14/2023 06:39:31 - INFO - __main__ -   test: [batch: 166/391 ] | Loss: 0.724 | Acc: 81.236% (17365/21376)/ 95.589% (20433/21376)
01/14/2023 06:39:34 - INFO - __main__ -   test: [batch: 167/391 ] | Loss: 0.726 | Acc: 81.171% (17455/21504)/ 95.568% (20551/21504)
01/14/2023 06:39:36 - INFO - __main__ -   test: [batch: 168/391 ] | Loss: 0.725 | Acc: 81.176% (17560/21632)/ 95.562% (20672/21632)
01/14/2023 06:39:39 - INFO - __main__ -   test: [batch: 169/391 ] | Loss: 0.727 | Acc: 81.149% (17658/21760)/ 95.524% (20786/21760)
01/14/2023 06:39:41 - INFO - __main__ -   test: [batch: 170/391 ] | Loss: 0.732 | Acc: 81.040% (17738/21888)/ 95.486% (20900/21888)
01/14/2023 06:39:44 - INFO - __main__ -   test: [batch: 171/391 ] | Loss: 0.735 | Acc: 80.982% (17829/22016)/ 95.453% (21015/22016)
01/14/2023 06:39:47 - INFO - __main__ -   test: [batch: 172/391 ] | Loss: 0.736 | Acc: 80.938% (17923/22144)/ 95.452% (21137/22144)
01/14/2023 06:39:49 - INFO - __main__ -   test: [batch: 173/391 ] | Loss: 0.738 | Acc: 80.877% (18013/22272)/ 95.429% (21254/22272)
01/14/2023 06:39:52 - INFO - __main__ -   test: [batch: 174/391 ] | Loss: 0.742 | Acc: 80.795% (18098/22400)/ 95.379% (21365/22400)
01/14/2023 06:39:54 - INFO - __main__ -   test: [batch: 175/391 ] | Loss: 0.741 | Acc: 80.837% (18211/22528)/ 95.384% (21488/22528)
01/14/2023 06:39:57 - INFO - __main__ -   test: [batch: 176/391 ] | Loss: 0.741 | Acc: 80.835% (18314/22656)/ 95.361% (21605/22656)
01/14/2023 06:39:59 - INFO - __main__ -   test: [batch: 177/391 ] | Loss: 0.745 | Acc: 80.767% (18402/22784)/ 95.312% (21716/22784)
01/14/2023 06:40:02 - INFO - __main__ -   test: [batch: 178/391 ] | Loss: 0.747 | Acc: 80.748% (18501/22912)/ 95.291% (21833/22912)
01/14/2023 06:40:05 - INFO - __main__ -   test: [batch: 179/391 ] | Loss: 0.750 | Acc: 80.681% (18589/23040)/ 95.247% (21945/23040)
01/14/2023 06:40:08 - INFO - __main__ -   test: [batch: 180/391 ] | Loss: 0.757 | Acc: 80.555% (18663/23168)/ 95.213% (22059/23168)
01/14/2023 06:40:10 - INFO - __main__ -   test: [batch: 181/391 ] | Loss: 0.763 | Acc: 80.426% (18736/23296)/ 95.158% (22168/23296)
01/14/2023 06:40:13 - INFO - __main__ -   test: [batch: 182/391 ] | Loss: 0.762 | Acc: 80.435% (18841/23424)/ 95.159% (22290/23424)
01/14/2023 06:40:15 - INFO - __main__ -   test: [batch: 183/391 ] | Loss: 0.768 | Acc: 80.320% (18917/23552)/ 95.066% (22390/23552)
01/14/2023 06:40:18 - INFO - __main__ -   test: [batch: 184/391 ] | Loss: 0.768 | Acc: 80.346% (19026/23680)/ 95.051% (22508/23680)
01/14/2023 06:40:21 - INFO - __main__ -   test: [batch: 185/391 ] | Loss: 0.768 | Acc: 80.355% (19131/23808)/ 95.044% (22628/23808)
01/14/2023 06:40:23 - INFO - __main__ -   test: [batch: 186/391 ] | Loss: 0.771 | Acc: 80.323% (19226/23936)/ 94.999% (22739/23936)
01/14/2023 06:40:26 - INFO - __main__ -   test: [batch: 187/391 ] | Loss: 0.776 | Acc: 80.207% (19301/24064)/ 94.968% (22853/24064)
01/14/2023 06:40:28 - INFO - __main__ -   test: [batch: 188/391 ] | Loss: 0.779 | Acc: 80.076% (19372/24192)/ 94.945% (22969/24192)
01/14/2023 06:40:31 - INFO - __main__ -   test: [batch: 189/391 ] | Loss: 0.780 | Acc: 80.012% (19459/24320)/ 94.947% (23091/24320)
01/14/2023 06:40:33 - INFO - __main__ -   test: [batch: 190/391 ] | Loss: 0.785 | Acc: 79.925% (19540/24448)/ 94.916% (23205/24448)
01/14/2023 06:40:36 - INFO - __main__ -   test: [batch: 191/391 ] | Loss: 0.788 | Acc: 79.887% (19633/24576)/ 94.893% (23321/24576)
01/14/2023 06:40:39 - INFO - __main__ -   test: [batch: 192/391 ] | Loss: 0.794 | Acc: 79.777% (19708/24704)/ 94.815% (23423/24704)
01/14/2023 06:40:41 - INFO - __main__ -   test: [batch: 193/391 ] | Loss: 0.794 | Acc: 79.796% (19815/24832)/ 94.813% (23544/24832)
01/14/2023 06:40:44 - INFO - __main__ -   test: [batch: 194/391 ] | Loss: 0.797 | Acc: 79.728% (19900/24960)/ 94.804% (23663/24960)
01/14/2023 06:40:46 - INFO - __main__ -   test: [batch: 195/391 ] | Loss: 0.802 | Acc: 79.656% (19984/25088)/ 94.743% (23769/25088)
01/14/2023 06:40:49 - INFO - __main__ -   test: [batch: 196/391 ] | Loss: 0.805 | Acc: 79.561% (20062/25216)/ 94.698% (23879/25216)
01/14/2023 06:40:51 - INFO - __main__ -   test: [batch: 197/391 ] | Loss: 0.810 | Acc: 79.486% (20145/25344)/ 94.673% (23994/25344)
01/14/2023 06:40:54 - INFO - __main__ -   test: [batch: 198/391 ] | Loss: 0.811 | Acc: 79.444% (20236/25472)/ 94.641% (24107/25472)
01/14/2023 06:40:57 - INFO - __main__ -   test: [batch: 199/391 ] | Loss: 0.811 | Acc: 79.426% (20333/25600)/ 94.645% (24229/25600)
01/14/2023 06:40:59 - INFO - __main__ -   test: [batch: 200/391 ] | Loss: 0.811 | Acc: 79.380% (20423/25728)/ 94.640% (24349/25728)
01/14/2023 06:41:02 - INFO - __main__ -   test: [batch: 201/391 ] | Loss: 0.815 | Acc: 79.293% (20502/25856)/ 94.605% (24461/25856)
01/14/2023 06:41:04 - INFO - __main__ -   test: [batch: 202/391 ] | Loss: 0.816 | Acc: 79.280% (20600/25984)/ 94.597% (24580/25984)
01/14/2023 06:41:07 - INFO - __main__ -   test: [batch: 203/391 ] | Loss: 0.816 | Acc: 79.270% (20699/26112)/ 94.593% (24700/26112)
01/14/2023 06:41:10 - INFO - __main__ -   test: [batch: 204/391 ] | Loss: 0.819 | Acc: 79.184% (20778/26240)/ 94.569% (24815/26240)
01/14/2023 06:41:12 - INFO - __main__ -   test: [batch: 205/391 ] | Loss: 0.822 | Acc: 79.100% (20857/26368)/ 94.535% (24927/26368)
01/14/2023 06:41:15 - INFO - __main__ -   test: [batch: 206/391 ] | Loss: 0.824 | Acc: 79.076% (20952/26496)/ 94.531% (25047/26496)
01/14/2023 06:41:17 - INFO - __main__ -   test: [batch: 207/391 ] | Loss: 0.827 | Acc: 78.996% (21032/26624)/ 94.490% (25157/26624)
01/14/2023 06:41:20 - INFO - __main__ -   test: [batch: 208/391 ] | Loss: 0.829 | Acc: 78.962% (21124/26752)/ 94.460% (25270/26752)
01/14/2023 06:41:23 - INFO - __main__ -   test: [batch: 209/391 ] | Loss: 0.829 | Acc: 78.962% (21225/26880)/ 94.479% (25396/26880)
01/14/2023 06:41:25 - INFO - __main__ -   test: [batch: 210/391 ] | Loss: 0.830 | Acc: 78.943% (21321/27008)/ 94.461% (25512/27008)
01/14/2023 06:41:28 - INFO - __main__ -   test: [batch: 211/391 ] | Loss: 0.832 | Acc: 78.884% (21406/27136)/ 94.450% (25630/27136)
01/14/2023 06:41:30 - INFO - __main__ -   test: [batch: 212/391 ] | Loss: 0.834 | Acc: 78.815% (21488/27264)/ 94.429% (25745/27264)
01/14/2023 06:41:33 - INFO - __main__ -   test: [batch: 213/391 ] | Loss: 0.834 | Acc: 78.822% (21591/27392)/ 94.436% (25868/27392)
01/14/2023 06:41:36 - INFO - __main__ -   test: [batch: 214/391 ] | Loss: 0.834 | Acc: 78.797% (21685/27520)/ 94.440% (25990/27520)
01/14/2023 06:41:38 - INFO - __main__ -   test: [batch: 215/391 ] | Loss: 0.834 | Acc: 78.812% (21790/27648)/ 94.434% (26109/27648)
01/14/2023 06:41:41 - INFO - __main__ -   test: [batch: 216/391 ] | Loss: 0.832 | Acc: 78.852% (21902/27776)/ 94.456% (26236/27776)
01/14/2023 06:41:43 - INFO - __main__ -   test: [batch: 217/391 ] | Loss: 0.837 | Acc: 78.788% (21985/27904)/ 94.391% (26339/27904)
01/14/2023 06:41:46 - INFO - __main__ -   test: [batch: 218/391 ] | Loss: 0.840 | Acc: 78.728% (22069/28032)/ 94.353% (26449/28032)
01/14/2023 06:41:49 - INFO - __main__ -   test: [batch: 219/391 ] | Loss: 0.839 | Acc: 78.761% (22179/28160)/ 94.357% (26571/28160)
01/14/2023 06:41:51 - INFO - __main__ -   test: [batch: 220/391 ] | Loss: 0.837 | Acc: 78.797% (22290/28288)/ 94.372% (26696/28288)
01/14/2023 06:41:54 - INFO - __main__ -   test: [batch: 221/391 ] | Loss: 0.839 | Acc: 78.755% (22379/28416)/ 94.359% (26813/28416)
01/14/2023 06:41:57 - INFO - __main__ -   test: [batch: 222/391 ] | Loss: 0.837 | Acc: 78.808% (22495/28544)/ 94.374% (26938/28544)
01/14/2023 06:41:59 - INFO - __main__ -   test: [batch: 223/391 ] | Loss: 0.837 | Acc: 78.809% (22596/28672)/ 94.371% (27058/28672)
01/14/2023 06:42:02 - INFO - __main__ -   test: [batch: 224/391 ] | Loss: 0.837 | Acc: 78.826% (22702/28800)/ 94.365% (27177/28800)
01/14/2023 06:42:04 - INFO - __main__ -   test: [batch: 225/391 ] | Loss: 0.836 | Acc: 78.823% (22802/28928)/ 94.376% (27301/28928)
01/14/2023 06:42:07 - INFO - __main__ -   test: [batch: 226/391 ] | Loss: 0.836 | Acc: 78.810% (22899/29056)/ 94.383% (27424/29056)
01/14/2023 06:42:10 - INFO - __main__ -   test: [batch: 227/391 ] | Loss: 0.838 | Acc: 78.797% (22996/29184)/ 94.363% (27539/29184)
01/14/2023 06:42:12 - INFO - __main__ -   test: [batch: 228/391 ] | Loss: 0.843 | Acc: 78.719% (23074/29312)/ 94.306% (27643/29312)
01/14/2023 06:42:15 - INFO - __main__ -   test: [batch: 229/391 ] | Loss: 0.846 | Acc: 78.665% (23159/29440)/ 94.266% (27752/29440)
01/14/2023 06:42:18 - INFO - __main__ -   test: [batch: 230/391 ] | Loss: 0.849 | Acc: 78.609% (23243/29568)/ 94.230% (27862/29568)
01/14/2023 06:42:20 - INFO - __main__ -   test: [batch: 231/391 ] | Loss: 0.849 | Acc: 78.603% (23342/29696)/ 94.215% (27978/29696)
01/14/2023 06:42:23 - INFO - __main__ -   test: [batch: 232/391 ] | Loss: 0.848 | Acc: 78.631% (23451/29824)/ 94.229% (28103/29824)
01/14/2023 06:42:25 - INFO - __main__ -   test: [batch: 233/391 ] | Loss: 0.850 | Acc: 78.582% (23537/29952)/ 94.211% (28218/29952)
01/14/2023 06:42:28 - INFO - __main__ -   test: [batch: 234/391 ] | Loss: 0.856 | Acc: 78.464% (23602/30080)/ 94.139% (28317/30080)
01/14/2023 06:42:31 - INFO - __main__ -   test: [batch: 235/391 ] | Loss: 0.856 | Acc: 78.453% (23699/30208)/ 94.134% (28436/30208)
01/14/2023 06:42:33 - INFO - __main__ -   test: [batch: 236/391 ] | Loss: 0.855 | Acc: 78.488% (23810/30336)/ 94.139% (28558/30336)
01/14/2023 06:42:36 - INFO - __main__ -   test: [batch: 237/391 ] | Loss: 0.856 | Acc: 78.483% (23909/30464)/ 94.108% (28669/30464)
01/14/2023 06:42:38 - INFO - __main__ -   test: [batch: 238/391 ] | Loss: 0.854 | Acc: 78.521% (24021/30592)/ 94.116% (28792/30592)
01/14/2023 06:42:41 - INFO - __main__ -   test: [batch: 239/391 ] | Loss: 0.853 | Acc: 78.558% (24133/30720)/ 94.118% (28913/30720)
01/14/2023 06:42:43 - INFO - __main__ -   test: [batch: 240/391 ] | Loss: 0.853 | Acc: 78.572% (24238/30848)/ 94.107% (29030/30848)
01/14/2023 06:42:46 - INFO - __main__ -   test: [batch: 241/391 ] | Loss: 0.856 | Acc: 78.487% (24312/30976)/ 94.066% (29138/30976)
01/14/2023 06:42:48 - INFO - __main__ -   test: [batch: 242/391 ] | Loss: 0.859 | Acc: 78.385% (24381/31104)/ 94.049% (29253/31104)
01/14/2023 06:42:51 - INFO - __main__ -   test: [batch: 243/391 ] | Loss: 0.865 | Acc: 78.243% (24437/31232)/ 93.981% (29352/31232)
01/14/2023 06:42:54 - INFO - __main__ -   test: [batch: 244/391 ] | Loss: 0.865 | Acc: 78.249% (24539/31360)/ 93.976% (29471/31360)
01/14/2023 06:42:57 - INFO - __main__ -   test: [batch: 245/391 ] | Loss: 0.864 | Acc: 78.255% (24641/31488)/ 93.969% (29589/31488)
01/14/2023 06:42:59 - INFO - __main__ -   test: [batch: 246/391 ] | Loss: 0.865 | Acc: 78.236% (24735/31616)/ 93.959% (29706/31616)
01/14/2023 06:43:02 - INFO - __main__ -   test: [batch: 247/391 ] | Loss: 0.870 | Acc: 78.141% (24805/31744)/ 93.892% (29805/31744)
01/14/2023 06:43:04 - INFO - __main__ -   test: [batch: 248/391 ] | Loss: 0.871 | Acc: 78.097% (24891/31872)/ 93.885% (29923/31872)
01/14/2023 06:43:07 - INFO - __main__ -   test: [batch: 249/391 ] | Loss: 0.873 | Acc: 77.972% (24951/32000)/ 93.875% (30040/32000)
01/14/2023 06:43:10 - INFO - __main__ -   test: [batch: 250/391 ] | Loss: 0.872 | Acc: 78.007% (25062/32128)/ 93.893% (30166/32128)
01/14/2023 06:43:12 - INFO - __main__ -   test: [batch: 251/391 ] | Loss: 0.874 | Acc: 77.970% (25150/32256)/ 93.865% (30277/32256)
01/14/2023 06:43:15 - INFO - __main__ -   test: [batch: 252/391 ] | Loss: 0.873 | Acc: 77.998% (25259/32384)/ 93.858% (30395/32384)
01/14/2023 06:43:17 - INFO - __main__ -   test: [batch: 253/391 ] | Loss: 0.875 | Acc: 77.974% (25351/32512)/ 93.842% (30510/32512)
01/14/2023 06:43:20 - INFO - __main__ -   test: [batch: 254/391 ] | Loss: 0.879 | Acc: 77.914% (25431/32640)/ 93.802% (30617/32640)
01/14/2023 06:43:23 - INFO - __main__ -   test: [batch: 255/391 ] | Loss: 0.881 | Acc: 77.878% (25519/32768)/ 93.793% (30734/32768)
01/14/2023 06:43:25 - INFO - __main__ -   test: [batch: 256/391 ] | Loss: 0.886 | Acc: 77.754% (25578/32896)/ 93.762% (30844/32896)
01/14/2023 06:43:28 - INFO - __main__ -   test: [batch: 257/391 ] | Loss: 0.887 | Acc: 77.743% (25674/33024)/ 93.750% (30960/33024)
01/14/2023 06:43:30 - INFO - __main__ -   test: [batch: 258/391 ] | Loss: 0.888 | Acc: 77.733% (25770/33152)/ 93.738% (31076/33152)
01/14/2023 06:43:33 - INFO - __main__ -   test: [batch: 259/391 ] | Loss: 0.892 | Acc: 77.617% (25831/33280)/ 93.723% (31191/33280)
01/14/2023 06:43:35 - INFO - __main__ -   test: [batch: 260/391 ] | Loss: 0.892 | Acc: 77.589% (25921/33408)/ 93.720% (31310/33408)
01/14/2023 06:43:38 - INFO - __main__ -   test: [batch: 261/391 ] | Loss: 0.890 | Acc: 77.633% (26035/33536)/ 93.741% (31437/33536)
01/14/2023 06:43:40 - INFO - __main__ -   test: [batch: 262/391 ] | Loss: 0.889 | Acc: 77.656% (26142/33664)/ 93.747% (31559/33664)
01/14/2023 06:43:43 - INFO - __main__ -   test: [batch: 263/391 ] | Loss: 0.892 | Acc: 77.586% (26218/33792)/ 93.720% (31670/33792)
01/14/2023 06:43:46 - INFO - __main__ -   test: [batch: 264/391 ] | Loss: 0.897 | Acc: 77.524% (26296/33920)/ 93.659% (31769/33920)
01/14/2023 06:43:48 - INFO - __main__ -   test: [batch: 265/391 ] | Loss: 0.896 | Acc: 77.552% (26405/34048)/ 93.656% (31888/34048)
01/14/2023 06:43:51 - INFO - __main__ -   test: [batch: 266/391 ] | Loss: 0.899 | Acc: 77.481% (26480/34176)/ 93.639% (32002/34176)
01/14/2023 06:43:53 - INFO - __main__ -   test: [batch: 267/391 ] | Loss: 0.898 | Acc: 77.510% (26589/34304)/ 93.639% (32122/34304)
01/14/2023 06:43:56 - INFO - __main__ -   test: [batch: 268/391 ] | Loss: 0.897 | Acc: 77.527% (26694/34432)/ 93.637% (32241/34432)
01/14/2023 06:43:59 - INFO - __main__ -   test: [batch: 269/391 ] | Loss: 0.899 | Acc: 77.468% (26773/34560)/ 93.617% (32354/34560)
01/14/2023 06:44:01 - INFO - __main__ -   test: [batch: 270/391 ] | Loss: 0.903 | Acc: 77.404% (26850/34688)/ 93.589% (32464/34688)
01/14/2023 06:44:04 - INFO - __main__ -   test: [batch: 271/391 ] | Loss: 0.903 | Acc: 77.413% (26952/34816)/ 93.583% (32582/34816)
01/14/2023 06:44:06 - INFO - __main__ -   test: [batch: 272/391 ] | Loss: 0.904 | Acc: 77.370% (27036/34944)/ 93.578% (32700/34944)
01/14/2023 06:44:09 - INFO - __main__ -   test: [batch: 273/391 ] | Loss: 0.904 | Acc: 77.378% (27138/35072)/ 93.573% (32818/35072)
01/14/2023 06:44:12 - INFO - __main__ -   test: [batch: 274/391 ] | Loss: 0.904 | Acc: 77.375% (27236/35200)/ 93.577% (32939/35200)
01/14/2023 06:44:14 - INFO - __main__ -   test: [batch: 275/391 ] | Loss: 0.906 | Acc: 77.347% (27325/35328)/ 93.572% (33057/35328)
01/14/2023 06:44:17 - INFO - __main__ -   test: [batch: 276/391 ] | Loss: 0.907 | Acc: 77.313% (27412/35456)/ 93.567% (33175/35456)
01/14/2023 06:44:19 - INFO - __main__ -   test: [batch: 277/391 ] | Loss: 0.907 | Acc: 77.288% (27502/35584)/ 93.553% (33290/35584)
01/14/2023 06:44:22 - INFO - __main__ -   test: [batch: 278/391 ] | Loss: 0.907 | Acc: 77.305% (27607/35712)/ 93.551% (33409/35712)
01/14/2023 06:44:25 - INFO - __main__ -   test: [batch: 279/391 ] | Loss: 0.907 | Acc: 77.313% (27709/35840)/ 93.544% (33526/35840)
01/14/2023 06:44:27 - INFO - __main__ -   test: [batch: 280/391 ] | Loss: 0.908 | Acc: 77.294% (27801/35968)/ 93.539% (33644/35968)
01/14/2023 06:44:30 - INFO - __main__ -   test: [batch: 281/391 ] | Loss: 0.908 | Acc: 77.305% (27904/36096)/ 93.534% (33762/36096)
01/14/2023 06:44:33 - INFO - __main__ -   test: [batch: 282/391 ] | Loss: 0.907 | Acc: 77.338% (28015/36224)/ 93.540% (33884/36224)
01/14/2023 06:44:35 - INFO - __main__ -   test: [batch: 283/391 ] | Loss: 0.908 | Acc: 77.313% (28105/36352)/ 93.535% (34002/36352)
01/14/2023 06:44:38 - INFO - __main__ -   test: [batch: 284/391 ] | Loss: 0.911 | Acc: 77.272% (28189/36480)/ 93.512% (34113/36480)
01/14/2023 06:44:40 - INFO - __main__ -   test: [batch: 285/391 ] | Loss: 0.913 | Acc: 77.229% (28272/36608)/ 93.474% (34219/36608)
01/14/2023 06:44:43 - INFO - __main__ -   test: [batch: 286/391 ] | Loss: 0.914 | Acc: 77.199% (28360/36736)/ 93.464% (34335/36736)
01/14/2023 06:44:45 - INFO - __main__ -   test: [batch: 287/391 ] | Loss: 0.915 | Acc: 77.200% (28459/36864)/ 93.457% (34452/36864)
01/14/2023 06:44:48 - INFO - __main__ -   test: [batch: 288/391 ] | Loss: 0.914 | Acc: 77.222% (28566/36992)/ 93.461% (34573/36992)
01/14/2023 06:44:51 - INFO - __main__ -   test: [batch: 289/391 ] | Loss: 0.916 | Acc: 77.182% (28650/37120)/ 93.429% (34681/37120)
01/14/2023 06:44:53 - INFO - __main__ -   test: [batch: 290/391 ] | Loss: 0.917 | Acc: 77.105% (28720/37248)/ 93.431% (34801/37248)
01/14/2023 06:44:56 - INFO - __main__ -   test: [batch: 291/391 ] | Loss: 0.917 | Acc: 77.119% (28824/37376)/ 93.421% (34917/37376)
01/14/2023 06:44:59 - INFO - __main__ -   test: [batch: 292/391 ] | Loss: 0.919 | Acc: 77.066% (28903/37504)/ 93.403% (35030/37504)
01/14/2023 06:45:01 - INFO - __main__ -   test: [batch: 293/391 ] | Loss: 0.919 | Acc: 77.065% (29001/37632)/ 93.397% (35147/37632)
01/14/2023 06:45:04 - INFO - __main__ -   test: [batch: 294/391 ] | Loss: 0.920 | Acc: 77.042% (29091/37760)/ 93.390% (35264/37760)
01/14/2023 06:45:06 - INFO - __main__ -   test: [batch: 295/391 ] | Loss: 0.919 | Acc: 77.075% (29202/37888)/ 93.396% (35386/37888)
01/14/2023 06:45:09 - INFO - __main__ -   test: [batch: 296/391 ] | Loss: 0.919 | Acc: 77.075% (29301/38016)/ 93.387% (35502/38016)
01/14/2023 06:45:12 - INFO - __main__ -   test: [batch: 297/391 ] | Loss: 0.921 | Acc: 77.055% (29392/38144)/ 93.362% (35612/38144)
01/14/2023 06:45:14 - INFO - __main__ -   test: [batch: 298/391 ] | Loss: 0.923 | Acc: 77.022% (29478/38272)/ 93.337% (35722/38272)
01/14/2023 06:45:17 - INFO - __main__ -   test: [batch: 299/391 ] | Loss: 0.924 | Acc: 77.016% (29574/38400)/ 93.318% (35834/38400)
01/14/2023 06:45:19 - INFO - __main__ -   test: [batch: 300/391 ] | Loss: 0.924 | Acc: 77.019% (29674/38528)/ 93.317% (35953/38528)
01/14/2023 06:45:22 - INFO - __main__ -   test: [batch: 301/391 ] | Loss: 0.925 | Acc: 77.005% (29767/38656)/ 93.302% (36067/38656)
01/14/2023 06:45:25 - INFO - __main__ -   test: [batch: 302/391 ] | Loss: 0.927 | Acc: 76.967% (29851/38784)/ 93.286% (36180/38784)
01/14/2023 06:45:27 - INFO - __main__ -   test: [batch: 303/391 ] | Loss: 0.928 | Acc: 76.945% (29941/38912)/ 93.272% (36294/38912)
01/14/2023 06:45:30 - INFO - __main__ -   test: [batch: 304/391 ] | Loss: 0.927 | Acc: 76.960% (30045/39040)/ 93.279% (36416/39040)
01/14/2023 06:45:33 - INFO - __main__ -   test: [batch: 305/391 ] | Loss: 0.928 | Acc: 76.915% (30126/39168)/ 93.275% (36534/39168)
01/14/2023 06:45:35 - INFO - __main__ -   test: [batch: 306/391 ] | Loss: 0.929 | Acc: 76.896% (30217/39296)/ 93.249% (36643/39296)
01/14/2023 06:45:38 - INFO - __main__ -   test: [batch: 307/391 ] | Loss: 0.930 | Acc: 76.895% (30315/39424)/ 93.235% (36757/39424)
01/14/2023 06:45:40 - INFO - __main__ -   test: [batch: 308/391 ] | Loss: 0.930 | Acc: 76.876% (30406/39552)/ 93.232% (36875/39552)
01/14/2023 06:45:43 - INFO - __main__ -   test: [batch: 309/391 ] | Loss: 0.931 | Acc: 76.875% (30504/39680)/ 93.213% (36987/39680)
01/14/2023 06:45:45 - INFO - __main__ -   test: [batch: 310/391 ] | Loss: 0.932 | Acc: 76.866% (30599/39808)/ 93.200% (37101/39808)
01/14/2023 06:45:48 - INFO - __main__ -   test: [batch: 311/391 ] | Loss: 0.933 | Acc: 76.845% (30689/39936)/ 93.189% (37216/39936)
01/14/2023 06:45:51 - INFO - __main__ -   test: [batch: 312/391 ] | Loss: 0.935 | Acc: 76.825% (30779/40064)/ 93.163% (37325/40064)
01/14/2023 06:45:53 - INFO - __main__ -   test: [batch: 313/391 ] | Loss: 0.933 | Acc: 76.864% (30893/40192)/ 93.180% (37451/40192)
01/14/2023 06:45:56 - INFO - __main__ -   test: [batch: 314/391 ] | Loss: 0.933 | Acc: 76.850% (30986/40320)/ 93.177% (37569/40320)
01/14/2023 06:45:58 - INFO - __main__ -   test: [batch: 315/391 ] | Loss: 0.934 | Acc: 76.834% (31078/40448)/ 93.162% (37682/40448)
01/14/2023 06:46:01 - INFO - __main__ -   test: [batch: 316/391 ] | Loss: 0.937 | Acc: 76.747% (31141/40576)/ 93.139% (37792/40576)
01/14/2023 06:46:04 - INFO - __main__ -   test: [batch: 317/391 ] | Loss: 0.938 | Acc: 76.703% (31221/40704)/ 93.119% (37903/40704)
01/14/2023 06:46:06 - INFO - __main__ -   test: [batch: 318/391 ] | Loss: 0.937 | Acc: 76.734% (31332/40832)/ 93.135% (38029/40832)
01/14/2023 06:46:09 - INFO - __main__ -   test: [batch: 319/391 ] | Loss: 0.940 | Acc: 76.680% (31408/40960)/ 93.103% (38135/40960)
01/14/2023 06:46:11 - INFO - __main__ -   test: [batch: 320/391 ] | Loss: 0.939 | Acc: 76.716% (31521/41088)/ 93.112% (38258/41088)
01/14/2023 06:46:14 - INFO - __main__ -   test: [batch: 321/391 ] | Loss: 0.938 | Acc: 76.737% (31628/41216)/ 93.112% (38377/41216)
01/14/2023 06:46:17 - INFO - __main__ -   test: [batch: 322/391 ] | Loss: 0.940 | Acc: 76.715% (31717/41344)/ 93.102% (38492/41344)
01/14/2023 06:46:20 - INFO - __main__ -   test: [batch: 323/391 ] | Loss: 0.942 | Acc: 76.683% (31802/41472)/ 93.080% (38602/41472)
01/14/2023 06:46:22 - INFO - __main__ -   test: [batch: 324/391 ] | Loss: 0.942 | Acc: 76.678% (31898/41600)/ 93.072% (38718/41600)
01/14/2023 06:46:25 - INFO - __main__ -   test: [batch: 325/391 ] | Loss: 0.942 | Acc: 76.685% (31999/41728)/ 93.069% (38836/41728)
01/14/2023 06:46:28 - INFO - __main__ -   test: [batch: 326/391 ] | Loss: 0.945 | Acc: 76.603% (32063/41856)/ 93.038% (38942/41856)
01/14/2023 06:46:30 - INFO - __main__ -   test: [batch: 327/391 ] | Loss: 0.949 | Acc: 76.520% (32126/41984)/ 93.004% (39047/41984)
01/14/2023 06:46:33 - INFO - __main__ -   test: [batch: 328/391 ] | Loss: 0.951 | Acc: 76.468% (32202/42112)/ 92.978% (39155/42112)
01/14/2023 06:46:35 - INFO - __main__ -   test: [batch: 329/391 ] | Loss: 0.951 | Acc: 76.463% (32298/42240)/ 92.978% (39274/42240)
01/14/2023 06:46:38 - INFO - __main__ -   test: [batch: 330/391 ] | Loss: 0.953 | Acc: 76.426% (32380/42368)/ 92.950% (39381/42368)
01/14/2023 06:46:41 - INFO - __main__ -   test: [batch: 331/391 ] | Loss: 0.953 | Acc: 76.393% (32464/42496)/ 92.964% (39506/42496)
01/14/2023 06:46:43 - INFO - __main__ -   test: [batch: 332/391 ] | Loss: 0.953 | Acc: 76.387% (32559/42624)/ 92.962% (39624/42624)
01/14/2023 06:46:46 - INFO - __main__ -   test: [batch: 333/391 ] | Loss: 0.952 | Acc: 76.415% (32669/42752)/ 92.971% (39747/42752)
01/14/2023 06:46:48 - INFO - __main__ -   test: [batch: 334/391 ] | Loss: 0.953 | Acc: 76.385% (32754/42880)/ 92.950% (39857/42880)
01/14/2023 06:46:51 - INFO - __main__ -   test: [batch: 335/391 ] | Loss: 0.954 | Acc: 76.365% (32843/43008)/ 92.936% (39970/43008)
01/14/2023 06:46:54 - INFO - __main__ -   test: [batch: 336/391 ] | Loss: 0.956 | Acc: 76.319% (32921/43136)/ 92.922% (40083/43136)
01/14/2023 06:46:56 - INFO - __main__ -   test: [batch: 337/391 ] | Loss: 0.956 | Acc: 76.313% (33016/43264)/ 92.916% (40199/43264)
01/14/2023 06:46:59 - INFO - __main__ -   test: [batch: 338/391 ] | Loss: 0.956 | Acc: 76.297% (33107/43392)/ 92.927% (40323/43392)
01/14/2023 06:47:01 - INFO - __main__ -   test: [batch: 339/391 ] | Loss: 0.959 | Acc: 76.261% (33189/43520)/ 92.904% (40432/43520)
01/14/2023 06:47:04 - INFO - __main__ -   test: [batch: 340/391 ] | Loss: 0.959 | Acc: 76.255% (33284/43648)/ 92.914% (40555/43648)
01/14/2023 06:47:07 - INFO - __main__ -   test: [batch: 341/391 ] | Loss: 0.957 | Acc: 76.295% (33399/43776)/ 92.932% (40682/43776)
01/14/2023 06:47:09 - INFO - __main__ -   test: [batch: 342/391 ] | Loss: 0.957 | Acc: 76.244% (33474/43904)/ 92.928% (40799/43904)
01/14/2023 06:47:12 - INFO - __main__ -   test: [batch: 343/391 ] | Loss: 0.957 | Acc: 76.242% (33571/44032)/ 92.932% (40920/44032)
01/14/2023 06:47:14 - INFO - __main__ -   test: [batch: 344/391 ] | Loss: 0.958 | Acc: 76.241% (33668/44160)/ 92.923% (41035/44160)
01/14/2023 06:47:17 - INFO - __main__ -   test: [batch: 345/391 ] | Loss: 0.961 | Acc: 76.174% (33736/44288)/ 92.885% (41137/44288)
01/14/2023 06:47:20 - INFO - __main__ -   test: [batch: 346/391 ] | Loss: 0.963 | Acc: 76.159% (33827/44416)/ 92.872% (41250/44416)
01/14/2023 06:47:22 - INFO - __main__ -   test: [batch: 347/391 ] | Loss: 0.962 | Acc: 76.172% (33930/44544)/ 92.881% (41373/44544)
01/14/2023 06:47:24 - INFO - __main__ -   test: [batch: 348/391 ] | Loss: 0.964 | Acc: 76.151% (34018/44672)/ 92.859% (41482/44672)
01/14/2023 06:47:27 - INFO - __main__ -   test: [batch: 349/391 ] | Loss: 0.963 | Acc: 76.152% (34116/44800)/ 92.868% (41605/44800)
01/14/2023 06:47:30 - INFO - __main__ -   test: [batch: 350/391 ] | Loss: 0.963 | Acc: 76.169% (34221/44928)/ 92.866% (41723/44928)
01/14/2023 06:47:32 - INFO - __main__ -   test: [batch: 351/391 ] | Loss: 0.965 | Acc: 76.112% (34293/45056)/ 92.847% (41833/45056)
01/14/2023 06:47:35 - INFO - __main__ -   test: [batch: 352/391 ] | Loss: 0.966 | Acc: 76.109% (34389/45184)/ 92.843% (41950/45184)
01/14/2023 06:47:38 - INFO - __main__ -   test: [batch: 353/391 ] | Loss: 0.968 | Acc: 76.066% (34467/45312)/ 92.801% (42050/45312)
01/14/2023 06:47:40 - INFO - __main__ -   test: [batch: 354/391 ] | Loss: 0.970 | Acc: 76.010% (34539/45440)/ 92.784% (42161/45440)
01/14/2023 06:47:43 - INFO - __main__ -   test: [batch: 355/391 ] | Loss: 0.973 | Acc: 75.946% (34607/45568)/ 92.778% (42277/45568)
01/14/2023 06:47:45 - INFO - __main__ -   test: [batch: 356/391 ] | Loss: 0.973 | Acc: 75.945% (34704/45696)/ 92.776% (42395/45696)
01/14/2023 06:47:48 - INFO - __main__ -   test: [batch: 357/391 ] | Loss: 0.972 | Acc: 75.980% (34817/45824)/ 92.790% (42520/45824)
01/14/2023 06:47:51 - INFO - __main__ -   test: [batch: 358/391 ] | Loss: 0.971 | Acc: 76.008% (34927/45952)/ 92.792% (42640/45952)
01/14/2023 06:47:53 - INFO - __main__ -   test: [batch: 359/391 ] | Loss: 0.971 | Acc: 76.018% (35029/46080)/ 92.789% (42757/46080)
01/14/2023 06:47:56 - INFO - __main__ -   test: [batch: 360/391 ] | Loss: 0.973 | Acc: 75.989% (35113/46208)/ 92.780% (42872/46208)
01/14/2023 06:47:58 - INFO - __main__ -   test: [batch: 361/391 ] | Loss: 0.973 | Acc: 75.991% (35211/46336)/ 92.787% (42994/46336)
01/14/2023 06:48:01 - INFO - __main__ -   test: [batch: 362/391 ] | Loss: 0.973 | Acc: 75.988% (35307/46464)/ 92.801% (43119/46464)
01/14/2023 06:48:04 - INFO - __main__ -   test: [batch: 363/391 ] | Loss: 0.973 | Acc: 75.974% (35398/46592)/ 92.791% (43233/46592)
01/14/2023 06:48:06 - INFO - __main__ -   test: [batch: 364/391 ] | Loss: 0.972 | Acc: 76.002% (35508/46720)/ 92.800% (43356/46720)
01/14/2023 06:48:09 - INFO - __main__ -   test: [batch: 365/391 ] | Loss: 0.971 | Acc: 76.016% (35612/46848)/ 92.807% (43478/46848)
01/14/2023 06:48:11 - INFO - __main__ -   test: [batch: 366/391 ] | Loss: 0.970 | Acc: 76.054% (35727/46976)/ 92.824% (43605/46976)
01/14/2023 06:48:14 - INFO - __main__ -   test: [batch: 367/391 ] | Loss: 0.969 | Acc: 76.078% (35836/47104)/ 92.837% (43730/47104)
01/14/2023 06:48:16 - INFO - __main__ -   test: [batch: 368/391 ] | Loss: 0.968 | Acc: 76.084% (35936/47232)/ 92.846% (43853/47232)
01/14/2023 06:48:19 - INFO - __main__ -   test: [batch: 369/391 ] | Loss: 0.967 | Acc: 76.102% (36042/47360)/ 92.855% (43976/47360)
01/14/2023 06:48:21 - INFO - __main__ -   test: [batch: 370/391 ] | Loss: 0.967 | Acc: 76.089% (36133/47488)/ 92.857% (44096/47488)
01/14/2023 06:48:24 - INFO - __main__ -   test: [batch: 371/391 ] | Loss: 0.967 | Acc: 76.098% (36235/47616)/ 92.862% (44217/47616)
01/14/2023 06:48:27 - INFO - __main__ -   test: [batch: 372/391 ] | Loss: 0.966 | Acc: 76.146% (36355/47744)/ 92.877% (44343/47744)
01/14/2023 06:48:29 - INFO - __main__ -   test: [batch: 373/391 ] | Loss: 0.964 | Acc: 76.184% (36471/47872)/ 92.885% (44466/47872)
01/14/2023 06:48:32 - INFO - __main__ -   test: [batch: 374/391 ] | Loss: 0.963 | Acc: 76.217% (36584/48000)/ 92.892% (44588/48000)
01/14/2023 06:48:35 - INFO - __main__ -   test: [batch: 375/391 ] | Loss: 0.966 | Acc: 76.153% (36651/48128)/ 92.854% (44689/48128)
01/14/2023 06:48:37 - INFO - __main__ -   test: [batch: 376/391 ] | Loss: 0.966 | Acc: 76.148% (36746/48256)/ 92.840% (44801/48256)
01/14/2023 06:48:40 - INFO - __main__ -   test: [batch: 377/391 ] | Loss: 0.967 | Acc: 76.137% (36838/48384)/ 92.832% (44916/48384)
01/14/2023 06:48:42 - INFO - __main__ -   test: [batch: 378/391 ] | Loss: 0.970 | Acc: 76.062% (36899/48512)/ 92.789% (45014/48512)
01/14/2023 06:48:45 - INFO - __main__ -   test: [batch: 379/391 ] | Loss: 0.971 | Acc: 76.049% (36990/48640)/ 92.794% (45135/48640)
01/14/2023 06:48:48 - INFO - __main__ -   test: [batch: 380/391 ] | Loss: 0.970 | Acc: 76.042% (37084/48768)/ 92.805% (45259/48768)
01/14/2023 06:48:50 - INFO - __main__ -   test: [batch: 381/391 ] | Loss: 0.972 | Acc: 75.994% (37158/48896)/ 92.801% (45376/48896)
01/14/2023 06:48:53 - INFO - __main__ -   test: [batch: 382/391 ] | Loss: 0.974 | Acc: 75.955% (37236/49024)/ 92.785% (45487/49024)
01/14/2023 06:48:55 - INFO - __main__ -   test: [batch: 383/391 ] | Loss: 0.974 | Acc: 75.960% (37336/49152)/ 92.782% (45604/49152)
01/14/2023 06:48:58 - INFO - __main__ -   test: [batch: 384/391 ] | Loss: 0.972 | Acc: 76.002% (37454/49280)/ 92.794% (45729/49280)
01/14/2023 06:49:01 - INFO - __main__ -   test: [batch: 385/391 ] | Loss: 0.971 | Acc: 76.014% (37557/49408)/ 92.803% (45852/49408)
01/14/2023 06:49:03 - INFO - __main__ -   test: [batch: 386/391 ] | Loss: 0.969 | Acc: 76.060% (37677/49536)/ 92.819% (45979/49536)
01/14/2023 06:49:06 - INFO - __main__ -   test: [batch: 387/391 ] | Loss: 0.967 | Acc: 76.099% (37794/49664)/ 92.830% (46103/49664)
01/14/2023 06:49:09 - INFO - __main__ -   test: [batch: 388/391 ] | Loss: 0.965 | Acc: 76.145% (37914/49792)/ 92.842% (46228/49792)
01/14/2023 06:49:11 - INFO - __main__ -   test: [batch: 389/391 ] | Loss: 0.965 | Acc: 76.146% (38012/49920)/ 92.849% (46350/49920)
01/14/2023 06:49:14 - INFO - __main__ -   test: [batch: 390/391 ] | Loss: 0.967 | Acc: 76.098% (38049/50000)/ 92.840% (46420/50000)
01/14/2023 06:49:14 - INFO - __main__ -   Final accuracy: 76.098
01/14/2023 06:49:14 - INFO - __main__ -   {'milestones': Counter({2: 1, 3: 1, 4: 1}), 'gamma': 0.1, 'base_lrs': [0.00025], 'last_epoch': 2, '_step_count': 3, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [2.5e-05]}
01/14/2023 06:49:14 - INFO - __main__ -   
Epoch: 2
01/14/2023 06:49:16 - INFO - __main__ -   test: [epoch: 2 | batch: 0/10010 ] | Loss: 0.967 | Acc: 78.125% (100/128)
01/14/2023 06:53:39 - INFO - __main__ -   test: [epoch: 2 | batch: 100/10010 ] | Loss: 0.829 | Acc: 79.695% (10303/12928)
01/14/2023 06:58:00 - INFO - __main__ -   test: [epoch: 2 | batch: 200/10010 ] | Loss: 0.837 | Acc: 79.081% (20346/25728)
01/14/2023 07:02:18 - INFO - __main__ -   test: [epoch: 2 | batch: 300/10010 ] | Loss: 0.842 | Acc: 79.044% (30454/38528)
01/14/2023 07:06:39 - INFO - __main__ -   test: [epoch: 2 | batch: 400/10010 ] | Loss: 0.848 | Acc: 78.856% (40475/51328)
01/14/2023 07:11:01 - INFO - __main__ -   test: [epoch: 2 | batch: 500/10010 ] | Loss: 0.851 | Acc: 78.789% (50526/64128)
01/14/2023 07:15:22 - INFO - __main__ -   test: [epoch: 2 | batch: 600/10010 ] | Loss: 0.855 | Acc: 78.729% (60565/76928)
01/14/2023 07:19:46 - INFO - __main__ -   test: [epoch: 2 | batch: 700/10010 ] | Loss: 0.856 | Acc: 78.681% (70599/89728)
01/14/2023 07:24:06 - INFO - __main__ -   test: [epoch: 2 | batch: 800/10010 ] | Loss: 0.857 | Acc: 78.645% (80633/102528)
01/14/2023 07:28:26 - INFO - __main__ -   test: [epoch: 2 | batch: 900/10010 ] | Loss: 0.855 | Acc: 78.680% (90740/115328)
01/14/2023 07:32:46 - INFO - __main__ -   test: [epoch: 2 | batch: 1000/10010 ] | Loss: 0.857 | Acc: 78.621% (100735/128128)
01/14/2023 07:37:08 - INFO - __main__ -   test: [epoch: 2 | batch: 1100/10010 ] | Loss: 0.858 | Acc: 78.617% (110793/140928)
01/14/2023 07:41:29 - INFO - __main__ -   test: [epoch: 2 | batch: 1200/10010 ] | Loss: 0.860 | Acc: 78.575% (120792/153728)
01/14/2023 07:45:51 - INFO - __main__ -   test: [epoch: 2 | batch: 1300/10010 ] | Loss: 0.858 | Acc: 78.600% (130891/166528)
01/14/2023 07:50:13 - INFO - __main__ -   test: [epoch: 2 | batch: 1400/10010 ] | Loss: 0.858 | Acc: 78.626% (140998/179328)
01/14/2023 07:54:32 - INFO - __main__ -   test: [epoch: 2 | batch: 1500/10010 ] | Loss: 0.858 | Acc: 78.592% (150998/192128)
01/14/2023 07:58:52 - INFO - __main__ -   test: [epoch: 2 | batch: 1600/10010 ] | Loss: 0.859 | Acc: 78.573% (161019/204928)
01/14/2023 08:03:12 - INFO - __main__ -   test: [epoch: 2 | batch: 1700/10010 ] | Loss: 0.859 | Acc: 78.572% (171073/217728)
01/14/2023 08:07:32 - INFO - __main__ -   test: [epoch: 2 | batch: 1800/10010 ] | Loss: 0.858 | Acc: 78.580% (181149/230528)
01/14/2023 08:11:52 - INFO - __main__ -   test: [epoch: 2 | batch: 1900/10010 ] | Loss: 0.857 | Acc: 78.600% (191256/243328)
01/14/2023 08:16:12 - INFO - __main__ -   test: [epoch: 2 | batch: 2000/10010 ] | Loss: 0.856 | Acc: 78.619% (201366/256128)
01/14/2023 08:20:32 - INFO - __main__ -   test: [epoch: 2 | batch: 2100/10010 ] | Loss: 0.856 | Acc: 78.612% (211409/268928)
01/14/2023 08:24:52 - INFO - __main__ -   test: [epoch: 2 | batch: 2200/10010 ] | Loss: 0.857 | Acc: 78.599% (221436/281728)
01/14/2023 08:29:12 - INFO - __main__ -   test: [epoch: 2 | batch: 2300/10010 ] | Loss: 0.858 | Acc: 78.580% (231441/294528)
01/14/2023 08:33:31 - INFO - __main__ -   test: [epoch: 2 | batch: 2400/10010 ] | Loss: 0.858 | Acc: 78.584% (241510/307328)
01/14/2023 08:37:52 - INFO - __main__ -   test: [epoch: 2 | batch: 2500/10010 ] | Loss: 0.858 | Acc: 78.585% (251571/320128)
01/14/2023 08:42:13 - INFO - __main__ -   test: [epoch: 2 | batch: 2600/10010 ] | Loss: 0.858 | Acc: 78.599% (261679/332928)
01/14/2023 08:46:33 - INFO - __main__ -   test: [epoch: 2 | batch: 2700/10010 ] | Loss: 0.857 | Acc: 78.636% (271865/345728)
01/14/2023 08:50:53 - INFO - __main__ -   test: [epoch: 2 | batch: 2800/10010 ] | Loss: 0.857 | Acc: 78.636% (281931/358528)
01/14/2023 08:55:15 - INFO - __main__ -   test: [epoch: 2 | batch: 2900/10010 ] | Loss: 0.857 | Acc: 78.636% (291997/371328)
01/14/2023 08:59:36 - INFO - __main__ -   test: [epoch: 2 | batch: 3000/10010 ] | Loss: 0.857 | Acc: 78.641% (302084/384128)
01/14/2023 09:03:55 - INFO - __main__ -   test: [epoch: 2 | batch: 3100/10010 ] | Loss: 0.857 | Acc: 78.651% (312188/396928)
01/14/2023 09:08:16 - INFO - __main__ -   test: [epoch: 2 | batch: 3200/10010 ] | Loss: 0.856 | Acc: 78.660% (322291/409728)
01/14/2023 09:12:36 - INFO - __main__ -   test: [epoch: 2 | batch: 3300/10010 ] | Loss: 0.857 | Acc: 78.646% (332300/422528)
01/14/2023 09:16:57 - INFO - __main__ -   test: [epoch: 2 | batch: 3400/10010 ] | Loss: 0.857 | Acc: 78.657% (342417/435328)
01/14/2023 09:21:18 - INFO - __main__ -   test: [epoch: 2 | batch: 3500/10010 ] | Loss: 0.857 | Acc: 78.659% (352495/448128)
01/14/2023 09:25:40 - INFO - __main__ -   test: [epoch: 2 | batch: 3600/10010 ] | Loss: 0.856 | Acc: 78.679% (362652/460928)
01/14/2023 09:30:02 - INFO - __main__ -   test: [epoch: 2 | batch: 3700/10010 ] | Loss: 0.857 | Acc: 78.662% (372643/473728)
01/14/2023 09:34:24 - INFO - __main__ -   test: [epoch: 2 | batch: 3800/10010 ] | Loss: 0.856 | Acc: 78.683% (382815/486528)
01/14/2023 09:38:44 - INFO - __main__ -   test: [epoch: 2 | batch: 3900/10010 ] | Loss: 0.856 | Acc: 78.675% (392845/499328)
01/14/2023 09:43:04 - INFO - __main__ -   test: [epoch: 2 | batch: 4000/10010 ] | Loss: 0.857 | Acc: 78.659% (402837/512128)
01/14/2023 09:47:24 - INFO - __main__ -   test: [epoch: 2 | batch: 4100/10010 ] | Loss: 0.857 | Acc: 78.662% (412919/524928)
01/14/2023 09:51:44 - INFO - __main__ -   test: [epoch: 2 | batch: 4200/10010 ] | Loss: 0.856 | Acc: 78.654% (422947/537728)
01/14/2023 09:56:04 - INFO - __main__ -   test: [epoch: 2 | batch: 4300/10010 ] | Loss: 0.857 | Acc: 78.653% (433005/550528)
01/14/2023 10:00:25 - INFO - __main__ -   test: [epoch: 2 | batch: 4400/10010 ] | Loss: 0.856 | Acc: 78.666% (443148/563328)
01/14/2023 10:04:48 - INFO - __main__ -   test: [epoch: 2 | batch: 4500/10010 ] | Loss: 0.857 | Acc: 78.659% (453178/576128)
01/14/2023 10:09:12 - INFO - __main__ -   test: [epoch: 2 | batch: 4600/10010 ] | Loss: 0.857 | Acc: 78.644% (463158/588928)
01/14/2023 10:13:33 - INFO - __main__ -   test: [epoch: 2 | batch: 4700/10010 ] | Loss: 0.857 | Acc: 78.643% (473215/601728)
01/14/2023 10:17:55 - INFO - __main__ -   test: [epoch: 2 | batch: 4800/10010 ] | Loss: 0.857 | Acc: 78.650% (483326/614528)
01/14/2023 10:22:14 - INFO - __main__ -   test: [epoch: 2 | batch: 4900/10010 ] | Loss: 0.857 | Acc: 78.646% (493367/627328)
01/14/2023 10:26:37 - INFO - __main__ -   test: [epoch: 2 | batch: 5000/10010 ] | Loss: 0.857 | Acc: 78.662% (503537/640128)
01/14/2023 10:30:57 - INFO - __main__ -   test: [epoch: 2 | batch: 5100/10010 ] | Loss: 0.857 | Acc: 78.653% (513545/652928)
01/14/2023 10:35:18 - INFO - __main__ -   test: [epoch: 2 | batch: 5200/10010 ] | Loss: 0.857 | Acc: 78.648% (523579/665728)
01/14/2023 10:39:36 - INFO - __main__ -   test: [epoch: 2 | batch: 5300/10010 ] | Loss: 0.857 | Acc: 78.645% (533631/678528)
01/14/2023 10:43:58 - INFO - __main__ -   test: [epoch: 2 | batch: 5400/10010 ] | Loss: 0.858 | Acc: 78.639% (543652/691328)
01/14/2023 10:48:19 - INFO - __main__ -   test: [epoch: 2 | batch: 5500/10010 ] | Loss: 0.858 | Acc: 78.638% (553710/704128)
01/14/2023 10:52:40 - INFO - __main__ -   test: [epoch: 2 | batch: 5600/10010 ] | Loss: 0.858 | Acc: 78.643% (563814/716928)
01/14/2023 10:57:02 - INFO - __main__ -   test: [epoch: 2 | batch: 5700/10010 ] | Loss: 0.857 | Acc: 78.645% (573894/729728)
01/14/2023 11:01:24 - INFO - __main__ -   test: [epoch: 2 | batch: 5800/10010 ] | Loss: 0.857 | Acc: 78.649% (583991/742528)
01/14/2023 11:05:43 - INFO - __main__ -   test: [epoch: 2 | batch: 5900/10010 ] | Loss: 0.857 | Acc: 78.659% (594133/755328)
01/14/2023 11:10:05 - INFO - __main__ -   test: [epoch: 2 | batch: 6000/10010 ] | Loss: 0.857 | Acc: 78.657% (604186/768128)
01/14/2023 11:14:28 - INFO - __main__ -   test: [epoch: 2 | batch: 6100/10010 ] | Loss: 0.857 | Acc: 78.644% (614153/780928)
01/14/2023 11:18:48 - INFO - __main__ -   test: [epoch: 2 | batch: 6200/10010 ] | Loss: 0.857 | Acc: 78.641% (624192/793728)
01/14/2023 11:23:12 - INFO - __main__ -   test: [epoch: 2 | batch: 6300/10010 ] | Loss: 0.857 | Acc: 78.641% (634261/806528)
01/14/2023 11:27:33 - INFO - __main__ -   test: [epoch: 2 | batch: 6400/10010 ] | Loss: 0.858 | Acc: 78.634% (644267/819328)
01/14/2023 11:31:55 - INFO - __main__ -   test: [epoch: 2 | batch: 6500/10010 ] | Loss: 0.857 | Acc: 78.642% (654405/832128)
01/14/2023 11:36:15 - INFO - __main__ -   test: [epoch: 2 | batch: 6600/10010 ] | Loss: 0.857 | Acc: 78.640% (664455/844928)
01/14/2023 11:40:37 - INFO - __main__ -   test: [epoch: 2 | batch: 6700/10010 ] | Loss: 0.858 | Acc: 78.636% (674483/857728)
01/14/2023 11:44:58 - INFO - __main__ -   test: [epoch: 2 | batch: 6800/10010 ] | Loss: 0.858 | Acc: 78.637% (684557/870528)
01/14/2023 11:49:20 - INFO - __main__ -   test: [epoch: 2 | batch: 6900/10010 ] | Loss: 0.857 | Acc: 78.640% (694649/883328)
01/14/2023 11:53:43 - INFO - __main__ -   test: [epoch: 2 | batch: 7000/10010 ] | Loss: 0.857 | Acc: 78.633% (704649/896128)
01/14/2023 11:58:04 - INFO - __main__ -   test: [epoch: 2 | batch: 7100/10010 ] | Loss: 0.857 | Acc: 78.638% (714759/908928)
01/14/2023 12:02:23 - INFO - __main__ -   test: [epoch: 2 | batch: 7200/10010 ] | Loss: 0.857 | Acc: 78.641% (724856/921728)
01/14/2023 12:06:45 - INFO - __main__ -   test: [epoch: 2 | batch: 7300/10010 ] | Loss: 0.857 | Acc: 78.639% (734906/934528)
01/14/2023 12:11:05 - INFO - __main__ -   test: [epoch: 2 | batch: 7400/10010 ] | Loss: 0.857 | Acc: 78.645% (745027/947328)
01/14/2023 12:15:28 - INFO - __main__ -   test: [epoch: 2 | batch: 7500/10010 ] | Loss: 0.857 | Acc: 78.652% (755158/960128)
01/14/2023 12:19:47 - INFO - __main__ -   test: [epoch: 2 | batch: 7600/10010 ] | Loss: 0.857 | Acc: 78.658% (765289/972928)
01/14/2023 12:24:08 - INFO - __main__ -   test: [epoch: 2 | batch: 7700/10010 ] | Loss: 0.857 | Acc: 78.653% (775300/985728)
01/14/2023 12:28:28 - INFO - __main__ -   test: [epoch: 2 | batch: 7800/10010 ] | Loss: 0.857 | Acc: 78.649% (785331/998528)
01/14/2023 12:32:51 - INFO - __main__ -   test: [epoch: 2 | batch: 7900/10010 ] | Loss: 0.857 | Acc: 78.653% (795438/1011328)
01/14/2023 12:37:13 - INFO - __main__ -   test: [epoch: 2 | batch: 8000/10010 ] | Loss: 0.857 | Acc: 78.651% (805489/1024128)
01/14/2023 12:41:34 - INFO - __main__ -   test: [epoch: 2 | batch: 8100/10010 ] | Loss: 0.857 | Acc: 78.658% (815622/1036928)
01/14/2023 12:45:55 - INFO - __main__ -   test: [epoch: 2 | batch: 8200/10010 ] | Loss: 0.857 | Acc: 78.655% (825664/1049728)
01/14/2023 12:50:17 - INFO - __main__ -   test: [epoch: 2 | batch: 8300/10010 ] | Loss: 0.857 | Acc: 78.652% (835695/1062528)
01/14/2023 12:54:39 - INFO - __main__ -   test: [epoch: 2 | batch: 8400/10010 ] | Loss: 0.857 | Acc: 78.653% (845778/1075328)
01/14/2023 12:58:59 - INFO - __main__ -   test: [epoch: 2 | batch: 8500/10010 ] | Loss: 0.857 | Acc: 78.657% (855889/1088128)
01/14/2023 13:03:20 - INFO - __main__ -   test: [epoch: 2 | batch: 8600/10010 ] | Loss: 0.857 | Acc: 78.663% (866023/1100928)
01/14/2023 13:07:40 - INFO - __main__ -   test: [epoch: 2 | batch: 8700/10010 ] | Loss: 0.857 | Acc: 78.664% (876105/1113728)
01/14/2023 13:12:01 - INFO - __main__ -   test: [epoch: 2 | batch: 8800/10010 ] | Loss: 0.857 | Acc: 78.670% (886245/1126528)
01/14/2023 13:16:22 - INFO - __main__ -   test: [epoch: 2 | batch: 8900/10010 ] | Loss: 0.857 | Acc: 78.670% (896315/1139328)
01/14/2023 13:20:45 - INFO - __main__ -   test: [epoch: 2 | batch: 9000/10010 ] | Loss: 0.857 | Acc: 78.664% (906308/1152128)
01/14/2023 13:25:05 - INFO - __main__ -   test: [epoch: 2 | batch: 9100/10010 ] | Loss: 0.857 | Acc: 78.659% (916316/1164928)
01/14/2023 13:29:27 - INFO - __main__ -   test: [epoch: 2 | batch: 9200/10010 ] | Loss: 0.857 | Acc: 78.663% (926432/1177728)
01/14/2023 13:33:48 - INFO - __main__ -   test: [epoch: 2 | batch: 9300/10010 ] | Loss: 0.857 | Acc: 78.661% (936476/1190528)
01/14/2023 13:38:09 - INFO - __main__ -   test: [epoch: 2 | batch: 9400/10010 ] | Loss: 0.857 | Acc: 78.663% (946570/1203328)
01/14/2023 13:42:30 - INFO - __main__ -   test: [epoch: 2 | batch: 9500/10010 ] | Loss: 0.857 | Acc: 78.659% (956591/1216128)
01/14/2023 13:46:50 - INFO - __main__ -   test: [epoch: 2 | batch: 9600/10010 ] | Loss: 0.857 | Acc: 78.657% (966642/1228928)
01/14/2023 13:51:09 - INFO - __main__ -   test: [epoch: 2 | batch: 9700/10010 ] | Loss: 0.857 | Acc: 78.653% (976657/1241728)
01/14/2023 13:55:32 - INFO - __main__ -   test: [epoch: 2 | batch: 9800/10010 ] | Loss: 0.857 | Acc: 78.652% (986708/1254528)
01/14/2023 13:59:52 - INFO - __main__ -   test: [epoch: 2 | batch: 9900/10010 ] | Loss: 0.857 | Acc: 78.653% (996792/1267328)
01/14/2023 14:04:13 - INFO - __main__ -   test: [epoch: 2 | batch: 10000/10010 ] | Loss: 0.857 | Acc: 78.650% (1006824/1280128)
01/14/2023 14:04:37 - INFO - __main__ -   Saving Checkpoint
01/14/2023 14:04:40 - INFO - __main__ -   test: [batch: 0/391 ] | Loss: 0.474 | Acc: 85.938% (110/128)/ 97.656% (125/128)
01/14/2023 14:04:43 - INFO - __main__ -   test: [batch: 1/391 ] | Loss: 0.470 | Acc: 85.938% (220/256)/ 98.047% (251/256)
01/14/2023 14:04:45 - INFO - __main__ -   test: [batch: 2/391 ] | Loss: 0.624 | Acc: 82.031% (315/384)/ 95.833% (368/384)
01/14/2023 14:04:48 - INFO - __main__ -   test: [batch: 3/391 ] | Loss: 0.579 | Acc: 83.984% (430/512)/ 96.484% (494/512)
01/14/2023 14:04:51 - INFO - __main__ -   test: [batch: 4/391 ] | Loss: 0.496 | Acc: 86.406% (553/640)/ 97.188% (622/640)
01/14/2023 14:04:53 - INFO - __main__ -   test: [batch: 5/391 ] | Loss: 0.439 | Acc: 87.630% (673/768)/ 97.656% (750/768)
01/14/2023 14:04:56 - INFO - __main__ -   test: [batch: 6/391 ] | Loss: 0.431 | Acc: 88.058% (789/896)/ 97.545% (874/896)
01/14/2023 14:04:58 - INFO - __main__ -   test: [batch: 7/391 ] | Loss: 0.411 | Acc: 89.062% (912/1024)/ 97.656% (1000/1024)
01/14/2023 14:05:01 - INFO - __main__ -   test: [batch: 8/391 ] | Loss: 0.428 | Acc: 88.976% (1025/1152)/ 97.656% (1125/1152)
01/14/2023 14:05:04 - INFO - __main__ -   test: [batch: 9/391 ] | Loss: 0.408 | Acc: 89.531% (1146/1280)/ 97.734% (1251/1280)
01/14/2023 14:05:06 - INFO - __main__ -   test: [batch: 10/391 ] | Loss: 0.457 | Acc: 88.139% (1241/1408)/ 97.727% (1376/1408)
01/14/2023 14:05:09 - INFO - __main__ -   test: [batch: 11/391 ] | Loss: 0.459 | Acc: 88.346% (1357/1536)/ 97.656% (1500/1536)
01/14/2023 14:05:12 - INFO - __main__ -   test: [batch: 12/391 ] | Loss: 0.505 | Acc: 87.260% (1452/1664)/ 97.296% (1619/1664)
01/14/2023 14:05:14 - INFO - __main__ -   test: [batch: 13/391 ] | Loss: 0.553 | Acc: 85.714% (1536/1792)/ 96.819% (1735/1792)
01/14/2023 14:05:17 - INFO - __main__ -   test: [batch: 14/391 ] | Loss: 0.574 | Acc: 84.948% (1631/1920)/ 96.875% (1860/1920)
01/14/2023 14:05:19 - INFO - __main__ -   test: [batch: 15/391 ] | Loss: 0.583 | Acc: 84.570% (1732/2048)/ 96.924% (1985/2048)
01/14/2023 14:05:22 - INFO - __main__ -   test: [batch: 16/391 ] | Loss: 0.592 | Acc: 84.375% (1836/2176)/ 96.737% (2105/2176)
01/14/2023 14:05:24 - INFO - __main__ -   test: [batch: 17/391 ] | Loss: 0.617 | Acc: 83.898% (1933/2304)/ 96.267% (2218/2304)
01/14/2023 14:05:27 - INFO - __main__ -   test: [batch: 18/391 ] | Loss: 0.641 | Acc: 83.347% (2027/2432)/ 96.094% (2337/2432)
01/14/2023 14:05:30 - INFO - __main__ -   test: [batch: 19/391 ] | Loss: 0.648 | Acc: 83.086% (2127/2560)/ 96.016% (2458/2560)
01/14/2023 14:05:33 - INFO - __main__ -   test: [batch: 20/391 ] | Loss: 0.645 | Acc: 83.185% (2236/2688)/ 95.908% (2578/2688)
01/14/2023 14:05:35 - INFO - __main__ -   test: [batch: 21/391 ] | Loss: 0.678 | Acc: 82.493% (2323/2816)/ 95.739% (2696/2816)
01/14/2023 14:05:38 - INFO - __main__ -   test: [batch: 22/391 ] | Loss: 0.677 | Acc: 82.371% (2425/2944)/ 95.720% (2818/2944)
01/14/2023 14:05:40 - INFO - __main__ -   test: [batch: 23/391 ] | Loss: 0.721 | Acc: 81.543% (2505/3072)/ 95.443% (2932/3072)
01/14/2023 14:05:43 - INFO - __main__ -   test: [batch: 24/391 ] | Loss: 0.740 | Acc: 81.062% (2594/3200)/ 95.250% (3048/3200)
01/14/2023 14:05:46 - INFO - __main__ -   test: [batch: 25/391 ] | Loss: 0.763 | Acc: 80.499% (2679/3328)/ 95.012% (3162/3328)
01/14/2023 14:05:48 - INFO - __main__ -   test: [batch: 26/391 ] | Loss: 0.777 | Acc: 79.774% (2757/3456)/ 95.023% (3284/3456)
01/14/2023 14:05:51 - INFO - __main__ -   test: [batch: 27/391 ] | Loss: 0.758 | Acc: 80.301% (2878/3584)/ 95.061% (3407/3584)
01/14/2023 14:05:53 - INFO - __main__ -   test: [batch: 28/391 ] | Loss: 0.768 | Acc: 79.795% (2962/3712)/ 95.124% (3531/3712)
01/14/2023 14:05:56 - INFO - __main__ -   test: [batch: 29/391 ] | Loss: 0.760 | Acc: 79.948% (3070/3840)/ 95.234% (3657/3840)
01/14/2023 14:05:58 - INFO - __main__ -   test: [batch: 30/391 ] | Loss: 0.774 | Acc: 79.889% (3170/3968)/ 95.111% (3774/3968)
01/14/2023 14:06:01 - INFO - __main__ -   test: [batch: 31/391 ] | Loss: 0.769 | Acc: 80.103% (3281/4096)/ 95.166% (3898/4096)
01/14/2023 14:06:04 - INFO - __main__ -   test: [batch: 32/391 ] | Loss: 0.754 | Acc: 80.421% (3397/4224)/ 95.265% (4024/4224)
01/14/2023 14:06:06 - INFO - __main__ -   test: [batch: 33/391 ] | Loss: 0.748 | Acc: 80.607% (3508/4352)/ 95.312% (4148/4352)
01/14/2023 14:06:09 - INFO - __main__ -   test: [batch: 34/391 ] | Loss: 0.736 | Acc: 81.004% (3629/4480)/ 95.357% (4272/4480)
01/14/2023 14:06:11 - INFO - __main__ -   test: [batch: 35/391 ] | Loss: 0.723 | Acc: 81.380% (3750/4608)/ 95.378% (4395/4608)
01/14/2023 14:06:14 - INFO - __main__ -   test: [batch: 36/391 ] | Loss: 0.708 | Acc: 81.820% (3875/4736)/ 95.481% (4522/4736)
01/14/2023 14:06:17 - INFO - __main__ -   test: [batch: 37/391 ] | Loss: 0.699 | Acc: 82.113% (3994/4864)/ 95.539% (4647/4864)
01/14/2023 14:06:19 - INFO - __main__ -   test: [batch: 38/391 ] | Loss: 0.692 | Acc: 82.252% (4106/4992)/ 95.593% (4772/4992)
01/14/2023 14:06:22 - INFO - __main__ -   test: [batch: 39/391 ] | Loss: 0.687 | Acc: 82.285% (4213/5120)/ 95.625% (4896/5120)
01/14/2023 14:06:24 - INFO - __main__ -   test: [batch: 40/391 ] | Loss: 0.685 | Acc: 82.412% (4325/5248)/ 95.560% (5015/5248)
01/14/2023 14:06:27 - INFO - __main__ -   test: [batch: 41/391 ] | Loss: 0.689 | Acc: 82.478% (4434/5376)/ 95.499% (5134/5376)
01/14/2023 14:06:30 - INFO - __main__ -   test: [batch: 42/391 ] | Loss: 0.689 | Acc: 82.449% (4538/5504)/ 95.512% (5257/5504)
01/14/2023 14:06:33 - INFO - __main__ -   test: [batch: 43/391 ] | Loss: 0.686 | Acc: 82.493% (4646/5632)/ 95.490% (5378/5632)
01/14/2023 14:06:35 - INFO - __main__ -   test: [batch: 44/391 ] | Loss: 0.690 | Acc: 82.483% (4751/5760)/ 95.399% (5495/5760)
01/14/2023 14:06:38 - INFO - __main__ -   test: [batch: 45/391 ] | Loss: 0.687 | Acc: 82.660% (4867/5888)/ 95.397% (5617/5888)
01/14/2023 14:06:41 - INFO - __main__ -   test: [batch: 46/391 ] | Loss: 0.690 | Acc: 82.646% (4972/6016)/ 95.429% (5741/6016)
01/14/2023 14:06:43 - INFO - __main__ -   test: [batch: 47/391 ] | Loss: 0.691 | Acc: 82.568% (5073/6144)/ 95.492% (5867/6144)
01/14/2023 14:06:46 - INFO - __main__ -   test: [batch: 48/391 ] | Loss: 0.696 | Acc: 82.430% (5170/6272)/ 95.456% (5987/6272)
01/14/2023 14:06:49 - INFO - __main__ -   test: [batch: 49/391 ] | Loss: 0.699 | Acc: 82.484% (5279/6400)/ 95.375% (6104/6400)
01/14/2023 14:06:51 - INFO - __main__ -   test: [batch: 50/391 ] | Loss: 0.691 | Acc: 82.690% (5398/6528)/ 95.420% (6229/6528)
01/14/2023 14:06:54 - INFO - __main__ -   test: [batch: 51/391 ] | Loss: 0.683 | Acc: 82.888% (5517/6656)/ 95.493% (6356/6656)
01/14/2023 14:06:56 - INFO - __main__ -   test: [batch: 52/391 ] | Loss: 0.681 | Acc: 82.960% (5628/6784)/ 95.519% (6480/6784)
01/14/2023 14:06:59 - INFO - __main__ -   test: [batch: 53/391 ] | Loss: 0.672 | Acc: 83.174% (5749/6912)/ 95.587% (6607/6912)
01/14/2023 14:07:02 - INFO - __main__ -   test: [batch: 54/391 ] | Loss: 0.665 | Acc: 83.324% (5866/7040)/ 95.625% (6732/7040)
01/14/2023 14:07:04 - INFO - __main__ -   test: [batch: 55/391 ] | Loss: 0.659 | Acc: 83.468% (5983/7168)/ 95.661% (6857/7168)
01/14/2023 14:07:07 - INFO - __main__ -   test: [batch: 56/391 ] | Loss: 0.650 | Acc: 83.676% (6105/7296)/ 95.710% (6983/7296)
01/14/2023 14:07:09 - INFO - __main__ -   test: [batch: 57/391 ] | Loss: 0.644 | Acc: 83.877% (6227/7424)/ 95.757% (7109/7424)
01/14/2023 14:07:12 - INFO - __main__ -   test: [batch: 58/391 ] | Loss: 0.642 | Acc: 83.938% (6339/7552)/ 95.749% (7231/7552)
01/14/2023 14:07:15 - INFO - __main__ -   test: [batch: 59/391 ] | Loss: 0.644 | Acc: 83.854% (6440/7680)/ 95.768% (7355/7680)
01/14/2023 14:07:17 - INFO - __main__ -   test: [batch: 60/391 ] | Loss: 0.648 | Acc: 83.747% (6539/7808)/ 95.735% (7475/7808)
01/14/2023 14:07:20 - INFO - __main__ -   test: [batch: 61/391 ] | Loss: 0.648 | Acc: 83.758% (6647/7936)/ 95.766% (7600/7936)
01/14/2023 14:07:22 - INFO - __main__ -   test: [batch: 62/391 ] | Loss: 0.648 | Acc: 83.705% (6750/8064)/ 95.771% (7723/8064)
01/14/2023 14:07:25 - INFO - __main__ -   test: [batch: 63/391 ] | Loss: 0.656 | Acc: 83.569% (6846/8192)/ 95.715% (7841/8192)
01/14/2023 14:07:28 - INFO - __main__ -   test: [batch: 64/391 ] | Loss: 0.664 | Acc: 83.365% (6936/8320)/ 95.649% (7958/8320)
01/14/2023 14:07:30 - INFO - __main__ -   test: [batch: 65/391 ] | Loss: 0.677 | Acc: 82.872% (7001/8448)/ 95.561% (8073/8448)
01/14/2023 14:07:33 - INFO - __main__ -   test: [batch: 66/391 ] | Loss: 0.682 | Acc: 82.859% (7106/8576)/ 95.522% (8192/8576)
01/14/2023 14:07:35 - INFO - __main__ -   test: [batch: 67/391 ] | Loss: 0.685 | Acc: 82.801% (7207/8704)/ 95.542% (8316/8704)
01/14/2023 14:07:38 - INFO - __main__ -   test: [batch: 68/391 ] | Loss: 0.686 | Acc: 82.756% (7309/8832)/ 95.562% (8440/8832)
01/14/2023 14:07:40 - INFO - __main__ -   test: [batch: 69/391 ] | Loss: 0.681 | Acc: 82.846% (7423/8960)/ 95.603% (8566/8960)
01/14/2023 14:07:43 - INFO - __main__ -   test: [batch: 70/391 ] | Loss: 0.683 | Acc: 82.779% (7523/9088)/ 95.599% (8688/9088)
01/14/2023 14:07:45 - INFO - __main__ -   test: [batch: 71/391 ] | Loss: 0.681 | Acc: 82.823% (7633/9216)/ 95.605% (8811/9216)
01/14/2023 14:07:48 - INFO - __main__ -   test: [batch: 72/391 ] | Loss: 0.683 | Acc: 82.716% (7729/9344)/ 95.623% (8935/9344)
01/14/2023 14:07:50 - INFO - __main__ -   test: [batch: 73/391 ] | Loss: 0.686 | Acc: 82.612% (7825/9472)/ 95.629% (9058/9472)
01/14/2023 14:07:53 - INFO - __main__ -   test: [batch: 74/391 ] | Loss: 0.687 | Acc: 82.594% (7929/9600)/ 95.615% (9179/9600)
01/14/2023 14:07:56 - INFO - __main__ -   test: [batch: 75/391 ] | Loss: 0.693 | Acc: 82.412% (8017/9728)/ 95.600% (9300/9728)
01/14/2023 14:07:58 - INFO - __main__ -   test: [batch: 76/391 ] | Loss: 0.693 | Acc: 82.386% (8120/9856)/ 95.607% (9423/9856)
01/14/2023 14:08:01 - INFO - __main__ -   test: [batch: 77/391 ] | Loss: 0.694 | Acc: 82.352% (8222/9984)/ 95.633% (9548/9984)
01/14/2023 14:08:03 - INFO - __main__ -   test: [batch: 78/391 ] | Loss: 0.693 | Acc: 82.239% (8316/10112)/ 95.669% (9674/10112)
01/14/2023 14:08:06 - INFO - __main__ -   test: [batch: 79/391 ] | Loss: 0.692 | Acc: 82.246% (8422/10240)/ 95.693% (9799/10240)
01/14/2023 14:08:09 - INFO - __main__ -   test: [batch: 80/391 ] | Loss: 0.692 | Acc: 82.215% (8524/10368)/ 95.689% (9921/10368)
01/14/2023 14:08:12 - INFO - __main__ -   test: [batch: 81/391 ] | Loss: 0.691 | Acc: 82.222% (8630/10496)/ 95.703% (10045/10496)
01/14/2023 14:08:14 - INFO - __main__ -   test: [batch: 82/391 ] | Loss: 0.691 | Acc: 82.238% (8737/10624)/ 95.698% (10167/10624)
01/14/2023 14:08:17 - INFO - __main__ -   test: [batch: 83/391 ] | Loss: 0.690 | Acc: 82.282% (8847/10752)/ 95.685% (10288/10752)
01/14/2023 14:08:19 - INFO - __main__ -   test: [batch: 84/391 ] | Loss: 0.687 | Acc: 82.390% (8964/10880)/ 95.717% (10414/10880)
01/14/2023 14:08:22 - INFO - __main__ -   test: [batch: 85/391 ] | Loss: 0.685 | Acc: 82.404% (9071/11008)/ 95.758% (10541/11008)
01/14/2023 14:08:25 - INFO - __main__ -   test: [batch: 86/391 ] | Loss: 0.689 | Acc: 82.328% (9168/11136)/ 95.735% (10661/11136)
01/14/2023 14:08:27 - INFO - __main__ -   test: [batch: 87/391 ] | Loss: 0.687 | Acc: 82.369% (9278/11264)/ 95.739% (10784/11264)
01/14/2023 14:08:30 - INFO - __main__ -   test: [batch: 88/391 ] | Loss: 0.694 | Acc: 82.303% (9376/11392)/ 95.672% (10899/11392)
01/14/2023 14:08:32 - INFO - __main__ -   test: [batch: 89/391 ] | Loss: 0.692 | Acc: 82.361% (9488/11520)/ 95.686% (11023/11520)
01/14/2023 14:08:35 - INFO - __main__ -   test: [batch: 90/391 ] | Loss: 0.693 | Acc: 82.280% (9584/11648)/ 95.690% (11146/11648)
01/14/2023 14:08:37 - INFO - __main__ -   test: [batch: 91/391 ] | Loss: 0.692 | Acc: 82.311% (9693/11776)/ 95.695% (11269/11776)
01/14/2023 14:08:40 - INFO - __main__ -   test: [batch: 92/391 ] | Loss: 0.693 | Acc: 82.292% (9796/11904)/ 95.674% (11389/11904)
01/14/2023 14:08:43 - INFO - __main__ -   test: [batch: 93/391 ] | Loss: 0.697 | Acc: 82.089% (9877/12032)/ 95.703% (11515/12032)
01/14/2023 14:08:46 - INFO - __main__ -   test: [batch: 94/391 ] | Loss: 0.699 | Acc: 81.965% (9967/12160)/ 95.715% (11639/12160)
01/14/2023 14:08:48 - INFO - __main__ -   test: [batch: 95/391 ] | Loss: 0.697 | Acc: 82.023% (10079/12288)/ 95.719% (11762/12288)
01/14/2023 14:08:51 - INFO - __main__ -   test: [batch: 96/391 ] | Loss: 0.700 | Acc: 81.943% (10174/12416)/ 95.731% (11886/12416)
01/14/2023 14:08:53 - INFO - __main__ -   test: [batch: 97/391 ] | Loss: 0.702 | Acc: 81.776% (10258/12544)/ 95.751% (12011/12544)
01/14/2023 14:08:56 - INFO - __main__ -   test: [batch: 98/391 ] | Loss: 0.698 | Acc: 81.881% (10376/12672)/ 95.786% (12138/12672)
01/14/2023 14:08:58 - INFO - __main__ -   test: [batch: 99/391 ] | Loss: 0.692 | Acc: 82.023% (10499/12800)/ 95.828% (12266/12800)
01/14/2023 14:09:01 - INFO - __main__ -   test: [batch: 100/391 ] | Loss: 0.692 | Acc: 82.078% (10611/12928)/ 95.846% (12391/12928)
01/14/2023 14:09:03 - INFO - __main__ -   test: [batch: 101/391 ] | Loss: 0.687 | Acc: 82.177% (10729/13056)/ 95.879% (12518/13056)
01/14/2023 14:09:05 - INFO - __main__ -   test: [batch: 102/391 ] | Loss: 0.684 | Acc: 82.304% (10851/13184)/ 95.904% (12644/13184)
01/14/2023 14:09:08 - INFO - __main__ -   test: [batch: 103/391 ] | Loss: 0.685 | Acc: 82.129% (10933/13312)/ 95.928% (12770/13312)
01/14/2023 14:09:10 - INFO - __main__ -   test: [batch: 104/391 ] | Loss: 0.685 | Acc: 82.068% (11030/13440)/ 95.923% (12892/13440)
01/14/2023 14:09:12 - INFO - __main__ -   test: [batch: 105/391 ] | Loss: 0.686 | Acc: 82.053% (11133/13568)/ 95.924% (13015/13568)
01/14/2023 14:09:14 - INFO - __main__ -   test: [batch: 106/391 ] | Loss: 0.694 | Acc: 81.929% (11221/13696)/ 95.860% (13129/13696)
01/14/2023 14:09:16 - INFO - __main__ -   test: [batch: 107/391 ] | Loss: 0.690 | Acc: 82.053% (11343/13824)/ 95.891% (13256/13824)
01/14/2023 14:09:18 - INFO - __main__ -   test: [batch: 108/391 ] | Loss: 0.693 | Acc: 81.909% (11428/13952)/ 95.893% (13379/13952)
01/14/2023 14:09:20 - INFO - __main__ -   test: [batch: 109/391 ] | Loss: 0.694 | Acc: 81.889% (11530/14080)/ 95.895% (13502/14080)
01/14/2023 14:09:23 - INFO - __main__ -   test: [batch: 110/391 ] | Loss: 0.697 | Acc: 81.700% (11608/14208)/ 95.904% (13626/14208)
01/14/2023 14:09:25 - INFO - __main__ -   test: [batch: 111/391 ] | Loss: 0.700 | Acc: 81.662% (11707/14336)/ 95.857% (13742/14336)
01/14/2023 14:09:27 - INFO - __main__ -   test: [batch: 112/391 ] | Loss: 0.701 | Acc: 81.679% (11814/14464)/ 95.873% (13867/14464)
01/14/2023 14:09:29 - INFO - __main__ -   test: [batch: 113/391 ] | Loss: 0.700 | Acc: 81.709% (11923/14592)/ 95.874% (13990/14592)
01/14/2023 14:09:31 - INFO - __main__ -   test: [batch: 114/391 ] | Loss: 0.697 | Acc: 81.800% (12041/14720)/ 95.904% (14117/14720)
01/14/2023 14:09:33 - INFO - __main__ -   test: [batch: 115/391 ] | Loss: 0.694 | Acc: 81.883% (12158/14848)/ 95.925% (14243/14848)
01/14/2023 14:09:35 - INFO - __main__ -   test: [batch: 116/391 ] | Loss: 0.697 | Acc: 81.844% (12257/14976)/ 95.893% (14361/14976)
01/14/2023 14:09:38 - INFO - __main__ -   test: [batch: 117/391 ] | Loss: 0.695 | Acc: 81.859% (12364/15104)/ 95.908% (14486/15104)
01/14/2023 14:09:40 - INFO - __main__ -   test: [batch: 118/391 ] | Loss: 0.699 | Acc: 81.749% (12452/15232)/ 95.923% (14611/15232)
01/14/2023 14:09:42 - INFO - __main__ -   test: [batch: 119/391 ] | Loss: 0.697 | Acc: 81.797% (12564/15360)/ 95.944% (14737/15360)
01/14/2023 14:09:44 - INFO - __main__ -   test: [batch: 120/391 ] | Loss: 0.696 | Acc: 81.838% (12675/15488)/ 95.958% (14862/15488)
01/14/2023 14:09:46 - INFO - __main__ -   test: [batch: 121/391 ] | Loss: 0.700 | Acc: 81.705% (12759/15616)/ 95.934% (14981/15616)
01/14/2023 14:09:49 - INFO - __main__ -   test: [batch: 122/391 ] | Loss: 0.702 | Acc: 81.650% (12855/15744)/ 95.910% (15100/15744)
01/14/2023 14:09:51 - INFO - __main__ -   test: [batch: 123/391 ] | Loss: 0.703 | Acc: 81.666% (12962/15872)/ 95.911% (15223/15872)
01/14/2023 14:09:53 - INFO - __main__ -   test: [batch: 124/391 ] | Loss: 0.702 | Acc: 81.681% (13069/16000)/ 95.925% (15348/16000)
01/14/2023 14:09:55 - INFO - __main__ -   test: [batch: 125/391 ] | Loss: 0.698 | Acc: 81.796% (13192/16128)/ 95.951% (15475/16128)
01/14/2023 14:09:57 - INFO - __main__ -   test: [batch: 126/391 ] | Loss: 0.694 | Acc: 81.890% (13312/16256)/ 95.965% (15600/16256)
01/14/2023 14:09:59 - INFO - __main__ -   test: [batch: 127/391 ] | Loss: 0.692 | Acc: 81.970% (13430/16384)/ 95.972% (15724/16384)
01/14/2023 14:10:02 - INFO - __main__ -   test: [batch: 128/391 ] | Loss: 0.693 | Acc: 81.953% (13532/16512)/ 95.948% (15843/16512)
01/14/2023 14:10:04 - INFO - __main__ -   test: [batch: 129/391 ] | Loss: 0.692 | Acc: 81.965% (13639/16640)/ 95.962% (15968/16640)
01/14/2023 14:10:06 - INFO - __main__ -   test: [batch: 130/391 ] | Loss: 0.689 | Acc: 82.061% (13760/16768)/ 95.980% (16094/16768)
01/14/2023 14:10:08 - INFO - __main__ -   test: [batch: 131/391 ] | Loss: 0.688 | Acc: 82.102% (13872/16896)/ 95.987% (16218/16896)
01/14/2023 14:10:10 - INFO - __main__ -   test: [batch: 132/391 ] | Loss: 0.686 | Acc: 82.166% (13988/17024)/ 96.000% (16343/17024)
01/14/2023 14:10:12 - INFO - __main__ -   test: [batch: 133/391 ] | Loss: 0.687 | Acc: 82.119% (14085/17152)/ 95.989% (16464/17152)
01/14/2023 14:10:14 - INFO - __main__ -   test: [batch: 134/391 ] | Loss: 0.685 | Acc: 82.141% (14194/17280)/ 96.007% (16590/17280)
01/14/2023 14:10:16 - INFO - __main__ -   test: [batch: 135/391 ] | Loss: 0.685 | Acc: 82.135% (14298/17408)/ 96.002% (16712/17408)
01/14/2023 14:10:19 - INFO - __main__ -   test: [batch: 136/391 ] | Loss: 0.684 | Acc: 82.088% (14395/17536)/ 96.025% (16839/17536)
01/14/2023 14:10:21 - INFO - __main__ -   test: [batch: 137/391 ] | Loss: 0.682 | Acc: 82.139% (14509/17664)/ 96.054% (16967/17664)
01/14/2023 14:10:23 - INFO - __main__ -   test: [batch: 138/391 ] | Loss: 0.681 | Acc: 82.172% (14620/17792)/ 96.066% (17092/17792)
01/14/2023 14:10:25 - INFO - __main__ -   test: [batch: 139/391 ] | Loss: 0.686 | Acc: 82.015% (14697/17920)/ 96.055% (17213/17920)
01/14/2023 14:10:27 - INFO - __main__ -   test: [batch: 140/391 ] | Loss: 0.688 | Acc: 81.943% (14789/18048)/ 96.049% (17335/18048)
01/14/2023 14:10:29 - INFO - __main__ -   test: [batch: 141/391 ] | Loss: 0.687 | Acc: 81.982% (14901/18176)/ 96.061% (17460/18176)
01/14/2023 14:10:31 - INFO - __main__ -   test: [batch: 142/391 ] | Loss: 0.684 | Acc: 82.059% (15020/18304)/ 96.077% (17586/18304)
01/14/2023 14:10:34 - INFO - __main__ -   test: [batch: 143/391 ] | Loss: 0.685 | Acc: 82.075% (15128/18432)/ 96.056% (17705/18432)
01/14/2023 14:10:36 - INFO - __main__ -   test: [batch: 144/391 ] | Loss: 0.687 | Acc: 82.042% (15227/18560)/ 96.024% (17822/18560)
01/14/2023 14:10:38 - INFO - __main__ -   test: [batch: 145/391 ] | Loss: 0.690 | Acc: 82.021% (15328/18688)/ 95.997% (17940/18688)
01/14/2023 14:10:40 - INFO - __main__ -   test: [batch: 146/391 ] | Loss: 0.691 | Acc: 81.999% (15429/18816)/ 95.982% (18060/18816)
01/14/2023 14:10:42 - INFO - __main__ -   test: [batch: 147/391 ] | Loss: 0.692 | Acc: 81.994% (15533/18944)/ 95.962% (18179/18944)
01/14/2023 14:10:44 - INFO - __main__ -   test: [batch: 148/391 ] | Loss: 0.693 | Acc: 81.926% (15625/19072)/ 95.957% (18301/19072)
01/14/2023 14:10:46 - INFO - __main__ -   test: [batch: 149/391 ] | Loss: 0.697 | Acc: 81.833% (15712/19200)/ 95.922% (18417/19200)
01/14/2023 14:10:49 - INFO - __main__ -   test: [batch: 150/391 ] | Loss: 0.697 | Acc: 81.767% (15804/19328)/ 95.928% (18541/19328)
01/14/2023 14:10:51 - INFO - __main__ -   test: [batch: 151/391 ] | Loss: 0.697 | Acc: 81.805% (15916/19456)/ 95.929% (18664/19456)
01/14/2023 14:10:53 - INFO - __main__ -   test: [batch: 152/391 ] | Loss: 0.699 | Acc: 81.791% (16018/19584)/ 95.920% (18785/19584)
01/14/2023 14:10:55 - INFO - __main__ -   test: [batch: 153/391 ] | Loss: 0.697 | Acc: 81.854% (16135/19712)/ 95.931% (18910/19712)
01/14/2023 14:10:57 - INFO - __main__ -   test: [batch: 154/391 ] | Loss: 0.697 | Acc: 81.850% (16239/19840)/ 95.902% (19027/19840)
01/14/2023 14:10:59 - INFO - __main__ -   test: [batch: 155/391 ] | Loss: 0.697 | Acc: 81.871% (16348/19968)/ 95.898% (19149/19968)
01/14/2023 14:11:01 - INFO - __main__ -   test: [batch: 156/391 ] | Loss: 0.700 | Acc: 81.792% (16437/20096)/ 95.875% (19267/20096)
01/14/2023 14:11:03 - INFO - __main__ -   test: [batch: 157/391 ] | Loss: 0.701 | Acc: 81.784% (16540/20224)/ 95.856% (19386/20224)
01/14/2023 14:11:05 - INFO - __main__ -   test: [batch: 158/391 ] | Loss: 0.702 | Acc: 81.741% (16636/20352)/ 95.833% (19504/20352)
01/14/2023 14:11:07 - INFO - __main__ -   test: [batch: 159/391 ] | Loss: 0.704 | Acc: 81.714% (16735/20480)/ 95.835% (19627/20480)
01/14/2023 14:11:09 - INFO - __main__ -   test: [batch: 160/391 ] | Loss: 0.704 | Acc: 81.692% (16835/20608)/ 95.822% (19747/20608)
01/14/2023 14:11:12 - INFO - __main__ -   test: [batch: 161/391 ] | Loss: 0.714 | Acc: 81.467% (16893/20736)/ 95.727% (19850/20736)
01/14/2023 14:11:14 - INFO - __main__ -   test: [batch: 162/391 ] | Loss: 0.719 | Acc: 81.365% (16976/20864)/ 95.662% (19959/20864)
01/14/2023 14:11:16 - INFO - __main__ -   test: [batch: 163/391 ] | Loss: 0.722 | Acc: 81.317% (17070/20992)/ 95.651% (20079/20992)
01/14/2023 14:11:18 - INFO - __main__ -   test: [batch: 164/391 ] | Loss: 0.721 | Acc: 81.307% (17172/21120)/ 95.668% (20205/21120)
01/14/2023 14:11:20 - INFO - __main__ -   test: [batch: 165/391 ] | Loss: 0.723 | Acc: 81.222% (17258/21248)/ 95.661% (20326/21248)
01/14/2023 14:11:22 - INFO - __main__ -   test: [batch: 166/391 ] | Loss: 0.723 | Acc: 81.231% (17364/21376)/ 95.649% (20446/21376)
01/14/2023 14:11:24 - INFO - __main__ -   test: [batch: 167/391 ] | Loss: 0.725 | Acc: 81.176% (17456/21504)/ 95.633% (20565/21504)
01/14/2023 14:11:27 - INFO - __main__ -   test: [batch: 168/391 ] | Loss: 0.725 | Acc: 81.181% (17561/21632)/ 95.631% (20687/21632)
01/14/2023 14:11:29 - INFO - __main__ -   test: [batch: 169/391 ] | Loss: 0.726 | Acc: 81.144% (17657/21760)/ 95.602% (20803/21760)
01/14/2023 14:11:31 - INFO - __main__ -   test: [batch: 170/391 ] | Loss: 0.731 | Acc: 81.031% (17736/21888)/ 95.559% (20916/21888)
01/14/2023 14:11:33 - INFO - __main__ -   test: [batch: 171/391 ] | Loss: 0.735 | Acc: 80.964% (17825/22016)/ 95.531% (21032/22016)
01/14/2023 14:11:35 - INFO - __main__ -   test: [batch: 172/391 ] | Loss: 0.736 | Acc: 80.920% (17919/22144)/ 95.529% (21154/22144)
01/14/2023 14:11:37 - INFO - __main__ -   test: [batch: 173/391 ] | Loss: 0.738 | Acc: 80.868% (18011/22272)/ 95.506% (21271/22272)
01/14/2023 14:11:39 - INFO - __main__ -   test: [batch: 174/391 ] | Loss: 0.742 | Acc: 80.781% (18095/22400)/ 95.460% (21383/22400)
01/14/2023 14:11:42 - INFO - __main__ -   test: [batch: 175/391 ] | Loss: 0.740 | Acc: 80.833% (18210/22528)/ 95.463% (21506/22528)
01/14/2023 14:11:44 - INFO - __main__ -   test: [batch: 176/391 ] | Loss: 0.741 | Acc: 80.826% (18312/22656)/ 95.441% (21623/22656)
01/14/2023 14:11:46 - INFO - __main__ -   test: [batch: 177/391 ] | Loss: 0.745 | Acc: 80.754% (18399/22784)/ 95.396% (21735/22784)
01/14/2023 14:11:48 - INFO - __main__ -   test: [batch: 178/391 ] | Loss: 0.746 | Acc: 80.731% (18497/22912)/ 95.369% (21851/22912)
01/14/2023 14:11:50 - INFO - __main__ -   test: [batch: 179/391 ] | Loss: 0.749 | Acc: 80.673% (18587/23040)/ 95.339% (21966/23040)
01/14/2023 14:11:52 - INFO - __main__ -   test: [batch: 180/391 ] | Loss: 0.755 | Acc: 80.546% (18661/23168)/ 95.295% (22078/23168)
01/14/2023 14:11:54 - INFO - __main__ -   test: [batch: 181/391 ] | Loss: 0.762 | Acc: 80.413% (18733/23296)/ 95.240% (22187/23296)
01/14/2023 14:11:57 - INFO - __main__ -   test: [batch: 182/391 ] | Loss: 0.761 | Acc: 80.426% (18839/23424)/ 95.240% (22309/23424)
01/14/2023 14:11:59 - INFO - __main__ -   test: [batch: 183/391 ] | Loss: 0.767 | Acc: 80.307% (18914/23552)/ 95.147% (22409/23552)
01/14/2023 14:12:01 - INFO - __main__ -   test: [batch: 184/391 ] | Loss: 0.767 | Acc: 80.329% (19022/23680)/ 95.131% (22527/23680)
01/14/2023 14:12:03 - INFO - __main__ -   test: [batch: 185/391 ] | Loss: 0.767 | Acc: 80.334% (19126/23808)/ 95.123% (22647/23808)
01/14/2023 14:12:05 - INFO - __main__ -   test: [batch: 186/391 ] | Loss: 0.770 | Acc: 80.297% (19220/23936)/ 95.091% (22761/23936)
01/14/2023 14:12:07 - INFO - __main__ -   test: [batch: 187/391 ] | Loss: 0.774 | Acc: 80.182% (19295/24064)/ 95.067% (22877/24064)
01/14/2023 14:12:09 - INFO - __main__ -   test: [batch: 188/391 ] | Loss: 0.778 | Acc: 80.051% (19366/24192)/ 95.048% (22994/24192)
01/14/2023 14:12:11 - INFO - __main__ -   test: [batch: 189/391 ] | Loss: 0.779 | Acc: 79.984% (19452/24320)/ 95.049% (23116/24320)
01/14/2023 14:12:14 - INFO - __main__ -   test: [batch: 190/391 ] | Loss: 0.784 | Acc: 79.892% (19532/24448)/ 95.022% (23231/24448)
01/14/2023 14:12:16 - INFO - __main__ -   test: [batch: 191/391 ] | Loss: 0.786 | Acc: 79.854% (19625/24576)/ 95.007% (23349/24576)
01/14/2023 14:12:18 - INFO - __main__ -   test: [batch: 192/391 ] | Loss: 0.792 | Acc: 79.748% (19701/24704)/ 94.924% (23450/24704)
01/14/2023 14:12:20 - INFO - __main__ -   test: [batch: 193/391 ] | Loss: 0.792 | Acc: 79.768% (19808/24832)/ 94.918% (23570/24832)
01/14/2023 14:12:22 - INFO - __main__ -   test: [batch: 194/391 ] | Loss: 0.795 | Acc: 79.712% (19896/24960)/ 94.904% (23688/24960)
01/14/2023 14:12:24 - INFO - __main__ -   test: [batch: 195/391 ] | Loss: 0.800 | Acc: 79.636% (19979/25088)/ 94.842% (23794/25088)
01/14/2023 14:12:26 - INFO - __main__ -   test: [batch: 196/391 ] | Loss: 0.803 | Acc: 79.529% (20054/25216)/ 94.801% (23905/25216)
01/14/2023 14:12:28 - INFO - __main__ -   test: [batch: 197/391 ] | Loss: 0.808 | Acc: 79.455% (20137/25344)/ 94.780% (24021/25344)
01/14/2023 14:12:31 - INFO - __main__ -   test: [batch: 198/391 ] | Loss: 0.810 | Acc: 79.421% (20230/25472)/ 94.747% (24134/25472)
01/14/2023 14:12:33 - INFO - __main__ -   test: [batch: 199/391 ] | Loss: 0.809 | Acc: 79.406% (20328/25600)/ 94.750% (24256/25600)
01/14/2023 14:12:35 - INFO - __main__ -   test: [batch: 200/391 ] | Loss: 0.809 | Acc: 79.365% (20419/25728)/ 94.745% (24376/25728)
01/14/2023 14:12:37 - INFO - __main__ -   test: [batch: 201/391 ] | Loss: 0.813 | Acc: 79.278% (20498/25856)/ 94.709% (24488/25856)
01/14/2023 14:12:39 - INFO - __main__ -   test: [batch: 202/391 ] | Loss: 0.814 | Acc: 79.260% (20595/25984)/ 94.701% (24607/25984)
01/14/2023 14:12:41 - INFO - __main__ -   test: [batch: 203/391 ] | Loss: 0.815 | Acc: 79.251% (20694/26112)/ 94.684% (24724/26112)
01/14/2023 14:12:44 - INFO - __main__ -   test: [batch: 204/391 ] | Loss: 0.818 | Acc: 79.165% (20773/26240)/ 94.665% (24840/26240)
01/14/2023 14:12:46 - INFO - __main__ -   test: [batch: 205/391 ] | Loss: 0.820 | Acc: 79.084% (20853/26368)/ 94.630% (24952/26368)
01/14/2023 14:12:48 - INFO - __main__ -   test: [batch: 206/391 ] | Loss: 0.822 | Acc: 79.061% (20948/26496)/ 94.622% (25071/26496)
01/14/2023 14:12:50 - INFO - __main__ -   test: [batch: 207/391 ] | Loss: 0.826 | Acc: 78.974% (21026/26624)/ 94.580% (25181/26624)
01/14/2023 14:12:52 - INFO - __main__ -   test: [batch: 208/391 ] | Loss: 0.827 | Acc: 78.932% (21116/26752)/ 94.554% (25295/26752)
01/14/2023 14:12:54 - INFO - __main__ -   test: [batch: 209/391 ] | Loss: 0.827 | Acc: 78.932% (21217/26880)/ 94.572% (25421/26880)
01/14/2023 14:12:56 - INFO - __main__ -   test: [batch: 210/391 ] | Loss: 0.828 | Acc: 78.917% (21314/27008)/ 94.553% (25537/27008)
01/14/2023 14:12:58 - INFO - __main__ -   test: [batch: 211/391 ] | Loss: 0.831 | Acc: 78.862% (21400/27136)/ 94.531% (25652/27136)
01/14/2023 14:13:00 - INFO - __main__ -   test: [batch: 212/391 ] | Loss: 0.833 | Acc: 78.807% (21486/27264)/ 94.509% (25767/27264)
01/14/2023 14:13:02 - INFO - __main__ -   test: [batch: 213/391 ] | Loss: 0.832 | Acc: 78.808% (21587/27392)/ 94.517% (25890/27392)
01/14/2023 14:13:05 - INFO - __main__ -   test: [batch: 214/391 ] | Loss: 0.833 | Acc: 78.783% (21681/27520)/ 94.517% (26011/27520)
01/14/2023 14:13:07 - INFO - __main__ -   test: [batch: 215/391 ] | Loss: 0.833 | Acc: 78.801% (21787/27648)/ 94.513% (26131/27648)
01/14/2023 14:13:09 - INFO - __main__ -   test: [batch: 216/391 ] | Loss: 0.831 | Acc: 78.841% (21899/27776)/ 94.531% (26257/27776)
01/14/2023 14:13:11 - INFO - __main__ -   test: [batch: 217/391 ] | Loss: 0.836 | Acc: 78.770% (21980/27904)/ 94.474% (26362/27904)
01/14/2023 14:13:13 - INFO - __main__ -   test: [batch: 218/391 ] | Loss: 0.839 | Acc: 78.717% (22066/28032)/ 94.435% (26472/28032)
01/14/2023 14:13:15 - INFO - __main__ -   test: [batch: 219/391 ] | Loss: 0.837 | Acc: 78.746% (22175/28160)/ 94.439% (26594/28160)
01/14/2023 14:13:17 - INFO - __main__ -   test: [batch: 220/391 ] | Loss: 0.835 | Acc: 78.783% (22286/28288)/ 94.457% (26720/28288)
01/14/2023 14:13:19 - INFO - __main__ -   test: [batch: 221/391 ] | Loss: 0.837 | Acc: 78.741% (22375/28416)/ 94.440% (26836/28416)
01/14/2023 14:13:21 - INFO - __main__ -   test: [batch: 222/391 ] | Loss: 0.835 | Acc: 78.798% (22492/28544)/ 94.454% (26961/28544)
01/14/2023 14:13:23 - INFO - __main__ -   test: [batch: 223/391 ] | Loss: 0.836 | Acc: 78.798% (22593/28672)/ 94.444% (27079/28672)
01/14/2023 14:13:25 - INFO - __main__ -   test: [batch: 224/391 ] | Loss: 0.835 | Acc: 78.819% (22700/28800)/ 94.438% (27198/28800)
01/14/2023 14:13:28 - INFO - __main__ -   test: [batch: 225/391 ] | Loss: 0.835 | Acc: 78.820% (22801/28928)/ 94.438% (27319/28928)
01/14/2023 14:13:30 - INFO - __main__ -   test: [batch: 226/391 ] | Loss: 0.835 | Acc: 78.803% (22897/29056)/ 94.449% (27443/29056)
01/14/2023 14:13:32 - INFO - __main__ -   test: [batch: 227/391 ] | Loss: 0.837 | Acc: 78.786% (22993/29184)/ 94.439% (27561/29184)
01/14/2023 14:13:34 - INFO - __main__ -   test: [batch: 228/391 ] | Loss: 0.841 | Acc: 78.712% (23072/29312)/ 94.378% (27664/29312)
01/14/2023 14:13:36 - INFO - __main__ -   test: [batch: 229/391 ] | Loss: 0.845 | Acc: 78.651% (23155/29440)/ 94.334% (27772/29440)
01/14/2023 14:13:38 - INFO - __main__ -   test: [batch: 230/391 ] | Loss: 0.847 | Acc: 78.595% (23239/29568)/ 94.295% (27881/29568)
01/14/2023 14:13:40 - INFO - __main__ -   test: [batch: 231/391 ] | Loss: 0.847 | Acc: 78.586% (23337/29696)/ 94.279% (27997/29696)
01/14/2023 14:13:42 - INFO - __main__ -   test: [batch: 232/391 ] | Loss: 0.846 | Acc: 78.615% (23446/29824)/ 94.293% (28122/29824)
01/14/2023 14:13:45 - INFO - __main__ -   test: [batch: 233/391 ] | Loss: 0.848 | Acc: 78.562% (23531/29952)/ 94.271% (28236/29952)
01/14/2023 14:13:47 - INFO - __main__ -   test: [batch: 234/391 ] | Loss: 0.855 | Acc: 78.444% (23596/30080)/ 94.199% (28335/30080)
01/14/2023 14:13:49 - INFO - __main__ -   test: [batch: 235/391 ] | Loss: 0.855 | Acc: 78.430% (23692/30208)/ 94.194% (28454/30208)
01/14/2023 14:13:51 - INFO - __main__ -   test: [batch: 236/391 ] | Loss: 0.854 | Acc: 78.474% (23806/30336)/ 94.198% (28576/30336)
01/14/2023 14:13:53 - INFO - __main__ -   test: [batch: 237/391 ] | Loss: 0.854 | Acc: 78.480% (23908/30464)/ 94.170% (28688/30464)
01/14/2023 14:13:55 - INFO - __main__ -   test: [batch: 238/391 ] | Loss: 0.853 | Acc: 78.521% (24021/30592)/ 94.178% (28811/30592)
01/14/2023 14:13:57 - INFO - __main__ -   test: [batch: 239/391 ] | Loss: 0.851 | Acc: 78.558% (24133/30720)/ 94.180% (28932/30720)
01/14/2023 14:13:59 - INFO - __main__ -   test: [batch: 240/391 ] | Loss: 0.852 | Acc: 78.576% (24239/30848)/ 94.165% (29048/30848)
01/14/2023 14:14:01 - INFO - __main__ -   test: [batch: 241/391 ] | Loss: 0.855 | Acc: 78.483% (24311/30976)/ 94.128% (29157/30976)
01/14/2023 14:14:03 - INFO - __main__ -   test: [batch: 242/391 ] | Loss: 0.857 | Acc: 78.379% (24379/31104)/ 94.110% (29272/31104)
01/14/2023 14:14:05 - INFO - __main__ -   test: [batch: 243/391 ] | Loss: 0.863 | Acc: 78.250% (24439/31232)/ 94.045% (29372/31232)
01/14/2023 14:14:08 - INFO - __main__ -   test: [batch: 244/391 ] | Loss: 0.863 | Acc: 78.259% (24542/31360)/ 94.040% (29491/31360)
01/14/2023 14:14:10 - INFO - __main__ -   test: [batch: 245/391 ] | Loss: 0.862 | Acc: 78.274% (24647/31488)/ 94.033% (29609/31488)
01/14/2023 14:14:12 - INFO - __main__ -   test: [batch: 246/391 ] | Loss: 0.863 | Acc: 78.252% (24740/31616)/ 94.022% (29726/31616)
01/14/2023 14:14:14 - INFO - __main__ -   test: [batch: 247/391 ] | Loss: 0.869 | Acc: 78.160% (24811/31744)/ 93.948% (29823/31744)
01/14/2023 14:14:16 - INFO - __main__ -   test: [batch: 248/391 ] | Loss: 0.869 | Acc: 78.119% (24898/31872)/ 93.938% (29940/31872)
01/14/2023 14:14:18 - INFO - __main__ -   test: [batch: 249/391 ] | Loss: 0.871 | Acc: 77.987% (24956/32000)/ 93.928% (30057/32000)
01/14/2023 14:14:20 - INFO - __main__ -   test: [batch: 250/391 ] | Loss: 0.870 | Acc: 78.022% (25067/32128)/ 93.943% (30182/32128)
01/14/2023 14:14:22 - INFO - __main__ -   test: [batch: 251/391 ] | Loss: 0.872 | Acc: 77.982% (25154/32256)/ 93.911% (30292/32256)
01/14/2023 14:14:24 - INFO - __main__ -   test: [batch: 252/391 ] | Loss: 0.872 | Acc: 78.008% (25262/32384)/ 93.904% (30410/32384)
01/14/2023 14:14:26 - INFO - __main__ -   test: [batch: 253/391 ] | Loss: 0.873 | Acc: 77.980% (25353/32512)/ 93.888% (30525/32512)
01/14/2023 14:14:29 - INFO - __main__ -   test: [batch: 254/391 ] | Loss: 0.877 | Acc: 77.917% (25432/32640)/ 93.842% (30630/32640)
01/14/2023 14:14:31 - INFO - __main__ -   test: [batch: 255/391 ] | Loss: 0.879 | Acc: 77.881% (25520/32768)/ 93.832% (30747/32768)
01/14/2023 14:14:33 - INFO - __main__ -   test: [batch: 256/391 ] | Loss: 0.884 | Acc: 77.757% (25579/32896)/ 93.799% (30856/32896)
01/14/2023 14:14:35 - INFO - __main__ -   test: [batch: 257/391 ] | Loss: 0.885 | Acc: 77.746% (25675/33024)/ 93.786% (30972/33024)
01/14/2023 14:14:37 - INFO - __main__ -   test: [batch: 258/391 ] | Loss: 0.886 | Acc: 77.739% (25772/33152)/ 93.774% (31088/33152)
01/14/2023 14:14:40 - INFO - __main__ -   test: [batch: 259/391 ] | Loss: 0.890 | Acc: 77.620% (25832/33280)/ 93.765% (31205/33280)
01/14/2023 14:14:42 - INFO - __main__ -   test: [batch: 260/391 ] | Loss: 0.890 | Acc: 77.592% (25922/33408)/ 93.756% (31322/33408)
01/14/2023 14:14:44 - INFO - __main__ -   test: [batch: 261/391 ] | Loss: 0.888 | Acc: 77.636% (26036/33536)/ 93.777% (31449/33536)
01/14/2023 14:14:46 - INFO - __main__ -   test: [batch: 262/391 ] | Loss: 0.887 | Acc: 77.656% (26142/33664)/ 93.786% (31572/33664)
01/14/2023 14:14:48 - INFO - __main__ -   test: [batch: 263/391 ] | Loss: 0.890 | Acc: 77.580% (26216/33792)/ 93.759% (31683/33792)
01/14/2023 14:14:50 - INFO - __main__ -   test: [batch: 264/391 ] | Loss: 0.895 | Acc: 77.521% (26295/33920)/ 93.700% (31783/33920)
01/14/2023 14:14:53 - INFO - __main__ -   test: [batch: 265/391 ] | Loss: 0.894 | Acc: 77.549% (26404/34048)/ 93.697% (31902/34048)
01/14/2023 14:14:55 - INFO - __main__ -   test: [batch: 266/391 ] | Loss: 0.897 | Acc: 77.478% (26479/34176)/ 93.680% (32016/34176)
01/14/2023 14:14:57 - INFO - __main__ -   test: [batch: 267/391 ] | Loss: 0.896 | Acc: 77.507% (26588/34304)/ 93.680% (32136/34304)
01/14/2023 14:14:59 - INFO - __main__ -   test: [batch: 268/391 ] | Loss: 0.896 | Acc: 77.524% (26693/34432)/ 93.677% (32255/34432)
01/14/2023 14:15:01 - INFO - __main__ -   test: [batch: 269/391 ] | Loss: 0.897 | Acc: 77.474% (26775/34560)/ 93.657% (32368/34560)
01/14/2023 14:15:03 - INFO - __main__ -   test: [batch: 270/391 ] | Loss: 0.901 | Acc: 77.410% (26852/34688)/ 93.626% (32477/34688)
01/14/2023 14:15:06 - INFO - __main__ -   test: [batch: 271/391 ] | Loss: 0.901 | Acc: 77.427% (26957/34816)/ 93.621% (32595/34816)
01/14/2023 14:15:08 - INFO - __main__ -   test: [batch: 272/391 ] | Loss: 0.902 | Acc: 77.390% (27043/34944)/ 93.615% (32713/34944)
01/14/2023 14:15:10 - INFO - __main__ -   test: [batch: 273/391 ] | Loss: 0.902 | Acc: 77.398% (27145/35072)/ 93.602% (32828/35072)
01/14/2023 14:15:12 - INFO - __main__ -   test: [batch: 274/391 ] | Loss: 0.902 | Acc: 77.398% (27244/35200)/ 93.608% (32950/35200)
01/14/2023 14:15:14 - INFO - __main__ -   test: [batch: 275/391 ] | Loss: 0.904 | Acc: 77.369% (27333/35328)/ 93.600% (33067/35328)
01/14/2023 14:15:16 - INFO - __main__ -   test: [batch: 276/391 ] | Loss: 0.905 | Acc: 77.316% (27413/35456)/ 93.595% (33185/35456)
01/14/2023 14:15:18 - INFO - __main__ -   test: [batch: 277/391 ] | Loss: 0.906 | Acc: 77.285% (27501/35584)/ 93.581% (33300/35584)
01/14/2023 14:15:20 - INFO - __main__ -   test: [batch: 278/391 ] | Loss: 0.905 | Acc: 77.293% (27603/35712)/ 93.579% (33419/35712)
01/14/2023 14:15:23 - INFO - __main__ -   test: [batch: 279/391 ] | Loss: 0.906 | Acc: 77.305% (27706/35840)/ 93.577% (33538/35840)
01/14/2023 14:15:25 - INFO - __main__ -   test: [batch: 280/391 ] | Loss: 0.906 | Acc: 77.288% (27799/35968)/ 93.572% (33656/35968)
01/14/2023 14:15:27 - INFO - __main__ -   test: [batch: 281/391 ] | Loss: 0.907 | Acc: 77.302% (27903/36096)/ 93.567% (33774/36096)
01/14/2023 14:15:29 - INFO - __main__ -   test: [batch: 282/391 ] | Loss: 0.905 | Acc: 77.338% (28015/36224)/ 93.573% (33896/36224)
01/14/2023 14:15:31 - INFO - __main__ -   test: [batch: 283/391 ] | Loss: 0.906 | Acc: 77.313% (28105/36352)/ 93.568% (34014/36352)
01/14/2023 14:15:33 - INFO - __main__ -   test: [batch: 284/391 ] | Loss: 0.910 | Acc: 77.259% (28184/36480)/ 93.539% (34123/36480)
01/14/2023 14:15:35 - INFO - __main__ -   test: [batch: 285/391 ] | Loss: 0.912 | Acc: 77.218% (28268/36608)/ 93.493% (34226/36608)
01/14/2023 14:15:37 - INFO - __main__ -   test: [batch: 286/391 ] | Loss: 0.913 | Acc: 77.186% (28355/36736)/ 93.483% (34342/36736)
01/14/2023 14:15:39 - INFO - __main__ -   test: [batch: 287/391 ] | Loss: 0.913 | Acc: 77.186% (28454/36864)/ 93.479% (34460/36864)
01/14/2023 14:15:41 - INFO - __main__ -   test: [batch: 288/391 ] | Loss: 0.912 | Acc: 77.206% (28560/36992)/ 93.482% (34581/36992)
01/14/2023 14:15:43 - INFO - __main__ -   test: [batch: 289/391 ] | Loss: 0.914 | Acc: 77.166% (28644/37120)/ 93.448% (34688/37120)
01/14/2023 14:15:46 - INFO - __main__ -   test: [batch: 290/391 ] | Loss: 0.916 | Acc: 77.091% (28715/37248)/ 93.449% (34808/37248)
01/14/2023 14:15:48 - INFO - __main__ -   test: [batch: 291/391 ] | Loss: 0.916 | Acc: 77.103% (28818/37376)/ 93.445% (34926/37376)
01/14/2023 14:15:50 - INFO - __main__ -   test: [batch: 292/391 ] | Loss: 0.918 | Acc: 77.045% (28895/37504)/ 93.427% (35039/37504)
01/14/2023 14:15:52 - INFO - __main__ -   test: [batch: 293/391 ] | Loss: 0.918 | Acc: 77.041% (28992/37632)/ 93.423% (35157/37632)
01/14/2023 14:15:54 - INFO - __main__ -   test: [batch: 294/391 ] | Loss: 0.919 | Acc: 77.023% (29084/37760)/ 93.419% (35275/37760)
01/14/2023 14:15:57 - INFO - __main__ -   test: [batch: 295/391 ] | Loss: 0.918 | Acc: 77.056% (29195/37888)/ 93.425% (35397/37888)
01/14/2023 14:15:59 - INFO - __main__ -   test: [batch: 296/391 ] | Loss: 0.918 | Acc: 77.054% (29293/38016)/ 93.416% (35513/38016)
01/14/2023 14:16:01 - INFO - __main__ -   test: [batch: 297/391 ] | Loss: 0.920 | Acc: 77.029% (29382/38144)/ 93.393% (35624/38144)
01/14/2023 14:16:03 - INFO - __main__ -   test: [batch: 298/391 ] | Loss: 0.922 | Acc: 76.999% (29469/38272)/ 93.371% (35735/38272)
01/14/2023 14:16:05 - INFO - __main__ -   test: [batch: 299/391 ] | Loss: 0.923 | Acc: 76.987% (29563/38400)/ 93.349% (35846/38400)
01/14/2023 14:16:07 - INFO - __main__ -   test: [batch: 300/391 ] | Loss: 0.923 | Acc: 76.991% (29663/38528)/ 93.343% (35963/38528)
01/14/2023 14:16:09 - INFO - __main__ -   test: [batch: 301/391 ] | Loss: 0.924 | Acc: 76.974% (29755/38656)/ 93.328% (36077/38656)
01/14/2023 14:16:12 - INFO - __main__ -   test: [batch: 302/391 ] | Loss: 0.925 | Acc: 76.936% (29839/38784)/ 93.312% (36190/38784)
01/14/2023 14:16:14 - INFO - __main__ -   test: [batch: 303/391 ] | Loss: 0.927 | Acc: 76.915% (29929/38912)/ 93.300% (36305/38912)
01/14/2023 14:16:16 - INFO - __main__ -   test: [batch: 304/391 ] | Loss: 0.926 | Acc: 76.934% (30035/39040)/ 93.304% (36426/39040)
01/14/2023 14:16:18 - INFO - __main__ -   test: [batch: 305/391 ] | Loss: 0.927 | Acc: 76.889% (30116/39168)/ 93.298% (36543/39168)
01/14/2023 14:16:20 - INFO - __main__ -   test: [batch: 306/391 ] | Loss: 0.928 | Acc: 76.870% (30207/39296)/ 93.274% (36653/39296)
01/14/2023 14:16:22 - INFO - __main__ -   test: [batch: 307/391 ] | Loss: 0.929 | Acc: 76.872% (30306/39424)/ 93.258% (36766/39424)
01/14/2023 14:16:24 - INFO - __main__ -   test: [batch: 308/391 ] | Loss: 0.929 | Acc: 76.858% (30399/39552)/ 93.252% (36883/39552)
01/14/2023 14:16:26 - INFO - __main__ -   test: [batch: 309/391 ] | Loss: 0.930 | Acc: 76.855% (30496/39680)/ 93.233% (36995/39680)
01/14/2023 14:16:29 - INFO - __main__ -   test: [batch: 310/391 ] | Loss: 0.931 | Acc: 76.849% (30592/39808)/ 93.220% (37109/39808)
01/14/2023 14:16:31 - INFO - __main__ -   test: [batch: 311/391 ] | Loss: 0.932 | Acc: 76.818% (30678/39936)/ 93.209% (37224/39936)
01/14/2023 14:16:33 - INFO - __main__ -   test: [batch: 312/391 ] | Loss: 0.934 | Acc: 76.800% (30769/40064)/ 93.188% (37335/40064)
01/14/2023 14:16:35 - INFO - __main__ -   test: [batch: 313/391 ] | Loss: 0.932 | Acc: 76.839% (30883/40192)/ 93.205% (37461/40192)
01/14/2023 14:16:37 - INFO - __main__ -   test: [batch: 314/391 ] | Loss: 0.932 | Acc: 76.835% (30980/40320)/ 93.202% (37579/40320)
01/14/2023 14:16:39 - INFO - __main__ -   test: [batch: 315/391 ] | Loss: 0.933 | Acc: 76.817% (31071/40448)/ 93.186% (37692/40448)
01/14/2023 14:16:42 - INFO - __main__ -   test: [batch: 316/391 ] | Loss: 0.936 | Acc: 76.725% (31132/40576)/ 93.166% (37803/40576)
01/14/2023 14:16:44 - INFO - __main__ -   test: [batch: 317/391 ] | Loss: 0.938 | Acc: 76.683% (31213/40704)/ 93.141% (37912/40704)
01/14/2023 14:16:46 - INFO - __main__ -   test: [batch: 318/391 ] | Loss: 0.936 | Acc: 76.712% (31323/40832)/ 93.157% (38038/40832)
01/14/2023 14:16:48 - INFO - __main__ -   test: [batch: 319/391 ] | Loss: 0.939 | Acc: 76.658% (31399/40960)/ 93.125% (38144/40960)
01/14/2023 14:16:50 - INFO - __main__ -   test: [batch: 320/391 ] | Loss: 0.938 | Acc: 76.694% (31512/41088)/ 93.134% (38267/41088)
01/14/2023 14:16:52 - INFO - __main__ -   test: [batch: 321/391 ] | Loss: 0.938 | Acc: 76.713% (31618/41216)/ 93.126% (38383/41216)
01/14/2023 14:16:55 - INFO - __main__ -   test: [batch: 322/391 ] | Loss: 0.939 | Acc: 76.686% (31705/41344)/ 93.121% (38500/41344)
01/14/2023 14:16:57 - INFO - __main__ -   test: [batch: 323/391 ] | Loss: 0.941 | Acc: 76.652% (31789/41472)/ 93.097% (38609/41472)
01/14/2023 14:16:59 - INFO - __main__ -   test: [batch: 324/391 ] | Loss: 0.941 | Acc: 76.647% (31885/41600)/ 93.089% (38725/41600)
01/14/2023 14:17:01 - INFO - __main__ -   test: [batch: 325/391 ] | Loss: 0.941 | Acc: 76.651% (31985/41728)/ 93.086% (38843/41728)
01/14/2023 14:17:03 - INFO - __main__ -   test: [batch: 326/391 ] | Loss: 0.944 | Acc: 76.570% (32049/41856)/ 93.057% (38950/41856)
01/14/2023 14:17:05 - INFO - __main__ -   test: [batch: 327/391 ] | Loss: 0.948 | Acc: 76.491% (32114/41984)/ 93.026% (39056/41984)
01/14/2023 14:17:07 - INFO - __main__ -   test: [batch: 328/391 ] | Loss: 0.949 | Acc: 76.439% (32190/42112)/ 93.004% (39166/42112)
01/14/2023 14:17:10 - INFO - __main__ -   test: [batch: 329/391 ] | Loss: 0.950 | Acc: 76.437% (32287/42240)/ 93.002% (39284/42240)
01/14/2023 14:17:12 - INFO - __main__ -   test: [batch: 330/391 ] | Loss: 0.951 | Acc: 76.395% (32367/42368)/ 92.978% (39393/42368)
01/14/2023 14:17:14 - INFO - __main__ -   test: [batch: 331/391 ] | Loss: 0.952 | Acc: 76.365% (32452/42496)/ 92.992% (39518/42496)
01/14/2023 14:17:16 - INFO - __main__ -   test: [batch: 332/391 ] | Loss: 0.952 | Acc: 76.356% (32546/42624)/ 92.992% (39637/42624)
01/14/2023 14:17:18 - INFO - __main__ -   test: [batch: 333/391 ] | Loss: 0.951 | Acc: 76.385% (32656/42752)/ 93.001% (39760/42752)
01/14/2023 14:17:20 - INFO - __main__ -   test: [batch: 334/391 ] | Loss: 0.952 | Acc: 76.353% (32740/42880)/ 92.983% (39871/42880)
01/14/2023 14:17:22 - INFO - __main__ -   test: [batch: 335/391 ] | Loss: 0.953 | Acc: 76.330% (32828/43008)/ 92.969% (39984/43008)
01/14/2023 14:17:24 - INFO - __main__ -   test: [batch: 336/391 ] | Loss: 0.955 | Acc: 76.284% (32906/43136)/ 92.955% (40097/43136)
01/14/2023 14:17:27 - INFO - __main__ -   test: [batch: 337/391 ] | Loss: 0.955 | Acc: 76.281% (33002/43264)/ 92.950% (40214/43264)
01/14/2023 14:17:29 - INFO - __main__ -   test: [batch: 338/391 ] | Loss: 0.955 | Acc: 76.270% (33095/43392)/ 92.960% (40337/43392)
01/14/2023 14:17:31 - INFO - __main__ -   test: [batch: 339/391 ] | Loss: 0.957 | Acc: 76.234% (33177/43520)/ 92.937% (40446/43520)
01/14/2023 14:17:33 - INFO - __main__ -   test: [batch: 340/391 ] | Loss: 0.957 | Acc: 76.226% (33271/43648)/ 92.946% (40569/43648)
01/14/2023 14:17:35 - INFO - __main__ -   test: [batch: 341/391 ] | Loss: 0.955 | Acc: 76.266% (33386/43776)/ 92.964% (40696/43776)
01/14/2023 14:17:37 - INFO - __main__ -   test: [batch: 342/391 ] | Loss: 0.956 | Acc: 76.209% (33459/43904)/ 92.957% (40812/43904)
01/14/2023 14:17:39 - INFO - __main__ -   test: [batch: 343/391 ] | Loss: 0.956 | Acc: 76.208% (33556/44032)/ 92.960% (40932/44032)
01/14/2023 14:17:41 - INFO - __main__ -   test: [batch: 344/391 ] | Loss: 0.956 | Acc: 76.205% (33652/44160)/ 92.951% (41047/44160)
01/14/2023 14:17:43 - INFO - __main__ -   test: [batch: 345/391 ] | Loss: 0.960 | Acc: 76.140% (33721/44288)/ 92.908% (41147/44288)
01/14/2023 14:17:45 - INFO - __main__ -   test: [batch: 346/391 ] | Loss: 0.962 | Acc: 76.123% (33811/44416)/ 92.894% (41260/44416)
01/14/2023 14:17:48 - INFO - __main__ -   test: [batch: 347/391 ] | Loss: 0.961 | Acc: 76.138% (33915/44544)/ 92.901% (41382/44544)
01/14/2023 14:17:50 - INFO - __main__ -   test: [batch: 348/391 ] | Loss: 0.962 | Acc: 76.115% (34002/44672)/ 92.875% (41489/44672)
01/14/2023 14:17:52 - INFO - __main__ -   test: [batch: 349/391 ] | Loss: 0.962 | Acc: 76.114% (34099/44800)/ 92.879% (41610/44800)
01/14/2023 14:17:54 - INFO - __main__ -   test: [batch: 350/391 ] | Loss: 0.962 | Acc: 76.128% (34203/44928)/ 92.877% (41728/44928)
01/14/2023 14:17:56 - INFO - __main__ -   test: [batch: 351/391 ] | Loss: 0.964 | Acc: 76.074% (34276/45056)/ 92.864% (41841/45056)
01/14/2023 14:17:58 - INFO - __main__ -   test: [batch: 352/391 ] | Loss: 0.964 | Acc: 76.073% (34373/45184)/ 92.863% (41959/45184)
01/14/2023 14:18:00 - INFO - __main__ -   test: [batch: 353/391 ] | Loss: 0.967 | Acc: 76.031% (34451/45312)/ 92.823% (42060/45312)
01/14/2023 14:18:03 - INFO - __main__ -   test: [batch: 354/391 ] | Loss: 0.969 | Acc: 75.973% (34522/45440)/ 92.799% (42168/45440)
01/14/2023 14:18:05 - INFO - __main__ -   test: [batch: 355/391 ] | Loss: 0.972 | Acc: 75.909% (34590/45568)/ 92.793% (42284/45568)
01/14/2023 14:18:07 - INFO - __main__ -   test: [batch: 356/391 ] | Loss: 0.972 | Acc: 75.908% (34687/45696)/ 92.794% (42403/45696)
01/14/2023 14:18:09 - INFO - __main__ -   test: [batch: 357/391 ] | Loss: 0.971 | Acc: 75.943% (34800/45824)/ 92.807% (42528/45824)
01/14/2023 14:18:11 - INFO - __main__ -   test: [batch: 358/391 ] | Loss: 0.970 | Acc: 75.971% (34910/45952)/ 92.810% (42648/45952)
01/14/2023 14:18:13 - INFO - __main__ -   test: [batch: 359/391 ] | Loss: 0.970 | Acc: 75.979% (35011/46080)/ 92.808% (42766/46080)
01/14/2023 14:18:15 - INFO - __main__ -   test: [batch: 360/391 ] | Loss: 0.972 | Acc: 75.948% (35094/46208)/ 92.800% (42881/46208)
01/14/2023 14:18:17 - INFO - __main__ -   test: [batch: 361/391 ] | Loss: 0.972 | Acc: 75.947% (35191/46336)/ 92.807% (43003/46336)
01/14/2023 14:18:19 - INFO - __main__ -   test: [batch: 362/391 ] | Loss: 0.972 | Acc: 75.945% (35287/46464)/ 92.820% (43128/46464)
01/14/2023 14:18:22 - INFO - __main__ -   test: [batch: 363/391 ] | Loss: 0.972 | Acc: 75.938% (35381/46592)/ 92.810% (43242/46592)
01/14/2023 14:18:24 - INFO - __main__ -   test: [batch: 364/391 ] | Loss: 0.971 | Acc: 75.965% (35491/46720)/ 92.819% (43365/46720)
01/14/2023 14:18:26 - INFO - __main__ -   test: [batch: 365/391 ] | Loss: 0.970 | Acc: 75.982% (35596/46848)/ 92.828% (43488/46848)
01/14/2023 14:18:28 - INFO - __main__ -   test: [batch: 366/391 ] | Loss: 0.969 | Acc: 76.020% (35711/46976)/ 92.845% (43615/46976)
01/14/2023 14:18:30 - INFO - __main__ -   test: [batch: 367/391 ] | Loss: 0.968 | Acc: 76.044% (35820/47104)/ 92.858% (43740/47104)
01/14/2023 14:18:32 - INFO - __main__ -   test: [batch: 368/391 ] | Loss: 0.967 | Acc: 76.050% (35920/47232)/ 92.867% (43863/47232)
01/14/2023 14:18:34 - INFO - __main__ -   test: [batch: 369/391 ] | Loss: 0.966 | Acc: 76.068% (36026/47360)/ 92.876% (43986/47360)
01/14/2023 14:18:36 - INFO - __main__ -   test: [batch: 370/391 ] | Loss: 0.967 | Acc: 76.053% (36116/47488)/ 92.876% (44105/47488)
01/14/2023 14:18:39 - INFO - __main__ -   test: [batch: 371/391 ] | Loss: 0.966 | Acc: 76.056% (36215/47616)/ 92.878% (44225/47616)
01/14/2023 14:18:41 - INFO - __main__ -   test: [batch: 372/391 ] | Loss: 0.965 | Acc: 76.102% (36334/47744)/ 92.893% (44351/47744)
01/14/2023 14:18:43 - INFO - __main__ -   test: [batch: 373/391 ] | Loss: 0.963 | Acc: 76.136% (36448/47872)/ 92.902% (44474/47872)
01/14/2023 14:18:45 - INFO - __main__ -   test: [batch: 374/391 ] | Loss: 0.962 | Acc: 76.167% (36560/48000)/ 92.908% (44596/48000)
01/14/2023 14:18:47 - INFO - __main__ -   test: [batch: 375/391 ] | Loss: 0.965 | Acc: 76.097% (36624/48128)/ 92.871% (44697/48128)
01/14/2023 14:18:49 - INFO - __main__ -   test: [batch: 376/391 ] | Loss: 0.965 | Acc: 76.092% (36719/48256)/ 92.857% (44809/48256)
01/14/2023 14:18:51 - INFO - __main__ -   test: [batch: 377/391 ] | Loss: 0.966 | Acc: 76.077% (36809/48384)/ 92.851% (44925/48384)
01/14/2023 14:18:53 - INFO - __main__ -   test: [batch: 378/391 ] | Loss: 0.969 | Acc: 76.004% (36871/48512)/ 92.814% (45026/48512)
01/14/2023 14:18:55 - INFO - __main__ -   test: [batch: 379/391 ] | Loss: 0.970 | Acc: 75.983% (36958/48640)/ 92.823% (45149/48640)
01/14/2023 14:18:57 - INFO - __main__ -   test: [batch: 380/391 ] | Loss: 0.969 | Acc: 75.978% (37053/48768)/ 92.831% (45272/48768)
01/14/2023 14:18:59 - INFO - __main__ -   test: [batch: 381/391 ] | Loss: 0.971 | Acc: 75.935% (37129/48896)/ 92.828% (45389/48896)
01/14/2023 14:19:01 - INFO - __main__ -   test: [batch: 382/391 ] | Loss: 0.972 | Acc: 75.900% (37209/49024)/ 92.812% (45500/49024)
01/14/2023 14:19:03 - INFO - __main__ -   test: [batch: 383/391 ] | Loss: 0.972 | Acc: 75.909% (37311/49152)/ 92.804% (45615/49152)
01/14/2023 14:19:06 - INFO - __main__ -   test: [batch: 384/391 ] | Loss: 0.971 | Acc: 75.948% (37427/49280)/ 92.817% (45740/49280)
01/14/2023 14:19:08 - INFO - __main__ -   test: [batch: 385/391 ] | Loss: 0.969 | Acc: 75.959% (37530/49408)/ 92.827% (45864/49408)
01/14/2023 14:19:10 - INFO - __main__ -   test: [batch: 386/391 ] | Loss: 0.967 | Acc: 76.005% (37650/49536)/ 92.844% (45991/49536)
01/14/2023 14:19:11 - INFO - __main__ -   test: [batch: 387/391 ] | Loss: 0.966 | Acc: 76.045% (37767/49664)/ 92.854% (46115/49664)
01/14/2023 14:19:13 - INFO - __main__ -   test: [batch: 388/391 ] | Loss: 0.964 | Acc: 76.091% (37887/49792)/ 92.866% (46240/49792)
01/14/2023 14:19:15 - INFO - __main__ -   test: [batch: 389/391 ] | Loss: 0.964 | Acc: 76.094% (37986/49920)/ 92.869% (46360/49920)
01/14/2023 14:19:16 - INFO - __main__ -   test: [batch: 390/391 ] | Loss: 0.966 | Acc: 76.048% (38024/50000)/ 92.860% (46430/50000)
01/14/2023 14:19:16 - INFO - __main__ -   Final accuracy: 76.048
