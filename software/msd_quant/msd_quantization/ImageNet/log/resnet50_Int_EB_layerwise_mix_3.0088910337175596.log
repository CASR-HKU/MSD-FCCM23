/home/jjc/miniconda3/envs/ant_quant/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
Namespace(a_low=75, a_up=150, abit=8, batch_size=128, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='csd_eb2', epoch=3, layer_4bit_l='28,40,18,0,39,27,33,40,27,33,39,13,43,24,45,48,2,3,9,25,12,16,22,25,49,47,41,35,32,4,1,10,8,7,41,32,35,47,23,14,49,31,37,21', layer_8bit_l=None, layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='resnet50', percent=100, ptq=False, resume=False, search=False, tag='', train=True, w_low=75, w_up=150, wbit=8)
01/15/2023 02:49:10 - INFO - __main__ -   output/resnet50_imagenet/int_W8A8_60809/gpu_0
01/15/2023 02:49:10 - INFO - __main__ -   Namespace(a_low=75, a_up=150, abit=8, batch_size=128, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='csd_eb2', epoch=3, layer_4bit_l='28,40,18,0,39,27,33,40,27,33,39,13,43,24,45,48,2,3,9,25,12,16,22,25,49,47,41,35,32,4,1,10,8,7,41,32,35,47,23,14,49,31,37,21', layer_8bit_l=None, layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='resnet50', percent=100, ptq=False, resume=False, search=False, tag='', train=True, w_low=75, w_up=150, wbit=8)
01/15/2023 02:49:10 - INFO - __main__ -   ==> Preparing data..
01/15/2023 02:49:12 - INFO - __main__ -   ==> Setting quantizer..
Namespace(a_low=75, a_up=150, abit=8, batch_size=128, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='csd_eb2', epoch=3, layer_4bit_l='28,40,18,0,39,27,33,40,27,33,39,13,43,24,45,48,2,3,9,25,12,16,22,25,49,47,41,35,32,4,1,10,8,7,41,32,35,47,23,14,49,31,37,21', layer_8bit_l=None, layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='resnet50', percent=100, ptq=False, resume=False, search=False, tag='', train=True, w_low=75, w_up=150, wbit=8)
01/15/2023 02:49:12 - INFO - __main__ -   Namespace(a_low=75, a_up=150, abit=8, batch_size=128, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='csd_eb2', epoch=3, layer_4bit_l='28,40,18,0,39,27,33,40,27,33,39,13,43,24,45,48,2,3,9,25,12,16,22,25,49,47,41,35,32,4,1,10,8,7,41,32,35,47,23,14,49,31,37,21', layer_8bit_l=None, layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='resnet50', percent=100, ptq=False, resume=False, search=False, tag='', train=True, w_low=75, w_up=150, wbit=8)
01/15/2023 02:49:12 - INFO - __main__ -   ==> Building model..
ResNet(
  (conv1): Conv2dQuantizer(
    (quant_weight): TensorQuantizer()
    (quant_input): TensorQuantizer()
  )
  (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (1): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (1): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): LinearQuantizer(
    (quant_weight): TensorQuantizer()
    (quant_input): TensorQuantizer()
  )
)
01/15/2023 02:49:12 - INFO - __main__ -   {'milestones': Counter({2: 1, 3: 1, 4: 1}), 'gamma': 0.1, 'base_lrs': [0.00025], 'last_epoch': 0, '_step_count': 1, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.00025]}
01/15/2023 02:49:12 - INFO - __main__ -   
Epoch: 0
Layer quant EB csd_eb2
int	8-bit 	 conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.3.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.3.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.3.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.3.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.3.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.3.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.4.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.4.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.4.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.5.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.5.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.5.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 fc.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 fc.quant_input,
set init to 1
------------- 8-bit EB1 Re-SET -------------
44
conv1.quant_weight 0
conv1.quant_input 0
layer1.0.conv1.quant_weight 1
layer1.0.conv1.quant_input 1
layer1.0.conv2.quant_weight 2
layer1.0.conv2.quant_input 2
layer1.0.conv3.quant_weight 3
layer1.0.conv3.quant_input 3
layer1.0.downsample.0.quant_weight 4
layer1.0.downsample.0.quant_input 4
layer1.1.conv3.quant_weight 7
layer1.1.conv3.quant_input 7
layer1.2.conv1.quant_weight 8
layer1.2.conv1.quant_input 8
layer1.2.conv2.quant_weight 9
layer1.2.conv2.quant_input 9
layer1.2.conv3.quant_weight 10
layer1.2.conv3.quant_input 10
layer2.0.conv2.quant_weight 12
layer2.0.conv2.quant_input 12
layer2.0.conv3.quant_weight 13
layer2.0.conv3.quant_input 13
layer2.0.downsample.0.quant_weight 14
layer2.0.downsample.0.quant_input 14
layer2.1.conv2.quant_weight 16
layer2.1.conv2.quant_input 16
layer2.2.conv1.quant_weight 18
layer2.2.conv1.quant_input 18
layer2.3.conv1.quant_weight 21
layer2.3.conv1.quant_input 21
layer2.3.conv2.quant_weight 22
layer2.3.conv2.quant_input 22
layer2.3.conv3.quant_weight 23
layer2.3.conv3.quant_input 23
layer3.0.conv1.quant_weight 24
layer3.0.conv1.quant_input 24
layer3.0.conv2.quant_weight 25
layer3.0.conv2.quant_input 25
layer3.0.downsample.0.quant_weight 27
layer3.0.downsample.0.quant_input 27
layer3.1.conv1.quant_weight 28
layer3.1.conv1.quant_input 28
layer3.2.conv1.quant_weight 31
layer3.2.conv1.quant_input 31
layer3.2.conv2.quant_weight 32
layer3.2.conv2.quant_input 32
layer3.2.conv3.quant_weight 33
layer3.2.conv3.quant_input 33
layer3.3.conv2.quant_weight 35
layer3.3.conv2.quant_input 35
layer3.4.conv1.quant_weight 37
layer3.4.conv1.quant_input 37
layer3.4.conv3.quant_weight 39
layer3.4.conv3.quant_input 39
layer3.5.conv1.quant_weight 40
layer3.5.conv1.quant_input 40
layer3.5.conv2.quant_weight 41
layer3.5.conv2.quant_input 41
layer4.0.conv1.quant_weight 43
layer4.0.conv1.quant_input 43
layer4.0.conv3.quant_weight 45
layer4.0.conv3.quant_input 45
layer4.1.conv1.quant_weight 47
layer4.1.conv1.quant_input 47
layer4.1.conv2.quant_weight 48
layer4.1.conv2.quant_input 48
layer4.1.conv3.quant_weight 49
layer4.1.conv3.quant_input 49
------------- 8-bit EB1 Re-SET -------------
Layer quant EB csd_eb1
int	8-bit 	 conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 conv1.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer1.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer1.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer1.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer1.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer1.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer1.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer1.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer1.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer2.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer2.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer2.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer2.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer2.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer2.3.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv1.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer2.3.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv2.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer2.3.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv3.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer3.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer3.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer3.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer3.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer3.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer3.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer3.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.3.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv1.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer3.3.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.3.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv3.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer3.4.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.4.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv2.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer3.4.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv3.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer3.5.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv1.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer3.5.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.5.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv3.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer4.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer4.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer4.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer4.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer4.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 fc.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 fc.quant_input,
set init to 1
01/15/2023 02:49:45 - INFO - __main__ -   test: [epoch: 0 | batch: 0/10010 ] | Loss: 0.750 | Acc: 78.906% (101/128)
01/15/2023 02:50:21 - INFO - __main__ -   test: [epoch: 0 | batch: 100/10010 ] | Loss: 0.940 | Acc: 77.065% (9963/12928)
01/15/2023 02:52:27 - INFO - __main__ -   test: [epoch: 0 | batch: 200/10010 ] | Loss: 0.927 | Acc: 77.091% (19834/25728)
01/15/2023 02:55:55 - INFO - __main__ -   test: [epoch: 0 | batch: 300/10010 ] | Loss: 0.943 | Acc: 76.736% (29565/38528)
01/15/2023 02:59:25 - INFO - __main__ -   test: [epoch: 0 | batch: 400/10010 ] | Loss: 0.954 | Acc: 76.453% (39242/51328)
01/15/2023 03:02:54 - INFO - __main__ -   test: [epoch: 0 | batch: 500/10010 ] | Loss: 0.956 | Acc: 76.352% (48963/64128)
01/15/2023 03:06:23 - INFO - __main__ -   test: [epoch: 0 | batch: 600/10010 ] | Loss: 0.958 | Acc: 76.339% (58726/76928)
01/15/2023 03:09:52 - INFO - __main__ -   test: [epoch: 0 | batch: 700/10010 ] | Loss: 0.959 | Acc: 76.367% (68523/89728)
01/15/2023 03:13:20 - INFO - __main__ -   test: [epoch: 0 | batch: 800/10010 ] | Loss: 0.961 | Acc: 76.309% (78238/102528)
01/15/2023 03:16:50 - INFO - __main__ -   test: [epoch: 0 | batch: 900/10010 ] | Loss: 0.961 | Acc: 76.271% (87962/115328)
01/15/2023 03:20:20 - INFO - __main__ -   test: [epoch: 0 | batch: 1000/10010 ] | Loss: 0.961 | Acc: 76.269% (97722/128128)
01/15/2023 03:23:49 - INFO - __main__ -   test: [epoch: 0 | batch: 1100/10010 ] | Loss: 0.963 | Acc: 76.279% (107498/140928)
01/15/2023 03:27:18 - INFO - __main__ -   test: [epoch: 0 | batch: 1200/10010 ] | Loss: 0.965 | Acc: 76.251% (117219/153728)
01/15/2023 03:30:46 - INFO - __main__ -   test: [epoch: 0 | batch: 1300/10010 ] | Loss: 0.963 | Acc: 76.291% (127046/166528)
01/15/2023 03:34:14 - INFO - __main__ -   test: [epoch: 0 | batch: 1400/10010 ] | Loss: 0.964 | Acc: 76.285% (136801/179328)
01/15/2023 03:37:44 - INFO - __main__ -   test: [epoch: 0 | batch: 1500/10010 ] | Loss: 0.962 | Acc: 76.310% (146612/192128)
01/15/2023 03:41:13 - INFO - __main__ -   test: [epoch: 0 | batch: 1600/10010 ] | Loss: 0.963 | Acc: 76.295% (156350/204928)
01/15/2023 03:44:43 - INFO - __main__ -   test: [epoch: 0 | batch: 1700/10010 ] | Loss: 0.963 | Acc: 76.307% (166142/217728)
01/15/2023 03:48:13 - INFO - __main__ -   test: [epoch: 0 | batch: 1800/10010 ] | Loss: 0.961 | Acc: 76.346% (175999/230528)
01/15/2023 03:51:43 - INFO - __main__ -   test: [epoch: 0 | batch: 1900/10010 ] | Loss: 0.960 | Acc: 76.351% (185784/243328)
01/15/2023 03:55:11 - INFO - __main__ -   test: [epoch: 0 | batch: 2000/10010 ] | Loss: 0.960 | Acc: 76.349% (195550/256128)
01/15/2023 03:58:40 - INFO - __main__ -   test: [epoch: 0 | batch: 2100/10010 ] | Loss: 0.961 | Acc: 76.339% (205296/268928)
01/15/2023 04:02:10 - INFO - __main__ -   test: [epoch: 0 | batch: 2200/10010 ] | Loss: 0.961 | Acc: 76.320% (215015/281728)
01/15/2023 04:05:39 - INFO - __main__ -   test: [epoch: 0 | batch: 2300/10010 ] | Loss: 0.962 | Acc: 76.304% (224736/294528)
01/15/2023 04:09:09 - INFO - __main__ -   test: [epoch: 0 | batch: 2400/10010 ] | Loss: 0.962 | Acc: 76.299% (234489/307328)
01/15/2023 04:12:38 - INFO - __main__ -   test: [epoch: 0 | batch: 2500/10010 ] | Loss: 0.962 | Acc: 76.305% (244275/320128)
01/15/2023 04:16:06 - INFO - __main__ -   test: [epoch: 0 | batch: 2600/10010 ] | Loss: 0.963 | Acc: 76.284% (253970/332928)
01/15/2023 04:19:35 - INFO - __main__ -   test: [epoch: 0 | batch: 2700/10010 ] | Loss: 0.962 | Acc: 76.301% (263795/345728)
01/15/2023 04:23:04 - INFO - __main__ -   test: [epoch: 0 | batch: 2800/10010 ] | Loss: 0.962 | Acc: 76.291% (273524/358528)
01/15/2023 04:26:32 - INFO - __main__ -   test: [epoch: 0 | batch: 2900/10010 ] | Loss: 0.962 | Acc: 76.296% (283309/371328)
01/15/2023 04:30:02 - INFO - __main__ -   test: [epoch: 0 | batch: 3000/10010 ] | Loss: 0.962 | Acc: 76.298% (293083/384128)
01/15/2023 04:33:33 - INFO - __main__ -   test: [epoch: 0 | batch: 3100/10010 ] | Loss: 0.962 | Acc: 76.292% (302826/396928)
01/15/2023 04:37:01 - INFO - __main__ -   test: [epoch: 0 | batch: 3200/10010 ] | Loss: 0.962 | Acc: 76.284% (312556/409728)
01/15/2023 04:40:30 - INFO - __main__ -   test: [epoch: 0 | batch: 3300/10010 ] | Loss: 0.963 | Acc: 76.270% (322260/422528)
01/15/2023 04:43:59 - INFO - __main__ -   test: [epoch: 0 | batch: 3400/10010 ] | Loss: 0.962 | Acc: 76.273% (332038/435328)
01/15/2023 04:47:29 - INFO - __main__ -   test: [epoch: 0 | batch: 3500/10010 ] | Loss: 0.962 | Acc: 76.273% (341800/448128)
01/15/2023 04:50:59 - INFO - __main__ -   test: [epoch: 0 | batch: 3600/10010 ] | Loss: 0.962 | Acc: 76.260% (351504/460928)
01/15/2023 04:54:28 - INFO - __main__ -   test: [epoch: 0 | batch: 3700/10010 ] | Loss: 0.963 | Acc: 76.240% (361170/473728)
01/15/2023 04:57:58 - INFO - __main__ -   test: [epoch: 0 | batch: 3800/10010 ] | Loss: 0.963 | Acc: 76.241% (370932/486528)
01/15/2023 05:01:27 - INFO - __main__ -   test: [epoch: 0 | batch: 3900/10010 ] | Loss: 0.963 | Acc: 76.230% (380638/499328)
01/15/2023 05:04:56 - INFO - __main__ -   test: [epoch: 0 | batch: 4000/10010 ] | Loss: 0.964 | Acc: 76.218% (390333/512128)
01/15/2023 05:08:25 - INFO - __main__ -   test: [epoch: 0 | batch: 4100/10010 ] | Loss: 0.963 | Acc: 76.218% (400089/524928)
01/15/2023 05:11:54 - INFO - __main__ -   test: [epoch: 0 | batch: 4200/10010 ] | Loss: 0.964 | Acc: 76.218% (409845/537728)
01/15/2023 05:15:23 - INFO - __main__ -   test: [epoch: 0 | batch: 4300/10010 ] | Loss: 0.964 | Acc: 76.219% (419606/550528)
01/15/2023 05:18:51 - INFO - __main__ -   test: [epoch: 0 | batch: 4400/10010 ] | Loss: 0.963 | Acc: 76.220% (429371/563328)
01/15/2023 05:22:20 - INFO - __main__ -   test: [epoch: 0 | batch: 4500/10010 ] | Loss: 0.964 | Acc: 76.208% (439058/576128)
01/15/2023 05:25:50 - INFO - __main__ -   test: [epoch: 0 | batch: 4600/10010 ] | Loss: 0.963 | Acc: 76.214% (448844/588928)
01/15/2023 05:29:18 - INFO - __main__ -   test: [epoch: 0 | batch: 4700/10010 ] | Loss: 0.964 | Acc: 76.214% (458603/601728)
01/15/2023 05:32:47 - INFO - __main__ -   test: [epoch: 0 | batch: 4800/10010 ] | Loss: 0.963 | Acc: 76.218% (468380/614528)
01/15/2023 05:36:17 - INFO - __main__ -   test: [epoch: 0 | batch: 4900/10010 ] | Loss: 0.964 | Acc: 76.221% (478155/627328)
01/15/2023 05:39:45 - INFO - __main__ -   test: [epoch: 0 | batch: 5000/10010 ] | Loss: 0.963 | Acc: 76.233% (487989/640128)
01/15/2023 05:43:15 - INFO - __main__ -   test: [epoch: 0 | batch: 5100/10010 ] | Loss: 0.963 | Acc: 76.242% (497804/652928)
01/15/2023 05:46:44 - INFO - __main__ -   test: [epoch: 0 | batch: 5200/10010 ] | Loss: 0.963 | Acc: 76.241% (507558/665728)
01/15/2023 05:50:12 - INFO - __main__ -   test: [epoch: 0 | batch: 5300/10010 ] | Loss: 0.963 | Acc: 76.240% (517312/678528)
01/15/2023 05:53:40 - INFO - __main__ -   test: [epoch: 0 | batch: 5400/10010 ] | Loss: 0.963 | Acc: 76.234% (527025/691328)
01/15/2023 05:57:09 - INFO - __main__ -   test: [epoch: 0 | batch: 5500/10010 ] | Loss: 0.964 | Acc: 76.227% (536735/704128)
01/15/2023 06:00:40 - INFO - __main__ -   test: [epoch: 0 | batch: 5600/10010 ] | Loss: 0.963 | Acc: 76.236% (546554/716928)
01/15/2023 06:04:08 - INFO - __main__ -   test: [epoch: 0 | batch: 5700/10010 ] | Loss: 0.963 | Acc: 76.238% (556329/729728)
01/15/2023 06:07:37 - INFO - __main__ -   test: [epoch: 0 | batch: 5800/10010 ] | Loss: 0.963 | Acc: 76.243% (566129/742528)
01/15/2023 06:11:07 - INFO - __main__ -   test: [epoch: 0 | batch: 5900/10010 ] | Loss: 0.963 | Acc: 76.245% (575898/755328)
01/15/2023 06:14:38 - INFO - __main__ -   test: [epoch: 0 | batch: 6000/10010 ] | Loss: 0.963 | Acc: 76.238% (585602/768128)
01/15/2023 06:18:08 - INFO - __main__ -   test: [epoch: 0 | batch: 6100/10010 ] | Loss: 0.964 | Acc: 76.233% (595323/780928)
01/15/2023 06:21:36 - INFO - __main__ -   test: [epoch: 0 | batch: 6200/10010 ] | Loss: 0.964 | Acc: 76.229% (605053/793728)
01/15/2023 06:25:05 - INFO - __main__ -   test: [epoch: 0 | batch: 6300/10010 ] | Loss: 0.964 | Acc: 76.229% (614805/806528)
01/15/2023 06:28:32 - INFO - __main__ -   test: [epoch: 0 | batch: 6400/10010 ] | Loss: 0.964 | Acc: 76.229% (624567/819328)
01/15/2023 06:31:59 - INFO - __main__ -   test: [epoch: 0 | batch: 6500/10010 ] | Loss: 0.964 | Acc: 76.228% (634317/832128)
01/15/2023 06:35:29 - INFO - __main__ -   test: [epoch: 0 | batch: 6600/10010 ] | Loss: 0.964 | Acc: 76.229% (644078/844928)
01/15/2023 06:38:59 - INFO - __main__ -   test: [epoch: 0 | batch: 6700/10010 ] | Loss: 0.964 | Acc: 76.228% (653827/857728)
01/15/2023 06:42:28 - INFO - __main__ -   test: [epoch: 0 | batch: 6800/10010 ] | Loss: 0.964 | Acc: 76.222% (663534/870528)
01/15/2023 06:45:56 - INFO - __main__ -   test: [epoch: 0 | batch: 6900/10010 ] | Loss: 0.964 | Acc: 76.223% (673296/883328)
01/15/2023 06:49:25 - INFO - __main__ -   test: [epoch: 0 | batch: 7000/10010 ] | Loss: 0.964 | Acc: 76.216% (682997/896128)
01/15/2023 06:52:55 - INFO - __main__ -   test: [epoch: 0 | batch: 7100/10010 ] | Loss: 0.963 | Acc: 76.225% (692829/908928)
01/15/2023 06:56:23 - INFO - __main__ -   test: [epoch: 0 | batch: 7200/10010 ] | Loss: 0.963 | Acc: 76.235% (702679/921728)
01/15/2023 06:59:52 - INFO - __main__ -   test: [epoch: 0 | batch: 7300/10010 ] | Loss: 0.963 | Acc: 76.228% (712374/934528)
01/15/2023 07:03:19 - INFO - __main__ -   test: [epoch: 0 | batch: 7400/10010 ] | Loss: 0.964 | Acc: 76.223% (722080/947328)
01/15/2023 07:06:45 - INFO - __main__ -   test: [epoch: 0 | batch: 7500/10010 ] | Loss: 0.964 | Acc: 76.227% (731879/960128)
01/15/2023 07:10:15 - INFO - __main__ -   test: [epoch: 0 | batch: 7600/10010 ] | Loss: 0.964 | Acc: 76.228% (741641/972928)
01/15/2023 07:13:44 - INFO - __main__ -   test: [epoch: 0 | batch: 7700/10010 ] | Loss: 0.964 | Acc: 76.219% (751312/985728)
01/15/2023 07:17:13 - INFO - __main__ -   test: [epoch: 0 | batch: 7800/10010 ] | Loss: 0.964 | Acc: 76.214% (761022/998528)
01/15/2023 07:20:41 - INFO - __main__ -   test: [epoch: 0 | batch: 7900/10010 ] | Loss: 0.964 | Acc: 76.219% (770824/1011328)
01/15/2023 07:24:11 - INFO - __main__ -   test: [epoch: 0 | batch: 8000/10010 ] | Loss: 0.964 | Acc: 76.215% (780536/1024128)
01/15/2023 07:27:39 - INFO - __main__ -   test: [epoch: 0 | batch: 8100/10010 ] | Loss: 0.964 | Acc: 76.222% (790365/1036928)
01/15/2023 07:31:08 - INFO - __main__ -   test: [epoch: 0 | batch: 8200/10010 ] | Loss: 0.964 | Acc: 76.218% (800082/1049728)
01/15/2023 07:34:37 - INFO - __main__ -   test: [epoch: 0 | batch: 8300/10010 ] | Loss: 0.964 | Acc: 76.220% (809862/1062528)
01/15/2023 07:38:05 - INFO - __main__ -   test: [epoch: 0 | batch: 8400/10010 ] | Loss: 0.964 | Acc: 76.223% (819651/1075328)
01/15/2023 07:41:36 - INFO - __main__ -   test: [epoch: 0 | batch: 8500/10010 ] | Loss: 0.964 | Acc: 76.229% (829470/1088128)
01/15/2023 07:45:05 - INFO - __main__ -   test: [epoch: 0 | batch: 8600/10010 ] | Loss: 0.964 | Acc: 76.222% (839144/1100928)
01/15/2023 07:48:33 - INFO - __main__ -   test: [epoch: 0 | batch: 8700/10010 ] | Loss: 0.964 | Acc: 76.222% (848901/1113728)
01/15/2023 07:52:03 - INFO - __main__ -   test: [epoch: 0 | batch: 8800/10010 ] | Loss: 0.964 | Acc: 76.223% (858678/1126528)
01/15/2023 07:55:32 - INFO - __main__ -   test: [epoch: 0 | batch: 8900/10010 ] | Loss: 0.964 | Acc: 76.225% (868458/1139328)
01/15/2023 07:58:59 - INFO - __main__ -   test: [epoch: 0 | batch: 9000/10010 ] | Loss: 0.964 | Acc: 76.230% (878267/1152128)
01/15/2023 08:02:28 - INFO - __main__ -   test: [epoch: 0 | batch: 9100/10010 ] | Loss: 0.964 | Acc: 76.227% (887993/1164928)
01/15/2023 08:05:56 - INFO - __main__ -   test: [epoch: 0 | batch: 9200/10010 ] | Loss: 0.964 | Acc: 76.224% (897715/1177728)
01/15/2023 08:09:25 - INFO - __main__ -   test: [epoch: 0 | batch: 9300/10010 ] | Loss: 0.964 | Acc: 76.228% (907518/1190528)
01/15/2023 08:12:52 - INFO - __main__ -   test: [epoch: 0 | batch: 9400/10010 ] | Loss: 0.964 | Acc: 76.225% (917240/1203328)
01/15/2023 08:16:22 - INFO - __main__ -   test: [epoch: 0 | batch: 9500/10010 ] | Loss: 0.964 | Acc: 76.228% (927032/1216128)
01/15/2023 08:19:52 - INFO - __main__ -   test: [epoch: 0 | batch: 9600/10010 ] | Loss: 0.964 | Acc: 76.226% (936764/1228928)
01/15/2023 08:23:21 - INFO - __main__ -   test: [epoch: 0 | batch: 9700/10010 ] | Loss: 0.964 | Acc: 76.225% (946504/1241728)
01/15/2023 08:26:50 - INFO - __main__ -   test: [epoch: 0 | batch: 9800/10010 ] | Loss: 0.964 | Acc: 76.221% (956217/1254528)
01/15/2023 08:30:16 - INFO - __main__ -   test: [epoch: 0 | batch: 9900/10010 ] | Loss: 0.964 | Acc: 76.225% (966020/1267328)
01/15/2023 08:33:46 - INFO - __main__ -   test: [epoch: 0 | batch: 10000/10010 ] | Loss: 0.964 | Acc: 76.223% (975750/1280128)
01/15/2023 08:34:05 - INFO - __main__ -   Saving Checkpoint
01/15/2023 08:34:07 - INFO - __main__ -   test: [batch: 0/391 ] | Loss: 0.540 | Acc: 87.500% (112/128)/ 95.312% (122/128)
01/15/2023 08:34:09 - INFO - __main__ -   test: [batch: 1/391 ] | Loss: 0.574 | Acc: 85.547% (219/256)/ 96.875% (248/256)
01/15/2023 08:34:11 - INFO - __main__ -   test: [batch: 2/391 ] | Loss: 0.776 | Acc: 79.688% (306/384)/ 94.271% (362/384)
01/15/2023 08:34:13 - INFO - __main__ -   test: [batch: 3/391 ] | Loss: 0.711 | Acc: 82.031% (420/512)/ 94.922% (486/512)
01/15/2023 08:34:15 - INFO - __main__ -   test: [batch: 4/391 ] | Loss: 0.612 | Acc: 84.688% (542/640)/ 95.625% (612/640)
01/15/2023 08:34:17 - INFO - __main__ -   test: [batch: 5/391 ] | Loss: 0.539 | Acc: 86.068% (661/768)/ 96.354% (740/768)
01/15/2023 08:34:19 - INFO - __main__ -   test: [batch: 6/391 ] | Loss: 0.525 | Acc: 86.496% (775/896)/ 96.429% (864/896)
01/15/2023 08:34:21 - INFO - __main__ -   test: [batch: 7/391 ] | Loss: 0.501 | Acc: 87.305% (894/1024)/ 96.582% (989/1024)
01/15/2023 08:34:24 - INFO - __main__ -   test: [batch: 8/391 ] | Loss: 0.506 | Acc: 87.326% (1006/1152)/ 96.615% (1113/1152)
01/15/2023 08:34:26 - INFO - __main__ -   test: [batch: 9/391 ] | Loss: 0.481 | Acc: 87.969% (1126/1280)/ 96.797% (1239/1280)
01/15/2023 08:34:28 - INFO - __main__ -   test: [batch: 10/391 ] | Loss: 0.526 | Acc: 86.648% (1220/1408)/ 96.733% (1362/1408)
01/15/2023 08:34:30 - INFO - __main__ -   test: [batch: 11/391 ] | Loss: 0.531 | Acc: 86.589% (1330/1536)/ 96.549% (1483/1536)
01/15/2023 08:34:32 - INFO - __main__ -   test: [batch: 12/391 ] | Loss: 0.584 | Acc: 85.276% (1419/1664)/ 96.094% (1599/1664)
01/15/2023 08:34:34 - INFO - __main__ -   test: [batch: 13/391 ] | Loss: 0.640 | Acc: 83.817% (1502/1792)/ 95.368% (1709/1792)
01/15/2023 08:34:36 - INFO - __main__ -   test: [batch: 14/391 ] | Loss: 0.655 | Acc: 83.021% (1594/1920)/ 95.521% (1834/1920)
01/15/2023 08:34:38 - INFO - __main__ -   test: [batch: 15/391 ] | Loss: 0.662 | Acc: 82.471% (1689/2048)/ 95.605% (1958/2048)
01/15/2023 08:34:41 - INFO - __main__ -   test: [batch: 16/391 ] | Loss: 0.670 | Acc: 82.399% (1793/2176)/ 95.404% (2076/2176)
01/15/2023 08:34:43 - INFO - __main__ -   test: [batch: 17/391 ] | Loss: 0.694 | Acc: 81.814% (1885/2304)/ 94.965% (2188/2304)
01/15/2023 08:34:45 - INFO - __main__ -   test: [batch: 18/391 ] | Loss: 0.719 | Acc: 81.291% (1977/2432)/ 94.819% (2306/2432)
01/15/2023 08:34:47 - INFO - __main__ -   test: [batch: 19/391 ] | Loss: 0.726 | Acc: 81.094% (2076/2560)/ 94.648% (2423/2560)
01/15/2023 08:34:49 - INFO - __main__ -   test: [batch: 20/391 ] | Loss: 0.721 | Acc: 81.176% (2182/2688)/ 94.643% (2544/2688)
01/15/2023 08:34:51 - INFO - __main__ -   test: [batch: 21/391 ] | Loss: 0.755 | Acc: 80.398% (2264/2816)/ 94.496% (2661/2816)
01/15/2023 08:34:53 - INFO - __main__ -   test: [batch: 22/391 ] | Loss: 0.755 | Acc: 80.265% (2363/2944)/ 94.531% (2783/2944)
01/15/2023 08:34:55 - INFO - __main__ -   test: [batch: 23/391 ] | Loss: 0.797 | Acc: 79.395% (2439/3072)/ 94.206% (2894/3072)
01/15/2023 08:34:57 - INFO - __main__ -   test: [batch: 24/391 ] | Loss: 0.813 | Acc: 78.906% (2525/3200)/ 94.094% (3011/3200)
01/15/2023 08:35:00 - INFO - __main__ -   test: [batch: 25/391 ] | Loss: 0.840 | Acc: 78.185% (2602/3328)/ 93.900% (3125/3328)
01/15/2023 08:35:02 - INFO - __main__ -   test: [batch: 26/391 ] | Loss: 0.856 | Acc: 77.488% (2678/3456)/ 93.895% (3245/3456)
01/15/2023 08:35:04 - INFO - __main__ -   test: [batch: 27/391 ] | Loss: 0.838 | Acc: 77.985% (2795/3584)/ 93.945% (3367/3584)
01/15/2023 08:35:06 - INFO - __main__ -   test: [batch: 28/391 ] | Loss: 0.843 | Acc: 77.478% (2876/3712)/ 94.073% (3492/3712)
01/15/2023 08:35:08 - INFO - __main__ -   test: [batch: 29/391 ] | Loss: 0.834 | Acc: 77.630% (2981/3840)/ 94.219% (3618/3840)
01/15/2023 08:35:10 - INFO - __main__ -   test: [batch: 30/391 ] | Loss: 0.846 | Acc: 77.470% (3074/3968)/ 94.052% (3732/3968)
01/15/2023 08:35:12 - INFO - __main__ -   test: [batch: 31/391 ] | Loss: 0.840 | Acc: 77.734% (3184/4096)/ 94.141% (3856/4096)
01/15/2023 08:35:14 - INFO - __main__ -   test: [batch: 32/391 ] | Loss: 0.826 | Acc: 78.054% (3297/4224)/ 94.247% (3981/4224)
01/15/2023 08:35:16 - INFO - __main__ -   test: [batch: 33/391 ] | Loss: 0.819 | Acc: 78.263% (3406/4352)/ 94.301% (4104/4352)
01/15/2023 08:35:18 - INFO - __main__ -   test: [batch: 34/391 ] | Loss: 0.804 | Acc: 78.705% (3526/4480)/ 94.375% (4228/4480)
01/15/2023 08:35:21 - INFO - __main__ -   test: [batch: 35/391 ] | Loss: 0.791 | Acc: 79.102% (3645/4608)/ 94.423% (4351/4608)
01/15/2023 08:35:23 - INFO - __main__ -   test: [batch: 36/391 ] | Loss: 0.775 | Acc: 79.582% (3769/4736)/ 94.552% (4478/4736)
01/15/2023 08:35:25 - INFO - __main__ -   test: [batch: 37/391 ] | Loss: 0.765 | Acc: 79.914% (3887/4864)/ 94.634% (4603/4864)
01/15/2023 08:35:27 - INFO - __main__ -   test: [batch: 38/391 ] | Loss: 0.757 | Acc: 80.168% (4002/4992)/ 94.712% (4728/4992)
01/15/2023 08:35:29 - INFO - __main__ -   test: [batch: 39/391 ] | Loss: 0.750 | Acc: 80.332% (4113/5120)/ 94.785% (4853/5120)
01/15/2023 08:35:31 - INFO - __main__ -   test: [batch: 40/391 ] | Loss: 0.748 | Acc: 80.450% (4222/5248)/ 94.703% (4970/5248)
01/15/2023 08:35:33 - INFO - __main__ -   test: [batch: 41/391 ] | Loss: 0.750 | Acc: 80.599% (4333/5376)/ 94.624% (5087/5376)
01/15/2023 08:35:35 - INFO - __main__ -   test: [batch: 42/391 ] | Loss: 0.750 | Acc: 80.614% (4437/5504)/ 94.695% (5212/5504)
01/15/2023 08:35:37 - INFO - __main__ -   test: [batch: 43/391 ] | Loss: 0.749 | Acc: 80.611% (4540/5632)/ 94.638% (5330/5632)
01/15/2023 08:35:39 - INFO - __main__ -   test: [batch: 44/391 ] | Loss: 0.754 | Acc: 80.608% (4643/5760)/ 94.497% (5443/5760)
01/15/2023 08:35:41 - INFO - __main__ -   test: [batch: 45/391 ] | Loss: 0.752 | Acc: 80.774% (4756/5888)/ 94.548% (5567/5888)
01/15/2023 08:35:43 - INFO - __main__ -   test: [batch: 46/391 ] | Loss: 0.754 | Acc: 80.701% (4855/6016)/ 94.598% (5691/6016)
01/15/2023 08:35:46 - INFO - __main__ -   test: [batch: 47/391 ] | Loss: 0.757 | Acc: 80.599% (4952/6144)/ 94.629% (5814/6144)
01/15/2023 08:35:48 - INFO - __main__ -   test: [batch: 48/391 ] | Loss: 0.762 | Acc: 80.469% (5047/6272)/ 94.611% (5934/6272)
01/15/2023 08:35:50 - INFO - __main__ -   test: [batch: 49/391 ] | Loss: 0.768 | Acc: 80.438% (5148/6400)/ 94.531% (6050/6400)
01/15/2023 08:35:52 - INFO - __main__ -   test: [batch: 50/391 ] | Loss: 0.759 | Acc: 80.622% (5263/6528)/ 94.608% (6176/6528)
01/15/2023 08:35:54 - INFO - __main__ -   test: [batch: 51/391 ] | Loss: 0.750 | Acc: 80.844% (5381/6656)/ 94.697% (6303/6656)
01/15/2023 08:35:56 - INFO - __main__ -   test: [batch: 52/391 ] | Loss: 0.747 | Acc: 80.911% (5489/6784)/ 94.738% (6427/6784)
01/15/2023 08:35:58 - INFO - __main__ -   test: [batch: 53/391 ] | Loss: 0.737 | Acc: 81.120% (5607/6912)/ 94.821% (6554/6912)
01/15/2023 08:36:00 - INFO - __main__ -   test: [batch: 54/391 ] | Loss: 0.730 | Acc: 81.264% (5721/7040)/ 94.844% (6677/7040)
01/15/2023 08:36:02 - INFO - __main__ -   test: [batch: 55/391 ] | Loss: 0.724 | Acc: 81.417% (5836/7168)/ 94.894% (6802/7168)
01/15/2023 08:36:04 - INFO - __main__ -   test: [batch: 56/391 ] | Loss: 0.715 | Acc: 81.675% (5959/7296)/ 94.956% (6928/7296)
01/15/2023 08:36:06 - INFO - __main__ -   test: [batch: 57/391 ] | Loss: 0.708 | Acc: 81.843% (6076/7424)/ 95.016% (7054/7424)
01/15/2023 08:36:08 - INFO - __main__ -   test: [batch: 58/391 ] | Loss: 0.704 | Acc: 81.925% (6187/7552)/ 95.034% (7177/7552)
01/15/2023 08:36:10 - INFO - __main__ -   test: [batch: 59/391 ] | Loss: 0.706 | Acc: 81.875% (6288/7680)/ 95.039% (7299/7680)
01/15/2023 08:36:13 - INFO - __main__ -   test: [batch: 60/391 ] | Loss: 0.709 | Acc: 81.814% (6388/7808)/ 95.018% (7419/7808)
01/15/2023 08:36:15 - INFO - __main__ -   test: [batch: 61/391 ] | Loss: 0.709 | Acc: 81.830% (6494/7936)/ 95.035% (7542/7936)
01/15/2023 08:36:17 - INFO - __main__ -   test: [batch: 62/391 ] | Loss: 0.707 | Acc: 81.808% (6597/8064)/ 95.077% (7667/8064)
01/15/2023 08:36:19 - INFO - __main__ -   test: [batch: 63/391 ] | Loss: 0.714 | Acc: 81.665% (6690/8192)/ 95.007% (7783/8192)
01/15/2023 08:36:21 - INFO - __main__ -   test: [batch: 64/391 ] | Loss: 0.721 | Acc: 81.454% (6777/8320)/ 94.952% (7900/8320)
01/15/2023 08:36:23 - INFO - __main__ -   test: [batch: 65/391 ] | Loss: 0.734 | Acc: 80.978% (6841/8448)/ 94.863% (8014/8448)
01/15/2023 08:36:25 - INFO - __main__ -   test: [batch: 66/391 ] | Loss: 0.739 | Acc: 80.947% (6942/8576)/ 94.823% (8132/8576)
01/15/2023 08:36:27 - INFO - __main__ -   test: [batch: 67/391 ] | Loss: 0.741 | Acc: 80.917% (7043/8704)/ 94.841% (8255/8704)
01/15/2023 08:36:29 - INFO - __main__ -   test: [batch: 68/391 ] | Loss: 0.742 | Acc: 80.854% (7141/8832)/ 94.848% (8377/8832)
01/15/2023 08:36:31 - INFO - __main__ -   test: [batch: 69/391 ] | Loss: 0.738 | Acc: 80.960% (7254/8960)/ 94.900% (8503/8960)
01/15/2023 08:36:34 - INFO - __main__ -   test: [batch: 70/391 ] | Loss: 0.738 | Acc: 80.887% (7351/9088)/ 94.927% (8627/9088)
01/15/2023 08:36:36 - INFO - __main__ -   test: [batch: 71/391 ] | Loss: 0.736 | Acc: 80.957% (7461/9216)/ 94.922% (8748/9216)
01/15/2023 08:36:38 - INFO - __main__ -   test: [batch: 72/391 ] | Loss: 0.739 | Acc: 80.843% (7554/9344)/ 94.949% (8872/9344)
01/15/2023 08:36:40 - INFO - __main__ -   test: [batch: 73/391 ] | Loss: 0.743 | Acc: 80.722% (7646/9472)/ 94.954% (8994/9472)
01/15/2023 08:36:42 - INFO - __main__ -   test: [batch: 74/391 ] | Loss: 0.743 | Acc: 80.729% (7750/9600)/ 94.938% (9114/9600)
01/15/2023 08:36:44 - INFO - __main__ -   test: [batch: 75/391 ] | Loss: 0.749 | Acc: 80.561% (7837/9728)/ 94.922% (9234/9728)
01/15/2023 08:36:46 - INFO - __main__ -   test: [batch: 76/391 ] | Loss: 0.748 | Acc: 80.580% (7942/9856)/ 94.947% (9358/9856)
01/15/2023 08:36:48 - INFO - __main__ -   test: [batch: 77/391 ] | Loss: 0.748 | Acc: 80.489% (8036/9984)/ 94.982% (9483/9984)
01/15/2023 08:36:50 - INFO - __main__ -   test: [batch: 78/391 ] | Loss: 0.747 | Acc: 80.449% (8135/10112)/ 95.026% (9609/10112)
01/15/2023 08:36:52 - INFO - __main__ -   test: [batch: 79/391 ] | Loss: 0.746 | Acc: 80.420% (8235/10240)/ 95.039% (9732/10240)
01/15/2023 08:36:54 - INFO - __main__ -   test: [batch: 80/391 ] | Loss: 0.746 | Acc: 80.392% (8335/10368)/ 95.023% (9852/10368)
01/15/2023 08:36:56 - INFO - __main__ -   test: [batch: 81/391 ] | Loss: 0.745 | Acc: 80.393% (8438/10496)/ 95.036% (9975/10496)
01/15/2023 08:36:58 - INFO - __main__ -   test: [batch: 82/391 ] | Loss: 0.745 | Acc: 80.412% (8543/10624)/ 95.049% (10098/10624)
01/15/2023 08:37:01 - INFO - __main__ -   test: [batch: 83/391 ] | Loss: 0.745 | Acc: 80.450% (8650/10752)/ 95.033% (10218/10752)
01/15/2023 08:37:03 - INFO - __main__ -   test: [batch: 84/391 ] | Loss: 0.741 | Acc: 80.588% (8768/10880)/ 95.064% (10343/10880)
01/15/2023 08:37:05 - INFO - __main__ -   test: [batch: 85/391 ] | Loss: 0.739 | Acc: 80.578% (8870/11008)/ 95.113% (10470/11008)
01/15/2023 08:37:07 - INFO - __main__ -   test: [batch: 86/391 ] | Loss: 0.742 | Acc: 80.487% (8963/11136)/ 95.070% (10587/11136)
01/15/2023 08:37:09 - INFO - __main__ -   test: [batch: 87/391 ] | Loss: 0.742 | Acc: 80.522% (9070/11264)/ 95.055% (10707/11264)
01/15/2023 08:37:11 - INFO - __main__ -   test: [batch: 88/391 ] | Loss: 0.748 | Acc: 80.451% (9165/11392)/ 95.005% (10823/11392)
01/15/2023 08:37:13 - INFO - __main__ -   test: [batch: 89/391 ] | Loss: 0.746 | Acc: 80.486% (9272/11520)/ 95.026% (10947/11520)
01/15/2023 08:37:16 - INFO - __main__ -   test: [batch: 90/391 ] | Loss: 0.747 | Acc: 80.400% (9365/11648)/ 95.038% (11070/11648)
01/15/2023 08:37:18 - INFO - __main__ -   test: [batch: 91/391 ] | Loss: 0.745 | Acc: 80.443% (9473/11776)/ 95.049% (11193/11776)
01/15/2023 08:37:20 - INFO - __main__ -   test: [batch: 92/391 ] | Loss: 0.747 | Acc: 80.402% (9571/11904)/ 95.027% (11312/11904)
01/15/2023 08:37:22 - INFO - __main__ -   test: [batch: 93/391 ] | Loss: 0.750 | Acc: 80.194% (9649/12032)/ 95.063% (11438/12032)
01/15/2023 08:37:24 - INFO - __main__ -   test: [batch: 94/391 ] | Loss: 0.753 | Acc: 80.049% (9734/12160)/ 95.066% (11560/12160)
01/15/2023 08:37:26 - INFO - __main__ -   test: [batch: 95/391 ] | Loss: 0.750 | Acc: 80.111% (9844/12288)/ 95.085% (11684/12288)
01/15/2023 08:37:28 - INFO - __main__ -   test: [batch: 96/391 ] | Loss: 0.754 | Acc: 79.969% (9929/12416)/ 95.079% (11805/12416)
01/15/2023 08:37:30 - INFO - __main__ -   test: [batch: 97/391 ] | Loss: 0.756 | Acc: 79.791% (10009/12544)/ 95.097% (11929/12544)
01/15/2023 08:37:32 - INFO - __main__ -   test: [batch: 98/391 ] | Loss: 0.753 | Acc: 79.893% (10124/12672)/ 95.123% (12054/12672)
01/15/2023 08:37:34 - INFO - __main__ -   test: [batch: 99/391 ] | Loss: 0.747 | Acc: 80.062% (10248/12800)/ 95.164% (12181/12800)
01/15/2023 08:37:36 - INFO - __main__ -   test: [batch: 100/391 ] | Loss: 0.746 | Acc: 80.128% (10359/12928)/ 95.173% (12304/12928)
01/15/2023 08:37:38 - INFO - __main__ -   test: [batch: 101/391 ] | Loss: 0.741 | Acc: 80.247% (10477/13056)/ 95.205% (12430/13056)
01/15/2023 08:37:40 - INFO - __main__ -   test: [batch: 102/391 ] | Loss: 0.738 | Acc: 80.317% (10589/13184)/ 95.237% (12556/13184)
01/15/2023 08:37:42 - INFO - __main__ -   test: [batch: 103/391 ] | Loss: 0.740 | Acc: 80.176% (10673/13312)/ 95.237% (12678/13312)
01/15/2023 08:37:45 - INFO - __main__ -   test: [batch: 104/391 ] | Loss: 0.739 | Acc: 80.134% (10770/13440)/ 95.238% (12800/13440)
01/15/2023 08:37:47 - INFO - __main__ -   test: [batch: 105/391 ] | Loss: 0.740 | Acc: 80.115% (10870/13568)/ 95.239% (12922/13568)
01/15/2023 08:37:49 - INFO - __main__ -   test: [batch: 106/391 ] | Loss: 0.748 | Acc: 80.045% (10963/13696)/ 95.174% (13035/13696)
01/15/2023 08:37:51 - INFO - __main__ -   test: [batch: 107/391 ] | Loss: 0.744 | Acc: 80.172% (11083/13824)/ 95.211% (13162/13824)
01/15/2023 08:37:53 - INFO - __main__ -   test: [batch: 108/391 ] | Loss: 0.747 | Acc: 80.024% (11165/13952)/ 95.219% (13285/13952)
01/15/2023 08:37:55 - INFO - __main__ -   test: [batch: 109/391 ] | Loss: 0.748 | Acc: 80.036% (11269/14080)/ 95.213% (13406/14080)
01/15/2023 08:37:57 - INFO - __main__ -   test: [batch: 110/391 ] | Loss: 0.750 | Acc: 79.856% (11346/14208)/ 95.235% (13531/14208)
01/15/2023 08:37:59 - INFO - __main__ -   test: [batch: 111/391 ] | Loss: 0.753 | Acc: 79.813% (11442/14336)/ 95.194% (13647/14336)
01/15/2023 08:38:01 - INFO - __main__ -   test: [batch: 112/391 ] | Loss: 0.753 | Acc: 79.840% (11548/14464)/ 95.209% (13771/14464)
01/15/2023 08:38:03 - INFO - __main__ -   test: [batch: 113/391 ] | Loss: 0.752 | Acc: 79.879% (11656/14592)/ 95.217% (13894/14592)
01/15/2023 08:38:06 - INFO - __main__ -   test: [batch: 114/391 ] | Loss: 0.748 | Acc: 79.986% (11774/14720)/ 95.245% (14020/14720)
01/15/2023 08:38:08 - INFO - __main__ -   test: [batch: 115/391 ] | Loss: 0.746 | Acc: 80.078% (11890/14848)/ 95.265% (14145/14848)
01/15/2023 08:38:10 - INFO - __main__ -   test: [batch: 116/391 ] | Loss: 0.749 | Acc: 80.055% (11989/14976)/ 95.239% (14263/14976)
01/15/2023 08:38:12 - INFO - __main__ -   test: [batch: 117/391 ] | Loss: 0.747 | Acc: 80.085% (12096/15104)/ 95.260% (14388/15104)
01/15/2023 08:38:14 - INFO - __main__ -   test: [batch: 118/391 ] | Loss: 0.751 | Acc: 79.930% (12175/15232)/ 95.267% (14511/15232)
01/15/2023 08:38:16 - INFO - __main__ -   test: [batch: 119/391 ] | Loss: 0.749 | Acc: 79.974% (12284/15360)/ 95.293% (14637/15360)
01/15/2023 08:38:18 - INFO - __main__ -   test: [batch: 120/391 ] | Loss: 0.748 | Acc: 80.023% (12394/15488)/ 95.312% (14762/15488)
01/15/2023 08:38:21 - INFO - __main__ -   test: [batch: 121/391 ] | Loss: 0.754 | Acc: 79.892% (12476/15616)/ 95.268% (14877/15616)
01/15/2023 08:38:23 - INFO - __main__ -   test: [batch: 122/391 ] | Loss: 0.757 | Acc: 79.840% (12570/15744)/ 95.230% (14993/15744)
01/15/2023 08:38:25 - INFO - __main__ -   test: [batch: 123/391 ] | Loss: 0.758 | Acc: 79.851% (12674/15872)/ 95.205% (15111/15872)
01/15/2023 08:38:27 - INFO - __main__ -   test: [batch: 124/391 ] | Loss: 0.757 | Acc: 79.888% (12782/16000)/ 95.219% (15235/16000)
01/15/2023 08:38:29 - INFO - __main__ -   test: [batch: 125/391 ] | Loss: 0.753 | Acc: 79.991% (12901/16128)/ 95.244% (15361/16128)
01/15/2023 08:38:31 - INFO - __main__ -   test: [batch: 126/391 ] | Loss: 0.750 | Acc: 80.087% (13019/16256)/ 95.245% (15483/16256)
01/15/2023 08:38:33 - INFO - __main__ -   test: [batch: 127/391 ] | Loss: 0.748 | Acc: 80.164% (13134/16384)/ 95.258% (15607/16384)
01/15/2023 08:38:36 - INFO - __main__ -   test: [batch: 128/391 ] | Loss: 0.749 | Acc: 80.160% (13236/16512)/ 95.234% (15725/16512)
01/15/2023 08:38:38 - INFO - __main__ -   test: [batch: 129/391 ] | Loss: 0.747 | Acc: 80.198% (13345/16640)/ 95.258% (15851/16640)
01/15/2023 08:38:40 - INFO - __main__ -   test: [batch: 130/391 ] | Loss: 0.743 | Acc: 80.308% (13466/16768)/ 95.283% (15977/16768)
01/15/2023 08:38:42 - INFO - __main__ -   test: [batch: 131/391 ] | Loss: 0.743 | Acc: 80.356% (13577/16896)/ 95.295% (16101/16896)
01/15/2023 08:38:44 - INFO - __main__ -   test: [batch: 132/391 ] | Loss: 0.741 | Acc: 80.434% (13693/17024)/ 95.312% (16226/17024)
01/15/2023 08:38:46 - INFO - __main__ -   test: [batch: 133/391 ] | Loss: 0.743 | Acc: 80.387% (13788/17152)/ 95.307% (16347/17152)
01/15/2023 08:38:48 - INFO - __main__ -   test: [batch: 134/391 ] | Loss: 0.741 | Acc: 80.411% (13895/17280)/ 95.324% (16472/17280)
01/15/2023 08:38:50 - INFO - __main__ -   test: [batch: 135/391 ] | Loss: 0.741 | Acc: 80.400% (13996/17408)/ 95.330% (16595/17408)
01/15/2023 08:38:52 - INFO - __main__ -   test: [batch: 136/391 ] | Loss: 0.739 | Acc: 80.389% (14097/17536)/ 95.352% (16721/17536)
01/15/2023 08:38:55 - INFO - __main__ -   test: [batch: 137/391 ] | Loss: 0.737 | Acc: 80.435% (14208/17664)/ 95.386% (16849/17664)
01/15/2023 08:38:57 - INFO - __main__ -   test: [batch: 138/391 ] | Loss: 0.736 | Acc: 80.458% (14315/17792)/ 95.402% (16974/17792)
01/15/2023 08:38:59 - INFO - __main__ -   test: [batch: 139/391 ] | Loss: 0.741 | Acc: 80.312% (14392/17920)/ 95.396% (17095/17920)
01/15/2023 08:39:01 - INFO - __main__ -   test: [batch: 140/391 ] | Loss: 0.743 | Acc: 80.236% (14481/18048)/ 95.390% (17216/18048)
01/15/2023 08:39:03 - INFO - __main__ -   test: [batch: 141/391 ] | Loss: 0.741 | Acc: 80.276% (14591/18176)/ 95.395% (17339/18176)
01/15/2023 08:39:05 - INFO - __main__ -   test: [batch: 142/391 ] | Loss: 0.739 | Acc: 80.354% (14708/18304)/ 95.411% (17464/18304)
01/15/2023 08:39:07 - INFO - __main__ -   test: [batch: 143/391 ] | Loss: 0.739 | Acc: 80.382% (14816/18432)/ 95.405% (17585/18432)
01/15/2023 08:39:09 - INFO - __main__ -   test: [batch: 144/391 ] | Loss: 0.741 | Acc: 80.361% (14915/18560)/ 95.372% (17701/18560)
01/15/2023 08:39:11 - INFO - __main__ -   test: [batch: 145/391 ] | Loss: 0.743 | Acc: 80.346% (15015/18688)/ 95.355% (17820/18688)
01/15/2023 08:39:13 - INFO - __main__ -   test: [batch: 146/391 ] | Loss: 0.745 | Acc: 80.315% (15112/18816)/ 95.334% (17938/18816)
01/15/2023 08:39:16 - INFO - __main__ -   test: [batch: 147/391 ] | Loss: 0.745 | Acc: 80.316% (15215/18944)/ 95.312% (18056/18944)
01/15/2023 08:39:18 - INFO - __main__ -   test: [batch: 148/391 ] | Loss: 0.747 | Acc: 80.228% (15301/19072)/ 95.312% (18178/19072)
01/15/2023 08:39:20 - INFO - __main__ -   test: [batch: 149/391 ] | Loss: 0.750 | Acc: 80.135% (15386/19200)/ 95.271% (18292/19200)
01/15/2023 08:39:22 - INFO - __main__ -   test: [batch: 150/391 ] | Loss: 0.751 | Acc: 80.096% (15481/19328)/ 95.281% (18416/19328)
01/15/2023 08:39:24 - INFO - __main__ -   test: [batch: 151/391 ] | Loss: 0.750 | Acc: 80.140% (15592/19456)/ 95.277% (18537/19456)
01/15/2023 08:39:26 - INFO - __main__ -   test: [batch: 152/391 ] | Loss: 0.752 | Acc: 80.106% (15688/19584)/ 95.261% (18656/19584)
01/15/2023 08:39:28 - INFO - __main__ -   test: [batch: 153/391 ] | Loss: 0.750 | Acc: 80.169% (15803/19712)/ 95.272% (18780/19712)
01/15/2023 08:39:30 - INFO - __main__ -   test: [batch: 154/391 ] | Loss: 0.751 | Acc: 80.156% (15903/19840)/ 95.222% (18892/19840)
01/15/2023 08:39:33 - INFO - __main__ -   test: [batch: 155/391 ] | Loss: 0.753 | Acc: 80.163% (16007/19968)/ 95.207% (19011/19968)
01/15/2023 08:39:35 - INFO - __main__ -   test: [batch: 156/391 ] | Loss: 0.755 | Acc: 80.086% (16094/20096)/ 95.188% (19129/20096)
01/15/2023 08:39:37 - INFO - __main__ -   test: [batch: 157/391 ] | Loss: 0.758 | Acc: 80.039% (16187/20224)/ 95.164% (19246/20224)
01/15/2023 08:39:39 - INFO - __main__ -   test: [batch: 158/391 ] | Loss: 0.759 | Acc: 80.002% (16282/20352)/ 95.141% (19363/20352)
01/15/2023 08:39:41 - INFO - __main__ -   test: [batch: 159/391 ] | Loss: 0.760 | Acc: 79.990% (16382/20480)/ 95.146% (19486/20480)
01/15/2023 08:39:43 - INFO - __main__ -   test: [batch: 160/391 ] | Loss: 0.760 | Acc: 79.979% (16482/20608)/ 95.118% (19602/20608)
01/15/2023 08:39:45 - INFO - __main__ -   test: [batch: 161/391 ] | Loss: 0.770 | Acc: 79.716% (16530/20736)/ 95.009% (19701/20736)
01/15/2023 08:39:47 - INFO - __main__ -   test: [batch: 162/391 ] | Loss: 0.776 | Acc: 79.606% (16609/20864)/ 94.929% (19806/20864)
01/15/2023 08:39:49 - INFO - __main__ -   test: [batch: 163/391 ] | Loss: 0.779 | Acc: 79.540% (16697/20992)/ 94.927% (19927/20992)
01/15/2023 08:39:51 - INFO - __main__ -   test: [batch: 164/391 ] | Loss: 0.779 | Acc: 79.517% (16794/21120)/ 94.929% (20049/21120)
01/15/2023 08:39:53 - INFO - __main__ -   test: [batch: 165/391 ] | Loss: 0.781 | Acc: 79.452% (16882/21248)/ 94.922% (20169/21248)
01/15/2023 08:39:55 - INFO - __main__ -   test: [batch: 166/391 ] | Loss: 0.781 | Acc: 79.482% (16990/21376)/ 94.910% (20288/21376)
01/15/2023 08:39:58 - INFO - __main__ -   test: [batch: 167/391 ] | Loss: 0.783 | Acc: 79.441% (17083/21504)/ 94.889% (20405/21504)
01/15/2023 08:40:00 - INFO - __main__ -   test: [batch: 168/391 ] | Loss: 0.782 | Acc: 79.461% (17189/21632)/ 94.887% (20526/21632)
01/15/2023 08:40:01 - INFO - __main__ -   test: [batch: 169/391 ] | Loss: 0.785 | Acc: 79.403% (17278/21760)/ 94.853% (20640/21760)
01/15/2023 08:40:04 - INFO - __main__ -   test: [batch: 170/391 ] | Loss: 0.789 | Acc: 79.308% (17359/21888)/ 94.810% (20752/21888)
01/15/2023 08:40:06 - INFO - __main__ -   test: [batch: 171/391 ] | Loss: 0.792 | Acc: 79.274% (17453/22016)/ 94.786% (20868/22016)
01/15/2023 08:40:08 - INFO - __main__ -   test: [batch: 172/391 ] | Loss: 0.793 | Acc: 79.236% (17546/22144)/ 94.784% (20989/22144)
01/15/2023 08:40:10 - INFO - __main__ -   test: [batch: 173/391 ] | Loss: 0.796 | Acc: 79.185% (17636/22272)/ 94.751% (21103/22272)
01/15/2023 08:40:12 - INFO - __main__ -   test: [batch: 174/391 ] | Loss: 0.800 | Acc: 79.076% (17713/22400)/ 94.710% (21215/22400)
01/15/2023 08:40:14 - INFO - __main__ -   test: [batch: 175/391 ] | Loss: 0.799 | Acc: 79.133% (17827/22528)/ 94.722% (21339/22528)
01/15/2023 08:40:16 - INFO - __main__ -   test: [batch: 176/391 ] | Loss: 0.799 | Acc: 79.127% (17927/22656)/ 94.695% (21454/22656)
01/15/2023 08:40:18 - INFO - __main__ -   test: [batch: 177/391 ] | Loss: 0.803 | Acc: 79.042% (18009/22784)/ 94.654% (21566/22784)
01/15/2023 08:40:20 - INFO - __main__ -   test: [batch: 178/391 ] | Loss: 0.804 | Acc: 79.059% (18114/22912)/ 94.636% (21683/22912)
01/15/2023 08:40:22 - INFO - __main__ -   test: [batch: 179/391 ] | Loss: 0.808 | Acc: 79.002% (18202/23040)/ 94.596% (21795/23040)
01/15/2023 08:40:24 - INFO - __main__ -   test: [batch: 180/391 ] | Loss: 0.813 | Acc: 78.867% (18272/23168)/ 94.549% (21905/23168)
01/15/2023 08:40:26 - INFO - __main__ -   test: [batch: 181/391 ] | Loss: 0.820 | Acc: 78.743% (18344/23296)/ 94.475% (22009/23296)
01/15/2023 08:40:29 - INFO - __main__ -   test: [batch: 182/391 ] | Loss: 0.819 | Acc: 78.761% (18449/23424)/ 94.480% (22131/23424)
01/15/2023 08:40:31 - INFO - __main__ -   test: [batch: 183/391 ] | Loss: 0.825 | Acc: 78.639% (18521/23552)/ 94.374% (22227/23552)
01/15/2023 08:40:33 - INFO - __main__ -   test: [batch: 184/391 ] | Loss: 0.825 | Acc: 78.670% (18629/23680)/ 94.371% (22347/23680)
01/15/2023 08:40:35 - INFO - __main__ -   test: [batch: 185/391 ] | Loss: 0.825 | Acc: 78.688% (18734/23808)/ 94.363% (22466/23808)
01/15/2023 08:40:37 - INFO - __main__ -   test: [batch: 186/391 ] | Loss: 0.828 | Acc: 78.651% (18826/23936)/ 94.322% (22577/23936)
01/15/2023 08:40:39 - INFO - __main__ -   test: [batch: 187/391 ] | Loss: 0.832 | Acc: 78.545% (18901/24064)/ 94.307% (22694/24064)
01/15/2023 08:40:41 - INFO - __main__ -   test: [batch: 188/391 ] | Loss: 0.836 | Acc: 78.435% (18975/24192)/ 94.279% (22808/24192)
01/15/2023 08:40:43 - INFO - __main__ -   test: [batch: 189/391 ] | Loss: 0.837 | Acc: 78.363% (19058/24320)/ 94.285% (22930/24320)
01/15/2023 08:40:45 - INFO - __main__ -   test: [batch: 190/391 ] | Loss: 0.842 | Acc: 78.276% (19137/24448)/ 94.257% (23044/24448)
01/15/2023 08:40:47 - INFO - __main__ -   test: [batch: 191/391 ] | Loss: 0.844 | Acc: 78.247% (19230/24576)/ 94.230% (23158/24576)
01/15/2023 08:40:49 - INFO - __main__ -   test: [batch: 192/391 ] | Loss: 0.851 | Acc: 78.105% (19295/24704)/ 94.114% (23250/24704)
01/15/2023 08:40:51 - INFO - __main__ -   test: [batch: 193/391 ] | Loss: 0.851 | Acc: 78.121% (19399/24832)/ 94.108% (23369/24832)
01/15/2023 08:40:53 - INFO - __main__ -   test: [batch: 194/391 ] | Loss: 0.855 | Acc: 78.049% (19481/24960)/ 94.071% (23480/24960)
01/15/2023 08:40:55 - INFO - __main__ -   test: [batch: 195/391 ] | Loss: 0.861 | Acc: 77.954% (19557/25088)/ 94.005% (23584/25088)
01/15/2023 08:40:57 - INFO - __main__ -   test: [batch: 196/391 ] | Loss: 0.864 | Acc: 77.859% (19633/25216)/ 93.960% (23693/25216)
01/15/2023 08:40:59 - INFO - __main__ -   test: [batch: 197/391 ] | Loss: 0.868 | Acc: 77.766% (19709/25344)/ 93.943% (23809/25344)
01/15/2023 08:41:01 - INFO - __main__ -   test: [batch: 198/391 ] | Loss: 0.870 | Acc: 77.721% (19797/25472)/ 93.895% (23917/25472)
01/15/2023 08:41:03 - INFO - __main__ -   test: [batch: 199/391 ] | Loss: 0.870 | Acc: 77.691% (19889/25600)/ 93.910% (24041/25600)
01/15/2023 08:41:06 - INFO - __main__ -   test: [batch: 200/391 ] | Loss: 0.870 | Acc: 77.662% (19981/25728)/ 93.913% (24162/25728)
01/15/2023 08:41:08 - INFO - __main__ -   test: [batch: 201/391 ] | Loss: 0.874 | Acc: 77.576% (20058/25856)/ 93.885% (24275/25856)
01/15/2023 08:41:10 - INFO - __main__ -   test: [batch: 202/391 ] | Loss: 0.875 | Acc: 77.559% (20153/25984)/ 93.881% (24394/25984)
01/15/2023 08:41:12 - INFO - __main__ -   test: [batch: 203/391 ] | Loss: 0.876 | Acc: 77.551% (20250/26112)/ 93.865% (24510/26112)
01/15/2023 08:41:14 - INFO - __main__ -   test: [batch: 204/391 ] | Loss: 0.879 | Acc: 77.454% (20324/26240)/ 93.838% (24623/26240)
01/15/2023 08:41:16 - INFO - __main__ -   test: [batch: 205/391 ] | Loss: 0.882 | Acc: 77.374% (20402/26368)/ 93.803% (24734/26368)
01/15/2023 08:41:18 - INFO - __main__ -   test: [batch: 206/391 ] | Loss: 0.883 | Acc: 77.348% (20494/26496)/ 93.803% (24854/26496)
01/15/2023 08:41:20 - INFO - __main__ -   test: [batch: 207/391 ] | Loss: 0.887 | Acc: 77.272% (20573/26624)/ 93.769% (24965/26624)
01/15/2023 08:41:22 - INFO - __main__ -   test: [batch: 208/391 ] | Loss: 0.889 | Acc: 77.232% (20661/26752)/ 93.743% (25078/26752)
01/15/2023 08:41:24 - INFO - __main__ -   test: [batch: 209/391 ] | Loss: 0.889 | Acc: 77.232% (20760/26880)/ 93.765% (25204/26880)
01/15/2023 08:41:26 - INFO - __main__ -   test: [batch: 210/391 ] | Loss: 0.890 | Acc: 77.203% (20851/27008)/ 93.750% (25320/27008)
01/15/2023 08:41:28 - INFO - __main__ -   test: [batch: 211/391 ] | Loss: 0.893 | Acc: 77.148% (20935/27136)/ 93.713% (25430/27136)
01/15/2023 08:41:30 - INFO - __main__ -   test: [batch: 212/391 ] | Loss: 0.895 | Acc: 77.080% (21015/27264)/ 93.702% (25547/27264)
01/15/2023 08:41:33 - INFO - __main__ -   test: [batch: 213/391 ] | Loss: 0.894 | Acc: 77.088% (21116/27392)/ 93.710% (25669/27392)
01/15/2023 08:41:35 - INFO - __main__ -   test: [batch: 214/391 ] | Loss: 0.894 | Acc: 77.082% (21213/27520)/ 93.714% (25790/27520)
01/15/2023 08:41:37 - INFO - __main__ -   test: [batch: 215/391 ] | Loss: 0.896 | Acc: 77.054% (21304/27648)/ 93.692% (25904/27648)
01/15/2023 08:41:39 - INFO - __main__ -   test: [batch: 216/391 ] | Loss: 0.893 | Acc: 77.106% (21417/27776)/ 93.707% (26028/27776)
01/15/2023 08:41:41 - INFO - __main__ -   test: [batch: 217/391 ] | Loss: 0.898 | Acc: 77.039% (21497/27904)/ 93.650% (26132/27904)
01/15/2023 08:41:43 - INFO - __main__ -   test: [batch: 218/391 ] | Loss: 0.902 | Acc: 76.966% (21575/28032)/ 93.611% (26241/28032)
01/15/2023 08:41:45 - INFO - __main__ -   test: [batch: 219/391 ] | Loss: 0.900 | Acc: 77.003% (21684/28160)/ 93.626% (26365/28160)
01/15/2023 08:41:47 - INFO - __main__ -   test: [batch: 220/391 ] | Loss: 0.898 | Acc: 77.040% (21793/28288)/ 93.633% (26487/28288)
01/15/2023 08:41:49 - INFO - __main__ -   test: [batch: 221/391 ] | Loss: 0.900 | Acc: 77.006% (21882/28416)/ 93.613% (26601/28416)
01/15/2023 08:41:51 - INFO - __main__ -   test: [batch: 222/391 ] | Loss: 0.898 | Acc: 77.056% (21995/28544)/ 93.631% (26726/28544)
01/15/2023 08:41:53 - INFO - __main__ -   test: [batch: 223/391 ] | Loss: 0.899 | Acc: 77.054% (22093/28672)/ 93.621% (26843/28672)
01/15/2023 08:41:55 - INFO - __main__ -   test: [batch: 224/391 ] | Loss: 0.898 | Acc: 77.069% (22196/28800)/ 93.622% (26963/28800)
01/15/2023 08:41:58 - INFO - __main__ -   test: [batch: 225/391 ] | Loss: 0.898 | Acc: 77.071% (22295/28928)/ 93.626% (27084/28928)
01/15/2023 08:42:00 - INFO - __main__ -   test: [batch: 226/391 ] | Loss: 0.898 | Acc: 77.041% (22385/29056)/ 93.626% (27204/29056)
01/15/2023 08:42:02 - INFO - __main__ -   test: [batch: 227/391 ] | Loss: 0.900 | Acc: 77.039% (22483/29184)/ 93.616% (27321/29184)
01/15/2023 08:42:04 - INFO - __main__ -   test: [batch: 228/391 ] | Loss: 0.904 | Acc: 76.958% (22558/29312)/ 93.552% (27422/29312)
01/15/2023 08:42:06 - INFO - __main__ -   test: [batch: 229/391 ] | Loss: 0.908 | Acc: 76.875% (22632/29440)/ 93.509% (27529/29440)
01/15/2023 08:42:08 - INFO - __main__ -   test: [batch: 230/391 ] | Loss: 0.911 | Acc: 76.803% (22709/29568)/ 93.459% (27634/29568)
01/15/2023 08:42:10 - INFO - __main__ -   test: [batch: 231/391 ] | Loss: 0.911 | Acc: 76.785% (22802/29696)/ 93.457% (27753/29696)
01/15/2023 08:42:12 - INFO - __main__ -   test: [batch: 232/391 ] | Loss: 0.910 | Acc: 76.814% (22909/29824)/ 93.468% (27876/29824)
01/15/2023 08:42:14 - INFO - __main__ -   test: [batch: 233/391 ] | Loss: 0.912 | Acc: 76.769% (22994/29952)/ 93.443% (27988/29952)
01/15/2023 08:42:16 - INFO - __main__ -   test: [batch: 234/391 ] | Loss: 0.918 | Acc: 76.659% (23059/30080)/ 93.381% (28089/30080)
01/15/2023 08:42:19 - INFO - __main__ -   test: [batch: 235/391 ] | Loss: 0.918 | Acc: 76.652% (23155/30208)/ 93.379% (28208/30208)
01/15/2023 08:42:21 - INFO - __main__ -   test: [batch: 236/391 ] | Loss: 0.916 | Acc: 76.708% (23270/30336)/ 93.391% (28331/30336)
01/15/2023 08:42:23 - INFO - __main__ -   test: [batch: 237/391 ] | Loss: 0.917 | Acc: 76.710% (23369/30464)/ 93.366% (28443/30464)
01/15/2023 08:42:25 - INFO - __main__ -   test: [batch: 238/391 ] | Loss: 0.916 | Acc: 76.749% (23479/30592)/ 93.374% (28565/30592)
01/15/2023 08:42:27 - INFO - __main__ -   test: [batch: 239/391 ] | Loss: 0.915 | Acc: 76.781% (23587/30720)/ 93.376% (28685/30720)
01/15/2023 08:42:29 - INFO - __main__ -   test: [batch: 240/391 ] | Loss: 0.915 | Acc: 76.786% (23687/30848)/ 93.361% (28800/30848)
01/15/2023 08:42:31 - INFO - __main__ -   test: [batch: 241/391 ] | Loss: 0.919 | Acc: 76.701% (23759/30976)/ 93.314% (28905/30976)
01/15/2023 08:42:33 - INFO - __main__ -   test: [batch: 242/391 ] | Loss: 0.921 | Acc: 76.591% (23823/31104)/ 93.303% (29021/31104)
01/15/2023 08:42:35 - INFO - __main__ -   test: [batch: 243/391 ] | Loss: 0.926 | Acc: 76.482% (23887/31232)/ 93.225% (29116/31232)
01/15/2023 08:42:37 - INFO - __main__ -   test: [batch: 244/391 ] | Loss: 0.927 | Acc: 76.483% (23985/31360)/ 93.217% (29233/31360)
01/15/2023 08:42:40 - INFO - __main__ -   test: [batch: 245/391 ] | Loss: 0.926 | Acc: 76.486% (24084/31488)/ 93.213% (29351/31488)
01/15/2023 08:42:42 - INFO - __main__ -   test: [batch: 246/391 ] | Loss: 0.927 | Acc: 76.468% (24176/31616)/ 93.206% (29468/31616)
01/15/2023 08:42:44 - INFO - __main__ -   test: [batch: 247/391 ] | Loss: 0.933 | Acc: 76.380% (24246/31744)/ 93.142% (29567/31744)
01/15/2023 08:42:46 - INFO - __main__ -   test: [batch: 248/391 ] | Loss: 0.934 | Acc: 76.349% (24334/31872)/ 93.141% (29686/31872)
01/15/2023 08:42:48 - INFO - __main__ -   test: [batch: 249/391 ] | Loss: 0.936 | Acc: 76.209% (24387/32000)/ 93.138% (29804/32000)
01/15/2023 08:42:50 - INFO - __main__ -   test: [batch: 250/391 ] | Loss: 0.935 | Acc: 76.242% (24495/32128)/ 93.149% (29927/32128)
01/15/2023 08:42:52 - INFO - __main__ -   test: [batch: 251/391 ] | Loss: 0.937 | Acc: 76.206% (24581/32256)/ 93.121% (30037/32256)
01/15/2023 08:42:54 - INFO - __main__ -   test: [batch: 252/391 ] | Loss: 0.936 | Acc: 76.232% (24687/32384)/ 93.111% (30153/32384)
01/15/2023 08:42:56 - INFO - __main__ -   test: [batch: 253/391 ] | Loss: 0.938 | Acc: 76.196% (24773/32512)/ 93.092% (30266/32512)
01/15/2023 08:42:58 - INFO - __main__ -   test: [batch: 254/391 ] | Loss: 0.942 | Acc: 76.121% (24846/32640)/ 93.039% (30368/32640)
01/15/2023 08:43:00 - INFO - __main__ -   test: [batch: 255/391 ] | Loss: 0.944 | Acc: 76.096% (24935/32768)/ 93.030% (30484/32768)
01/15/2023 08:43:02 - INFO - __main__ -   test: [batch: 256/391 ] | Loss: 0.949 | Acc: 75.973% (24992/32896)/ 92.981% (30587/32896)
01/15/2023 08:43:04 - INFO - __main__ -   test: [batch: 257/391 ] | Loss: 0.950 | Acc: 75.957% (25084/33024)/ 92.978% (30705/33024)
01/15/2023 08:43:06 - INFO - __main__ -   test: [batch: 258/391 ] | Loss: 0.951 | Acc: 75.956% (25181/33152)/ 92.963% (30819/33152)
01/15/2023 08:43:08 - INFO - __main__ -   test: [batch: 259/391 ] | Loss: 0.954 | Acc: 75.838% (25239/33280)/ 92.960% (30937/33280)
01/15/2023 08:43:10 - INFO - __main__ -   test: [batch: 260/391 ] | Loss: 0.955 | Acc: 75.820% (25330/33408)/ 92.957% (31055/33408)
01/15/2023 08:43:13 - INFO - __main__ -   test: [batch: 261/391 ] | Loss: 0.952 | Acc: 75.865% (25442/33536)/ 92.981% (31182/33536)
01/15/2023 08:43:15 - INFO - __main__ -   test: [batch: 262/391 ] | Loss: 0.951 | Acc: 75.888% (25547/33664)/ 92.990% (31304/33664)
01/15/2023 08:43:17 - INFO - __main__ -   test: [batch: 263/391 ] | Loss: 0.954 | Acc: 75.823% (25622/33792)/ 92.966% (31415/33792)
01/15/2023 08:43:19 - INFO - __main__ -   test: [batch: 264/391 ] | Loss: 0.959 | Acc: 75.746% (25693/33920)/ 92.901% (31512/33920)
01/15/2023 08:43:21 - INFO - __main__ -   test: [batch: 265/391 ] | Loss: 0.958 | Acc: 75.781% (25802/34048)/ 92.898% (31630/34048)
01/15/2023 08:43:23 - INFO - __main__ -   test: [batch: 266/391 ] | Loss: 0.961 | Acc: 75.726% (25880/34176)/ 92.884% (31744/34176)
01/15/2023 08:43:25 - INFO - __main__ -   test: [batch: 267/391 ] | Loss: 0.960 | Acc: 75.764% (25990/34304)/ 92.887% (31864/34304)
01/15/2023 08:43:27 - INFO - __main__ -   test: [batch: 268/391 ] | Loss: 0.960 | Acc: 75.778% (26092/34432)/ 92.882% (31981/34432)
01/15/2023 08:43:29 - INFO - __main__ -   test: [batch: 269/391 ] | Loss: 0.961 | Acc: 75.720% (26169/34560)/ 92.867% (32095/34560)
01/15/2023 08:43:31 - INFO - __main__ -   test: [batch: 270/391 ] | Loss: 0.964 | Acc: 75.672% (26249/34688)/ 92.842% (32205/34688)
01/15/2023 08:43:33 - INFO - __main__ -   test: [batch: 271/391 ] | Loss: 0.964 | Acc: 75.686% (26351/34816)/ 92.834% (32321/34816)
01/15/2023 08:43:35 - INFO - __main__ -   test: [batch: 272/391 ] | Loss: 0.965 | Acc: 75.658% (26438/34944)/ 92.831% (32439/34944)
01/15/2023 08:43:37 - INFO - __main__ -   test: [batch: 273/391 ] | Loss: 0.965 | Acc: 75.673% (26540/35072)/ 92.809% (32550/35072)
01/15/2023 08:43:39 - INFO - __main__ -   test: [batch: 274/391 ] | Loss: 0.965 | Acc: 75.676% (26638/35200)/ 92.815% (32671/35200)
01/15/2023 08:43:42 - INFO - __main__ -   test: [batch: 275/391 ] | Loss: 0.967 | Acc: 75.643% (26723/35328)/ 92.805% (32786/35328)
01/15/2023 08:43:44 - INFO - __main__ -   test: [batch: 276/391 ] | Loss: 0.968 | Acc: 75.604% (26806/35456)/ 92.791% (32900/35456)
01/15/2023 08:43:46 - INFO - __main__ -   test: [batch: 277/391 ] | Loss: 0.969 | Acc: 75.573% (26892/35584)/ 92.780% (33015/35584)
01/15/2023 08:43:48 - INFO - __main__ -   test: [batch: 278/391 ] | Loss: 0.969 | Acc: 75.588% (26994/35712)/ 92.781% (33134/35712)
01/15/2023 08:43:50 - INFO - __main__ -   test: [batch: 279/391 ] | Loss: 0.969 | Acc: 75.608% (27098/35840)/ 92.776% (33251/35840)
01/15/2023 08:43:52 - INFO - __main__ -   test: [batch: 280/391 ] | Loss: 0.969 | Acc: 75.584% (27186/35968)/ 92.769% (33367/35968)
01/15/2023 08:43:54 - INFO - __main__ -   test: [batch: 281/391 ] | Loss: 0.970 | Acc: 75.579% (27281/36096)/ 92.755% (33481/36096)
01/15/2023 08:43:56 - INFO - __main__ -   test: [batch: 282/391 ] | Loss: 0.969 | Acc: 75.613% (27390/36224)/ 92.753% (33599/36224)
01/15/2023 08:43:58 - INFO - __main__ -   test: [batch: 283/391 ] | Loss: 0.970 | Acc: 75.600% (27482/36352)/ 92.746% (33715/36352)
01/15/2023 08:44:00 - INFO - __main__ -   test: [batch: 284/391 ] | Loss: 0.973 | Acc: 75.551% (27561/36480)/ 92.692% (33814/36480)
01/15/2023 08:44:02 - INFO - __main__ -   test: [batch: 285/391 ] | Loss: 0.976 | Acc: 75.519% (27646/36608)/ 92.646% (33916/36608)
01/15/2023 08:44:04 - INFO - __main__ -   test: [batch: 286/391 ] | Loss: 0.978 | Acc: 75.482% (27729/36736)/ 92.623% (34026/36736)
01/15/2023 08:44:06 - INFO - __main__ -   test: [batch: 287/391 ] | Loss: 0.978 | Acc: 75.472% (27822/36864)/ 92.616% (34142/36864)
01/15/2023 08:44:08 - INFO - __main__ -   test: [batch: 288/391 ] | Loss: 0.977 | Acc: 75.492% (27926/36992)/ 92.617% (34261/36992)
01/15/2023 08:44:11 - INFO - __main__ -   test: [batch: 289/391 ] | Loss: 0.979 | Acc: 75.463% (28012/37120)/ 92.584% (34367/37120)
01/15/2023 08:44:13 - INFO - __main__ -   test: [batch: 290/391 ] | Loss: 0.981 | Acc: 75.379% (28077/37248)/ 92.579% (34484/37248)
01/15/2023 08:44:15 - INFO - __main__ -   test: [batch: 291/391 ] | Loss: 0.981 | Acc: 75.388% (28177/37376)/ 92.581% (34603/37376)
01/15/2023 08:44:17 - INFO - __main__ -   test: [batch: 292/391 ] | Loss: 0.983 | Acc: 75.331% (28252/37504)/ 92.566% (34716/37504)
01/15/2023 08:44:20 - INFO - __main__ -   test: [batch: 293/391 ] | Loss: 0.983 | Acc: 75.311% (28341/37632)/ 92.565% (34834/37632)
01/15/2023 08:44:22 - INFO - __main__ -   test: [batch: 294/391 ] | Loss: 0.984 | Acc: 75.286% (28428/37760)/ 92.550% (34947/37760)
01/15/2023 08:44:24 - INFO - __main__ -   test: [batch: 295/391 ] | Loss: 0.984 | Acc: 75.309% (28533/37888)/ 92.549% (35065/37888)
01/15/2023 08:44:26 - INFO - __main__ -   test: [batch: 296/391 ] | Loss: 0.984 | Acc: 75.297% (28625/38016)/ 92.540% (35180/38016)
01/15/2023 08:44:28 - INFO - __main__ -   test: [batch: 297/391 ] | Loss: 0.986 | Acc: 75.275% (28713/38144)/ 92.515% (35289/38144)
01/15/2023 08:44:30 - INFO - __main__ -   test: [batch: 298/391 ] | Loss: 0.988 | Acc: 75.227% (28791/38272)/ 92.491% (35398/38272)
01/15/2023 08:44:32 - INFO - __main__ -   test: [batch: 299/391 ] | Loss: 0.990 | Acc: 75.214% (28882/38400)/ 92.466% (35507/38400)
01/15/2023 08:44:35 - INFO - __main__ -   test: [batch: 300/391 ] | Loss: 0.990 | Acc: 75.231% (28985/38528)/ 92.463% (35624/38528)
01/15/2023 08:44:37 - INFO - __main__ -   test: [batch: 301/391 ] | Loss: 0.991 | Acc: 75.220% (29077/38656)/ 92.451% (35738/38656)
01/15/2023 08:44:39 - INFO - __main__ -   test: [batch: 302/391 ] | Loss: 0.992 | Acc: 75.193% (29163/38784)/ 92.435% (35850/38784)
01/15/2023 08:44:41 - INFO - __main__ -   test: [batch: 303/391 ] | Loss: 0.993 | Acc: 75.164% (29248/38912)/ 92.421% (35963/38912)
01/15/2023 08:44:43 - INFO - __main__ -   test: [batch: 304/391 ] | Loss: 0.993 | Acc: 75.179% (29350/39040)/ 92.423% (36082/39040)
01/15/2023 08:44:45 - INFO - __main__ -   test: [batch: 305/391 ] | Loss: 0.993 | Acc: 75.140% (29431/39168)/ 92.420% (36199/39168)
01/15/2023 08:44:47 - INFO - __main__ -   test: [batch: 306/391 ] | Loss: 0.995 | Acc: 75.135% (29525/39296)/ 92.396% (36308/39296)
01/15/2023 08:44:49 - INFO - __main__ -   test: [batch: 307/391 ] | Loss: 0.996 | Acc: 75.104% (29609/39424)/ 92.383% (36421/39424)
01/15/2023 08:44:51 - INFO - __main__ -   test: [batch: 308/391 ] | Loss: 0.997 | Acc: 75.088% (29699/39552)/ 92.372% (36535/39552)
01/15/2023 08:44:53 - INFO - __main__ -   test: [batch: 309/391 ] | Loss: 0.998 | Acc: 75.078% (29791/39680)/ 92.351% (36645/39680)
01/15/2023 08:44:55 - INFO - __main__ -   test: [batch: 310/391 ] | Loss: 0.999 | Acc: 75.065% (29882/39808)/ 92.336% (36757/39808)
01/15/2023 08:44:58 - INFO - __main__ -   test: [batch: 311/391 ] | Loss: 1.000 | Acc: 75.048% (29971/39936)/ 92.323% (36870/39936)
01/15/2023 08:45:00 - INFO - __main__ -   test: [batch: 312/391 ] | Loss: 1.001 | Acc: 75.035% (30062/40064)/ 92.307% (36982/40064)
01/15/2023 08:45:02 - INFO - __main__ -   test: [batch: 313/391 ] | Loss: 0.999 | Acc: 75.082% (30177/40192)/ 92.332% (37110/40192)
01/15/2023 08:45:04 - INFO - __main__ -   test: [batch: 314/391 ] | Loss: 0.999 | Acc: 75.069% (30268/40320)/ 92.324% (37225/40320)
01/15/2023 08:45:06 - INFO - __main__ -   test: [batch: 315/391 ] | Loss: 1.000 | Acc: 75.057% (30359/40448)/ 92.304% (37335/40448)
01/15/2023 08:45:08 - INFO - __main__ -   test: [batch: 316/391 ] | Loss: 1.003 | Acc: 74.965% (30418/40576)/ 92.276% (37442/40576)
01/15/2023 08:45:10 - INFO - __main__ -   test: [batch: 317/391 ] | Loss: 1.005 | Acc: 74.924% (30497/40704)/ 92.242% (37546/40704)
01/15/2023 08:45:12 - INFO - __main__ -   test: [batch: 318/391 ] | Loss: 1.004 | Acc: 74.956% (30606/40832)/ 92.261% (37672/40832)
01/15/2023 08:45:15 - INFO - __main__ -   test: [batch: 319/391 ] | Loss: 1.006 | Acc: 74.907% (30682/40960)/ 92.229% (37777/40960)
01/15/2023 08:45:17 - INFO - __main__ -   test: [batch: 320/391 ] | Loss: 1.005 | Acc: 74.951% (30796/41088)/ 92.239% (37899/41088)
01/15/2023 08:45:19 - INFO - __main__ -   test: [batch: 321/391 ] | Loss: 1.005 | Acc: 74.971% (30900/41216)/ 92.234% (38015/41216)
01/15/2023 08:45:21 - INFO - __main__ -   test: [batch: 322/391 ] | Loss: 1.006 | Acc: 74.937% (30982/41344)/ 92.221% (38128/41344)
01/15/2023 08:45:23 - INFO - __main__ -   test: [batch: 323/391 ] | Loss: 1.009 | Acc: 74.901% (31063/41472)/ 92.195% (38235/41472)
01/15/2023 08:45:25 - INFO - __main__ -   test: [batch: 324/391 ] | Loss: 1.009 | Acc: 74.899% (31158/41600)/ 92.188% (38350/41600)
01/15/2023 08:45:27 - INFO - __main__ -   test: [batch: 325/391 ] | Loss: 1.008 | Acc: 74.899% (31254/41728)/ 92.192% (38470/41728)
01/15/2023 08:45:29 - INFO - __main__ -   test: [batch: 326/391 ] | Loss: 1.012 | Acc: 74.811% (31313/41856)/ 92.144% (38568/41856)
01/15/2023 08:45:31 - INFO - __main__ -   test: [batch: 327/391 ] | Loss: 1.015 | Acc: 74.731% (31375/41984)/ 92.104% (38669/41984)
01/15/2023 08:45:34 - INFO - __main__ -   test: [batch: 328/391 ] | Loss: 1.017 | Acc: 74.679% (31449/42112)/ 92.071% (38773/42112)
01/15/2023 08:45:36 - INFO - __main__ -   test: [batch: 329/391 ] | Loss: 1.017 | Acc: 74.678% (31544/42240)/ 92.081% (38895/42240)
01/15/2023 08:45:38 - INFO - __main__ -   test: [batch: 330/391 ] | Loss: 1.019 | Acc: 74.641% (31624/42368)/ 92.067% (39007/42368)
01/15/2023 08:45:40 - INFO - __main__ -   test: [batch: 331/391 ] | Loss: 1.019 | Acc: 74.605% (31704/42496)/ 92.079% (39130/42496)
01/15/2023 08:45:42 - INFO - __main__ -   test: [batch: 332/391 ] | Loss: 1.020 | Acc: 74.582% (31790/42624)/ 92.075% (39246/42624)
01/15/2023 08:45:44 - INFO - __main__ -   test: [batch: 333/391 ] | Loss: 1.019 | Acc: 74.612% (31898/42752)/ 92.082% (39367/42752)
01/15/2023 08:45:46 - INFO - __main__ -   test: [batch: 334/391 ] | Loss: 1.020 | Acc: 74.573% (31977/42880)/ 92.076% (39482/42880)
01/15/2023 08:45:48 - INFO - __main__ -   test: [batch: 335/391 ] | Loss: 1.021 | Acc: 74.547% (32061/43008)/ 92.050% (39589/43008)
01/15/2023 08:45:50 - INFO - __main__ -   test: [batch: 336/391 ] | Loss: 1.023 | Acc: 74.504% (32138/43136)/ 92.028% (39697/43136)
01/15/2023 08:45:52 - INFO - __main__ -   test: [batch: 337/391 ] | Loss: 1.023 | Acc: 74.501% (32232/43264)/ 92.019% (39811/43264)
01/15/2023 08:45:54 - INFO - __main__ -   test: [batch: 338/391 ] | Loss: 1.023 | Acc: 74.502% (32328/43392)/ 92.031% (39934/43392)
01/15/2023 08:45:57 - INFO - __main__ -   test: [batch: 339/391 ] | Loss: 1.026 | Acc: 74.467% (32408/43520)/ 91.999% (40038/43520)
01/15/2023 08:45:59 - INFO - __main__ -   test: [batch: 340/391 ] | Loss: 1.025 | Acc: 74.466% (32503/43648)/ 92.011% (40161/43648)
01/15/2023 08:46:01 - INFO - __main__ -   test: [batch: 341/391 ] | Loss: 1.024 | Acc: 74.511% (32618/43776)/ 92.030% (40287/43776)
01/15/2023 08:46:03 - INFO - __main__ -   test: [batch: 342/391 ] | Loss: 1.024 | Acc: 74.467% (32694/43904)/ 92.019% (40400/43904)
01/15/2023 08:46:05 - INFO - __main__ -   test: [batch: 343/391 ] | Loss: 1.024 | Acc: 74.464% (32788/44032)/ 92.017% (40517/44032)
01/15/2023 08:46:07 - INFO - __main__ -   test: [batch: 344/391 ] | Loss: 1.025 | Acc: 74.466% (32884/44160)/ 92.004% (40629/44160)
01/15/2023 08:46:09 - INFO - __main__ -   test: [batch: 345/391 ] | Loss: 1.028 | Acc: 74.395% (32948/44288)/ 91.971% (40732/44288)
01/15/2023 08:46:11 - INFO - __main__ -   test: [batch: 346/391 ] | Loss: 1.029 | Acc: 74.381% (33037/44416)/ 91.962% (40846/44416)
01/15/2023 08:46:13 - INFO - __main__ -   test: [batch: 347/391 ] | Loss: 1.029 | Acc: 74.392% (33137/44544)/ 91.963% (40964/44544)
01/15/2023 08:46:15 - INFO - __main__ -   test: [batch: 348/391 ] | Loss: 1.030 | Acc: 74.366% (33221/44672)/ 91.943% (41073/44672)
01/15/2023 08:46:17 - INFO - __main__ -   test: [batch: 349/391 ] | Loss: 1.030 | Acc: 74.366% (33316/44800)/ 91.953% (41195/44800)
01/15/2023 08:46:19 - INFO - __main__ -   test: [batch: 350/391 ] | Loss: 1.030 | Acc: 74.375% (33415/44928)/ 91.952% (41312/44928)
01/15/2023 08:46:21 - INFO - __main__ -   test: [batch: 351/391 ] | Loss: 1.032 | Acc: 74.319% (33485/45056)/ 91.932% (41421/45056)
01/15/2023 08:46:23 - INFO - __main__ -   test: [batch: 352/391 ] | Loss: 1.032 | Acc: 74.316% (33579/45184)/ 91.931% (41538/45184)
01/15/2023 08:46:25 - INFO - __main__ -   test: [batch: 353/391 ] | Loss: 1.035 | Acc: 74.270% (33653/45312)/ 91.885% (41635/45312)
01/15/2023 08:46:28 - INFO - __main__ -   test: [batch: 354/391 ] | Loss: 1.038 | Acc: 74.212% (33722/45440)/ 91.868% (41745/45440)
01/15/2023 08:46:30 - INFO - __main__ -   test: [batch: 355/391 ] | Loss: 1.040 | Acc: 74.149% (33788/45568)/ 91.856% (41857/45568)
01/15/2023 08:46:32 - INFO - __main__ -   test: [batch: 356/391 ] | Loss: 1.041 | Acc: 74.138% (33878/45696)/ 91.861% (41977/45696)
01/15/2023 08:46:34 - INFO - __main__ -   test: [batch: 357/391 ] | Loss: 1.039 | Acc: 74.175% (33990/45824)/ 91.875% (42101/45824)
01/15/2023 08:46:36 - INFO - __main__ -   test: [batch: 358/391 ] | Loss: 1.038 | Acc: 74.201% (34097/45952)/ 91.878% (42220/45952)
01/15/2023 08:46:38 - INFO - __main__ -   test: [batch: 359/391 ] | Loss: 1.039 | Acc: 74.204% (34193/46080)/ 91.875% (42336/46080)
01/15/2023 08:46:40 - INFO - __main__ -   test: [batch: 360/391 ] | Loss: 1.040 | Acc: 74.167% (34271/46208)/ 91.876% (42454/46208)
01/15/2023 08:46:42 - INFO - __main__ -   test: [batch: 361/391 ] | Loss: 1.040 | Acc: 74.169% (34367/46336)/ 91.877% (42572/46336)
01/15/2023 08:46:45 - INFO - __main__ -   test: [batch: 362/391 ] | Loss: 1.040 | Acc: 74.165% (34460/46464)/ 91.888% (42695/46464)
01/15/2023 08:46:47 - INFO - __main__ -   test: [batch: 363/391 ] | Loss: 1.041 | Acc: 74.141% (34544/46592)/ 91.874% (42806/46592)
01/15/2023 08:46:49 - INFO - __main__ -   test: [batch: 364/391 ] | Loss: 1.040 | Acc: 74.167% (34651/46720)/ 91.884% (42928/46720)
01/15/2023 08:46:51 - INFO - __main__ -   test: [batch: 365/391 ] | Loss: 1.040 | Acc: 74.170% (34747/46848)/ 91.882% (43045/46848)
01/15/2023 08:46:53 - INFO - __main__ -   test: [batch: 366/391 ] | Loss: 1.038 | Acc: 74.204% (34858/46976)/ 91.900% (43171/46976)
01/15/2023 08:46:55 - INFO - __main__ -   test: [batch: 367/391 ] | Loss: 1.037 | Acc: 74.223% (34962/47104)/ 91.909% (43293/47104)
01/15/2023 08:46:57 - INFO - __main__ -   test: [batch: 368/391 ] | Loss: 1.037 | Acc: 74.217% (35054/47232)/ 91.919% (43415/47232)
01/15/2023 08:46:59 - INFO - __main__ -   test: [batch: 369/391 ] | Loss: 1.036 | Acc: 74.240% (35160/47360)/ 91.930% (43538/47360)
01/15/2023 08:47:02 - INFO - __main__ -   test: [batch: 370/391 ] | Loss: 1.036 | Acc: 74.217% (35244/47488)/ 91.933% (43657/47488)
01/15/2023 08:47:04 - INFO - __main__ -   test: [batch: 371/391 ] | Loss: 1.036 | Acc: 74.231% (35346/47616)/ 91.931% (43774/47616)
01/15/2023 08:47:06 - INFO - __main__ -   test: [batch: 372/391 ] | Loss: 1.035 | Acc: 74.277% (35463/47744)/ 91.949% (43900/47744)
01/15/2023 08:47:08 - INFO - __main__ -   test: [batch: 373/391 ] | Loss: 1.033 | Acc: 74.311% (35574/47872)/ 91.960% (44023/47872)
01/15/2023 08:47:10 - INFO - __main__ -   test: [batch: 374/391 ] | Loss: 1.032 | Acc: 74.335% (35681/48000)/ 91.958% (44140/48000)
01/15/2023 08:47:12 - INFO - __main__ -   test: [batch: 375/391 ] | Loss: 1.035 | Acc: 74.277% (35748/48128)/ 91.919% (44239/48128)
01/15/2023 08:47:14 - INFO - __main__ -   test: [batch: 376/391 ] | Loss: 1.036 | Acc: 74.264% (35837/48256)/ 91.902% (44348/48256)
01/15/2023 08:47:16 - INFO - __main__ -   test: [batch: 377/391 ] | Loss: 1.037 | Acc: 74.260% (35930/48384)/ 91.894% (44462/48384)
01/15/2023 08:47:18 - INFO - __main__ -   test: [batch: 378/391 ] | Loss: 1.040 | Acc: 74.198% (35995/48512)/ 91.854% (44560/48512)
01/15/2023 08:47:21 - INFO - __main__ -   test: [batch: 379/391 ] | Loss: 1.040 | Acc: 74.190% (36086/48640)/ 91.867% (44684/48640)
01/15/2023 08:47:23 - INFO - __main__ -   test: [batch: 380/391 ] | Loss: 1.039 | Acc: 74.200% (36186/48768)/ 91.882% (44809/48768)
01/15/2023 08:47:25 - INFO - __main__ -   test: [batch: 381/391 ] | Loss: 1.041 | Acc: 74.155% (36259/48896)/ 91.885% (44928/48896)
01/15/2023 08:47:27 - INFO - __main__ -   test: [batch: 382/391 ] | Loss: 1.042 | Acc: 74.127% (36340/49024)/ 91.873% (45040/49024)
01/15/2023 08:47:29 - INFO - __main__ -   test: [batch: 383/391 ] | Loss: 1.042 | Acc: 74.131% (36437/49152)/ 91.866% (45154/49152)
01/15/2023 08:47:31 - INFO - __main__ -   test: [batch: 384/391 ] | Loss: 1.041 | Acc: 74.168% (36550/49280)/ 91.881% (45279/49280)
01/15/2023 08:47:33 - INFO - __main__ -   test: [batch: 385/391 ] | Loss: 1.039 | Acc: 74.182% (36652/49408)/ 91.894% (45403/49408)
01/15/2023 08:47:35 - INFO - __main__ -   test: [batch: 386/391 ] | Loss: 1.037 | Acc: 74.223% (36767/49536)/ 91.911% (45529/49536)
01/15/2023 08:47:37 - INFO - __main__ -   test: [batch: 387/391 ] | Loss: 1.036 | Acc: 74.267% (36884/49664)/ 91.924% (45653/49664)
01/15/2023 08:47:39 - INFO - __main__ -   test: [batch: 388/391 ] | Loss: 1.034 | Acc: 74.319% (37005/49792)/ 91.938% (45778/49792)
01/15/2023 08:47:41 - INFO - __main__ -   test: [batch: 389/391 ] | Loss: 1.033 | Acc: 74.321% (37101/49920)/ 91.943% (45898/49920)
01/15/2023 08:47:44 - INFO - __main__ -   test: [batch: 390/391 ] | Loss: 1.035 | Acc: 74.278% (37139/50000)/ 91.932% (45966/50000)
01/15/2023 08:47:44 - INFO - __main__ -   Final accuracy: 74.278
01/15/2023 08:47:44 - INFO - __main__ -   {'milestones': Counter({2: 1, 3: 1, 4: 1}), 'gamma': 0.1, 'base_lrs': [0.00025], 'last_epoch': 1, '_step_count': 2, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.00025]}
01/15/2023 08:47:44 - INFO - __main__ -   
Epoch: 1
01/15/2023 08:47:46 - INFO - __main__ -   test: [epoch: 1 | batch: 0/10010 ] | Loss: 0.703 | Acc: 84.375% (108/128)
01/15/2023 08:51:18 - INFO - __main__ -   test: [epoch: 1 | batch: 100/10010 ] | Loss: 0.934 | Acc: 77.205% (9981/12928)
01/15/2023 08:54:48 - INFO - __main__ -   test: [epoch: 1 | batch: 200/10010 ] | Loss: 0.931 | Acc: 77.013% (19814/25728)
01/15/2023 08:58:18 - INFO - __main__ -   test: [epoch: 1 | batch: 300/10010 ] | Loss: 0.946 | Acc: 76.672% (29540/38528)
01/15/2023 09:01:46 - INFO - __main__ -   test: [epoch: 1 | batch: 400/10010 ] | Loss: 0.957 | Acc: 76.368% (39198/51328)
01/15/2023 09:05:14 - INFO - __main__ -   test: [epoch: 1 | batch: 500/10010 ] | Loss: 0.958 | Acc: 76.297% (48928/64128)
01/15/2023 09:08:43 - INFO - __main__ -   test: [epoch: 1 | batch: 600/10010 ] | Loss: 0.961 | Acc: 76.280% (58681/76928)
01/15/2023 09:12:12 - INFO - __main__ -   test: [epoch: 1 | batch: 700/10010 ] | Loss: 0.961 | Acc: 76.255% (68422/89728)
01/15/2023 09:15:40 - INFO - __main__ -   test: [epoch: 1 | batch: 800/10010 ] | Loss: 0.960 | Acc: 76.279% (78207/102528)
01/15/2023 09:19:09 - INFO - __main__ -   test: [epoch: 1 | batch: 900/10010 ] | Loss: 0.960 | Acc: 76.295% (87989/115328)
01/15/2023 09:22:39 - INFO - __main__ -   test: [epoch: 1 | batch: 1000/10010 ] | Loss: 0.960 | Acc: 76.267% (97719/128128)
01/15/2023 09:26:07 - INFO - __main__ -   test: [epoch: 1 | batch: 1100/10010 ] | Loss: 0.963 | Acc: 76.234% (107435/140928)
01/15/2023 09:29:38 - INFO - __main__ -   test: [epoch: 1 | batch: 1200/10010 ] | Loss: 0.963 | Acc: 76.246% (117212/153728)
01/15/2023 09:33:07 - INFO - __main__ -   test: [epoch: 1 | batch: 1300/10010 ] | Loss: 0.959 | Acc: 76.339% (127126/166528)
01/15/2023 09:36:36 - INFO - __main__ -   test: [epoch: 1 | batch: 1400/10010 ] | Loss: 0.960 | Acc: 76.349% (136916/179328)
01/15/2023 09:40:05 - INFO - __main__ -   test: [epoch: 1 | batch: 1500/10010 ] | Loss: 0.959 | Acc: 76.320% (146632/192128)
01/15/2023 09:43:35 - INFO - __main__ -   test: [epoch: 1 | batch: 1600/10010 ] | Loss: 0.962 | Acc: 76.290% (156339/204928)
01/15/2023 09:47:03 - INFO - __main__ -   test: [epoch: 1 | batch: 1700/10010 ] | Loss: 0.961 | Acc: 76.318% (166165/217728)
01/15/2023 09:50:33 - INFO - __main__ -   test: [epoch: 1 | batch: 1800/10010 ] | Loss: 0.961 | Acc: 76.322% (175943/230528)
01/15/2023 09:54:04 - INFO - __main__ -   test: [epoch: 1 | batch: 1900/10010 ] | Loss: 0.960 | Acc: 76.334% (185743/243328)
01/15/2023 09:57:34 - INFO - __main__ -   test: [epoch: 1 | batch: 2000/10010 ] | Loss: 0.960 | Acc: 76.346% (195543/256128)
01/15/2023 10:01:02 - INFO - __main__ -   test: [epoch: 1 | batch: 2100/10010 ] | Loss: 0.959 | Acc: 76.343% (205308/268928)
01/15/2023 10:04:30 - INFO - __main__ -   test: [epoch: 1 | batch: 2200/10010 ] | Loss: 0.961 | Acc: 76.317% (215007/281728)
01/15/2023 10:07:59 - INFO - __main__ -   test: [epoch: 1 | batch: 2300/10010 ] | Loss: 0.961 | Acc: 76.321% (224787/294528)
01/15/2023 10:11:29 - INFO - __main__ -   test: [epoch: 1 | batch: 2400/10010 ] | Loss: 0.961 | Acc: 76.358% (234669/307328)
01/15/2023 10:14:58 - INFO - __main__ -   test: [epoch: 1 | batch: 2500/10010 ] | Loss: 0.962 | Acc: 76.329% (244352/320128)
01/15/2023 10:18:26 - INFO - __main__ -   test: [epoch: 1 | batch: 2600/10010 ] | Loss: 0.963 | Acc: 76.316% (254077/332928)
01/15/2023 10:21:55 - INFO - __main__ -   test: [epoch: 1 | batch: 2700/10010 ] | Loss: 0.963 | Acc: 76.316% (263847/345728)
01/15/2023 10:25:24 - INFO - __main__ -   test: [epoch: 1 | batch: 2800/10010 ] | Loss: 0.962 | Acc: 76.310% (273591/358528)
01/15/2023 10:28:53 - INFO - __main__ -   test: [epoch: 1 | batch: 2900/10010 ] | Loss: 0.963 | Acc: 76.310% (283362/371328)
01/15/2023 10:32:21 - INFO - __main__ -   test: [epoch: 1 | batch: 3000/10010 ] | Loss: 0.962 | Acc: 76.331% (293207/384128)
01/15/2023 10:35:51 - INFO - __main__ -   test: [epoch: 1 | batch: 3100/10010 ] | Loss: 0.962 | Acc: 76.326% (302958/396928)
01/15/2023 10:39:22 - INFO - __main__ -   test: [epoch: 1 | batch: 3200/10010 ] | Loss: 0.962 | Acc: 76.310% (312663/409728)
01/15/2023 10:42:50 - INFO - __main__ -   test: [epoch: 1 | batch: 3300/10010 ] | Loss: 0.963 | Acc: 76.300% (322390/422528)
01/15/2023 10:46:19 - INFO - __main__ -   test: [epoch: 1 | batch: 3400/10010 ] | Loss: 0.963 | Acc: 76.307% (332187/435328)
01/15/2023 10:49:47 - INFO - __main__ -   test: [epoch: 1 | batch: 3500/10010 ] | Loss: 0.963 | Acc: 76.305% (341943/448128)
01/15/2023 10:53:18 - INFO - __main__ -   test: [epoch: 1 | batch: 3600/10010 ] | Loss: 0.963 | Acc: 76.299% (351684/460928)
01/15/2023 10:56:48 - INFO - __main__ -   test: [epoch: 1 | batch: 3700/10010 ] | Loss: 0.964 | Acc: 76.281% (361364/473728)
01/15/2023 11:00:18 - INFO - __main__ -   test: [epoch: 1 | batch: 3800/10010 ] | Loss: 0.964 | Acc: 76.278% (371116/486528)
01/15/2023 11:03:49 - INFO - __main__ -   test: [epoch: 1 | batch: 3900/10010 ] | Loss: 0.963 | Acc: 76.285% (380914/499328)
01/15/2023 11:07:17 - INFO - __main__ -   test: [epoch: 1 | batch: 4000/10010 ] | Loss: 0.964 | Acc: 76.279% (390645/512128)
01/15/2023 11:10:46 - INFO - __main__ -   test: [epoch: 1 | batch: 4100/10010 ] | Loss: 0.963 | Acc: 76.287% (400453/524928)
01/15/2023 11:14:16 - INFO - __main__ -   test: [epoch: 1 | batch: 4200/10010 ] | Loss: 0.963 | Acc: 76.286% (410212/537728)
01/15/2023 11:17:43 - INFO - __main__ -   test: [epoch: 1 | batch: 4300/10010 ] | Loss: 0.963 | Acc: 76.292% (420008/550528)
01/15/2023 11:21:13 - INFO - __main__ -   test: [epoch: 1 | batch: 4400/10010 ] | Loss: 0.963 | Acc: 76.292% (429775/563328)
01/15/2023 11:24:43 - INFO - __main__ -   test: [epoch: 1 | batch: 4500/10010 ] | Loss: 0.963 | Acc: 76.280% (439473/576128)
01/15/2023 11:28:11 - INFO - __main__ -   test: [epoch: 1 | batch: 4600/10010 ] | Loss: 0.963 | Acc: 76.276% (449211/588928)
01/15/2023 11:31:42 - INFO - __main__ -   test: [epoch: 1 | batch: 4700/10010 ] | Loss: 0.963 | Acc: 76.287% (459041/601728)
01/15/2023 11:35:11 - INFO - __main__ -   test: [epoch: 1 | batch: 4800/10010 ] | Loss: 0.962 | Acc: 76.299% (468877/614528)
01/15/2023 11:38:38 - INFO - __main__ -   test: [epoch: 1 | batch: 4900/10010 ] | Loss: 0.962 | Acc: 76.294% (478611/627328)
01/15/2023 11:42:06 - INFO - __main__ -   test: [epoch: 1 | batch: 5000/10010 ] | Loss: 0.962 | Acc: 76.307% (488464/640128)
01/15/2023 11:45:35 - INFO - __main__ -   test: [epoch: 1 | batch: 5100/10010 ] | Loss: 0.962 | Acc: 76.304% (498211/652928)
01/15/2023 11:49:03 - INFO - __main__ -   test: [epoch: 1 | batch: 5200/10010 ] | Loss: 0.962 | Acc: 76.294% (507913/665728)
01/15/2023 11:52:31 - INFO - __main__ -   test: [epoch: 1 | batch: 5300/10010 ] | Loss: 0.962 | Acc: 76.296% (517692/678528)
01/15/2023 11:56:01 - INFO - __main__ -   test: [epoch: 1 | batch: 5400/10010 ] | Loss: 0.962 | Acc: 76.296% (527457/691328)
01/15/2023 11:59:30 - INFO - __main__ -   test: [epoch: 1 | batch: 5500/10010 ] | Loss: 0.963 | Acc: 76.293% (537199/704128)
01/15/2023 12:02:57 - INFO - __main__ -   test: [epoch: 1 | batch: 5600/10010 ] | Loss: 0.962 | Acc: 76.304% (547046/716928)
01/15/2023 12:06:27 - INFO - __main__ -   test: [epoch: 1 | batch: 5700/10010 ] | Loss: 0.962 | Acc: 76.306% (556828/729728)
01/15/2023 12:09:55 - INFO - __main__ -   test: [epoch: 1 | batch: 5800/10010 ] | Loss: 0.962 | Acc: 76.316% (566667/742528)
01/15/2023 12:13:23 - INFO - __main__ -   test: [epoch: 1 | batch: 5900/10010 ] | Loss: 0.962 | Acc: 76.312% (576406/755328)
01/15/2023 12:16:51 - INFO - __main__ -   test: [epoch: 1 | batch: 6000/10010 ] | Loss: 0.962 | Acc: 76.316% (586203/768128)
01/15/2023 12:20:20 - INFO - __main__ -   test: [epoch: 1 | batch: 6100/10010 ] | Loss: 0.962 | Acc: 76.307% (595904/780928)
01/15/2023 12:23:49 - INFO - __main__ -   test: [epoch: 1 | batch: 6200/10010 ] | Loss: 0.962 | Acc: 76.315% (605730/793728)
01/15/2023 12:27:19 - INFO - __main__ -   test: [epoch: 1 | batch: 6300/10010 ] | Loss: 0.962 | Acc: 76.312% (615480/806528)
01/15/2023 12:30:47 - INFO - __main__ -   test: [epoch: 1 | batch: 6400/10010 ] | Loss: 0.962 | Acc: 76.299% (625137/819328)
01/15/2023 12:34:16 - INFO - __main__ -   test: [epoch: 1 | batch: 6500/10010 ] | Loss: 0.962 | Acc: 76.306% (634967/832128)
01/15/2023 12:37:44 - INFO - __main__ -   test: [epoch: 1 | batch: 6600/10010 ] | Loss: 0.962 | Acc: 76.313% (644790/844928)
01/15/2023 12:41:13 - INFO - __main__ -   test: [epoch: 1 | batch: 6700/10010 ] | Loss: 0.962 | Acc: 76.313% (654557/857728)
01/15/2023 12:44:41 - INFO - __main__ -   test: [epoch: 1 | batch: 6800/10010 ] | Loss: 0.962 | Acc: 76.307% (664272/870528)
01/15/2023 12:48:10 - INFO - __main__ -   test: [epoch: 1 | batch: 6900/10010 ] | Loss: 0.962 | Acc: 76.312% (674086/883328)
01/15/2023 12:51:36 - INFO - __main__ -   test: [epoch: 1 | batch: 7000/10010 ] | Loss: 0.962 | Acc: 76.305% (683788/896128)
01/15/2023 12:55:03 - INFO - __main__ -   test: [epoch: 1 | batch: 7100/10010 ] | Loss: 0.961 | Acc: 76.312% (693618/908928)
01/15/2023 12:58:31 - INFO - __main__ -   test: [epoch: 1 | batch: 7200/10010 ] | Loss: 0.961 | Acc: 76.315% (703421/921728)
01/15/2023 13:01:59 - INFO - __main__ -   test: [epoch: 1 | batch: 7300/10010 ] | Loss: 0.962 | Acc: 76.310% (713141/934528)
01/15/2023 13:05:29 - INFO - __main__ -   test: [epoch: 1 | batch: 7400/10010 ] | Loss: 0.962 | Acc: 76.299% (722806/947328)
01/15/2023 13:08:58 - INFO - __main__ -   test: [epoch: 1 | batch: 7500/10010 ] | Loss: 0.962 | Acc: 76.300% (732573/960128)
01/15/2023 13:12:27 - INFO - __main__ -   test: [epoch: 1 | batch: 7600/10010 ] | Loss: 0.962 | Acc: 76.293% (742278/972928)
01/15/2023 13:15:56 - INFO - __main__ -   test: [epoch: 1 | batch: 7700/10010 ] | Loss: 0.962 | Acc: 76.292% (752030/985728)
01/15/2023 13:19:25 - INFO - __main__ -   test: [epoch: 1 | batch: 7800/10010 ] | Loss: 0.963 | Acc: 76.286% (761737/998528)
01/15/2023 13:22:55 - INFO - __main__ -   test: [epoch: 1 | batch: 7900/10010 ] | Loss: 0.962 | Acc: 76.291% (771553/1011328)
01/15/2023 13:26:23 - INFO - __main__ -   test: [epoch: 1 | batch: 8000/10010 ] | Loss: 0.962 | Acc: 76.295% (781363/1024128)
01/15/2023 13:29:54 - INFO - __main__ -   test: [epoch: 1 | batch: 8100/10010 ] | Loss: 0.962 | Acc: 76.305% (791225/1036928)
01/15/2023 13:33:22 - INFO - __main__ -   test: [epoch: 1 | batch: 8200/10010 ] | Loss: 0.962 | Acc: 76.300% (800940/1049728)
01/15/2023 13:36:51 - INFO - __main__ -   test: [epoch: 1 | batch: 8300/10010 ] | Loss: 0.962 | Acc: 76.296% (810663/1062528)
01/15/2023 13:40:18 - INFO - __main__ -   test: [epoch: 1 | batch: 8400/10010 ] | Loss: 0.962 | Acc: 76.295% (820417/1075328)
01/15/2023 13:43:45 - INFO - __main__ -   test: [epoch: 1 | batch: 8500/10010 ] | Loss: 0.962 | Acc: 76.295% (830192/1088128)
01/15/2023 13:47:15 - INFO - __main__ -   test: [epoch: 1 | batch: 8600/10010 ] | Loss: 0.962 | Acc: 76.293% (839934/1100928)
01/15/2023 13:50:45 - INFO - __main__ -   test: [epoch: 1 | batch: 8700/10010 ] | Loss: 0.962 | Acc: 76.303% (849809/1113728)
01/15/2023 13:54:16 - INFO - __main__ -   test: [epoch: 1 | batch: 8800/10010 ] | Loss: 0.962 | Acc: 76.306% (859611/1126528)
01/15/2023 13:57:44 - INFO - __main__ -   test: [epoch: 1 | batch: 8900/10010 ] | Loss: 0.962 | Acc: 76.310% (869416/1139328)
01/15/2023 14:01:12 - INFO - __main__ -   test: [epoch: 1 | batch: 9000/10010 ] | Loss: 0.962 | Acc: 76.313% (879218/1152128)
01/15/2023 14:04:40 - INFO - __main__ -   test: [epoch: 1 | batch: 9100/10010 ] | Loss: 0.962 | Acc: 76.315% (889016/1164928)
01/15/2023 14:08:07 - INFO - __main__ -   test: [epoch: 1 | batch: 9200/10010 ] | Loss: 0.961 | Acc: 76.318% (898819/1177728)
01/15/2023 14:11:38 - INFO - __main__ -   test: [epoch: 1 | batch: 9300/10010 ] | Loss: 0.961 | Acc: 76.321% (908625/1190528)
01/15/2023 14:15:07 - INFO - __main__ -   test: [epoch: 1 | batch: 9400/10010 ] | Loss: 0.962 | Acc: 76.317% (918348/1203328)
01/15/2023 14:18:36 - INFO - __main__ -   test: [epoch: 1 | batch: 9500/10010 ] | Loss: 0.961 | Acc: 76.318% (928125/1216128)
01/15/2023 14:22:05 - INFO - __main__ -   test: [epoch: 1 | batch: 9600/10010 ] | Loss: 0.962 | Acc: 76.317% (937880/1228928)
01/15/2023 14:25:35 - INFO - __main__ -   test: [epoch: 1 | batch: 9700/10010 ] | Loss: 0.962 | Acc: 76.316% (947640/1241728)
01/15/2023 14:29:03 - INFO - __main__ -   test: [epoch: 1 | batch: 9800/10010 ] | Loss: 0.962 | Acc: 76.312% (957357/1254528)
01/15/2023 14:32:33 - INFO - __main__ -   test: [epoch: 1 | batch: 9900/10010 ] | Loss: 0.962 | Acc: 76.311% (967113/1267328)
01/15/2023 14:36:02 - INFO - __main__ -   test: [epoch: 1 | batch: 10000/10010 ] | Loss: 0.962 | Acc: 76.307% (976827/1280128)
01/15/2023 14:36:21 - INFO - __main__ -   Saving Checkpoint
01/15/2023 14:36:23 - INFO - __main__ -   test: [batch: 0/391 ] | Loss: 0.513 | Acc: 87.500% (112/128)/ 96.094% (123/128)
01/15/2023 14:36:25 - INFO - __main__ -   test: [batch: 1/391 ] | Loss: 0.554 | Acc: 84.375% (216/256)/ 97.266% (249/256)
01/15/2023 14:36:28 - INFO - __main__ -   test: [batch: 2/391 ] | Loss: 0.762 | Acc: 78.906% (303/384)/ 94.792% (364/384)
01/15/2023 14:36:30 - INFO - __main__ -   test: [batch: 3/391 ] | Loss: 0.698 | Acc: 81.641% (418/512)/ 95.117% (487/512)
01/15/2023 14:36:32 - INFO - __main__ -   test: [batch: 4/391 ] | Loss: 0.602 | Acc: 84.375% (540/640)/ 95.781% (613/640)
01/15/2023 14:36:34 - INFO - __main__ -   test: [batch: 5/391 ] | Loss: 0.530 | Acc: 86.068% (661/768)/ 96.484% (741/768)
01/15/2023 14:36:36 - INFO - __main__ -   test: [batch: 6/391 ] | Loss: 0.516 | Acc: 86.607% (776/896)/ 96.540% (865/896)
01/15/2023 14:36:38 - INFO - __main__ -   test: [batch: 7/391 ] | Loss: 0.494 | Acc: 87.500% (896/1024)/ 96.680% (990/1024)
01/15/2023 14:36:40 - INFO - __main__ -   test: [batch: 8/391 ] | Loss: 0.498 | Acc: 87.587% (1009/1152)/ 96.701% (1114/1152)
01/15/2023 14:36:42 - INFO - __main__ -   test: [batch: 9/391 ] | Loss: 0.473 | Acc: 88.203% (1129/1280)/ 96.875% (1240/1280)
01/15/2023 14:36:44 - INFO - __main__ -   test: [batch: 10/391 ] | Loss: 0.516 | Acc: 87.003% (1225/1408)/ 96.875% (1364/1408)
01/15/2023 14:36:47 - INFO - __main__ -   test: [batch: 11/391 ] | Loss: 0.521 | Acc: 87.109% (1338/1536)/ 96.680% (1485/1536)
01/15/2023 14:36:49 - INFO - __main__ -   test: [batch: 12/391 ] | Loss: 0.574 | Acc: 85.877% (1429/1664)/ 96.214% (1601/1664)
01/15/2023 14:36:51 - INFO - __main__ -   test: [batch: 13/391 ] | Loss: 0.634 | Acc: 84.319% (1511/1792)/ 95.424% (1710/1792)
01/15/2023 14:36:53 - INFO - __main__ -   test: [batch: 14/391 ] | Loss: 0.650 | Acc: 83.490% (1603/1920)/ 95.625% (1836/1920)
01/15/2023 14:36:55 - INFO - __main__ -   test: [batch: 15/391 ] | Loss: 0.658 | Acc: 82.861% (1697/2048)/ 95.703% (1960/2048)
01/15/2023 14:36:57 - INFO - __main__ -   test: [batch: 16/391 ] | Loss: 0.665 | Acc: 82.767% (1801/2176)/ 95.496% (2078/2176)
01/15/2023 14:36:59 - INFO - __main__ -   test: [batch: 17/391 ] | Loss: 0.690 | Acc: 82.205% (1894/2304)/ 95.052% (2190/2304)
01/15/2023 14:37:01 - INFO - __main__ -   test: [batch: 18/391 ] | Loss: 0.714 | Acc: 81.743% (1988/2432)/ 94.860% (2307/2432)
01/15/2023 14:37:03 - INFO - __main__ -   test: [batch: 19/391 ] | Loss: 0.724 | Acc: 81.484% (2086/2560)/ 94.648% (2423/2560)
01/15/2023 14:37:05 - INFO - __main__ -   test: [batch: 20/391 ] | Loss: 0.719 | Acc: 81.585% (2193/2688)/ 94.643% (2544/2688)
01/15/2023 14:37:07 - INFO - __main__ -   test: [batch: 21/391 ] | Loss: 0.755 | Acc: 80.682% (2272/2816)/ 94.460% (2660/2816)
01/15/2023 14:37:09 - INFO - __main__ -   test: [batch: 22/391 ] | Loss: 0.753 | Acc: 80.571% (2372/2944)/ 94.429% (2780/2944)
01/15/2023 14:37:12 - INFO - __main__ -   test: [batch: 23/391 ] | Loss: 0.796 | Acc: 79.818% (2452/3072)/ 94.141% (2892/3072)
01/15/2023 14:37:14 - INFO - __main__ -   test: [batch: 24/391 ] | Loss: 0.813 | Acc: 79.344% (2539/3200)/ 94.031% (3009/3200)
01/15/2023 14:37:16 - INFO - __main__ -   test: [batch: 25/391 ] | Loss: 0.840 | Acc: 78.636% (2617/3328)/ 93.780% (3121/3328)
01/15/2023 14:37:18 - INFO - __main__ -   test: [batch: 26/391 ] | Loss: 0.858 | Acc: 77.836% (2690/3456)/ 93.808% (3242/3456)
01/15/2023 14:37:20 - INFO - __main__ -   test: [batch: 27/391 ] | Loss: 0.839 | Acc: 78.348% (2808/3584)/ 93.862% (3364/3584)
01/15/2023 14:37:22 - INFO - __main__ -   test: [batch: 28/391 ] | Loss: 0.846 | Acc: 77.802% (2888/3712)/ 93.992% (3489/3712)
01/15/2023 14:37:24 - INFO - __main__ -   test: [batch: 29/391 ] | Loss: 0.836 | Acc: 77.969% (2994/3840)/ 94.141% (3615/3840)
01/15/2023 14:37:26 - INFO - __main__ -   test: [batch: 30/391 ] | Loss: 0.848 | Acc: 77.848% (3089/3968)/ 93.977% (3729/3968)
01/15/2023 14:37:28 - INFO - __main__ -   test: [batch: 31/391 ] | Loss: 0.842 | Acc: 78.125% (3200/4096)/ 94.092% (3854/4096)
01/15/2023 14:37:30 - INFO - __main__ -   test: [batch: 32/391 ] | Loss: 0.828 | Acc: 78.456% (3314/4224)/ 94.176% (3978/4224)
01/15/2023 14:37:32 - INFO - __main__ -   test: [batch: 33/391 ] | Loss: 0.822 | Acc: 78.585% (3420/4352)/ 94.233% (4101/4352)
01/15/2023 14:37:34 - INFO - __main__ -   test: [batch: 34/391 ] | Loss: 0.807 | Acc: 79.040% (3541/4480)/ 94.330% (4226/4480)
01/15/2023 14:37:36 - INFO - __main__ -   test: [batch: 35/391 ] | Loss: 0.794 | Acc: 79.405% (3659/4608)/ 94.379% (4349/4608)
01/15/2023 14:37:38 - INFO - __main__ -   test: [batch: 36/391 ] | Loss: 0.778 | Acc: 79.878% (3783/4736)/ 94.510% (4476/4736)
01/15/2023 14:37:40 - INFO - __main__ -   test: [batch: 37/391 ] | Loss: 0.768 | Acc: 80.201% (3901/4864)/ 94.593% (4601/4864)
01/15/2023 14:37:42 - INFO - __main__ -   test: [batch: 38/391 ] | Loss: 0.759 | Acc: 80.449% (4016/4992)/ 94.671% (4726/4992)
01/15/2023 14:37:45 - INFO - __main__ -   test: [batch: 39/391 ] | Loss: 0.753 | Acc: 80.625% (4128/5120)/ 94.746% (4851/5120)
01/15/2023 14:37:47 - INFO - __main__ -   test: [batch: 40/391 ] | Loss: 0.751 | Acc: 80.736% (4237/5248)/ 94.665% (4968/5248)
01/15/2023 14:37:49 - INFO - __main__ -   test: [batch: 41/391 ] | Loss: 0.752 | Acc: 80.897% (4349/5376)/ 94.587% (5085/5376)
01/15/2023 14:37:51 - INFO - __main__ -   test: [batch: 42/391 ] | Loss: 0.752 | Acc: 80.905% (4453/5504)/ 94.658% (5210/5504)
01/15/2023 14:37:53 - INFO - __main__ -   test: [batch: 43/391 ] | Loss: 0.751 | Acc: 80.877% (4555/5632)/ 94.602% (5328/5632)
01/15/2023 14:37:55 - INFO - __main__ -   test: [batch: 44/391 ] | Loss: 0.756 | Acc: 80.868% (4658/5760)/ 94.497% (5443/5760)
01/15/2023 14:37:57 - INFO - __main__ -   test: [batch: 45/391 ] | Loss: 0.753 | Acc: 80.995% (4769/5888)/ 94.548% (5567/5888)
01/15/2023 14:38:00 - INFO - __main__ -   test: [batch: 46/391 ] | Loss: 0.757 | Acc: 80.918% (4868/6016)/ 94.614% (5692/6016)
01/15/2023 14:38:02 - INFO - __main__ -   test: [batch: 47/391 ] | Loss: 0.760 | Acc: 80.745% (4961/6144)/ 94.645% (5815/6144)
01/15/2023 14:38:04 - INFO - __main__ -   test: [batch: 48/391 ] | Loss: 0.766 | Acc: 80.644% (5058/6272)/ 94.627% (5935/6272)
01/15/2023 14:38:06 - INFO - __main__ -   test: [batch: 49/391 ] | Loss: 0.771 | Acc: 80.641% (5161/6400)/ 94.516% (6049/6400)
01/15/2023 14:38:08 - INFO - __main__ -   test: [batch: 50/391 ] | Loss: 0.762 | Acc: 80.821% (5276/6528)/ 94.593% (6175/6528)
01/15/2023 14:38:10 - INFO - __main__ -   test: [batch: 51/391 ] | Loss: 0.753 | Acc: 81.025% (5393/6656)/ 94.681% (6302/6656)
01/15/2023 14:38:12 - INFO - __main__ -   test: [batch: 52/391 ] | Loss: 0.750 | Acc: 81.103% (5502/6784)/ 94.723% (6426/6784)
01/15/2023 14:38:14 - INFO - __main__ -   test: [batch: 53/391 ] | Loss: 0.740 | Acc: 81.337% (5622/6912)/ 94.792% (6552/6912)
01/15/2023 14:38:17 - INFO - __main__ -   test: [batch: 54/391 ] | Loss: 0.733 | Acc: 81.491% (5737/7040)/ 94.815% (6675/7040)
01/15/2023 14:38:19 - INFO - __main__ -   test: [batch: 55/391 ] | Loss: 0.726 | Acc: 81.641% (5852/7168)/ 94.866% (6800/7168)
01/15/2023 14:38:21 - INFO - __main__ -   test: [batch: 56/391 ] | Loss: 0.718 | Acc: 81.880% (5974/7296)/ 94.942% (6927/7296)
01/15/2023 14:38:23 - INFO - __main__ -   test: [batch: 57/391 ] | Loss: 0.711 | Acc: 82.045% (6091/7424)/ 94.989% (7052/7424)
01/15/2023 14:38:25 - INFO - __main__ -   test: [batch: 58/391 ] | Loss: 0.706 | Acc: 82.150% (6204/7552)/ 94.981% (7173/7552)
01/15/2023 14:38:27 - INFO - __main__ -   test: [batch: 59/391 ] | Loss: 0.708 | Acc: 82.096% (6305/7680)/ 94.974% (7294/7680)
01/15/2023 14:38:29 - INFO - __main__ -   test: [batch: 60/391 ] | Loss: 0.711 | Acc: 82.057% (6407/7808)/ 94.967% (7415/7808)
01/15/2023 14:38:31 - INFO - __main__ -   test: [batch: 61/391 ] | Loss: 0.711 | Acc: 82.056% (6512/7936)/ 94.972% (7537/7936)
01/15/2023 14:38:33 - INFO - __main__ -   test: [batch: 62/391 ] | Loss: 0.709 | Acc: 82.019% (6614/8064)/ 95.015% (7662/8064)
01/15/2023 14:38:35 - INFO - __main__ -   test: [batch: 63/391 ] | Loss: 0.716 | Acc: 81.885% (6708/8192)/ 94.958% (7779/8192)
01/15/2023 14:38:37 - INFO - __main__ -   test: [batch: 64/391 ] | Loss: 0.724 | Acc: 81.647% (6793/8320)/ 94.916% (7897/8320)
01/15/2023 14:38:39 - INFO - __main__ -   test: [batch: 65/391 ] | Loss: 0.736 | Acc: 81.203% (6860/8448)/ 94.839% (8012/8448)
01/15/2023 14:38:41 - INFO - __main__ -   test: [batch: 66/391 ] | Loss: 0.741 | Acc: 81.203% (6964/8576)/ 94.799% (8130/8576)
01/15/2023 14:38:44 - INFO - __main__ -   test: [batch: 67/391 ] | Loss: 0.744 | Acc: 81.158% (7064/8704)/ 94.807% (8252/8704)
01/15/2023 14:38:46 - INFO - __main__ -   test: [batch: 68/391 ] | Loss: 0.745 | Acc: 81.091% (7162/8832)/ 94.814% (8374/8832)
01/15/2023 14:38:48 - INFO - __main__ -   test: [batch: 69/391 ] | Loss: 0.741 | Acc: 81.172% (7273/8960)/ 94.866% (8500/8960)
01/15/2023 14:38:50 - INFO - __main__ -   test: [batch: 70/391 ] | Loss: 0.741 | Acc: 81.096% (7370/9088)/ 94.894% (8624/9088)
01/15/2023 14:38:52 - INFO - __main__ -   test: [batch: 71/391 ] | Loss: 0.738 | Acc: 81.163% (7480/9216)/ 94.889% (8745/9216)
01/15/2023 14:38:54 - INFO - __main__ -   test: [batch: 72/391 ] | Loss: 0.741 | Acc: 81.015% (7570/9344)/ 94.906% (8868/9344)
01/15/2023 14:38:56 - INFO - __main__ -   test: [batch: 73/391 ] | Loss: 0.745 | Acc: 80.891% (7662/9472)/ 94.901% (8989/9472)
01/15/2023 14:38:58 - INFO - __main__ -   test: [batch: 74/391 ] | Loss: 0.746 | Acc: 80.885% (7765/9600)/ 94.885% (9109/9600)
01/15/2023 14:39:00 - INFO - __main__ -   test: [batch: 75/391 ] | Loss: 0.752 | Acc: 80.705% (7851/9728)/ 94.870% (9229/9728)
01/15/2023 14:39:02 - INFO - __main__ -   test: [batch: 76/391 ] | Loss: 0.751 | Acc: 80.722% (7956/9856)/ 94.897% (9353/9856)
01/15/2023 14:39:04 - INFO - __main__ -   test: [batch: 77/391 ] | Loss: 0.751 | Acc: 80.629% (8050/9984)/ 94.932% (9478/9984)
01/15/2023 14:39:06 - INFO - __main__ -   test: [batch: 78/391 ] | Loss: 0.750 | Acc: 80.597% (8150/10112)/ 94.976% (9604/10112)
01/15/2023 14:39:09 - INFO - __main__ -   test: [batch: 79/391 ] | Loss: 0.748 | Acc: 80.586% (8252/10240)/ 95.010% (9729/10240)
01/15/2023 14:39:11 - INFO - __main__ -   test: [batch: 80/391 ] | Loss: 0.749 | Acc: 80.556% (8352/10368)/ 94.994% (9849/10368)
01/15/2023 14:39:13 - INFO - __main__ -   test: [batch: 81/391 ] | Loss: 0.748 | Acc: 80.564% (8456/10496)/ 95.017% (9973/10496)
01/15/2023 14:39:15 - INFO - __main__ -   test: [batch: 82/391 ] | Loss: 0.747 | Acc: 80.582% (8561/10624)/ 95.030% (10096/10624)
01/15/2023 14:39:17 - INFO - __main__ -   test: [batch: 83/391 ] | Loss: 0.747 | Acc: 80.618% (8668/10752)/ 95.006% (10215/10752)
01/15/2023 14:39:19 - INFO - __main__ -   test: [batch: 84/391 ] | Loss: 0.743 | Acc: 80.744% (8785/10880)/ 95.037% (10340/10880)
01/15/2023 14:39:21 - INFO - __main__ -   test: [batch: 85/391 ] | Loss: 0.741 | Acc: 80.732% (8887/11008)/ 95.085% (10467/11008)
01/15/2023 14:39:23 - INFO - __main__ -   test: [batch: 86/391 ] | Loss: 0.745 | Acc: 80.648% (8981/11136)/ 95.034% (10583/11136)
01/15/2023 14:39:25 - INFO - __main__ -   test: [batch: 87/391 ] | Loss: 0.744 | Acc: 80.700% (9090/11264)/ 95.020% (10703/11264)
01/15/2023 14:39:27 - INFO - __main__ -   test: [batch: 88/391 ] | Loss: 0.751 | Acc: 80.618% (9184/11392)/ 94.961% (10818/11392)
01/15/2023 14:39:29 - INFO - __main__ -   test: [batch: 89/391 ] | Loss: 0.749 | Acc: 80.668% (9293/11520)/ 94.983% (10942/11520)
01/15/2023 14:39:31 - INFO - __main__ -   test: [batch: 90/391 ] | Loss: 0.750 | Acc: 80.580% (9386/11648)/ 94.995% (11065/11648)
01/15/2023 14:39:33 - INFO - __main__ -   test: [batch: 91/391 ] | Loss: 0.748 | Acc: 80.605% (9492/11776)/ 94.998% (11187/11776)
01/15/2023 14:39:35 - INFO - __main__ -   test: [batch: 92/391 ] | Loss: 0.750 | Acc: 80.553% (9589/11904)/ 94.976% (11306/11904)
01/15/2023 14:39:37 - INFO - __main__ -   test: [batch: 93/391 ] | Loss: 0.752 | Acc: 80.352% (9668/12032)/ 95.013% (11432/12032)
01/15/2023 14:39:40 - INFO - __main__ -   test: [batch: 94/391 ] | Loss: 0.755 | Acc: 80.222% (9755/12160)/ 95.033% (11556/12160)
01/15/2023 14:39:42 - INFO - __main__ -   test: [batch: 95/391 ] | Loss: 0.753 | Acc: 80.290% (9866/12288)/ 95.060% (11681/12288)
01/15/2023 14:39:44 - INFO - __main__ -   test: [batch: 96/391 ] | Loss: 0.757 | Acc: 80.130% (9949/12416)/ 95.063% (11803/12416)
01/15/2023 14:39:46 - INFO - __main__ -   test: [batch: 97/391 ] | Loss: 0.759 | Acc: 79.943% (10028/12544)/ 95.081% (11927/12544)
01/15/2023 14:39:48 - INFO - __main__ -   test: [batch: 98/391 ] | Loss: 0.756 | Acc: 80.043% (10143/12672)/ 95.092% (12050/12672)
01/15/2023 14:39:50 - INFO - __main__ -   test: [batch: 99/391 ] | Loss: 0.750 | Acc: 80.211% (10267/12800)/ 95.133% (12177/12800)
01/15/2023 14:39:52 - INFO - __main__ -   test: [batch: 100/391 ] | Loss: 0.749 | Acc: 80.275% (10378/12928)/ 95.150% (12301/12928)
01/15/2023 14:39:54 - INFO - __main__ -   test: [batch: 101/391 ] | Loss: 0.744 | Acc: 80.384% (10495/13056)/ 95.182% (12427/13056)
01/15/2023 14:39:56 - INFO - __main__ -   test: [batch: 102/391 ] | Loss: 0.741 | Acc: 80.469% (10609/13184)/ 95.214% (12553/13184)
01/15/2023 14:39:58 - INFO - __main__ -   test: [batch: 103/391 ] | Loss: 0.743 | Acc: 80.334% (10694/13312)/ 95.222% (12676/13312)
01/15/2023 14:40:00 - INFO - __main__ -   test: [batch: 104/391 ] | Loss: 0.742 | Acc: 80.275% (10789/13440)/ 95.231% (12799/13440)
01/15/2023 14:40:02 - INFO - __main__ -   test: [batch: 105/391 ] | Loss: 0.743 | Acc: 80.270% (10891/13568)/ 95.231% (12921/13568)
01/15/2023 14:40:04 - INFO - __main__ -   test: [batch: 106/391 ] | Loss: 0.751 | Acc: 80.199% (10984/13696)/ 95.174% (13035/13696)
01/15/2023 14:40:06 - INFO - __main__ -   test: [batch: 107/391 ] | Loss: 0.747 | Acc: 80.324% (11104/13824)/ 95.211% (13162/13824)
01/15/2023 14:40:08 - INFO - __main__ -   test: [batch: 108/391 ] | Loss: 0.750 | Acc: 80.189% (11188/13952)/ 95.226% (13286/13952)
01/15/2023 14:40:11 - INFO - __main__ -   test: [batch: 109/391 ] | Loss: 0.750 | Acc: 80.185% (11290/14080)/ 95.227% (13408/14080)
01/15/2023 14:40:13 - INFO - __main__ -   test: [batch: 110/391 ] | Loss: 0.753 | Acc: 79.997% (11366/14208)/ 95.249% (13533/14208)
01/15/2023 14:40:15 - INFO - __main__ -   test: [batch: 111/391 ] | Loss: 0.756 | Acc: 79.953% (11462/14336)/ 95.215% (13650/14336)
01/15/2023 14:40:17 - INFO - __main__ -   test: [batch: 112/391 ] | Loss: 0.756 | Acc: 79.992% (11570/14464)/ 95.236% (13775/14464)
01/15/2023 14:40:19 - INFO - __main__ -   test: [batch: 113/391 ] | Loss: 0.755 | Acc: 80.030% (11678/14592)/ 95.251% (13899/14592)
01/15/2023 14:40:21 - INFO - __main__ -   test: [batch: 114/391 ] | Loss: 0.751 | Acc: 80.129% (11795/14720)/ 95.279% (14025/14720)
01/15/2023 14:40:23 - INFO - __main__ -   test: [batch: 115/391 ] | Loss: 0.748 | Acc: 80.220% (11911/14848)/ 95.299% (14150/14848)
01/15/2023 14:40:26 - INFO - __main__ -   test: [batch: 116/391 ] | Loss: 0.751 | Acc: 80.188% (12009/14976)/ 95.259% (14266/14976)
01/15/2023 14:40:28 - INFO - __main__ -   test: [batch: 117/391 ] | Loss: 0.750 | Acc: 80.230% (12118/15104)/ 95.279% (14391/15104)
01/15/2023 14:40:30 - INFO - __main__ -   test: [batch: 118/391 ] | Loss: 0.754 | Acc: 80.068% (12196/15232)/ 95.293% (14515/15232)
01/15/2023 14:40:32 - INFO - __main__ -   test: [batch: 119/391 ] | Loss: 0.752 | Acc: 80.117% (12306/15360)/ 95.319% (14641/15360)
01/15/2023 14:40:34 - INFO - __main__ -   test: [batch: 120/391 ] | Loss: 0.751 | Acc: 80.172% (12417/15488)/ 95.332% (14765/15488)
01/15/2023 14:40:36 - INFO - __main__ -   test: [batch: 121/391 ] | Loss: 0.756 | Acc: 80.040% (12499/15616)/ 95.287% (14880/15616)
01/15/2023 14:40:38 - INFO - __main__ -   test: [batch: 122/391 ] | Loss: 0.759 | Acc: 79.992% (12594/15744)/ 95.268% (14999/15744)
01/15/2023 14:40:40 - INFO - __main__ -   test: [batch: 123/391 ] | Loss: 0.760 | Acc: 80.015% (12700/15872)/ 95.262% (15120/15872)
01/15/2023 14:40:42 - INFO - __main__ -   test: [batch: 124/391 ] | Loss: 0.759 | Acc: 80.044% (12807/16000)/ 95.275% (15244/16000)
01/15/2023 14:40:44 - INFO - __main__ -   test: [batch: 125/391 ] | Loss: 0.755 | Acc: 80.140% (12925/16128)/ 95.300% (15370/16128)
01/15/2023 14:40:46 - INFO - __main__ -   test: [batch: 126/391 ] | Loss: 0.752 | Acc: 80.229% (13042/16256)/ 95.300% (15492/16256)
01/15/2023 14:40:49 - INFO - __main__ -   test: [batch: 127/391 ] | Loss: 0.750 | Acc: 80.298% (13156/16384)/ 95.312% (15616/16384)
01/15/2023 14:40:51 - INFO - __main__ -   test: [batch: 128/391 ] | Loss: 0.751 | Acc: 80.305% (13260/16512)/ 95.294% (15735/16512)
01/15/2023 14:40:53 - INFO - __main__ -   test: [batch: 129/391 ] | Loss: 0.749 | Acc: 80.349% (13370/16640)/ 95.312% (15860/16640)
01/15/2023 14:40:55 - INFO - __main__ -   test: [batch: 130/391 ] | Loss: 0.745 | Acc: 80.457% (13491/16768)/ 95.336% (15986/16768)
01/15/2023 14:40:57 - INFO - __main__ -   test: [batch: 131/391 ] | Loss: 0.745 | Acc: 80.504% (13602/16896)/ 95.348% (16110/16896)
01/15/2023 14:40:59 - INFO - __main__ -   test: [batch: 132/391 ] | Loss: 0.742 | Acc: 80.574% (13717/17024)/ 95.354% (16233/17024)
01/15/2023 14:41:01 - INFO - __main__ -   test: [batch: 133/391 ] | Loss: 0.744 | Acc: 80.533% (13813/17152)/ 95.347% (16354/17152)
01/15/2023 14:41:03 - INFO - __main__ -   test: [batch: 134/391 ] | Loss: 0.742 | Acc: 80.561% (13921/17280)/ 95.365% (16479/17280)
01/15/2023 14:41:05 - INFO - __main__ -   test: [batch: 135/391 ] | Loss: 0.743 | Acc: 80.549% (14022/17408)/ 95.370% (16602/17408)
01/15/2023 14:41:07 - INFO - __main__ -   test: [batch: 136/391 ] | Loss: 0.741 | Acc: 80.531% (14122/17536)/ 95.392% (16728/17536)
01/15/2023 14:41:09 - INFO - __main__ -   test: [batch: 137/391 ] | Loss: 0.738 | Acc: 80.571% (14232/17664)/ 95.426% (16856/17664)
01/15/2023 14:41:11 - INFO - __main__ -   test: [batch: 138/391 ] | Loss: 0.737 | Acc: 80.604% (14341/17792)/ 95.436% (16980/17792)
01/15/2023 14:41:13 - INFO - __main__ -   test: [batch: 139/391 ] | Loss: 0.742 | Acc: 80.463% (14419/17920)/ 95.430% (17101/17920)
01/15/2023 14:41:15 - INFO - __main__ -   test: [batch: 140/391 ] | Loss: 0.744 | Acc: 80.397% (14510/18048)/ 95.429% (17223/18048)
01/15/2023 14:41:17 - INFO - __main__ -   test: [batch: 141/391 ] | Loss: 0.742 | Acc: 80.441% (14621/18176)/ 95.434% (17346/18176)
01/15/2023 14:41:20 - INFO - __main__ -   test: [batch: 142/391 ] | Loss: 0.740 | Acc: 80.518% (14738/18304)/ 95.444% (17470/18304)
01/15/2023 14:41:22 - INFO - __main__ -   test: [batch: 143/391 ] | Loss: 0.740 | Acc: 80.550% (14847/18432)/ 95.437% (17591/18432)
01/15/2023 14:41:23 - INFO - __main__ -   test: [batch: 144/391 ] | Loss: 0.742 | Acc: 80.523% (14945/18560)/ 95.409% (17708/18560)
01/15/2023 14:41:26 - INFO - __main__ -   test: [batch: 145/391 ] | Loss: 0.745 | Acc: 80.512% (15046/18688)/ 95.387% (17826/18688)
01/15/2023 14:41:28 - INFO - __main__ -   test: [batch: 146/391 ] | Loss: 0.747 | Acc: 80.474% (15142/18816)/ 95.366% (17944/18816)
01/15/2023 14:41:30 - INFO - __main__ -   test: [batch: 147/391 ] | Loss: 0.747 | Acc: 80.485% (15247/18944)/ 95.349% (18063/18944)
01/15/2023 14:41:32 - INFO - __main__ -   test: [batch: 148/391 ] | Loss: 0.748 | Acc: 80.390% (15332/19072)/ 95.349% (18185/19072)
01/15/2023 14:41:34 - INFO - __main__ -   test: [batch: 149/391 ] | Loss: 0.752 | Acc: 80.323% (15422/19200)/ 95.302% (18298/19200)
01/15/2023 14:41:36 - INFO - __main__ -   test: [batch: 150/391 ] | Loss: 0.752 | Acc: 80.277% (15516/19328)/ 95.307% (18421/19328)
01/15/2023 14:41:38 - INFO - __main__ -   test: [batch: 151/391 ] | Loss: 0.751 | Acc: 80.320% (15627/19456)/ 95.307% (18543/19456)
01/15/2023 14:41:41 - INFO - __main__ -   test: [batch: 152/391 ] | Loss: 0.753 | Acc: 80.295% (15725/19584)/ 95.297% (18663/19584)
01/15/2023 14:41:42 - INFO - __main__ -   test: [batch: 153/391 ] | Loss: 0.751 | Acc: 80.357% (15840/19712)/ 95.307% (18787/19712)
01/15/2023 14:41:44 - INFO - __main__ -   test: [batch: 154/391 ] | Loss: 0.753 | Acc: 80.348% (15941/19840)/ 95.267% (18901/19840)
01/15/2023 14:41:46 - INFO - __main__ -   test: [batch: 155/391 ] | Loss: 0.754 | Acc: 80.354% (16045/19968)/ 95.257% (19021/19968)
01/15/2023 14:41:48 - INFO - __main__ -   test: [batch: 156/391 ] | Loss: 0.756 | Acc: 80.280% (16133/20096)/ 95.233% (19138/20096)
01/15/2023 14:41:51 - INFO - __main__ -   test: [batch: 157/391 ] | Loss: 0.759 | Acc: 80.251% (16230/20224)/ 95.209% (19255/20224)
01/15/2023 14:41:53 - INFO - __main__ -   test: [batch: 158/391 ] | Loss: 0.760 | Acc: 80.228% (16328/20352)/ 95.180% (19371/20352)
01/15/2023 14:41:55 - INFO - __main__ -   test: [batch: 159/391 ] | Loss: 0.761 | Acc: 80.220% (16429/20480)/ 95.186% (19494/20480)
01/15/2023 14:41:57 - INFO - __main__ -   test: [batch: 160/391 ] | Loss: 0.761 | Acc: 80.207% (16529/20608)/ 95.157% (19610/20608)
01/15/2023 14:41:59 - INFO - __main__ -   test: [batch: 161/391 ] | Loss: 0.771 | Acc: 79.953% (16579/20736)/ 95.057% (19711/20736)
01/15/2023 14:42:01 - INFO - __main__ -   test: [batch: 162/391 ] | Loss: 0.777 | Acc: 79.850% (16660/20864)/ 94.991% (19819/20864)
01/15/2023 14:42:03 - INFO - __main__ -   test: [batch: 163/391 ] | Loss: 0.780 | Acc: 79.788% (16749/20992)/ 94.979% (19938/20992)
01/15/2023 14:42:06 - INFO - __main__ -   test: [batch: 164/391 ] | Loss: 0.780 | Acc: 79.773% (16848/21120)/ 94.981% (20060/21120)
01/15/2023 14:42:08 - INFO - __main__ -   test: [batch: 165/391 ] | Loss: 0.782 | Acc: 79.706% (16936/21248)/ 94.969% (20179/21248)
01/15/2023 14:42:10 - INFO - __main__ -   test: [batch: 166/391 ] | Loss: 0.782 | Acc: 79.725% (17042/21376)/ 94.962% (20299/21376)
01/15/2023 14:42:12 - INFO - __main__ -   test: [batch: 167/391 ] | Loss: 0.784 | Acc: 79.692% (17137/21504)/ 94.936% (20415/21504)
01/15/2023 14:42:14 - INFO - __main__ -   test: [batch: 168/391 ] | Loss: 0.783 | Acc: 79.706% (17242/21632)/ 94.938% (20537/21632)
01/15/2023 14:42:16 - INFO - __main__ -   test: [batch: 169/391 ] | Loss: 0.786 | Acc: 79.637% (17329/21760)/ 94.899% (20650/21760)
01/15/2023 14:42:18 - INFO - __main__ -   test: [batch: 170/391 ] | Loss: 0.790 | Acc: 79.546% (17411/21888)/ 94.865% (20764/21888)
01/15/2023 14:42:20 - INFO - __main__ -   test: [batch: 171/391 ] | Loss: 0.794 | Acc: 79.501% (17503/22016)/ 94.836% (20879/22016)
01/15/2023 14:42:22 - INFO - __main__ -   test: [batch: 172/391 ] | Loss: 0.795 | Acc: 79.466% (17597/22144)/ 94.829% (20999/22144)
01/15/2023 14:42:24 - INFO - __main__ -   test: [batch: 173/391 ] | Loss: 0.797 | Acc: 79.423% (17689/22272)/ 94.801% (21114/22272)
01/15/2023 14:42:27 - INFO - __main__ -   test: [batch: 174/391 ] | Loss: 0.802 | Acc: 79.312% (17766/22400)/ 94.754% (21225/22400)
01/15/2023 14:42:29 - INFO - __main__ -   test: [batch: 175/391 ] | Loss: 0.800 | Acc: 79.359% (17878/22528)/ 94.767% (21349/22528)
01/15/2023 14:42:31 - INFO - __main__ -   test: [batch: 176/391 ] | Loss: 0.801 | Acc: 79.348% (17977/22656)/ 94.739% (21464/22656)
01/15/2023 14:42:33 - INFO - __main__ -   test: [batch: 177/391 ] | Loss: 0.805 | Acc: 79.271% (18061/22784)/ 94.698% (21576/22784)
01/15/2023 14:42:35 - INFO - __main__ -   test: [batch: 178/391 ] | Loss: 0.806 | Acc: 79.286% (18166/22912)/ 94.680% (21693/22912)
01/15/2023 14:42:37 - INFO - __main__ -   test: [batch: 179/391 ] | Loss: 0.810 | Acc: 79.223% (18253/23040)/ 94.627% (21802/23040)
01/15/2023 14:42:39 - INFO - __main__ -   test: [batch: 180/391 ] | Loss: 0.815 | Acc: 79.075% (18320/23168)/ 94.583% (21913/23168)
01/15/2023 14:42:41 - INFO - __main__ -   test: [batch: 181/391 ] | Loss: 0.822 | Acc: 78.949% (18392/23296)/ 94.505% (22016/23296)
01/15/2023 14:42:43 - INFO - __main__ -   test: [batch: 182/391 ] | Loss: 0.821 | Acc: 78.966% (18497/23424)/ 94.506% (22137/23424)
01/15/2023 14:42:45 - INFO - __main__ -   test: [batch: 183/391 ] | Loss: 0.828 | Acc: 78.838% (18568/23552)/ 94.391% (22231/23552)
01/15/2023 14:42:47 - INFO - __main__ -   test: [batch: 184/391 ] | Loss: 0.828 | Acc: 78.868% (18676/23680)/ 94.383% (22350/23680)
01/15/2023 14:42:50 - INFO - __main__ -   test: [batch: 185/391 ] | Loss: 0.827 | Acc: 78.877% (18779/23808)/ 94.384% (22471/23808)
01/15/2023 14:42:52 - INFO - __main__ -   test: [batch: 186/391 ] | Loss: 0.830 | Acc: 78.844% (18872/23936)/ 94.347% (22583/23936)
01/15/2023 14:42:54 - INFO - __main__ -   test: [batch: 187/391 ] | Loss: 0.834 | Acc: 78.719% (18943/24064)/ 94.332% (22700/24064)
01/15/2023 14:42:56 - INFO - __main__ -   test: [batch: 188/391 ] | Loss: 0.838 | Acc: 78.592% (19013/24192)/ 94.304% (22814/24192)
01/15/2023 14:42:58 - INFO - __main__ -   test: [batch: 189/391 ] | Loss: 0.839 | Acc: 78.520% (19096/24320)/ 94.305% (22935/24320)
01/15/2023 14:43:00 - INFO - __main__ -   test: [batch: 190/391 ] | Loss: 0.844 | Acc: 78.428% (19174/24448)/ 94.274% (23048/24448)
01/15/2023 14:43:02 - INFO - __main__ -   test: [batch: 191/391 ] | Loss: 0.847 | Acc: 78.398% (19267/24576)/ 94.246% (23162/24576)
01/15/2023 14:43:04 - INFO - __main__ -   test: [batch: 192/391 ] | Loss: 0.853 | Acc: 78.267% (19335/24704)/ 94.143% (23257/24704)
01/15/2023 14:43:06 - INFO - __main__ -   test: [batch: 193/391 ] | Loss: 0.853 | Acc: 78.290% (19441/24832)/ 94.137% (23376/24832)
01/15/2023 14:43:09 - INFO - __main__ -   test: [batch: 194/391 ] | Loss: 0.857 | Acc: 78.217% (19523/24960)/ 94.103% (23488/24960)
01/15/2023 14:43:11 - INFO - __main__ -   test: [batch: 195/391 ] | Loss: 0.862 | Acc: 78.121% (19599/25088)/ 94.037% (23592/25088)
01/15/2023 14:43:13 - INFO - __main__ -   test: [batch: 196/391 ] | Loss: 0.866 | Acc: 78.026% (19675/25216)/ 93.996% (23702/25216)
01/15/2023 14:43:15 - INFO - __main__ -   test: [batch: 197/391 ] | Loss: 0.870 | Acc: 77.932% (19751/25344)/ 93.979% (23818/25344)
01/15/2023 14:43:17 - INFO - __main__ -   test: [batch: 198/391 ] | Loss: 0.872 | Acc: 77.886% (19839/25472)/ 93.938% (23928/25472)
01/15/2023 14:43:19 - INFO - __main__ -   test: [batch: 199/391 ] | Loss: 0.872 | Acc: 77.855% (19931/25600)/ 93.953% (24052/25600)
01/15/2023 14:43:21 - INFO - __main__ -   test: [batch: 200/391 ] | Loss: 0.872 | Acc: 77.822% (20022/25728)/ 93.960% (24174/25728)
01/15/2023 14:43:23 - INFO - __main__ -   test: [batch: 201/391 ] | Loss: 0.876 | Acc: 77.738% (20100/25856)/ 93.932% (24287/25856)
01/15/2023 14:43:25 - INFO - __main__ -   test: [batch: 202/391 ] | Loss: 0.877 | Acc: 77.729% (20197/25984)/ 93.923% (24405/25984)
01/15/2023 14:43:27 - INFO - __main__ -   test: [batch: 203/391 ] | Loss: 0.877 | Acc: 77.723% (20295/26112)/ 93.919% (24524/26112)
01/15/2023 14:43:29 - INFO - __main__ -   test: [batch: 204/391 ] | Loss: 0.881 | Acc: 77.630% (20370/26240)/ 93.887% (24636/26240)
01/15/2023 14:43:32 - INFO - __main__ -   test: [batch: 205/391 ] | Loss: 0.883 | Acc: 77.556% (20450/26368)/ 93.860% (24749/26368)
01/15/2023 14:43:34 - INFO - __main__ -   test: [batch: 206/391 ] | Loss: 0.885 | Acc: 77.521% (20540/26496)/ 93.852% (24867/26496)
01/15/2023 14:43:36 - INFO - __main__ -   test: [batch: 207/391 ] | Loss: 0.889 | Acc: 77.441% (20618/26624)/ 93.818% (24978/26624)
01/15/2023 14:43:38 - INFO - __main__ -   test: [batch: 208/391 ] | Loss: 0.890 | Acc: 77.400% (20706/26752)/ 93.795% (25092/26752)
01/15/2023 14:43:40 - INFO - __main__ -   test: [batch: 209/391 ] | Loss: 0.890 | Acc: 77.400% (20805/26880)/ 93.817% (25218/26880)
01/15/2023 14:43:42 - INFO - __main__ -   test: [batch: 210/391 ] | Loss: 0.891 | Acc: 77.366% (20895/27008)/ 93.798% (25333/27008)
01/15/2023 14:43:44 - INFO - __main__ -   test: [batch: 211/391 ] | Loss: 0.894 | Acc: 77.311% (20979/27136)/ 93.757% (25442/27136)
01/15/2023 14:43:46 - INFO - __main__ -   test: [batch: 212/391 ] | Loss: 0.896 | Acc: 77.245% (21060/27264)/ 93.754% (25561/27264)
01/15/2023 14:43:48 - INFO - __main__ -   test: [batch: 213/391 ] | Loss: 0.896 | Acc: 77.252% (21161/27392)/ 93.757% (25682/27392)
01/15/2023 14:43:50 - INFO - __main__ -   test: [batch: 214/391 ] | Loss: 0.896 | Acc: 77.238% (21256/27520)/ 93.761% (25803/27520)
01/15/2023 14:43:52 - INFO - __main__ -   test: [batch: 215/391 ] | Loss: 0.897 | Acc: 77.214% (21348/27648)/ 93.739% (25917/27648)
01/15/2023 14:43:54 - INFO - __main__ -   test: [batch: 216/391 ] | Loss: 0.895 | Acc: 77.265% (21461/27776)/ 93.757% (26042/27776)
01/15/2023 14:43:56 - INFO - __main__ -   test: [batch: 217/391 ] | Loss: 0.900 | Acc: 77.200% (21542/27904)/ 93.700% (26146/27904)
01/15/2023 14:43:58 - INFO - __main__ -   test: [batch: 218/391 ] | Loss: 0.903 | Acc: 77.130% (21621/28032)/ 93.661% (26255/28032)
01/15/2023 14:44:00 - INFO - __main__ -   test: [batch: 219/391 ] | Loss: 0.901 | Acc: 77.166% (21730/28160)/ 93.675% (26379/28160)
01/15/2023 14:44:02 - INFO - __main__ -   test: [batch: 220/391 ] | Loss: 0.900 | Acc: 77.206% (21840/28288)/ 93.683% (26501/28288)
01/15/2023 14:44:04 - INFO - __main__ -   test: [batch: 221/391 ] | Loss: 0.902 | Acc: 77.171% (21929/28416)/ 93.662% (26615/28416)
01/15/2023 14:44:06 - INFO - __main__ -   test: [batch: 222/391 ] | Loss: 0.900 | Acc: 77.221% (22042/28544)/ 93.676% (26739/28544)
01/15/2023 14:44:08 - INFO - __main__ -   test: [batch: 223/391 ] | Loss: 0.900 | Acc: 77.211% (22138/28672)/ 93.663% (26855/28672)
01/15/2023 14:44:10 - INFO - __main__ -   test: [batch: 224/391 ] | Loss: 0.900 | Acc: 77.233% (22243/28800)/ 93.663% (26975/28800)
01/15/2023 14:44:12 - INFO - __main__ -   test: [batch: 225/391 ] | Loss: 0.900 | Acc: 77.230% (22341/28928)/ 93.667% (27096/28928)
01/15/2023 14:44:14 - INFO - __main__ -   test: [batch: 226/391 ] | Loss: 0.900 | Acc: 77.199% (22431/29056)/ 93.664% (27215/29056)
01/15/2023 14:44:17 - INFO - __main__ -   test: [batch: 227/391 ] | Loss: 0.901 | Acc: 77.200% (22530/29184)/ 93.657% (27333/29184)
01/15/2023 14:44:18 - INFO - __main__ -   test: [batch: 228/391 ] | Loss: 0.906 | Acc: 77.115% (22604/29312)/ 93.590% (27433/29312)
01/15/2023 14:44:21 - INFO - __main__ -   test: [batch: 229/391 ] | Loss: 0.910 | Acc: 77.035% (22679/29440)/ 93.539% (27538/29440)
01/15/2023 14:44:23 - INFO - __main__ -   test: [batch: 230/391 ] | Loss: 0.913 | Acc: 76.955% (22754/29568)/ 93.503% (27647/29568)
01/15/2023 14:44:25 - INFO - __main__ -   test: [batch: 231/391 ] | Loss: 0.913 | Acc: 76.950% (22851/29696)/ 93.497% (27765/29696)
01/15/2023 14:44:27 - INFO - __main__ -   test: [batch: 232/391 ] | Loss: 0.912 | Acc: 76.978% (22958/29824)/ 93.512% (27889/29824)
01/15/2023 14:44:29 - INFO - __main__ -   test: [batch: 233/391 ] | Loss: 0.914 | Acc: 76.940% (23045/29952)/ 93.486% (28001/29952)
01/15/2023 14:44:31 - INFO - __main__ -   test: [batch: 234/391 ] | Loss: 0.920 | Acc: 76.818% (23107/30080)/ 93.421% (28101/30080)
01/15/2023 14:44:33 - INFO - __main__ -   test: [batch: 235/391 ] | Loss: 0.920 | Acc: 76.811% (23203/30208)/ 93.416% (28219/30208)
01/15/2023 14:44:35 - INFO - __main__ -   test: [batch: 236/391 ] | Loss: 0.919 | Acc: 76.862% (23317/30336)/ 93.427% (28342/30336)
01/15/2023 14:44:37 - INFO - __main__ -   test: [batch: 237/391 ] | Loss: 0.919 | Acc: 76.861% (23415/30464)/ 93.399% (28453/30464)
01/15/2023 14:44:39 - INFO - __main__ -   test: [batch: 238/391 ] | Loss: 0.918 | Acc: 76.889% (23522/30592)/ 93.410% (28576/30592)
01/15/2023 14:44:42 - INFO - __main__ -   test: [batch: 239/391 ] | Loss: 0.917 | Acc: 76.917% (23629/30720)/ 93.415% (28697/30720)
01/15/2023 14:44:44 - INFO - __main__ -   test: [batch: 240/391 ] | Loss: 0.918 | Acc: 76.926% (23730/30848)/ 93.406% (28814/30848)
01/15/2023 14:44:46 - INFO - __main__ -   test: [batch: 241/391 ] | Loss: 0.921 | Acc: 76.840% (23802/30976)/ 93.363% (28920/30976)
01/15/2023 14:44:48 - INFO - __main__ -   test: [batch: 242/391 ] | Loss: 0.923 | Acc: 76.739% (23869/31104)/ 93.345% (29034/31104)
01/15/2023 14:44:50 - INFO - __main__ -   test: [batch: 243/391 ] | Loss: 0.929 | Acc: 76.627% (23932/31232)/ 93.276% (29132/31232)
01/15/2023 14:44:52 - INFO - __main__ -   test: [batch: 244/391 ] | Loss: 0.929 | Acc: 76.623% (24029/31360)/ 93.265% (29248/31360)
01/15/2023 14:44:54 - INFO - __main__ -   test: [batch: 245/391 ] | Loss: 0.929 | Acc: 76.626% (24128/31488)/ 93.258% (29365/31488)
01/15/2023 14:44:56 - INFO - __main__ -   test: [batch: 246/391 ] | Loss: 0.929 | Acc: 76.604% (24219/31616)/ 93.250% (29482/31616)
01/15/2023 14:44:58 - INFO - __main__ -   test: [batch: 247/391 ] | Loss: 0.935 | Acc: 76.512% (24288/31744)/ 93.186% (29581/31744)
01/15/2023 14:45:00 - INFO - __main__ -   test: [batch: 248/391 ] | Loss: 0.936 | Acc: 76.475% (24374/31872)/ 93.185% (29700/31872)
01/15/2023 14:45:03 - INFO - __main__ -   test: [batch: 249/391 ] | Loss: 0.938 | Acc: 76.322% (24423/32000)/ 93.181% (29818/32000)
01/15/2023 14:45:05 - INFO - __main__ -   test: [batch: 250/391 ] | Loss: 0.937 | Acc: 76.351% (24530/32128)/ 93.196% (29942/32128)
01/15/2023 14:45:07 - INFO - __main__ -   test: [batch: 251/391 ] | Loss: 0.939 | Acc: 76.308% (24614/32256)/ 93.170% (30053/32256)
01/15/2023 14:45:09 - INFO - __main__ -   test: [batch: 252/391 ] | Loss: 0.938 | Acc: 76.334% (24720/32384)/ 93.163% (30170/32384)
01/15/2023 14:45:11 - INFO - __main__ -   test: [batch: 253/391 ] | Loss: 0.940 | Acc: 76.295% (24805/32512)/ 93.144% (30283/32512)
01/15/2023 14:45:13 - INFO - __main__ -   test: [batch: 254/391 ] | Loss: 0.944 | Acc: 76.222% (24879/32640)/ 93.097% (30387/32640)
01/15/2023 14:45:15 - INFO - __main__ -   test: [batch: 255/391 ] | Loss: 0.946 | Acc: 76.196% (24968/32768)/ 93.091% (30504/32768)
01/15/2023 14:45:17 - INFO - __main__ -   test: [batch: 256/391 ] | Loss: 0.951 | Acc: 76.067% (25023/32896)/ 93.048% (30609/32896)
01/15/2023 14:45:19 - INFO - __main__ -   test: [batch: 257/391 ] | Loss: 0.951 | Acc: 76.057% (25117/33024)/ 93.044% (30727/33024)
01/15/2023 14:45:21 - INFO - __main__ -   test: [batch: 258/391 ] | Loss: 0.952 | Acc: 76.053% (25213/33152)/ 93.026% (30840/33152)
01/15/2023 14:45:23 - INFO - __main__ -   test: [batch: 259/391 ] | Loss: 0.956 | Acc: 75.925% (25268/33280)/ 93.020% (30957/33280)
01/15/2023 14:45:25 - INFO - __main__ -   test: [batch: 260/391 ] | Loss: 0.957 | Acc: 75.904% (25358/33408)/ 93.017% (31075/33408)
01/15/2023 14:45:27 - INFO - __main__ -   test: [batch: 261/391 ] | Loss: 0.954 | Acc: 75.945% (25469/33536)/ 93.040% (31202/33536)
01/15/2023 14:45:30 - INFO - __main__ -   test: [batch: 262/391 ] | Loss: 0.954 | Acc: 75.968% (25574/33664)/ 93.049% (31324/33664)
01/15/2023 14:45:32 - INFO - __main__ -   test: [batch: 263/391 ] | Loss: 0.956 | Acc: 75.906% (25650/33792)/ 93.025% (31435/33792)
01/15/2023 14:45:34 - INFO - __main__ -   test: [batch: 264/391 ] | Loss: 0.961 | Acc: 75.837% (25724/33920)/ 92.957% (31531/33920)
01/15/2023 14:45:36 - INFO - __main__ -   test: [batch: 265/391 ] | Loss: 0.961 | Acc: 75.869% (25832/34048)/ 92.957% (31650/34048)
01/15/2023 14:45:38 - INFO - __main__ -   test: [batch: 266/391 ] | Loss: 0.963 | Acc: 75.811% (25909/34176)/ 92.939% (31763/34176)
01/15/2023 14:45:40 - INFO - __main__ -   test: [batch: 267/391 ] | Loss: 0.962 | Acc: 75.840% (26016/34304)/ 92.943% (31883/34304)
01/15/2023 14:45:42 - INFO - __main__ -   test: [batch: 268/391 ] | Loss: 0.962 | Acc: 75.848% (26116/34432)/ 92.940% (32001/34432)
01/15/2023 14:45:44 - INFO - __main__ -   test: [batch: 269/391 ] | Loss: 0.964 | Acc: 75.799% (26196/34560)/ 92.922% (32114/34560)
01/15/2023 14:45:46 - INFO - __main__ -   test: [batch: 270/391 ] | Loss: 0.967 | Acc: 75.750% (26276/34688)/ 92.900% (32225/34688)
01/15/2023 14:45:48 - INFO - __main__ -   test: [batch: 271/391 ] | Loss: 0.967 | Acc: 75.764% (26378/34816)/ 92.897% (32343/34816)
01/15/2023 14:45:51 - INFO - __main__ -   test: [batch: 272/391 ] | Loss: 0.968 | Acc: 75.735% (26465/34944)/ 92.889% (32459/34944)
01/15/2023 14:45:53 - INFO - __main__ -   test: [batch: 273/391 ] | Loss: 0.968 | Acc: 75.747% (26566/35072)/ 92.866% (32570/35072)
01/15/2023 14:45:55 - INFO - __main__ -   test: [batch: 274/391 ] | Loss: 0.967 | Acc: 75.753% (26665/35200)/ 92.872% (32691/35200)
01/15/2023 14:45:57 - INFO - __main__ -   test: [batch: 275/391 ] | Loss: 0.969 | Acc: 75.713% (26748/35328)/ 92.864% (32807/35328)
01/15/2023 14:45:59 - INFO - __main__ -   test: [batch: 276/391 ] | Loss: 0.970 | Acc: 75.666% (26828/35456)/ 92.856% (32923/35456)
01/15/2023 14:46:01 - INFO - __main__ -   test: [batch: 277/391 ] | Loss: 0.971 | Acc: 75.638% (26915/35584)/ 92.842% (33037/35584)
01/15/2023 14:46:03 - INFO - __main__ -   test: [batch: 278/391 ] | Loss: 0.971 | Acc: 75.652% (27017/35712)/ 92.843% (33156/35712)
01/15/2023 14:46:05 - INFO - __main__ -   test: [batch: 279/391 ] | Loss: 0.971 | Acc: 75.672% (27121/35840)/ 92.838% (33273/35840)
01/15/2023 14:46:07 - INFO - __main__ -   test: [batch: 280/391 ] | Loss: 0.972 | Acc: 75.648% (27209/35968)/ 92.833% (33390/35968)
01/15/2023 14:46:09 - INFO - __main__ -   test: [batch: 281/391 ] | Loss: 0.973 | Acc: 75.646% (27305/36096)/ 92.819% (33504/36096)
01/15/2023 14:46:11 - INFO - __main__ -   test: [batch: 282/391 ] | Loss: 0.972 | Acc: 75.682% (27415/36224)/ 92.817% (33622/36224)
01/15/2023 14:46:13 - INFO - __main__ -   test: [batch: 283/391 ] | Loss: 0.972 | Acc: 75.663% (27505/36352)/ 92.809% (33738/36352)
01/15/2023 14:46:16 - INFO - __main__ -   test: [batch: 284/391 ] | Loss: 0.976 | Acc: 75.611% (27583/36480)/ 92.755% (33837/36480)
01/15/2023 14:46:18 - INFO - __main__ -   test: [batch: 285/391 ] | Loss: 0.979 | Acc: 75.579% (27668/36608)/ 92.709% (33939/36608)
01/15/2023 14:46:20 - INFO - __main__ -   test: [batch: 286/391 ] | Loss: 0.980 | Acc: 75.542% (27751/36736)/ 92.694% (34052/36736)
01/15/2023 14:46:22 - INFO - __main__ -   test: [batch: 287/391 ] | Loss: 0.981 | Acc: 75.532% (27844/36864)/ 92.681% (34166/36864)
01/15/2023 14:46:24 - INFO - __main__ -   test: [batch: 288/391 ] | Loss: 0.980 | Acc: 75.549% (27947/36992)/ 92.685% (34286/36992)
01/15/2023 14:46:26 - INFO - __main__ -   test: [batch: 289/391 ] | Loss: 0.981 | Acc: 75.520% (28033/37120)/ 92.651% (34392/37120)
01/15/2023 14:46:28 - INFO - __main__ -   test: [batch: 290/391 ] | Loss: 0.983 | Acc: 75.432% (28097/37248)/ 92.647% (34509/37248)
01/15/2023 14:46:30 - INFO - __main__ -   test: [batch: 291/391 ] | Loss: 0.983 | Acc: 75.439% (28196/37376)/ 92.648% (34628/37376)
01/15/2023 14:46:32 - INFO - __main__ -   test: [batch: 292/391 ] | Loss: 0.985 | Acc: 75.379% (28270/37504)/ 92.625% (34738/37504)
01/15/2023 14:46:34 - INFO - __main__ -   test: [batch: 293/391 ] | Loss: 0.986 | Acc: 75.353% (28357/37632)/ 92.623% (34856/37632)
01/15/2023 14:46:36 - INFO - __main__ -   test: [batch: 294/391 ] | Loss: 0.987 | Acc: 75.334% (28446/37760)/ 92.614% (34971/37760)
01/15/2023 14:46:38 - INFO - __main__ -   test: [batch: 295/391 ] | Loss: 0.986 | Acc: 75.354% (28550/37888)/ 92.610% (35088/37888)
01/15/2023 14:46:40 - INFO - __main__ -   test: [batch: 296/391 ] | Loss: 0.986 | Acc: 75.350% (28645/38016)/ 92.606% (35205/38016)
01/15/2023 14:46:42 - INFO - __main__ -   test: [batch: 297/391 ] | Loss: 0.988 | Acc: 75.328% (28733/38144)/ 92.583% (35315/38144)
01/15/2023 14:46:44 - INFO - __main__ -   test: [batch: 298/391 ] | Loss: 0.990 | Acc: 75.282% (28812/38272)/ 92.559% (35424/38272)
01/15/2023 14:46:46 - INFO - __main__ -   test: [batch: 299/391 ] | Loss: 0.992 | Acc: 75.268% (28903/38400)/ 92.534% (35533/38400)
01/15/2023 14:46:48 - INFO - __main__ -   test: [batch: 300/391 ] | Loss: 0.992 | Acc: 75.283% (29005/38528)/ 92.535% (35652/38528)
01/15/2023 14:46:50 - INFO - __main__ -   test: [batch: 301/391 ] | Loss: 0.993 | Acc: 75.269% (29096/38656)/ 92.524% (35766/38656)
01/15/2023 14:46:52 - INFO - __main__ -   test: [batch: 302/391 ] | Loss: 0.994 | Acc: 75.235% (29179/38784)/ 92.505% (35877/38784)
01/15/2023 14:46:55 - INFO - __main__ -   test: [batch: 303/391 ] | Loss: 0.996 | Acc: 75.211% (29266/38912)/ 92.488% (35989/38912)
01/15/2023 14:46:56 - INFO - __main__ -   test: [batch: 304/391 ] | Loss: 0.995 | Acc: 75.228% (29369/39040)/ 92.495% (36110/39040)
01/15/2023 14:46:58 - INFO - __main__ -   test: [batch: 305/391 ] | Loss: 0.995 | Acc: 75.189% (29450/39168)/ 92.491% (36227/39168)
01/15/2023 14:47:01 - INFO - __main__ -   test: [batch: 306/391 ] | Loss: 0.997 | Acc: 75.188% (29546/39296)/ 92.467% (36336/39296)
01/15/2023 14:47:03 - INFO - __main__ -   test: [batch: 307/391 ] | Loss: 0.998 | Acc: 75.157% (29630/39424)/ 92.454% (36449/39424)
01/15/2023 14:47:05 - INFO - __main__ -   test: [batch: 308/391 ] | Loss: 0.998 | Acc: 75.147% (29722/39552)/ 92.445% (36564/39552)
01/15/2023 14:47:07 - INFO - __main__ -   test: [batch: 309/391 ] | Loss: 1.000 | Acc: 75.134% (29813/39680)/ 92.422% (36673/39680)
01/15/2023 14:47:08 - INFO - __main__ -   test: [batch: 310/391 ] | Loss: 1.001 | Acc: 75.116% (29902/39808)/ 92.406% (36785/39808)
01/15/2023 14:47:10 - INFO - __main__ -   test: [batch: 311/391 ] | Loss: 1.002 | Acc: 75.103% (29993/39936)/ 92.390% (36897/39936)
01/15/2023 14:47:13 - INFO - __main__ -   test: [batch: 312/391 ] | Loss: 1.003 | Acc: 75.090% (30084/40064)/ 92.372% (37008/40064)
01/15/2023 14:47:15 - INFO - __main__ -   test: [batch: 313/391 ] | Loss: 1.001 | Acc: 75.144% (30202/40192)/ 92.396% (37136/40192)
01/15/2023 14:47:17 - INFO - __main__ -   test: [batch: 314/391 ] | Loss: 1.002 | Acc: 75.131% (30293/40320)/ 92.388% (37251/40320)
01/15/2023 14:47:19 - INFO - __main__ -   test: [batch: 315/391 ] | Loss: 1.002 | Acc: 75.119% (30384/40448)/ 92.373% (37363/40448)
01/15/2023 14:47:21 - INFO - __main__ -   test: [batch: 316/391 ] | Loss: 1.005 | Acc: 75.025% (30442/40576)/ 92.350% (37472/40576)
01/15/2023 14:47:23 - INFO - __main__ -   test: [batch: 317/391 ] | Loss: 1.007 | Acc: 74.980% (30520/40704)/ 92.320% (37578/40704)
01/15/2023 14:47:25 - INFO - __main__ -   test: [batch: 318/391 ] | Loss: 1.006 | Acc: 75.010% (30628/40832)/ 92.339% (37704/40832)
01/15/2023 14:47:27 - INFO - __main__ -   test: [batch: 319/391 ] | Loss: 1.008 | Acc: 74.958% (30703/40960)/ 92.314% (37812/40960)
01/15/2023 14:47:29 - INFO - __main__ -   test: [batch: 320/391 ] | Loss: 1.007 | Acc: 74.998% (30815/41088)/ 92.324% (37934/41088)
01/15/2023 14:47:31 - INFO - __main__ -   test: [batch: 321/391 ] | Loss: 1.007 | Acc: 75.015% (30918/41216)/ 92.319% (38050/41216)
01/15/2023 14:47:33 - INFO - __main__ -   test: [batch: 322/391 ] | Loss: 1.008 | Acc: 74.976% (30998/41344)/ 92.306% (38163/41344)
01/15/2023 14:47:35 - INFO - __main__ -   test: [batch: 323/391 ] | Loss: 1.010 | Acc: 74.942% (31080/41472)/ 92.279% (38270/41472)
01/15/2023 14:47:38 - INFO - __main__ -   test: [batch: 324/391 ] | Loss: 1.010 | Acc: 74.945% (31177/41600)/ 92.274% (38386/41600)
01/15/2023 14:47:40 - INFO - __main__ -   test: [batch: 325/391 ] | Loss: 1.010 | Acc: 74.947% (31274/41728)/ 92.281% (38507/41728)
01/15/2023 14:47:42 - INFO - __main__ -   test: [batch: 326/391 ] | Loss: 1.014 | Acc: 74.861% (31334/41856)/ 92.231% (38604/41856)
01/15/2023 14:47:44 - INFO - __main__ -   test: [batch: 327/391 ] | Loss: 1.017 | Acc: 74.776% (31394/41984)/ 92.190% (38705/41984)
01/15/2023 14:47:46 - INFO - __main__ -   test: [batch: 328/391 ] | Loss: 1.019 | Acc: 74.725% (31468/42112)/ 92.154% (38808/42112)
01/15/2023 14:47:48 - INFO - __main__ -   test: [batch: 329/391 ] | Loss: 1.020 | Acc: 74.728% (31565/42240)/ 92.157% (38927/42240)
01/15/2023 14:47:50 - INFO - __main__ -   test: [batch: 330/391 ] | Loss: 1.021 | Acc: 74.686% (31643/42368)/ 92.140% (39038/42368)
01/15/2023 14:47:52 - INFO - __main__ -   test: [batch: 331/391 ] | Loss: 1.021 | Acc: 74.652% (31724/42496)/ 92.155% (39162/42496)
01/15/2023 14:47:55 - INFO - __main__ -   test: [batch: 332/391 ] | Loss: 1.022 | Acc: 74.622% (31807/42624)/ 92.148% (39277/42624)
01/15/2023 14:47:57 - INFO - __main__ -   test: [batch: 333/391 ] | Loss: 1.021 | Acc: 74.656% (31917/42752)/ 92.155% (39398/42752)
01/15/2023 14:47:59 - INFO - __main__ -   test: [batch: 334/391 ] | Loss: 1.022 | Acc: 74.620% (31997/42880)/ 92.148% (39513/42880)
01/15/2023 14:48:01 - INFO - __main__ -   test: [batch: 335/391 ] | Loss: 1.023 | Acc: 74.591% (32080/43008)/ 92.127% (39622/43008)
01/15/2023 14:48:03 - INFO - __main__ -   test: [batch: 336/391 ] | Loss: 1.025 | Acc: 74.543% (32155/43136)/ 92.106% (39731/43136)
01/15/2023 14:48:05 - INFO - __main__ -   test: [batch: 337/391 ] | Loss: 1.025 | Acc: 74.540% (32249/43264)/ 92.097% (39845/43264)
01/15/2023 14:48:07 - INFO - __main__ -   test: [batch: 338/391 ] | Loss: 1.025 | Acc: 74.534% (32342/43392)/ 92.109% (39968/43392)
01/15/2023 14:48:09 - INFO - __main__ -   test: [batch: 339/391 ] | Loss: 1.028 | Acc: 74.499% (32422/43520)/ 92.082% (40074/43520)
01/15/2023 14:48:12 - INFO - __main__ -   test: [batch: 340/391 ] | Loss: 1.027 | Acc: 74.496% (32516/43648)/ 92.096% (40198/43648)
01/15/2023 14:48:14 - INFO - __main__ -   test: [batch: 341/391 ] | Loss: 1.025 | Acc: 74.539% (32630/43776)/ 92.114% (40324/43776)
01/15/2023 14:48:16 - INFO - __main__ -   test: [batch: 342/391 ] | Loss: 1.026 | Acc: 74.488% (32703/43904)/ 92.101% (40436/43904)
01/15/2023 14:48:18 - INFO - __main__ -   test: [batch: 343/391 ] | Loss: 1.026 | Acc: 74.482% (32796/44032)/ 92.099% (40553/44032)
01/15/2023 14:48:20 - INFO - __main__ -   test: [batch: 344/391 ] | Loss: 1.027 | Acc: 74.486% (32893/44160)/ 92.090% (40667/44160)
01/15/2023 14:48:22 - INFO - __main__ -   test: [batch: 345/391 ] | Loss: 1.030 | Acc: 74.415% (32957/44288)/ 92.052% (40768/44288)
01/15/2023 14:48:24 - INFO - __main__ -   test: [batch: 346/391 ] | Loss: 1.032 | Acc: 74.397% (33044/44416)/ 92.043% (40882/44416)
01/15/2023 14:48:26 - INFO - __main__ -   test: [batch: 347/391 ] | Loss: 1.031 | Acc: 74.407% (33144/44544)/ 92.046% (41001/44544)
01/15/2023 14:48:28 - INFO - __main__ -   test: [batch: 348/391 ] | Loss: 1.033 | Acc: 74.380% (33227/44672)/ 92.035% (41114/44672)
01/15/2023 14:48:30 - INFO - __main__ -   test: [batch: 349/391 ] | Loss: 1.032 | Acc: 74.377% (33321/44800)/ 92.049% (41238/44800)
01/15/2023 14:48:32 - INFO - __main__ -   test: [batch: 350/391 ] | Loss: 1.032 | Acc: 74.388% (33421/44928)/ 92.045% (41354/44928)
01/15/2023 14:48:34 - INFO - __main__ -   test: [batch: 351/391 ] | Loss: 1.034 | Acc: 74.330% (33490/45056)/ 92.028% (41464/45056)
01/15/2023 14:48:36 - INFO - __main__ -   test: [batch: 352/391 ] | Loss: 1.035 | Acc: 74.325% (33583/45184)/ 92.024% (41580/45184)
01/15/2023 14:48:38 - INFO - __main__ -   test: [batch: 353/391 ] | Loss: 1.038 | Acc: 74.281% (33658/45312)/ 91.980% (41678/45312)
01/15/2023 14:48:40 - INFO - __main__ -   test: [batch: 354/391 ] | Loss: 1.040 | Acc: 74.223% (33727/45440)/ 91.961% (41787/45440)
01/15/2023 14:48:42 - INFO - __main__ -   test: [batch: 355/391 ] | Loss: 1.042 | Acc: 74.159% (33793/45568)/ 91.955% (41902/45568)
01/15/2023 14:48:45 - INFO - __main__ -   test: [batch: 356/391 ] | Loss: 1.043 | Acc: 74.149% (33883/45696)/ 91.960% (42022/45696)
01/15/2023 14:48:47 - INFO - __main__ -   test: [batch: 357/391 ] | Loss: 1.041 | Acc: 74.184% (33994/45824)/ 91.974% (42146/45824)
01/15/2023 14:48:49 - INFO - __main__ -   test: [batch: 358/391 ] | Loss: 1.041 | Acc: 74.212% (34102/45952)/ 91.979% (42266/45952)
01/15/2023 14:48:51 - INFO - __main__ -   test: [batch: 359/391 ] | Loss: 1.041 | Acc: 74.219% (34200/46080)/ 91.975% (42382/46080)
01/15/2023 14:48:53 - INFO - __main__ -   test: [batch: 360/391 ] | Loss: 1.042 | Acc: 74.178% (34276/46208)/ 91.975% (42500/46208)
01/15/2023 14:48:55 - INFO - __main__ -   test: [batch: 361/391 ] | Loss: 1.043 | Acc: 74.180% (34372/46336)/ 91.976% (42618/46336)
01/15/2023 14:48:57 - INFO - __main__ -   test: [batch: 362/391 ] | Loss: 1.042 | Acc: 74.184% (34469/46464)/ 91.987% (42741/46464)
01/15/2023 14:49:00 - INFO - __main__ -   test: [batch: 363/391 ] | Loss: 1.043 | Acc: 74.161% (34553/46592)/ 91.975% (42853/46592)
01/15/2023 14:49:02 - INFO - __main__ -   test: [batch: 364/391 ] | Loss: 1.042 | Acc: 74.187% (34660/46720)/ 91.984% (42975/46720)
01/15/2023 14:49:04 - INFO - __main__ -   test: [batch: 365/391 ] | Loss: 1.042 | Acc: 74.189% (34756/46848)/ 91.985% (43093/46848)
01/15/2023 14:49:06 - INFO - __main__ -   test: [batch: 366/391 ] | Loss: 1.040 | Acc: 74.225% (34868/46976)/ 92.002% (43219/46976)
01/15/2023 14:49:08 - INFO - __main__ -   test: [batch: 367/391 ] | Loss: 1.039 | Acc: 74.240% (34970/47104)/ 92.011% (43341/47104)
01/15/2023 14:49:10 - INFO - __main__ -   test: [batch: 368/391 ] | Loss: 1.039 | Acc: 74.234% (35062/47232)/ 92.020% (43463/47232)
01/15/2023 14:49:12 - INFO - __main__ -   test: [batch: 369/391 ] | Loss: 1.038 | Acc: 74.255% (35167/47360)/ 92.031% (43586/47360)
01/15/2023 14:49:14 - INFO - __main__ -   test: [batch: 370/391 ] | Loss: 1.039 | Acc: 74.229% (35250/47488)/ 92.034% (43705/47488)
01/15/2023 14:49:16 - INFO - __main__ -   test: [batch: 371/391 ] | Loss: 1.038 | Acc: 74.246% (35353/47616)/ 92.034% (43823/47616)
01/15/2023 14:49:18 - INFO - __main__ -   test: [batch: 372/391 ] | Loss: 1.037 | Acc: 74.294% (35471/47744)/ 92.053% (43950/47744)
01/15/2023 14:49:20 - INFO - __main__ -   test: [batch: 373/391 ] | Loss: 1.035 | Acc: 74.329% (35583/47872)/ 92.066% (44074/47872)
01/15/2023 14:49:22 - INFO - __main__ -   test: [batch: 374/391 ] | Loss: 1.034 | Acc: 74.352% (35689/48000)/ 92.067% (44192/48000)
01/15/2023 14:49:24 - INFO - __main__ -   test: [batch: 375/391 ] | Loss: 1.037 | Acc: 74.300% (35759/48128)/ 92.032% (44293/48128)
01/15/2023 14:49:27 - INFO - __main__ -   test: [batch: 376/391 ] | Loss: 1.038 | Acc: 74.285% (35847/48256)/ 92.013% (44402/48256)
01/15/2023 14:49:29 - INFO - __main__ -   test: [batch: 377/391 ] | Loss: 1.039 | Acc: 74.281% (35940/48384)/ 92.006% (44516/48384)
01/15/2023 14:49:31 - INFO - __main__ -   test: [batch: 378/391 ] | Loss: 1.042 | Acc: 74.215% (36003/48512)/ 91.965% (44614/48512)
01/15/2023 14:49:33 - INFO - __main__ -   test: [batch: 379/391 ] | Loss: 1.042 | Acc: 74.202% (36092/48640)/ 91.978% (44738/48640)
01/15/2023 14:49:35 - INFO - __main__ -   test: [batch: 380/391 ] | Loss: 1.041 | Acc: 74.213% (36192/48768)/ 91.993% (44863/48768)
01/15/2023 14:49:37 - INFO - __main__ -   test: [batch: 381/391 ] | Loss: 1.043 | Acc: 74.166% (36264/48896)/ 91.991% (44980/48896)
01/15/2023 14:49:39 - INFO - __main__ -   test: [batch: 382/391 ] | Loss: 1.044 | Acc: 74.139% (36346/49024)/ 91.979% (45092/49024)
01/15/2023 14:49:41 - INFO - __main__ -   test: [batch: 383/391 ] | Loss: 1.044 | Acc: 74.146% (36444/49152)/ 91.972% (45206/49152)
01/15/2023 14:49:43 - INFO - __main__ -   test: [batch: 384/391 ] | Loss: 1.042 | Acc: 74.180% (36556/49280)/ 91.987% (45331/49280)
01/15/2023 14:49:45 - INFO - __main__ -   test: [batch: 385/391 ] | Loss: 1.041 | Acc: 74.194% (36658/49408)/ 92.001% (45456/49408)
01/15/2023 14:49:47 - INFO - __main__ -   test: [batch: 386/391 ] | Loss: 1.039 | Acc: 74.231% (36771/49536)/ 92.018% (45582/49536)
01/15/2023 14:49:49 - INFO - __main__ -   test: [batch: 387/391 ] | Loss: 1.038 | Acc: 74.275% (36888/49664)/ 92.030% (45706/49664)
01/15/2023 14:49:51 - INFO - __main__ -   test: [batch: 388/391 ] | Loss: 1.036 | Acc: 74.327% (37009/49792)/ 92.045% (45831/49792)
01/15/2023 14:49:54 - INFO - __main__ -   test: [batch: 389/391 ] | Loss: 1.035 | Acc: 74.327% (37104/49920)/ 92.049% (45951/49920)
01/15/2023 14:49:56 - INFO - __main__ -   test: [batch: 390/391 ] | Loss: 1.037 | Acc: 74.282% (37141/50000)/ 92.038% (46019/50000)
01/15/2023 14:49:56 - INFO - __main__ -   Final accuracy: 74.282
01/15/2023 14:49:56 - INFO - __main__ -   {'milestones': Counter({2: 1, 3: 1, 4: 1}), 'gamma': 0.1, 'base_lrs': [0.00025], 'last_epoch': 2, '_step_count': 3, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [2.5e-05]}
01/15/2023 14:49:56 - INFO - __main__ -   
Epoch: 2
01/15/2023 14:49:58 - INFO - __main__ -   test: [epoch: 2 | batch: 0/10010 ] | Loss: 1.108 | Acc: 75.000% (96/128)
01/15/2023 14:53:30 - INFO - __main__ -   test: [epoch: 2 | batch: 100/10010 ] | Loss: 0.933 | Acc: 77.444% (10012/12928)
01/15/2023 14:57:04 - INFO - __main__ -   test: [epoch: 2 | batch: 200/10010 ] | Loss: 0.940 | Acc: 76.920% (19790/25728)
01/15/2023 15:00:38 - INFO - __main__ -   test: [epoch: 2 | batch: 300/10010 ] | Loss: 0.945 | Acc: 76.830% (29601/38528)
01/15/2023 15:04:10 - INFO - __main__ -   test: [epoch: 2 | batch: 400/10010 ] | Loss: 0.951 | Acc: 76.640% (39338/51328)
01/15/2023 15:07:40 - INFO - __main__ -   test: [epoch: 2 | batch: 500/10010 ] | Loss: 0.958 | Acc: 76.519% (49070/64128)
01/15/2023 15:11:09 - INFO - __main__ -   test: [epoch: 2 | batch: 600/10010 ] | Loss: 0.960 | Acc: 76.443% (58806/76928)
01/15/2023 15:14:37 - INFO - __main__ -   test: [epoch: 2 | batch: 700/10010 ] | Loss: 0.960 | Acc: 76.392% (68545/89728)
01/15/2023 15:18:05 - INFO - __main__ -   test: [epoch: 2 | batch: 800/10010 ] | Loss: 0.962 | Acc: 76.344% (78274/102528)
01/15/2023 15:21:35 - INFO - __main__ -   test: [epoch: 2 | batch: 900/10010 ] | Loss: 0.961 | Acc: 76.382% (88090/115328)
01/15/2023 15:25:03 - INFO - __main__ -   test: [epoch: 2 | batch: 1000/10010 ] | Loss: 0.961 | Acc: 76.366% (97846/128128)
01/15/2023 15:28:31 - INFO - __main__ -   test: [epoch: 2 | batch: 1100/10010 ] | Loss: 0.963 | Acc: 76.316% (107551/140928)
01/15/2023 15:31:59 - INFO - __main__ -   test: [epoch: 2 | batch: 1200/10010 ] | Loss: 0.964 | Acc: 76.308% (117307/153728)
01/15/2023 15:35:28 - INFO - __main__ -   test: [epoch: 2 | batch: 1300/10010 ] | Loss: 0.963 | Acc: 76.330% (127111/166528)
01/15/2023 15:38:58 - INFO - __main__ -   test: [epoch: 2 | batch: 1400/10010 ] | Loss: 0.963 | Acc: 76.341% (136900/179328)
01/15/2023 15:42:25 - INFO - __main__ -   test: [epoch: 2 | batch: 1500/10010 ] | Loss: 0.963 | Acc: 76.314% (146621/192128)
01/15/2023 15:45:54 - INFO - __main__ -   test: [epoch: 2 | batch: 1600/10010 ] | Loss: 0.964 | Acc: 76.287% (156334/204928)
01/15/2023 15:49:22 - INFO - __main__ -   test: [epoch: 2 | batch: 1700/10010 ] | Loss: 0.964 | Acc: 76.285% (166094/217728)
01/15/2023 15:52:52 - INFO - __main__ -   test: [epoch: 2 | batch: 1800/10010 ] | Loss: 0.963 | Acc: 76.310% (175915/230528)
01/15/2023 15:56:22 - INFO - __main__ -   test: [epoch: 2 | batch: 1900/10010 ] | Loss: 0.962 | Acc: 76.321% (185711/243328)
01/15/2023 15:59:50 - INFO - __main__ -   test: [epoch: 2 | batch: 2000/10010 ] | Loss: 0.961 | Acc: 76.336% (195517/256128)
01/15/2023 16:03:19 - INFO - __main__ -   test: [epoch: 2 | batch: 2100/10010 ] | Loss: 0.961 | Acc: 76.326% (205263/268928)
01/15/2023 16:06:48 - INFO - __main__ -   test: [epoch: 2 | batch: 2200/10010 ] | Loss: 0.962 | Acc: 76.306% (214974/281728)
01/15/2023 16:10:18 - INFO - __main__ -   test: [epoch: 2 | batch: 2300/10010 ] | Loss: 0.963 | Acc: 76.276% (224654/294528)
01/15/2023 16:13:48 - INFO - __main__ -   test: [epoch: 2 | batch: 2400/10010 ] | Loss: 0.963 | Acc: 76.278% (234424/307328)
01/15/2023 16:17:17 - INFO - __main__ -   test: [epoch: 2 | batch: 2500/10010 ] | Loss: 0.963 | Acc: 76.277% (244183/320128)
01/15/2023 16:20:45 - INFO - __main__ -   test: [epoch: 2 | batch: 2600/10010 ] | Loss: 0.964 | Acc: 76.269% (253921/332928)
01/15/2023 16:24:14 - INFO - __main__ -   test: [epoch: 2 | batch: 2700/10010 ] | Loss: 0.963 | Acc: 76.297% (263780/345728)
01/15/2023 16:27:44 - INFO - __main__ -   test: [epoch: 2 | batch: 2800/10010 ] | Loss: 0.963 | Acc: 76.288% (273514/358528)
01/15/2023 16:31:14 - INFO - __main__ -   test: [epoch: 2 | batch: 2900/10010 ] | Loss: 0.962 | Acc: 76.279% (283245/371328)
01/15/2023 16:34:43 - INFO - __main__ -   test: [epoch: 2 | batch: 3000/10010 ] | Loss: 0.962 | Acc: 76.278% (293005/384128)
01/15/2023 16:38:11 - INFO - __main__ -   test: [epoch: 2 | batch: 3100/10010 ] | Loss: 0.962 | Acc: 76.296% (302840/396928)
01/15/2023 16:41:42 - INFO - __main__ -   test: [epoch: 2 | batch: 3200/10010 ] | Loss: 0.962 | Acc: 76.286% (312564/409728)
01/15/2023 16:45:11 - INFO - __main__ -   test: [epoch: 2 | batch: 3300/10010 ] | Loss: 0.963 | Acc: 76.275% (322282/422528)
01/15/2023 16:48:41 - INFO - __main__ -   test: [epoch: 2 | batch: 3400/10010 ] | Loss: 0.962 | Acc: 76.280% (332070/435328)
01/15/2023 16:52:09 - INFO - __main__ -   test: [epoch: 2 | batch: 3500/10010 ] | Loss: 0.962 | Acc: 76.283% (341846/448128)
01/15/2023 16:55:40 - INFO - __main__ -   test: [epoch: 2 | batch: 3600/10010 ] | Loss: 0.962 | Acc: 76.301% (351691/460928)
01/15/2023 16:59:10 - INFO - __main__ -   test: [epoch: 2 | batch: 3700/10010 ] | Loss: 0.962 | Acc: 76.290% (361406/473728)
01/15/2023 17:02:38 - INFO - __main__ -   test: [epoch: 2 | batch: 3800/10010 ] | Loss: 0.961 | Acc: 76.318% (371309/486528)
01/15/2023 17:06:07 - INFO - __main__ -   test: [epoch: 2 | batch: 3900/10010 ] | Loss: 0.961 | Acc: 76.303% (381001/499328)
01/15/2023 17:09:36 - INFO - __main__ -   test: [epoch: 2 | batch: 4000/10010 ] | Loss: 0.962 | Acc: 76.285% (390675/512128)
01/15/2023 17:13:04 - INFO - __main__ -   test: [epoch: 2 | batch: 4100/10010 ] | Loss: 0.962 | Acc: 76.289% (400461/524928)
01/15/2023 17:16:34 - INFO - __main__ -   test: [epoch: 2 | batch: 4200/10010 ] | Loss: 0.962 | Acc: 76.288% (410222/537728)
01/15/2023 17:20:01 - INFO - __main__ -   test: [epoch: 2 | batch: 4300/10010 ] | Loss: 0.962 | Acc: 76.291% (420005/550528)
01/15/2023 17:23:29 - INFO - __main__ -   test: [epoch: 2 | batch: 4400/10010 ] | Loss: 0.962 | Acc: 76.305% (429849/563328)
01/15/2023 17:26:59 - INFO - __main__ -   test: [epoch: 2 | batch: 4500/10010 ] | Loss: 0.962 | Acc: 76.298% (439576/576128)
01/15/2023 17:30:30 - INFO - __main__ -   test: [epoch: 2 | batch: 4600/10010 ] | Loss: 0.962 | Acc: 76.296% (449329/588928)
01/15/2023 17:33:57 - INFO - __main__ -   test: [epoch: 2 | batch: 4700/10010 ] | Loss: 0.962 | Acc: 76.295% (459087/601728)
01/15/2023 17:37:26 - INFO - __main__ -   test: [epoch: 2 | batch: 4800/10010 ] | Loss: 0.962 | Acc: 76.299% (468879/614528)
01/15/2023 17:40:56 - INFO - __main__ -   test: [epoch: 2 | batch: 4900/10010 ] | Loss: 0.962 | Acc: 76.300% (478651/627328)
01/15/2023 17:44:24 - INFO - __main__ -   test: [epoch: 2 | batch: 5000/10010 ] | Loss: 0.962 | Acc: 76.310% (488479/640128)
01/15/2023 17:47:55 - INFO - __main__ -   test: [epoch: 2 | batch: 5100/10010 ] | Loss: 0.962 | Acc: 76.301% (498191/652928)
01/15/2023 17:51:23 - INFO - __main__ -   test: [epoch: 2 | batch: 5200/10010 ] | Loss: 0.962 | Acc: 76.296% (507921/665728)
01/15/2023 17:54:51 - INFO - __main__ -   test: [epoch: 2 | batch: 5300/10010 ] | Loss: 0.962 | Acc: 76.289% (517645/678528)
01/15/2023 17:58:19 - INFO - __main__ -   test: [epoch: 2 | batch: 5400/10010 ] | Loss: 0.963 | Acc: 76.287% (527395/691328)
01/15/2023 18:01:47 - INFO - __main__ -   test: [epoch: 2 | batch: 5500/10010 ] | Loss: 0.963 | Acc: 76.288% (537164/704128)
01/15/2023 18:05:17 - INFO - __main__ -   test: [epoch: 2 | batch: 5600/10010 ] | Loss: 0.963 | Acc: 76.291% (546951/716928)
01/15/2023 18:08:47 - INFO - __main__ -   test: [epoch: 2 | batch: 5700/10010 ] | Loss: 0.963 | Acc: 76.293% (556728/729728)
01/15/2023 18:12:18 - INFO - __main__ -   test: [epoch: 2 | batch: 5800/10010 ] | Loss: 0.962 | Acc: 76.293% (566499/742528)
01/15/2023 18:15:46 - INFO - __main__ -   test: [epoch: 2 | batch: 5900/10010 ] | Loss: 0.962 | Acc: 76.299% (576310/755328)
01/15/2023 18:19:13 - INFO - __main__ -   test: [epoch: 2 | batch: 6000/10010 ] | Loss: 0.962 | Acc: 76.298% (586070/768128)
01/15/2023 18:22:44 - INFO - __main__ -   test: [epoch: 2 | batch: 6100/10010 ] | Loss: 0.963 | Acc: 76.286% (595742/780928)
01/15/2023 18:26:13 - INFO - __main__ -   test: [epoch: 2 | batch: 6200/10010 ] | Loss: 0.963 | Acc: 76.288% (605518/793728)
01/15/2023 18:29:41 - INFO - __main__ -   test: [epoch: 2 | batch: 6300/10010 ] | Loss: 0.963 | Acc: 76.287% (615272/806528)
01/15/2023 18:33:11 - INFO - __main__ -   test: [epoch: 2 | batch: 6400/10010 ] | Loss: 0.963 | Acc: 76.281% (624993/819328)
01/15/2023 18:36:42 - INFO - __main__ -   test: [epoch: 2 | batch: 6500/10010 ] | Loss: 0.963 | Acc: 76.289% (634824/832128)
01/15/2023 18:40:12 - INFO - __main__ -   test: [epoch: 2 | batch: 6600/10010 ] | Loss: 0.963 | Acc: 76.284% (644547/844928)
01/15/2023 18:43:40 - INFO - __main__ -   test: [epoch: 2 | batch: 6700/10010 ] | Loss: 0.963 | Acc: 76.278% (654262/857728)
01/15/2023 18:47:11 - INFO - __main__ -   test: [epoch: 2 | batch: 6800/10010 ] | Loss: 0.963 | Acc: 76.278% (664018/870528)
01/15/2023 18:50:42 - INFO - __main__ -   test: [epoch: 2 | batch: 6900/10010 ] | Loss: 0.963 | Acc: 76.274% (673749/883328)
01/15/2023 18:54:11 - INFO - __main__ -   test: [epoch: 2 | batch: 7000/10010 ] | Loss: 0.963 | Acc: 76.266% (683445/896128)
01/15/2023 18:57:41 - INFO - __main__ -   test: [epoch: 2 | batch: 7100/10010 ] | Loss: 0.963 | Acc: 76.270% (693243/908928)
01/15/2023 19:01:11 - INFO - __main__ -   test: [epoch: 2 | batch: 7200/10010 ] | Loss: 0.962 | Acc: 76.274% (703037/921728)
01/15/2023 19:04:39 - INFO - __main__ -   test: [epoch: 2 | batch: 7300/10010 ] | Loss: 0.962 | Acc: 76.271% (712777/934528)
01/15/2023 19:08:09 - INFO - __main__ -   test: [epoch: 2 | batch: 7400/10010 ] | Loss: 0.962 | Acc: 76.274% (722564/947328)
01/15/2023 19:11:39 - INFO - __main__ -   test: [epoch: 2 | batch: 7500/10010 ] | Loss: 0.962 | Acc: 76.277% (732361/960128)
01/15/2023 19:15:07 - INFO - __main__ -   test: [epoch: 2 | batch: 7600/10010 ] | Loss: 0.962 | Acc: 76.280% (742152/972928)
01/15/2023 19:18:37 - INFO - __main__ -   test: [epoch: 2 | batch: 7700/10010 ] | Loss: 0.962 | Acc: 76.277% (751879/985728)
01/15/2023 19:22:05 - INFO - __main__ -   test: [epoch: 2 | batch: 7800/10010 ] | Loss: 0.963 | Acc: 76.268% (761560/998528)
01/15/2023 19:25:33 - INFO - __main__ -   test: [epoch: 2 | batch: 7900/10010 ] | Loss: 0.962 | Acc: 76.271% (771353/1011328)
01/15/2023 19:29:02 - INFO - __main__ -   test: [epoch: 2 | batch: 8000/10010 ] | Loss: 0.963 | Acc: 76.269% (781093/1024128)
01/15/2023 19:32:31 - INFO - __main__ -   test: [epoch: 2 | batch: 8100/10010 ] | Loss: 0.962 | Acc: 76.276% (790927/1036928)
01/15/2023 19:36:02 - INFO - __main__ -   test: [epoch: 2 | batch: 8200/10010 ] | Loss: 0.962 | Acc: 76.275% (800679/1049728)
01/15/2023 19:39:30 - INFO - __main__ -   test: [epoch: 2 | batch: 8300/10010 ] | Loss: 0.962 | Acc: 76.271% (810398/1062528)
01/15/2023 19:42:59 - INFO - __main__ -   test: [epoch: 2 | batch: 8400/10010 ] | Loss: 0.962 | Acc: 76.271% (820165/1075328)
01/15/2023 19:46:27 - INFO - __main__ -   test: [epoch: 2 | batch: 8500/10010 ] | Loss: 0.962 | Acc: 76.277% (829992/1088128)
01/15/2023 19:49:56 - INFO - __main__ -   test: [epoch: 2 | batch: 8600/10010 ] | Loss: 0.962 | Acc: 76.281% (839800/1100928)
01/15/2023 19:53:24 - INFO - __main__ -   test: [epoch: 2 | batch: 8700/10010 ] | Loss: 0.962 | Acc: 76.280% (849551/1113728)
01/15/2023 19:56:52 - INFO - __main__ -   test: [epoch: 2 | batch: 8800/10010 ] | Loss: 0.962 | Acc: 76.288% (859405/1126528)
01/15/2023 20:00:22 - INFO - __main__ -   test: [epoch: 2 | batch: 8900/10010 ] | Loss: 0.962 | Acc: 76.288% (869169/1139328)
01/15/2023 20:03:49 - INFO - __main__ -   test: [epoch: 2 | batch: 9000/10010 ] | Loss: 0.962 | Acc: 76.282% (878866/1152128)
01/15/2023 20:07:17 - INFO - __main__ -   test: [epoch: 2 | batch: 9100/10010 ] | Loss: 0.962 | Acc: 76.279% (888598/1164928)
01/15/2023 20:10:47 - INFO - __main__ -   test: [epoch: 2 | batch: 9200/10010 ] | Loss: 0.962 | Acc: 76.282% (898394/1177728)
01/15/2023 20:14:16 - INFO - __main__ -   test: [epoch: 2 | batch: 9300/10010 ] | Loss: 0.962 | Acc: 76.276% (908089/1190528)
01/15/2023 20:17:46 - INFO - __main__ -   test: [epoch: 2 | batch: 9400/10010 ] | Loss: 0.962 | Acc: 76.277% (917864/1203328)
01/15/2023 20:21:15 - INFO - __main__ -   test: [epoch: 2 | batch: 9500/10010 ] | Loss: 0.962 | Acc: 76.277% (927625/1216128)
01/15/2023 20:24:43 - INFO - __main__ -   test: [epoch: 2 | batch: 9600/10010 ] | Loss: 0.962 | Acc: 76.277% (937394/1228928)
01/15/2023 20:28:11 - INFO - __main__ -   test: [epoch: 2 | batch: 9700/10010 ] | Loss: 0.962 | Acc: 76.273% (947097/1241728)
01/15/2023 20:31:40 - INFO - __main__ -   test: [epoch: 2 | batch: 9800/10010 ] | Loss: 0.962 | Acc: 76.271% (956841/1254528)
01/15/2023 20:35:08 - INFO - __main__ -   test: [epoch: 2 | batch: 9900/10010 ] | Loss: 0.962 | Acc: 76.272% (966622/1267328)
01/15/2023 20:38:36 - INFO - __main__ -   test: [epoch: 2 | batch: 10000/10010 ] | Loss: 0.963 | Acc: 76.269% (976344/1280128)
01/15/2023 20:38:55 - INFO - __main__ -   Saving Checkpoint
01/15/2023 20:38:58 - INFO - __main__ -   test: [batch: 0/391 ] | Loss: 0.543 | Acc: 86.719% (111/128)/ 95.312% (122/128)
01/15/2023 20:39:00 - INFO - __main__ -   test: [batch: 1/391 ] | Loss: 0.580 | Acc: 85.156% (218/256)/ 96.484% (247/256)
01/15/2023 20:39:02 - INFO - __main__ -   test: [batch: 2/391 ] | Loss: 0.779 | Acc: 79.427% (305/384)/ 94.271% (362/384)
01/15/2023 20:39:04 - INFO - __main__ -   test: [batch: 3/391 ] | Loss: 0.715 | Acc: 82.031% (420/512)/ 94.922% (486/512)
01/15/2023 20:39:06 - INFO - __main__ -   test: [batch: 4/391 ] | Loss: 0.616 | Acc: 84.688% (542/640)/ 95.625% (612/640)
01/15/2023 20:39:09 - INFO - __main__ -   test: [batch: 5/391 ] | Loss: 0.543 | Acc: 86.198% (662/768)/ 96.354% (740/768)
01/15/2023 20:39:11 - INFO - __main__ -   test: [batch: 6/391 ] | Loss: 0.529 | Acc: 86.719% (777/896)/ 96.429% (864/896)
01/15/2023 20:39:13 - INFO - __main__ -   test: [batch: 7/391 ] | Loss: 0.503 | Acc: 87.695% (898/1024)/ 96.582% (989/1024)
01/15/2023 20:39:15 - INFO - __main__ -   test: [batch: 8/391 ] | Loss: 0.509 | Acc: 87.674% (1010/1152)/ 96.615% (1113/1152)
01/15/2023 20:39:17 - INFO - __main__ -   test: [batch: 9/391 ] | Loss: 0.484 | Acc: 88.281% (1130/1280)/ 96.797% (1239/1280)
01/15/2023 20:39:19 - INFO - __main__ -   test: [batch: 10/391 ] | Loss: 0.526 | Acc: 86.932% (1224/1408)/ 96.804% (1363/1408)
01/15/2023 20:39:21 - INFO - __main__ -   test: [batch: 11/391 ] | Loss: 0.531 | Acc: 86.849% (1334/1536)/ 96.615% (1484/1536)
01/15/2023 20:39:24 - INFO - __main__ -   test: [batch: 12/391 ] | Loss: 0.580 | Acc: 85.637% (1425/1664)/ 96.154% (1600/1664)
01/15/2023 20:39:26 - INFO - __main__ -   test: [batch: 13/391 ] | Loss: 0.633 | Acc: 84.152% (1508/1792)/ 95.424% (1710/1792)
01/15/2023 20:39:28 - INFO - __main__ -   test: [batch: 14/391 ] | Loss: 0.649 | Acc: 83.385% (1601/1920)/ 95.573% (1835/1920)
01/15/2023 20:39:30 - INFO - __main__ -   test: [batch: 15/391 ] | Loss: 0.657 | Acc: 82.812% (1696/2048)/ 95.605% (1958/2048)
01/15/2023 20:39:32 - INFO - __main__ -   test: [batch: 16/391 ] | Loss: 0.662 | Acc: 82.721% (1800/2176)/ 95.496% (2078/2176)
01/15/2023 20:39:34 - INFO - __main__ -   test: [batch: 17/391 ] | Loss: 0.685 | Acc: 82.205% (1894/2304)/ 95.052% (2190/2304)
01/15/2023 20:39:37 - INFO - __main__ -   test: [batch: 18/391 ] | Loss: 0.710 | Acc: 81.702% (1987/2432)/ 94.901% (2308/2432)
01/15/2023 20:39:39 - INFO - __main__ -   test: [batch: 19/391 ] | Loss: 0.718 | Acc: 81.484% (2086/2560)/ 94.727% (2425/2560)
01/15/2023 20:39:41 - INFO - __main__ -   test: [batch: 20/391 ] | Loss: 0.713 | Acc: 81.622% (2194/2688)/ 94.717% (2546/2688)
01/15/2023 20:39:43 - INFO - __main__ -   test: [batch: 21/391 ] | Loss: 0.749 | Acc: 80.753% (2274/2816)/ 94.531% (2662/2816)
01/15/2023 20:39:45 - INFO - __main__ -   test: [batch: 22/391 ] | Loss: 0.749 | Acc: 80.571% (2372/2944)/ 94.531% (2783/2944)
01/15/2023 20:39:47 - INFO - __main__ -   test: [batch: 23/391 ] | Loss: 0.792 | Acc: 79.785% (2451/3072)/ 94.238% (2895/3072)
01/15/2023 20:39:49 - INFO - __main__ -   test: [batch: 24/391 ] | Loss: 0.809 | Acc: 79.250% (2536/3200)/ 94.125% (3012/3200)
01/15/2023 20:39:51 - INFO - __main__ -   test: [batch: 25/391 ] | Loss: 0.834 | Acc: 78.576% (2615/3328)/ 93.900% (3125/3328)
01/15/2023 20:39:53 - INFO - __main__ -   test: [batch: 26/391 ] | Loss: 0.851 | Acc: 77.894% (2692/3456)/ 93.924% (3246/3456)
01/15/2023 20:39:55 - INFO - __main__ -   test: [batch: 27/391 ] | Loss: 0.832 | Acc: 78.432% (2811/3584)/ 93.973% (3368/3584)
01/15/2023 20:39:57 - INFO - __main__ -   test: [batch: 28/391 ] | Loss: 0.838 | Acc: 77.883% (2891/3712)/ 94.100% (3493/3712)
01/15/2023 20:39:59 - INFO - __main__ -   test: [batch: 29/391 ] | Loss: 0.829 | Acc: 78.073% (2998/3840)/ 94.245% (3619/3840)
01/15/2023 20:40:01 - INFO - __main__ -   test: [batch: 30/391 ] | Loss: 0.840 | Acc: 77.923% (3092/3968)/ 94.103% (3734/3968)
01/15/2023 20:40:03 - INFO - __main__ -   test: [batch: 31/391 ] | Loss: 0.834 | Acc: 78.198% (3203/4096)/ 94.214% (3859/4096)
01/15/2023 20:40:05 - INFO - __main__ -   test: [batch: 32/391 ] | Loss: 0.819 | Acc: 78.527% (3317/4224)/ 94.342% (3985/4224)
01/15/2023 20:40:08 - INFO - __main__ -   test: [batch: 33/391 ] | Loss: 0.813 | Acc: 78.722% (3426/4352)/ 94.393% (4108/4352)
01/15/2023 20:40:10 - INFO - __main__ -   test: [batch: 34/391 ] | Loss: 0.800 | Acc: 79.152% (3546/4480)/ 94.464% (4232/4480)
01/15/2023 20:40:12 - INFO - __main__ -   test: [batch: 35/391 ] | Loss: 0.787 | Acc: 79.492% (3663/4608)/ 94.488% (4354/4608)
01/15/2023 20:40:14 - INFO - __main__ -   test: [batch: 36/391 ] | Loss: 0.771 | Acc: 79.962% (3787/4736)/ 94.595% (4480/4736)
01/15/2023 20:40:16 - INFO - __main__ -   test: [batch: 37/391 ] | Loss: 0.762 | Acc: 80.243% (3903/4864)/ 94.675% (4605/4864)
01/15/2023 20:40:18 - INFO - __main__ -   test: [batch: 38/391 ] | Loss: 0.753 | Acc: 80.509% (4019/4992)/ 94.732% (4729/4992)
01/15/2023 20:40:20 - INFO - __main__ -   test: [batch: 39/391 ] | Loss: 0.748 | Acc: 80.664% (4130/5120)/ 94.785% (4853/5120)
01/15/2023 20:40:22 - INFO - __main__ -   test: [batch: 40/391 ] | Loss: 0.745 | Acc: 80.755% (4238/5248)/ 94.722% (4971/5248)
01/15/2023 20:40:25 - INFO - __main__ -   test: [batch: 41/391 ] | Loss: 0.747 | Acc: 80.897% (4349/5376)/ 94.643% (5088/5376)
01/15/2023 20:40:27 - INFO - __main__ -   test: [batch: 42/391 ] | Loss: 0.747 | Acc: 80.905% (4453/5504)/ 94.713% (5213/5504)
01/15/2023 20:40:29 - INFO - __main__ -   test: [batch: 43/391 ] | Loss: 0.747 | Acc: 80.913% (4557/5632)/ 94.656% (5331/5632)
01/15/2023 20:40:31 - INFO - __main__ -   test: [batch: 44/391 ] | Loss: 0.752 | Acc: 80.903% (4660/5760)/ 94.531% (5445/5760)
01/15/2023 20:40:33 - INFO - __main__ -   test: [batch: 45/391 ] | Loss: 0.750 | Acc: 81.046% (4772/5888)/ 94.582% (5569/5888)
01/15/2023 20:40:35 - INFO - __main__ -   test: [batch: 46/391 ] | Loss: 0.753 | Acc: 81.017% (4874/6016)/ 94.631% (5693/6016)
01/15/2023 20:40:37 - INFO - __main__ -   test: [batch: 47/391 ] | Loss: 0.756 | Acc: 80.892% (4970/6144)/ 94.661% (5816/6144)
01/15/2023 20:40:39 - INFO - __main__ -   test: [batch: 48/391 ] | Loss: 0.761 | Acc: 80.772% (5066/6272)/ 94.611% (5934/6272)
01/15/2023 20:40:41 - INFO - __main__ -   test: [batch: 49/391 ] | Loss: 0.766 | Acc: 80.750% (5168/6400)/ 94.547% (6051/6400)
01/15/2023 20:40:43 - INFO - __main__ -   test: [batch: 50/391 ] | Loss: 0.758 | Acc: 80.944% (5284/6528)/ 94.623% (6177/6528)
01/15/2023 20:40:45 - INFO - __main__ -   test: [batch: 51/391 ] | Loss: 0.750 | Acc: 81.160% (5402/6656)/ 94.712% (6304/6656)
01/15/2023 20:40:47 - INFO - __main__ -   test: [batch: 52/391 ] | Loss: 0.747 | Acc: 81.206% (5509/6784)/ 94.738% (6427/6784)
01/15/2023 20:40:49 - INFO - __main__ -   test: [batch: 53/391 ] | Loss: 0.737 | Acc: 81.424% (5628/6912)/ 94.821% (6554/6912)
01/15/2023 20:40:51 - INFO - __main__ -   test: [batch: 54/391 ] | Loss: 0.730 | Acc: 81.562% (5742/7040)/ 94.844% (6677/7040)
01/15/2023 20:40:53 - INFO - __main__ -   test: [batch: 55/391 ] | Loss: 0.724 | Acc: 81.696% (5856/7168)/ 94.894% (6802/7168)
01/15/2023 20:40:55 - INFO - __main__ -   test: [batch: 56/391 ] | Loss: 0.715 | Acc: 81.935% (5978/7296)/ 94.956% (6928/7296)
01/15/2023 20:40:57 - INFO - __main__ -   test: [batch: 57/391 ] | Loss: 0.708 | Acc: 82.099% (6095/7424)/ 95.003% (7053/7424)
01/15/2023 20:40:59 - INFO - __main__ -   test: [batch: 58/391 ] | Loss: 0.704 | Acc: 82.190% (6207/7552)/ 95.008% (7175/7552)
01/15/2023 20:41:02 - INFO - __main__ -   test: [batch: 59/391 ] | Loss: 0.706 | Acc: 82.161% (6310/7680)/ 95.013% (7297/7680)
01/15/2023 20:41:04 - INFO - __main__ -   test: [batch: 60/391 ] | Loss: 0.709 | Acc: 82.121% (6412/7808)/ 94.992% (7417/7808)
01/15/2023 20:41:06 - INFO - __main__ -   test: [batch: 61/391 ] | Loss: 0.708 | Acc: 82.119% (6517/7936)/ 95.010% (7540/7936)
01/15/2023 20:41:08 - INFO - __main__ -   test: [batch: 62/391 ] | Loss: 0.706 | Acc: 82.081% (6619/8064)/ 95.052% (7665/8064)
01/15/2023 20:41:10 - INFO - __main__ -   test: [batch: 63/391 ] | Loss: 0.713 | Acc: 81.934% (6712/8192)/ 94.995% (7782/8192)
01/15/2023 20:41:12 - INFO - __main__ -   test: [batch: 64/391 ] | Loss: 0.721 | Acc: 81.719% (6799/8320)/ 94.952% (7900/8320)
01/15/2023 20:41:14 - INFO - __main__ -   test: [batch: 65/391 ] | Loss: 0.734 | Acc: 81.238% (6863/8448)/ 94.863% (8014/8448)
01/15/2023 20:41:16 - INFO - __main__ -   test: [batch: 66/391 ] | Loss: 0.738 | Acc: 81.250% (6968/8576)/ 94.823% (8132/8576)
01/15/2023 20:41:18 - INFO - __main__ -   test: [batch: 67/391 ] | Loss: 0.741 | Acc: 81.181% (7066/8704)/ 94.830% (8254/8704)
01/15/2023 20:41:21 - INFO - __main__ -   test: [batch: 68/391 ] | Loss: 0.742 | Acc: 81.114% (7164/8832)/ 94.837% (8376/8832)
01/15/2023 20:41:23 - INFO - __main__ -   test: [batch: 69/391 ] | Loss: 0.738 | Acc: 81.217% (7277/8960)/ 94.888% (8502/8960)
01/15/2023 20:41:25 - INFO - __main__ -   test: [batch: 70/391 ] | Loss: 0.738 | Acc: 81.140% (7374/9088)/ 94.916% (8626/9088)
01/15/2023 20:41:27 - INFO - __main__ -   test: [batch: 71/391 ] | Loss: 0.736 | Acc: 81.207% (7484/9216)/ 94.911% (8747/9216)
01/15/2023 20:41:29 - INFO - __main__ -   test: [batch: 72/391 ] | Loss: 0.739 | Acc: 81.057% (7574/9344)/ 94.938% (8871/9344)
01/15/2023 20:41:31 - INFO - __main__ -   test: [batch: 73/391 ] | Loss: 0.743 | Acc: 80.933% (7666/9472)/ 94.932% (8992/9472)
01/15/2023 20:41:33 - INFO - __main__ -   test: [batch: 74/391 ] | Loss: 0.743 | Acc: 80.938% (7770/9600)/ 94.917% (9112/9600)
01/15/2023 20:41:36 - INFO - __main__ -   test: [batch: 75/391 ] | Loss: 0.750 | Acc: 80.767% (7857/9728)/ 94.891% (9231/9728)
01/15/2023 20:41:38 - INFO - __main__ -   test: [batch: 76/391 ] | Loss: 0.748 | Acc: 80.783% (7962/9856)/ 94.917% (9355/9856)
01/15/2023 20:41:40 - INFO - __main__ -   test: [batch: 77/391 ] | Loss: 0.749 | Acc: 80.699% (8057/9984)/ 94.952% (9480/9984)
01/15/2023 20:41:42 - INFO - __main__ -   test: [batch: 78/391 ] | Loss: 0.748 | Acc: 80.657% (8156/10112)/ 94.996% (9606/10112)
01/15/2023 20:41:44 - INFO - __main__ -   test: [batch: 79/391 ] | Loss: 0.747 | Acc: 80.625% (8256/10240)/ 95.020% (9730/10240)
01/15/2023 20:41:46 - INFO - __main__ -   test: [batch: 80/391 ] | Loss: 0.747 | Acc: 80.594% (8356/10368)/ 94.994% (9849/10368)
01/15/2023 20:41:48 - INFO - __main__ -   test: [batch: 81/391 ] | Loss: 0.747 | Acc: 80.593% (8459/10496)/ 95.017% (9973/10496)
01/15/2023 20:41:50 - INFO - __main__ -   test: [batch: 82/391 ] | Loss: 0.746 | Acc: 80.610% (8564/10624)/ 95.030% (10096/10624)
01/15/2023 20:41:52 - INFO - __main__ -   test: [batch: 83/391 ] | Loss: 0.746 | Acc: 80.655% (8672/10752)/ 95.006% (10215/10752)
01/15/2023 20:41:54 - INFO - __main__ -   test: [batch: 84/391 ] | Loss: 0.742 | Acc: 80.772% (8788/10880)/ 95.037% (10340/10880)
01/15/2023 20:41:56 - INFO - __main__ -   test: [batch: 85/391 ] | Loss: 0.740 | Acc: 80.769% (8891/11008)/ 95.094% (10468/11008)
01/15/2023 20:41:58 - INFO - __main__ -   test: [batch: 86/391 ] | Loss: 0.744 | Acc: 80.693% (8986/11136)/ 95.043% (10584/11136)
01/15/2023 20:42:00 - INFO - __main__ -   test: [batch: 87/391 ] | Loss: 0.742 | Acc: 80.735% (9094/11264)/ 95.037% (10705/11264)
01/15/2023 20:42:02 - INFO - __main__ -   test: [batch: 88/391 ] | Loss: 0.749 | Acc: 80.653% (9188/11392)/ 94.988% (10821/11392)
01/15/2023 20:42:04 - INFO - __main__ -   test: [batch: 89/391 ] | Loss: 0.747 | Acc: 80.694% (9296/11520)/ 95.009% (10945/11520)
01/15/2023 20:42:06 - INFO - __main__ -   test: [batch: 90/391 ] | Loss: 0.748 | Acc: 80.598% (9388/11648)/ 95.021% (11068/11648)
01/15/2023 20:42:08 - INFO - __main__ -   test: [batch: 91/391 ] | Loss: 0.747 | Acc: 80.622% (9494/11776)/ 95.024% (11190/11776)
01/15/2023 20:42:10 - INFO - __main__ -   test: [batch: 92/391 ] | Loss: 0.749 | Acc: 80.570% (9591/11904)/ 94.993% (11308/11904)
01/15/2023 20:42:12 - INFO - __main__ -   test: [batch: 93/391 ] | Loss: 0.751 | Acc: 80.394% (9673/12032)/ 95.030% (11434/12032)
01/15/2023 20:42:15 - INFO - __main__ -   test: [batch: 94/391 ] | Loss: 0.754 | Acc: 80.263% (9760/12160)/ 95.033% (11556/12160)
01/15/2023 20:42:17 - INFO - __main__ -   test: [batch: 95/391 ] | Loss: 0.752 | Acc: 80.330% (9871/12288)/ 95.052% (11680/12288)
01/15/2023 20:42:19 - INFO - __main__ -   test: [batch: 96/391 ] | Loss: 0.756 | Acc: 80.179% (9955/12416)/ 95.047% (11801/12416)
01/15/2023 20:42:21 - INFO - __main__ -   test: [batch: 97/391 ] | Loss: 0.758 | Acc: 79.982% (10033/12544)/ 95.065% (11925/12544)
01/15/2023 20:42:23 - INFO - __main__ -   test: [batch: 98/391 ] | Loss: 0.754 | Acc: 80.082% (10148/12672)/ 95.084% (12049/12672)
01/15/2023 20:42:25 - INFO - __main__ -   test: [batch: 99/391 ] | Loss: 0.748 | Acc: 80.250% (10272/12800)/ 95.125% (12176/12800)
01/15/2023 20:42:27 - INFO - __main__ -   test: [batch: 100/391 ] | Loss: 0.747 | Acc: 80.330% (10385/12928)/ 95.142% (12300/12928)
01/15/2023 20:42:29 - INFO - __main__ -   test: [batch: 101/391 ] | Loss: 0.743 | Acc: 80.438% (10502/13056)/ 95.175% (12426/13056)
01/15/2023 20:42:31 - INFO - __main__ -   test: [batch: 102/391 ] | Loss: 0.740 | Acc: 80.522% (10616/13184)/ 95.206% (12552/13184)
01/15/2023 20:42:33 - INFO - __main__ -   test: [batch: 103/391 ] | Loss: 0.741 | Acc: 80.379% (10700/13312)/ 95.215% (12675/13312)
01/15/2023 20:42:36 - INFO - __main__ -   test: [batch: 104/391 ] | Loss: 0.740 | Acc: 80.298% (10792/13440)/ 95.216% (12797/13440)
01/15/2023 20:42:38 - INFO - __main__ -   test: [batch: 105/391 ] | Loss: 0.741 | Acc: 80.292% (10894/13568)/ 95.209% (12918/13568)
01/15/2023 20:42:40 - INFO - __main__ -   test: [batch: 106/391 ] | Loss: 0.749 | Acc: 80.228% (10988/13696)/ 95.137% (13030/13696)
01/15/2023 20:42:42 - INFO - __main__ -   test: [batch: 107/391 ] | Loss: 0.745 | Acc: 80.353% (11108/13824)/ 95.175% (13157/13824)
01/15/2023 20:42:44 - INFO - __main__ -   test: [batch: 108/391 ] | Loss: 0.748 | Acc: 80.196% (11189/13952)/ 95.183% (13280/13952)
01/15/2023 20:42:46 - INFO - __main__ -   test: [batch: 109/391 ] | Loss: 0.749 | Acc: 80.199% (11292/14080)/ 95.185% (13402/14080)
01/15/2023 20:42:48 - INFO - __main__ -   test: [batch: 110/391 ] | Loss: 0.751 | Acc: 80.011% (11368/14208)/ 95.207% (13527/14208)
01/15/2023 20:42:50 - INFO - __main__ -   test: [batch: 111/391 ] | Loss: 0.754 | Acc: 79.967% (11464/14336)/ 95.173% (13644/14336)
01/15/2023 20:42:52 - INFO - __main__ -   test: [batch: 112/391 ] | Loss: 0.754 | Acc: 80.006% (11572/14464)/ 95.195% (13769/14464)
01/15/2023 20:42:54 - INFO - __main__ -   test: [batch: 113/391 ] | Loss: 0.753 | Acc: 80.051% (11681/14592)/ 95.210% (13893/14592)
01/15/2023 20:42:57 - INFO - __main__ -   test: [batch: 114/391 ] | Loss: 0.749 | Acc: 80.156% (11799/14720)/ 95.238% (14019/14720)
01/15/2023 20:42:59 - INFO - __main__ -   test: [batch: 115/391 ] | Loss: 0.747 | Acc: 80.246% (11915/14848)/ 95.259% (14144/14848)
01/15/2023 20:43:01 - INFO - __main__ -   test: [batch: 116/391 ] | Loss: 0.750 | Acc: 80.222% (12014/14976)/ 95.232% (14262/14976)
01/15/2023 20:43:03 - INFO - __main__ -   test: [batch: 117/391 ] | Loss: 0.749 | Acc: 80.257% (12122/15104)/ 95.253% (14387/15104)
01/15/2023 20:43:05 - INFO - __main__ -   test: [batch: 118/391 ] | Loss: 0.752 | Acc: 80.095% (12200/15232)/ 95.260% (14510/15232)
01/15/2023 20:43:07 - INFO - __main__ -   test: [batch: 119/391 ] | Loss: 0.750 | Acc: 80.130% (12308/15360)/ 95.280% (14635/15360)
01/15/2023 20:43:09 - INFO - __main__ -   test: [batch: 120/391 ] | Loss: 0.749 | Acc: 80.172% (12417/15488)/ 95.300% (14760/15488)
01/15/2023 20:43:11 - INFO - __main__ -   test: [batch: 121/391 ] | Loss: 0.754 | Acc: 80.046% (12500/15616)/ 95.261% (14876/15616)
01/15/2023 20:43:14 - INFO - __main__ -   test: [batch: 122/391 ] | Loss: 0.757 | Acc: 80.005% (12596/15744)/ 95.249% (14996/15744)
01/15/2023 20:43:16 - INFO - __main__ -   test: [batch: 123/391 ] | Loss: 0.758 | Acc: 80.028% (12702/15872)/ 95.237% (15116/15872)
01/15/2023 20:43:18 - INFO - __main__ -   test: [batch: 124/391 ] | Loss: 0.757 | Acc: 80.044% (12807/16000)/ 95.250% (15240/16000)
01/15/2023 20:43:20 - INFO - __main__ -   test: [batch: 125/391 ] | Loss: 0.753 | Acc: 80.146% (12926/16128)/ 95.275% (15366/16128)
01/15/2023 20:43:22 - INFO - __main__ -   test: [batch: 126/391 ] | Loss: 0.750 | Acc: 80.241% (13044/16256)/ 95.276% (15488/16256)
01/15/2023 20:43:24 - INFO - __main__ -   test: [batch: 127/391 ] | Loss: 0.748 | Acc: 80.310% (13158/16384)/ 95.288% (15612/16384)
01/15/2023 20:43:26 - INFO - __main__ -   test: [batch: 128/391 ] | Loss: 0.748 | Acc: 80.311% (13261/16512)/ 95.270% (15731/16512)
01/15/2023 20:43:28 - INFO - __main__ -   test: [batch: 129/391 ] | Loss: 0.747 | Acc: 80.355% (13371/16640)/ 95.294% (15857/16640)
01/15/2023 20:43:30 - INFO - __main__ -   test: [batch: 130/391 ] | Loss: 0.743 | Acc: 80.463% (13492/16768)/ 95.318% (15983/16768)
01/15/2023 20:43:32 - INFO - __main__ -   test: [batch: 131/391 ] | Loss: 0.743 | Acc: 80.510% (13603/16896)/ 95.330% (16107/16896)
01/15/2023 20:43:34 - INFO - __main__ -   test: [batch: 132/391 ] | Loss: 0.740 | Acc: 80.586% (13719/17024)/ 95.342% (16231/17024)
01/15/2023 20:43:36 - INFO - __main__ -   test: [batch: 133/391 ] | Loss: 0.742 | Acc: 80.533% (13813/17152)/ 95.336% (16352/17152)
01/15/2023 20:43:38 - INFO - __main__ -   test: [batch: 134/391 ] | Loss: 0.740 | Acc: 80.561% (13921/17280)/ 95.353% (16477/17280)
01/15/2023 20:43:40 - INFO - __main__ -   test: [batch: 135/391 ] | Loss: 0.740 | Acc: 80.543% (14021/17408)/ 95.358% (16600/17408)
01/15/2023 20:43:42 - INFO - __main__ -   test: [batch: 136/391 ] | Loss: 0.739 | Acc: 80.543% (14124/17536)/ 95.381% (16726/17536)
01/15/2023 20:43:44 - INFO - __main__ -   test: [batch: 137/391 ] | Loss: 0.736 | Acc: 80.582% (14234/17664)/ 95.414% (16854/17664)
01/15/2023 20:43:47 - INFO - __main__ -   test: [batch: 138/391 ] | Loss: 0.735 | Acc: 80.621% (14344/17792)/ 95.425% (16978/17792)
01/15/2023 20:43:49 - INFO - __main__ -   test: [batch: 139/391 ] | Loss: 0.740 | Acc: 80.480% (14422/17920)/ 95.419% (17099/17920)
01/15/2023 20:43:51 - INFO - __main__ -   test: [batch: 140/391 ] | Loss: 0.742 | Acc: 80.408% (14512/18048)/ 95.423% (17222/18048)
01/15/2023 20:43:53 - INFO - __main__ -   test: [batch: 141/391 ] | Loss: 0.740 | Acc: 80.458% (14624/18176)/ 95.434% (17346/18176)
01/15/2023 20:43:55 - INFO - __main__ -   test: [batch: 142/391 ] | Loss: 0.738 | Acc: 80.534% (14741/18304)/ 95.449% (17471/18304)
01/15/2023 20:43:57 - INFO - __main__ -   test: [batch: 143/391 ] | Loss: 0.738 | Acc: 80.566% (14850/18432)/ 95.437% (17591/18432)
01/15/2023 20:43:59 - INFO - __main__ -   test: [batch: 144/391 ] | Loss: 0.740 | Acc: 80.533% (14947/18560)/ 95.404% (17707/18560)
01/15/2023 20:44:01 - INFO - __main__ -   test: [batch: 145/391 ] | Loss: 0.743 | Acc: 80.528% (15049/18688)/ 95.382% (17825/18688)
01/15/2023 20:44:03 - INFO - __main__ -   test: [batch: 146/391 ] | Loss: 0.744 | Acc: 80.495% (15146/18816)/ 95.360% (17943/18816)
01/15/2023 20:44:05 - INFO - __main__ -   test: [batch: 147/391 ] | Loss: 0.744 | Acc: 80.495% (15249/18944)/ 95.344% (18062/18944)
01/15/2023 20:44:07 - INFO - __main__ -   test: [batch: 148/391 ] | Loss: 0.746 | Acc: 80.411% (15336/19072)/ 95.349% (18185/19072)
01/15/2023 20:44:09 - INFO - __main__ -   test: [batch: 149/391 ] | Loss: 0.750 | Acc: 80.339% (15425/19200)/ 95.307% (18299/19200)
01/15/2023 20:44:11 - INFO - __main__ -   test: [batch: 150/391 ] | Loss: 0.750 | Acc: 80.298% (15520/19328)/ 95.312% (18422/19328)
01/15/2023 20:44:13 - INFO - __main__ -   test: [batch: 151/391 ] | Loss: 0.750 | Acc: 80.345% (15632/19456)/ 95.307% (18543/19456)
01/15/2023 20:44:16 - INFO - __main__ -   test: [batch: 152/391 ] | Loss: 0.751 | Acc: 80.316% (15729/19584)/ 95.287% (18661/19584)
01/15/2023 20:44:18 - INFO - __main__ -   test: [batch: 153/391 ] | Loss: 0.749 | Acc: 80.367% (15842/19712)/ 95.297% (18785/19712)
01/15/2023 20:44:20 - INFO - __main__ -   test: [batch: 154/391 ] | Loss: 0.751 | Acc: 80.348% (15941/19840)/ 95.252% (18898/19840)
01/15/2023 20:44:22 - INFO - __main__ -   test: [batch: 155/391 ] | Loss: 0.752 | Acc: 80.354% (16045/19968)/ 95.237% (19017/19968)
01/15/2023 20:44:24 - INFO - __main__ -   test: [batch: 156/391 ] | Loss: 0.755 | Acc: 80.275% (16132/20096)/ 95.218% (19135/20096)
01/15/2023 20:44:26 - INFO - __main__ -   test: [batch: 157/391 ] | Loss: 0.757 | Acc: 80.246% (16229/20224)/ 95.199% (19253/20224)
01/15/2023 20:44:28 - INFO - __main__ -   test: [batch: 158/391 ] | Loss: 0.758 | Acc: 80.218% (16326/20352)/ 95.175% (19370/20352)
01/15/2023 20:44:30 - INFO - __main__ -   test: [batch: 159/391 ] | Loss: 0.759 | Acc: 80.200% (16425/20480)/ 95.181% (19493/20480)
01/15/2023 20:44:32 - INFO - __main__ -   test: [batch: 160/391 ] | Loss: 0.760 | Acc: 80.187% (16525/20608)/ 95.157% (19610/20608)
01/15/2023 20:44:34 - INFO - __main__ -   test: [batch: 161/391 ] | Loss: 0.770 | Acc: 79.919% (16572/20736)/ 95.052% (19710/20736)
01/15/2023 20:44:36 - INFO - __main__ -   test: [batch: 162/391 ] | Loss: 0.776 | Acc: 79.817% (16653/20864)/ 94.977% (19816/20864)
01/15/2023 20:44:38 - INFO - __main__ -   test: [batch: 163/391 ] | Loss: 0.778 | Acc: 79.754% (16742/20992)/ 94.965% (19935/20992)
01/15/2023 20:44:40 - INFO - __main__ -   test: [batch: 164/391 ] | Loss: 0.778 | Acc: 79.740% (16841/21120)/ 94.976% (20059/21120)
01/15/2023 20:44:42 - INFO - __main__ -   test: [batch: 165/391 ] | Loss: 0.780 | Acc: 79.673% (16929/21248)/ 94.969% (20179/21248)
01/15/2023 20:44:44 - INFO - __main__ -   test: [batch: 166/391 ] | Loss: 0.780 | Acc: 79.697% (17036/21376)/ 94.957% (20298/21376)
01/15/2023 20:44:46 - INFO - __main__ -   test: [batch: 167/391 ] | Loss: 0.782 | Acc: 79.660% (17130/21504)/ 94.931% (20414/21504)
01/15/2023 20:44:49 - INFO - __main__ -   test: [batch: 168/391 ] | Loss: 0.782 | Acc: 79.669% (17234/21632)/ 94.920% (20533/21632)
01/15/2023 20:44:51 - INFO - __main__ -   test: [batch: 169/391 ] | Loss: 0.784 | Acc: 79.600% (17321/21760)/ 94.881% (20646/21760)
01/15/2023 20:44:53 - INFO - __main__ -   test: [batch: 170/391 ] | Loss: 0.789 | Acc: 79.505% (17402/21888)/ 94.842% (20759/21888)
01/15/2023 20:44:55 - INFO - __main__ -   test: [batch: 171/391 ] | Loss: 0.792 | Acc: 79.451% (17492/22016)/ 94.813% (20874/22016)
01/15/2023 20:44:57 - INFO - __main__ -   test: [batch: 172/391 ] | Loss: 0.793 | Acc: 79.398% (17582/22144)/ 94.807% (20994/22144)
01/15/2023 20:44:59 - INFO - __main__ -   test: [batch: 173/391 ] | Loss: 0.796 | Acc: 79.351% (17673/22272)/ 94.783% (21110/22272)
01/15/2023 20:45:01 - INFO - __main__ -   test: [batch: 174/391 ] | Loss: 0.800 | Acc: 79.250% (17752/22400)/ 94.737% (21221/22400)
01/15/2023 20:45:03 - INFO - __main__ -   test: [batch: 175/391 ] | Loss: 0.799 | Acc: 79.301% (17865/22528)/ 94.749% (21345/22528)
01/15/2023 20:45:05 - INFO - __main__ -   test: [batch: 176/391 ] | Loss: 0.799 | Acc: 79.286% (17963/22656)/ 94.721% (21460/22656)
01/15/2023 20:45:07 - INFO - __main__ -   test: [batch: 177/391 ] | Loss: 0.803 | Acc: 79.209% (18047/22784)/ 94.694% (21575/22784)
01/15/2023 20:45:10 - INFO - __main__ -   test: [batch: 178/391 ] | Loss: 0.804 | Acc: 79.225% (18152/22912)/ 94.675% (21692/22912)
01/15/2023 20:45:12 - INFO - __main__ -   test: [batch: 179/391 ] | Loss: 0.808 | Acc: 79.175% (18242/23040)/ 94.622% (21801/23040)
01/15/2023 20:45:14 - INFO - __main__ -   test: [batch: 180/391 ] | Loss: 0.814 | Acc: 79.044% (18313/23168)/ 94.574% (21911/23168)
01/15/2023 20:45:16 - INFO - __main__ -   test: [batch: 181/391 ] | Loss: 0.820 | Acc: 78.919% (18385/23296)/ 94.505% (22016/23296)
01/15/2023 20:45:18 - INFO - __main__ -   test: [batch: 182/391 ] | Loss: 0.819 | Acc: 78.932% (18489/23424)/ 94.501% (22136/23424)
01/15/2023 20:45:20 - INFO - __main__ -   test: [batch: 183/391 ] | Loss: 0.826 | Acc: 78.809% (18561/23552)/ 94.387% (22230/23552)
01/15/2023 20:45:22 - INFO - __main__ -   test: [batch: 184/391 ] | Loss: 0.826 | Acc: 78.834% (18668/23680)/ 94.375% (22348/23680)
01/15/2023 20:45:24 - INFO - __main__ -   test: [batch: 185/391 ] | Loss: 0.825 | Acc: 78.843% (18771/23808)/ 94.376% (22469/23808)
01/15/2023 20:45:26 - INFO - __main__ -   test: [batch: 186/391 ] | Loss: 0.828 | Acc: 78.806% (18863/23936)/ 94.335% (22580/23936)
01/15/2023 20:45:28 - INFO - __main__ -   test: [batch: 187/391 ] | Loss: 0.832 | Acc: 78.694% (18937/24064)/ 94.319% (22697/24064)
01/15/2023 20:45:30 - INFO - __main__ -   test: [batch: 188/391 ] | Loss: 0.836 | Acc: 78.571% (19008/24192)/ 94.292% (22811/24192)
01/15/2023 20:45:32 - INFO - __main__ -   test: [batch: 189/391 ] | Loss: 0.838 | Acc: 78.483% (19087/24320)/ 94.301% (22934/24320)
01/15/2023 20:45:34 - INFO - __main__ -   test: [batch: 190/391 ] | Loss: 0.842 | Acc: 78.399% (19167/24448)/ 94.274% (23048/24448)
01/15/2023 20:45:36 - INFO - __main__ -   test: [batch: 191/391 ] | Loss: 0.845 | Acc: 78.365% (19259/24576)/ 94.246% (23162/24576)
01/15/2023 20:45:38 - INFO - __main__ -   test: [batch: 192/391 ] | Loss: 0.851 | Acc: 78.238% (19328/24704)/ 94.143% (23257/24704)
01/15/2023 20:45:41 - INFO - __main__ -   test: [batch: 193/391 ] | Loss: 0.851 | Acc: 78.258% (19433/24832)/ 94.137% (23376/24832)
01/15/2023 20:45:43 - INFO - __main__ -   test: [batch: 194/391 ] | Loss: 0.855 | Acc: 78.193% (19517/24960)/ 94.107% (23489/24960)
01/15/2023 20:45:45 - INFO - __main__ -   test: [batch: 195/391 ] | Loss: 0.860 | Acc: 78.093% (19592/25088)/ 94.033% (23591/25088)
01/15/2023 20:45:47 - INFO - __main__ -   test: [batch: 196/391 ] | Loss: 0.864 | Acc: 77.998% (19668/25216)/ 93.992% (23701/25216)
01/15/2023 20:45:49 - INFO - __main__ -   test: [batch: 197/391 ] | Loss: 0.868 | Acc: 77.904% (19744/25344)/ 93.975% (23817/25344)
01/15/2023 20:45:51 - INFO - __main__ -   test: [batch: 198/391 ] | Loss: 0.870 | Acc: 77.858% (19832/25472)/ 93.927% (23925/25472)
01/15/2023 20:45:53 - INFO - __main__ -   test: [batch: 199/391 ] | Loss: 0.870 | Acc: 77.832% (19925/25600)/ 93.938% (24048/25600)
01/15/2023 20:45:55 - INFO - __main__ -   test: [batch: 200/391 ] | Loss: 0.870 | Acc: 77.806% (20018/25728)/ 93.944% (24170/25728)
01/15/2023 20:45:57 - INFO - __main__ -   test: [batch: 201/391 ] | Loss: 0.873 | Acc: 77.727% (20097/25856)/ 93.916% (24283/25856)
01/15/2023 20:45:59 - INFO - __main__ -   test: [batch: 202/391 ] | Loss: 0.874 | Acc: 77.702% (20190/25984)/ 93.908% (24401/25984)
01/15/2023 20:46:01 - INFO - __main__ -   test: [batch: 203/391 ] | Loss: 0.875 | Acc: 77.688% (20286/26112)/ 93.892% (24517/26112)
01/15/2023 20:46:03 - INFO - __main__ -   test: [batch: 204/391 ] | Loss: 0.878 | Acc: 77.595% (20361/26240)/ 93.861% (24629/26240)
01/15/2023 20:46:06 - INFO - __main__ -   test: [batch: 205/391 ] | Loss: 0.881 | Acc: 77.514% (20439/26368)/ 93.833% (24742/26368)
01/15/2023 20:46:08 - INFO - __main__ -   test: [batch: 206/391 ] | Loss: 0.883 | Acc: 77.483% (20530/26496)/ 93.833% (24862/26496)
01/15/2023 20:46:10 - INFO - __main__ -   test: [batch: 207/391 ] | Loss: 0.886 | Acc: 77.404% (20608/26624)/ 93.791% (24971/26624)
01/15/2023 20:46:12 - INFO - __main__ -   test: [batch: 208/391 ] | Loss: 0.888 | Acc: 77.362% (20696/26752)/ 93.761% (25083/26752)
01/15/2023 20:46:14 - INFO - __main__ -   test: [batch: 209/391 ] | Loss: 0.888 | Acc: 77.362% (20795/26880)/ 93.783% (25209/26880)
01/15/2023 20:46:16 - INFO - __main__ -   test: [batch: 210/391 ] | Loss: 0.889 | Acc: 77.329% (20885/27008)/ 93.761% (25323/27008)
01/15/2023 20:46:18 - INFO - __main__ -   test: [batch: 211/391 ] | Loss: 0.892 | Acc: 77.274% (20969/27136)/ 93.728% (25434/27136)
01/15/2023 20:46:20 - INFO - __main__ -   test: [batch: 212/391 ] | Loss: 0.894 | Acc: 77.208% (21050/27264)/ 93.721% (25552/27264)
01/15/2023 20:46:23 - INFO - __main__ -   test: [batch: 213/391 ] | Loss: 0.893 | Acc: 77.209% (21149/27392)/ 93.728% (25674/27392)
01/15/2023 20:46:25 - INFO - __main__ -   test: [batch: 214/391 ] | Loss: 0.894 | Acc: 77.191% (21243/27520)/ 93.732% (25795/27520)
01/15/2023 20:46:27 - INFO - __main__ -   test: [batch: 215/391 ] | Loss: 0.896 | Acc: 77.167% (21335/27648)/ 93.703% (25907/27648)
01/15/2023 20:46:29 - INFO - __main__ -   test: [batch: 216/391 ] | Loss: 0.893 | Acc: 77.218% (21448/27776)/ 93.718% (26031/27776)
01/15/2023 20:46:31 - INFO - __main__ -   test: [batch: 217/391 ] | Loss: 0.898 | Acc: 77.154% (21529/27904)/ 93.657% (26134/27904)
01/15/2023 20:46:33 - INFO - __main__ -   test: [batch: 218/391 ] | Loss: 0.901 | Acc: 77.083% (21608/28032)/ 93.625% (26245/28032)
01/15/2023 20:46:35 - INFO - __main__ -   test: [batch: 219/391 ] | Loss: 0.900 | Acc: 77.124% (21718/28160)/ 93.636% (26368/28160)
01/15/2023 20:46:37 - INFO - __main__ -   test: [batch: 220/391 ] | Loss: 0.898 | Acc: 77.167% (21829/28288)/ 93.647% (26491/28288)
01/15/2023 20:46:39 - INFO - __main__ -   test: [batch: 221/391 ] | Loss: 0.900 | Acc: 77.129% (21917/28416)/ 93.627% (26605/28416)
01/15/2023 20:46:42 - INFO - __main__ -   test: [batch: 222/391 ] | Loss: 0.898 | Acc: 77.183% (22031/28544)/ 93.641% (26729/28544)
01/15/2023 20:46:44 - INFO - __main__ -   test: [batch: 223/391 ] | Loss: 0.899 | Acc: 77.169% (22126/28672)/ 93.628% (26845/28672)
01/15/2023 20:46:46 - INFO - __main__ -   test: [batch: 224/391 ] | Loss: 0.898 | Acc: 77.188% (22230/28800)/ 93.628% (26965/28800)
01/15/2023 20:46:48 - INFO - __main__ -   test: [batch: 225/391 ] | Loss: 0.898 | Acc: 77.192% (22330/28928)/ 93.629% (27085/28928)
01/15/2023 20:46:50 - INFO - __main__ -   test: [batch: 226/391 ] | Loss: 0.898 | Acc: 77.172% (22423/29056)/ 93.630% (27205/29056)
01/15/2023 20:46:52 - INFO - __main__ -   test: [batch: 227/391 ] | Loss: 0.899 | Acc: 77.172% (22522/29184)/ 93.623% (27323/29184)
01/15/2023 20:46:54 - INFO - __main__ -   test: [batch: 228/391 ] | Loss: 0.904 | Acc: 77.081% (22594/29312)/ 93.556% (27423/29312)
01/15/2023 20:46:56 - INFO - __main__ -   test: [batch: 229/391 ] | Loss: 0.908 | Acc: 76.994% (22667/29440)/ 93.512% (27530/29440)
01/15/2023 20:46:58 - INFO - __main__ -   test: [batch: 230/391 ] | Loss: 0.911 | Acc: 76.918% (22743/29568)/ 93.466% (27636/29568)
01/15/2023 20:47:00 - INFO - __main__ -   test: [batch: 231/391 ] | Loss: 0.911 | Acc: 76.906% (22838/29696)/ 93.457% (27753/29696)
01/15/2023 20:47:02 - INFO - __main__ -   test: [batch: 232/391 ] | Loss: 0.910 | Acc: 76.935% (22945/29824)/ 93.465% (27875/29824)
01/15/2023 20:47:04 - INFO - __main__ -   test: [batch: 233/391 ] | Loss: 0.912 | Acc: 76.896% (23032/29952)/ 93.443% (27988/29952)
01/15/2023 20:47:06 - INFO - __main__ -   test: [batch: 234/391 ] | Loss: 0.918 | Acc: 76.775% (23094/30080)/ 93.378% (28088/30080)
01/15/2023 20:47:09 - INFO - __main__ -   test: [batch: 235/391 ] | Loss: 0.918 | Acc: 76.771% (23191/30208)/ 93.376% (28207/30208)
01/15/2023 20:47:11 - INFO - __main__ -   test: [batch: 236/391 ] | Loss: 0.917 | Acc: 76.820% (23304/30336)/ 93.387% (28330/30336)
01/15/2023 20:47:13 - INFO - __main__ -   test: [batch: 237/391 ] | Loss: 0.917 | Acc: 76.822% (23403/30464)/ 93.359% (28441/30464)
01/15/2023 20:47:15 - INFO - __main__ -   test: [batch: 238/391 ] | Loss: 0.916 | Acc: 76.867% (23515/30592)/ 93.368% (28563/30592)
01/15/2023 20:47:17 - INFO - __main__ -   test: [batch: 239/391 ] | Loss: 0.915 | Acc: 76.898% (23623/30720)/ 93.372% (28684/30720)
01/15/2023 20:47:20 - INFO - __main__ -   test: [batch: 240/391 ] | Loss: 0.916 | Acc: 76.906% (23724/30848)/ 93.364% (28801/30848)
01/15/2023 20:47:22 - INFO - __main__ -   test: [batch: 241/391 ] | Loss: 0.919 | Acc: 76.811% (23793/30976)/ 93.317% (28906/30976)
01/15/2023 20:47:24 - INFO - __main__ -   test: [batch: 242/391 ] | Loss: 0.921 | Acc: 76.704% (23858/31104)/ 93.310% (29023/31104)
01/15/2023 20:47:26 - INFO - __main__ -   test: [batch: 243/391 ] | Loss: 0.926 | Acc: 76.598% (23923/31232)/ 93.235% (29119/31232)
01/15/2023 20:47:28 - INFO - __main__ -   test: [batch: 244/391 ] | Loss: 0.927 | Acc: 76.598% (24021/31360)/ 93.230% (29237/31360)
01/15/2023 20:47:30 - INFO - __main__ -   test: [batch: 245/391 ] | Loss: 0.926 | Acc: 76.597% (24119/31488)/ 93.220% (29353/31488)
01/15/2023 20:47:32 - INFO - __main__ -   test: [batch: 246/391 ] | Loss: 0.927 | Acc: 76.569% (24208/31616)/ 93.212% (29470/31616)
01/15/2023 20:47:34 - INFO - __main__ -   test: [batch: 247/391 ] | Loss: 0.933 | Acc: 76.481% (24278/31744)/ 93.151% (29570/31744)
01/15/2023 20:47:36 - INFO - __main__ -   test: [batch: 248/391 ] | Loss: 0.934 | Acc: 76.440% (24363/31872)/ 93.148% (29688/31872)
01/15/2023 20:47:39 - INFO - __main__ -   test: [batch: 249/391 ] | Loss: 0.936 | Acc: 76.287% (24412/32000)/ 93.144% (29806/32000)
01/15/2023 20:47:41 - INFO - __main__ -   test: [batch: 250/391 ] | Loss: 0.935 | Acc: 76.320% (24520/32128)/ 93.152% (29928/32128)
01/15/2023 20:47:43 - INFO - __main__ -   test: [batch: 251/391 ] | Loss: 0.937 | Acc: 76.287% (24607/32256)/ 93.127% (30039/32256)
01/15/2023 20:47:45 - INFO - __main__ -   test: [batch: 252/391 ] | Loss: 0.936 | Acc: 76.312% (24713/32384)/ 93.117% (30155/32384)
01/15/2023 20:47:47 - INFO - __main__ -   test: [batch: 253/391 ] | Loss: 0.938 | Acc: 76.276% (24799/32512)/ 93.098% (30268/32512)
01/15/2023 20:47:49 - INFO - __main__ -   test: [batch: 254/391 ] | Loss: 0.942 | Acc: 76.195% (24870/32640)/ 93.045% (30370/32640)
01/15/2023 20:47:51 - INFO - __main__ -   test: [batch: 255/391 ] | Loss: 0.944 | Acc: 76.169% (24959/32768)/ 93.036% (30486/32768)
01/15/2023 20:47:53 - INFO - __main__ -   test: [batch: 256/391 ] | Loss: 0.949 | Acc: 76.049% (25017/32896)/ 92.987% (30589/32896)
01/15/2023 20:47:55 - INFO - __main__ -   test: [batch: 257/391 ] | Loss: 0.950 | Acc: 76.042% (25112/33024)/ 92.981% (30706/33024)
01/15/2023 20:47:57 - INFO - __main__ -   test: [batch: 258/391 ] | Loss: 0.951 | Acc: 76.038% (25208/33152)/ 92.963% (30819/33152)
01/15/2023 20:47:59 - INFO - __main__ -   test: [batch: 259/391 ] | Loss: 0.954 | Acc: 75.925% (25268/33280)/ 92.957% (30936/33280)
01/15/2023 20:48:01 - INFO - __main__ -   test: [batch: 260/391 ] | Loss: 0.954 | Acc: 75.904% (25358/33408)/ 92.951% (31053/33408)
01/15/2023 20:48:03 - INFO - __main__ -   test: [batch: 261/391 ] | Loss: 0.952 | Acc: 75.951% (25471/33536)/ 92.975% (31180/33536)
01/15/2023 20:48:05 - INFO - __main__ -   test: [batch: 262/391 ] | Loss: 0.951 | Acc: 75.965% (25573/33664)/ 92.984% (31302/33664)
01/15/2023 20:48:07 - INFO - __main__ -   test: [batch: 263/391 ] | Loss: 0.954 | Acc: 75.900% (25648/33792)/ 92.960% (31413/33792)
01/15/2023 20:48:10 - INFO - __main__ -   test: [batch: 264/391 ] | Loss: 0.959 | Acc: 75.823% (25719/33920)/ 92.889% (31508/33920)
01/15/2023 20:48:12 - INFO - __main__ -   test: [batch: 265/391 ] | Loss: 0.958 | Acc: 75.858% (25828/34048)/ 92.889% (31627/34048)
01/15/2023 20:48:14 - INFO - __main__ -   test: [batch: 266/391 ] | Loss: 0.961 | Acc: 75.802% (25906/34176)/ 92.875% (31741/34176)
01/15/2023 20:48:16 - INFO - __main__ -   test: [batch: 267/391 ] | Loss: 0.960 | Acc: 75.837% (26015/34304)/ 92.878% (31861/34304)
01/15/2023 20:48:18 - INFO - __main__ -   test: [batch: 268/391 ] | Loss: 0.960 | Acc: 75.842% (26114/34432)/ 92.870% (31977/34432)
01/15/2023 20:48:20 - INFO - __main__ -   test: [batch: 269/391 ] | Loss: 0.962 | Acc: 75.787% (26192/34560)/ 92.853% (32090/34560)
01/15/2023 20:48:22 - INFO - __main__ -   test: [batch: 270/391 ] | Loss: 0.964 | Acc: 75.738% (26272/34688)/ 92.830% (32201/34688)
01/15/2023 20:48:24 - INFO - __main__ -   test: [batch: 271/391 ] | Loss: 0.965 | Acc: 75.755% (26375/34816)/ 92.828% (32319/34816)
01/15/2023 20:48:27 - INFO - __main__ -   test: [batch: 272/391 ] | Loss: 0.965 | Acc: 75.724% (26461/34944)/ 92.826% (32437/34944)
01/15/2023 20:48:29 - INFO - __main__ -   test: [batch: 273/391 ] | Loss: 0.966 | Acc: 75.738% (26563/35072)/ 92.803% (32548/35072)
01/15/2023 20:48:31 - INFO - __main__ -   test: [batch: 274/391 ] | Loss: 0.965 | Acc: 75.741% (26661/35200)/ 92.810% (32669/35200)
01/15/2023 20:48:33 - INFO - __main__ -   test: [batch: 275/391 ] | Loss: 0.967 | Acc: 75.705% (26745/35328)/ 92.799% (32784/35328)
01/15/2023 20:48:35 - INFO - __main__ -   test: [batch: 276/391 ] | Loss: 0.968 | Acc: 75.660% (26826/35456)/ 92.788% (32899/35456)
01/15/2023 20:48:37 - INFO - __main__ -   test: [batch: 277/391 ] | Loss: 0.969 | Acc: 75.627% (26911/35584)/ 92.769% (33011/35584)
01/15/2023 20:48:39 - INFO - __main__ -   test: [batch: 278/391 ] | Loss: 0.969 | Acc: 75.644% (27014/35712)/ 92.770% (33130/35712)
01/15/2023 20:48:41 - INFO - __main__ -   test: [batch: 279/391 ] | Loss: 0.969 | Acc: 75.670% (27120/35840)/ 92.765% (33247/35840)
01/15/2023 20:48:44 - INFO - __main__ -   test: [batch: 280/391 ] | Loss: 0.970 | Acc: 75.645% (27208/35968)/ 92.760% (33364/35968)
01/15/2023 20:48:46 - INFO - __main__ -   test: [batch: 281/391 ] | Loss: 0.971 | Acc: 75.640% (27303/36096)/ 92.747% (33478/36096)
01/15/2023 20:48:48 - INFO - __main__ -   test: [batch: 282/391 ] | Loss: 0.970 | Acc: 75.671% (27411/36224)/ 92.745% (33596/36224)
01/15/2023 20:48:50 - INFO - __main__ -   test: [batch: 283/391 ] | Loss: 0.970 | Acc: 75.657% (27503/36352)/ 92.740% (33713/36352)
01/15/2023 20:48:52 - INFO - __main__ -   test: [batch: 284/391 ] | Loss: 0.974 | Acc: 75.606% (27581/36480)/ 92.684% (33811/36480)
01/15/2023 20:48:54 - INFO - __main__ -   test: [batch: 285/391 ] | Loss: 0.977 | Acc: 75.571% (27665/36608)/ 92.635% (33912/36608)
01/15/2023 20:48:56 - INFO - __main__ -   test: [batch: 286/391 ] | Loss: 0.978 | Acc: 75.534% (27748/36736)/ 92.618% (34024/36736)
01/15/2023 20:48:58 - INFO - __main__ -   test: [batch: 287/391 ] | Loss: 0.978 | Acc: 75.524% (27841/36864)/ 92.611% (34140/36864)
01/15/2023 20:49:00 - INFO - __main__ -   test: [batch: 288/391 ] | Loss: 0.978 | Acc: 75.546% (27946/36992)/ 92.615% (34260/36992)
01/15/2023 20:49:02 - INFO - __main__ -   test: [batch: 289/391 ] | Loss: 0.979 | Acc: 75.523% (28034/37120)/ 92.581% (34366/37120)
01/15/2023 20:49:04 - INFO - __main__ -   test: [batch: 290/391 ] | Loss: 0.981 | Acc: 75.440% (28100/37248)/ 92.577% (34483/37248)
01/15/2023 20:49:06 - INFO - __main__ -   test: [batch: 291/391 ] | Loss: 0.981 | Acc: 75.444% (28198/37376)/ 92.578% (34602/37376)
01/15/2023 20:49:08 - INFO - __main__ -   test: [batch: 292/391 ] | Loss: 0.983 | Acc: 75.384% (28272/37504)/ 92.558% (34713/37504)
01/15/2023 20:49:10 - INFO - __main__ -   test: [batch: 293/391 ] | Loss: 0.984 | Acc: 75.361% (28360/37632)/ 92.560% (34832/37632)
01/15/2023 20:49:12 - INFO - __main__ -   test: [batch: 294/391 ] | Loss: 0.985 | Acc: 75.339% (28448/37760)/ 92.545% (34945/37760)
01/15/2023 20:49:14 - INFO - __main__ -   test: [batch: 295/391 ] | Loss: 0.984 | Acc: 75.362% (28553/37888)/ 92.546% (35064/37888)
01/15/2023 20:49:16 - INFO - __main__ -   test: [batch: 296/391 ] | Loss: 0.985 | Acc: 75.352% (28646/38016)/ 92.537% (35179/38016)
01/15/2023 20:49:18 - INFO - __main__ -   test: [batch: 297/391 ] | Loss: 0.986 | Acc: 75.333% (28735/38144)/ 92.520% (35291/38144)
01/15/2023 20:49:20 - INFO - __main__ -   test: [batch: 298/391 ] | Loss: 0.988 | Acc: 75.293% (28816/38272)/ 92.504% (35403/38272)
01/15/2023 20:49:22 - INFO - __main__ -   test: [batch: 299/391 ] | Loss: 0.990 | Acc: 75.279% (28907/38400)/ 92.479% (35512/38400)
01/15/2023 20:49:24 - INFO - __main__ -   test: [batch: 300/391 ] | Loss: 0.990 | Acc: 75.293% (29009/38528)/ 92.478% (35630/38528)
01/15/2023 20:49:26 - INFO - __main__ -   test: [batch: 301/391 ] | Loss: 0.991 | Acc: 75.282% (29101/38656)/ 92.467% (35744/38656)
01/15/2023 20:49:28 - INFO - __main__ -   test: [batch: 302/391 ] | Loss: 0.993 | Acc: 75.248% (29184/38784)/ 92.448% (35855/38784)
01/15/2023 20:49:30 - INFO - __main__ -   test: [batch: 303/391 ] | Loss: 0.994 | Acc: 75.216% (29268/38912)/ 92.437% (35969/38912)
01/15/2023 20:49:33 - INFO - __main__ -   test: [batch: 304/391 ] | Loss: 0.993 | Acc: 75.231% (29370/39040)/ 92.444% (36090/39040)
01/15/2023 20:49:35 - INFO - __main__ -   test: [batch: 305/391 ] | Loss: 0.994 | Acc: 75.194% (29452/39168)/ 92.438% (36206/39168)
01/15/2023 20:49:36 - INFO - __main__ -   test: [batch: 306/391 ] | Loss: 0.995 | Acc: 75.188% (29546/39296)/ 92.414% (36315/39296)
01/15/2023 20:49:39 - INFO - __main__ -   test: [batch: 307/391 ] | Loss: 0.996 | Acc: 75.167% (29634/39424)/ 92.401% (36428/39424)
01/15/2023 20:49:41 - INFO - __main__ -   test: [batch: 308/391 ] | Loss: 0.997 | Acc: 75.159% (29727/39552)/ 92.390% (36542/39552)
01/15/2023 20:49:43 - INFO - __main__ -   test: [batch: 309/391 ] | Loss: 0.998 | Acc: 75.149% (29819/39680)/ 92.366% (36651/39680)
01/15/2023 20:49:45 - INFO - __main__ -   test: [batch: 310/391 ] | Loss: 0.999 | Acc: 75.131% (29908/39808)/ 92.348% (36762/39808)
01/15/2023 20:49:47 - INFO - __main__ -   test: [batch: 311/391 ] | Loss: 1.001 | Acc: 75.113% (29997/39936)/ 92.333% (36874/39936)
01/15/2023 20:49:49 - INFO - __main__ -   test: [batch: 312/391 ] | Loss: 1.002 | Acc: 75.097% (30087/40064)/ 92.317% (36986/40064)
01/15/2023 20:49:51 - INFO - __main__ -   test: [batch: 313/391 ] | Loss: 1.000 | Acc: 75.144% (30202/40192)/ 92.342% (37114/40192)
01/15/2023 20:49:53 - INFO - __main__ -   test: [batch: 314/391 ] | Loss: 1.000 | Acc: 75.131% (30293/40320)/ 92.334% (37229/40320)
01/15/2023 20:49:55 - INFO - __main__ -   test: [batch: 315/391 ] | Loss: 1.001 | Acc: 75.121% (30385/40448)/ 92.319% (37341/40448)
01/15/2023 20:49:57 - INFO - __main__ -   test: [batch: 316/391 ] | Loss: 1.003 | Acc: 75.027% (30443/40576)/ 92.298% (37451/40576)
01/15/2023 20:49:59 - INFO - __main__ -   test: [batch: 317/391 ] | Loss: 1.005 | Acc: 74.983% (30521/40704)/ 92.266% (37556/40704)
01/15/2023 20:50:01 - INFO - __main__ -   test: [batch: 318/391 ] | Loss: 1.004 | Acc: 75.012% (30629/40832)/ 92.285% (37682/40832)
01/15/2023 20:50:03 - INFO - __main__ -   test: [batch: 319/391 ] | Loss: 1.007 | Acc: 74.961% (30704/40960)/ 92.258% (37789/40960)
01/15/2023 20:50:06 - INFO - __main__ -   test: [batch: 320/391 ] | Loss: 1.005 | Acc: 75.002% (30817/41088)/ 92.268% (37911/41088)
01/15/2023 20:50:08 - INFO - __main__ -   test: [batch: 321/391 ] | Loss: 1.005 | Acc: 75.022% (30921/41216)/ 92.263% (38027/41216)
01/15/2023 20:50:10 - INFO - __main__ -   test: [batch: 322/391 ] | Loss: 1.007 | Acc: 74.978% (30999/41344)/ 92.253% (38141/41344)
01/15/2023 20:50:12 - INFO - __main__ -   test: [batch: 323/391 ] | Loss: 1.009 | Acc: 74.942% (31080/41472)/ 92.226% (38248/41472)
01/15/2023 20:50:14 - INFO - __main__ -   test: [batch: 324/391 ] | Loss: 1.009 | Acc: 74.942% (31176/41600)/ 92.219% (38363/41600)
01/15/2023 20:50:16 - INFO - __main__ -   test: [batch: 325/391 ] | Loss: 1.009 | Acc: 74.950% (31275/41728)/ 92.221% (38482/41728)
01/15/2023 20:50:18 - INFO - __main__ -   test: [batch: 326/391 ] | Loss: 1.012 | Acc: 74.864% (31335/41856)/ 92.171% (38579/41856)
01/15/2023 20:50:20 - INFO - __main__ -   test: [batch: 327/391 ] | Loss: 1.016 | Acc: 74.778% (31395/41984)/ 92.126% (38678/41984)
01/15/2023 20:50:22 - INFO - __main__ -   test: [batch: 328/391 ] | Loss: 1.018 | Acc: 74.725% (31468/42112)/ 92.093% (38782/42112)
01/15/2023 20:50:24 - INFO - __main__ -   test: [batch: 329/391 ] | Loss: 1.018 | Acc: 74.728% (31565/42240)/ 92.098% (38902/42240)
01/15/2023 20:50:26 - INFO - __main__ -   test: [batch: 330/391 ] | Loss: 1.019 | Acc: 74.693% (31646/42368)/ 92.079% (39012/42368)
01/15/2023 20:50:28 - INFO - __main__ -   test: [batch: 331/391 ] | Loss: 1.019 | Acc: 74.659% (31727/42496)/ 92.096% (39137/42496)
01/15/2023 20:50:31 - INFO - __main__ -   test: [batch: 332/391 ] | Loss: 1.020 | Acc: 74.639% (31814/42624)/ 92.089% (39252/42624)
01/15/2023 20:50:33 - INFO - __main__ -   test: [batch: 333/391 ] | Loss: 1.019 | Acc: 74.670% (31923/42752)/ 92.096% (39373/42752)
01/15/2023 20:50:35 - INFO - __main__ -   test: [batch: 334/391 ] | Loss: 1.020 | Acc: 74.634% (32003/42880)/ 92.090% (39488/42880)
01/15/2023 20:50:37 - INFO - __main__ -   test: [batch: 335/391 ] | Loss: 1.021 | Acc: 74.607% (32087/43008)/ 92.071% (39598/43008)
01/15/2023 20:50:39 - INFO - __main__ -   test: [batch: 336/391 ] | Loss: 1.023 | Acc: 74.562% (32163/43136)/ 92.048% (39706/43136)
01/15/2023 20:50:41 - INFO - __main__ -   test: [batch: 337/391 ] | Loss: 1.023 | Acc: 74.561% (32258/43264)/ 92.040% (39820/43264)
01/15/2023 20:50:43 - INFO - __main__ -   test: [batch: 338/391 ] | Loss: 1.023 | Acc: 74.558% (32352/43392)/ 92.049% (39942/43392)
01/15/2023 20:50:45 - INFO - __main__ -   test: [batch: 339/391 ] | Loss: 1.026 | Acc: 74.522% (32432/43520)/ 92.022% (40048/43520)
01/15/2023 20:50:47 - INFO - __main__ -   test: [batch: 340/391 ] | Loss: 1.025 | Acc: 74.519% (32526/43648)/ 92.034% (40171/43648)
01/15/2023 20:50:49 - INFO - __main__ -   test: [batch: 341/391 ] | Loss: 1.023 | Acc: 74.561% (32640/43776)/ 92.053% (40297/43776)
01/15/2023 20:50:52 - INFO - __main__ -   test: [batch: 342/391 ] | Loss: 1.024 | Acc: 74.510% (32713/43904)/ 92.039% (40409/43904)
01/15/2023 20:50:54 - INFO - __main__ -   test: [batch: 343/391 ] | Loss: 1.024 | Acc: 74.505% (32806/44032)/ 92.038% (40526/44032)
01/15/2023 20:50:56 - INFO - __main__ -   test: [batch: 344/391 ] | Loss: 1.025 | Acc: 74.502% (32900/44160)/ 92.029% (40640/44160)
01/15/2023 20:50:58 - INFO - __main__ -   test: [batch: 345/391 ] | Loss: 1.028 | Acc: 74.431% (32964/44288)/ 91.996% (40743/44288)
01/15/2023 20:51:00 - INFO - __main__ -   test: [batch: 346/391 ] | Loss: 1.030 | Acc: 74.412% (33051/44416)/ 91.985% (40856/44416)
01/15/2023 20:51:02 - INFO - __main__ -   test: [batch: 347/391 ] | Loss: 1.029 | Acc: 74.428% (33153/44544)/ 91.990% (40976/44544)
01/15/2023 20:51:04 - INFO - __main__ -   test: [batch: 348/391 ] | Loss: 1.030 | Acc: 74.402% (33237/44672)/ 91.973% (41086/44672)
01/15/2023 20:51:06 - INFO - __main__ -   test: [batch: 349/391 ] | Loss: 1.030 | Acc: 74.402% (33332/44800)/ 91.984% (41209/44800)
01/15/2023 20:51:08 - INFO - __main__ -   test: [batch: 350/391 ] | Loss: 1.030 | Acc: 74.415% (33433/44928)/ 91.981% (41325/44928)
01/15/2023 20:51:11 - INFO - __main__ -   test: [batch: 351/391 ] | Loss: 1.032 | Acc: 74.356% (33502/45056)/ 91.966% (41436/45056)
01/15/2023 20:51:13 - INFO - __main__ -   test: [batch: 352/391 ] | Loss: 1.032 | Acc: 74.352% (33595/45184)/ 91.962% (41552/45184)
01/15/2023 20:51:15 - INFO - __main__ -   test: [batch: 353/391 ] | Loss: 1.035 | Acc: 74.303% (33668/45312)/ 91.920% (41651/45312)
01/15/2023 20:51:17 - INFO - __main__ -   test: [batch: 354/391 ] | Loss: 1.038 | Acc: 74.247% (33738/45440)/ 91.904% (41761/45440)
01/15/2023 20:51:19 - INFO - __main__ -   test: [batch: 355/391 ] | Loss: 1.040 | Acc: 74.179% (33802/45568)/ 91.889% (41872/45568)
01/15/2023 20:51:21 - INFO - __main__ -   test: [batch: 356/391 ] | Loss: 1.041 | Acc: 74.166% (33891/45696)/ 91.896% (41993/45696)
01/15/2023 20:51:23 - INFO - __main__ -   test: [batch: 357/391 ] | Loss: 1.039 | Acc: 74.201% (34002/45824)/ 91.910% (42117/45824)
01/15/2023 20:51:25 - INFO - __main__ -   test: [batch: 358/391 ] | Loss: 1.038 | Acc: 74.223% (34107/45952)/ 91.915% (42237/45952)
01/15/2023 20:51:28 - INFO - __main__ -   test: [batch: 359/391 ] | Loss: 1.039 | Acc: 74.227% (34204/46080)/ 91.912% (42353/46080)
01/15/2023 20:51:30 - INFO - __main__ -   test: [batch: 360/391 ] | Loss: 1.040 | Acc: 74.188% (34281/46208)/ 91.913% (42471/46208)
01/15/2023 20:51:32 - INFO - __main__ -   test: [batch: 361/391 ] | Loss: 1.041 | Acc: 74.186% (34375/46336)/ 91.913% (42589/46336)
01/15/2023 20:51:34 - INFO - __main__ -   test: [batch: 362/391 ] | Loss: 1.040 | Acc: 74.191% (34472/46464)/ 91.925% (42712/46464)
01/15/2023 20:51:36 - INFO - __main__ -   test: [batch: 363/391 ] | Loss: 1.041 | Acc: 74.172% (34558/46592)/ 91.911% (42823/46592)
01/15/2023 20:51:38 - INFO - __main__ -   test: [batch: 364/391 ] | Loss: 1.040 | Acc: 74.197% (34665/46720)/ 91.922% (42946/46720)
01/15/2023 20:51:40 - INFO - __main__ -   test: [batch: 365/391 ] | Loss: 1.040 | Acc: 74.200% (34761/46848)/ 91.919% (43062/46848)
01/15/2023 20:51:42 - INFO - __main__ -   test: [batch: 366/391 ] | Loss: 1.038 | Acc: 74.236% (34873/46976)/ 91.936% (43188/46976)
01/15/2023 20:51:44 - INFO - __main__ -   test: [batch: 367/391 ] | Loss: 1.037 | Acc: 74.251% (34975/47104)/ 91.945% (43310/47104)
01/15/2023 20:51:47 - INFO - __main__ -   test: [batch: 368/391 ] | Loss: 1.037 | Acc: 74.244% (35067/47232)/ 91.952% (43431/47232)
01/15/2023 20:51:49 - INFO - __main__ -   test: [batch: 369/391 ] | Loss: 1.036 | Acc: 74.261% (35170/47360)/ 91.964% (43554/47360)
01/15/2023 20:51:51 - INFO - __main__ -   test: [batch: 370/391 ] | Loss: 1.037 | Acc: 74.238% (35254/47488)/ 91.966% (43673/47488)
01/15/2023 20:51:53 - INFO - __main__ -   test: [batch: 371/391 ] | Loss: 1.037 | Acc: 74.254% (35357/47616)/ 91.965% (43790/47616)
01/15/2023 20:51:55 - INFO - __main__ -   test: [batch: 372/391 ] | Loss: 1.035 | Acc: 74.303% (35475/47744)/ 91.982% (43916/47744)
01/15/2023 20:51:57 - INFO - __main__ -   test: [batch: 373/391 ] | Loss: 1.034 | Acc: 74.336% (35586/47872)/ 91.991% (44038/47872)
01/15/2023 20:51:59 - INFO - __main__ -   test: [batch: 374/391 ] | Loss: 1.033 | Acc: 74.358% (35692/48000)/ 91.992% (44156/48000)
01/15/2023 20:52:01 - INFO - __main__ -   test: [batch: 375/391 ] | Loss: 1.036 | Acc: 74.306% (35762/48128)/ 91.957% (44257/48128)
01/15/2023 20:52:03 - INFO - __main__ -   test: [batch: 376/391 ] | Loss: 1.037 | Acc: 74.293% (35851/48256)/ 91.941% (44367/48256)
01/15/2023 20:52:05 - INFO - __main__ -   test: [batch: 377/391 ] | Loss: 1.037 | Acc: 74.289% (35944/48384)/ 91.933% (44481/48384)
01/15/2023 20:52:07 - INFO - __main__ -   test: [batch: 378/391 ] | Loss: 1.040 | Acc: 74.219% (36005/48512)/ 91.893% (44579/48512)
01/15/2023 20:52:09 - INFO - __main__ -   test: [batch: 379/391 ] | Loss: 1.040 | Acc: 74.204% (36093/48640)/ 91.904% (44702/48640)
01/15/2023 20:52:11 - INFO - __main__ -   test: [batch: 380/391 ] | Loss: 1.040 | Acc: 74.215% (36193/48768)/ 91.919% (44827/48768)
01/15/2023 20:52:13 - INFO - __main__ -   test: [batch: 381/391 ] | Loss: 1.041 | Acc: 74.170% (36266/48896)/ 91.918% (44944/48896)
01/15/2023 20:52:15 - INFO - __main__ -   test: [batch: 382/391 ] | Loss: 1.042 | Acc: 74.137% (36345/49024)/ 91.908% (45057/49024)
01/15/2023 20:52:17 - INFO - __main__ -   test: [batch: 383/391 ] | Loss: 1.043 | Acc: 74.139% (36441/49152)/ 91.903% (45172/49152)
01/15/2023 20:52:20 - INFO - __main__ -   test: [batch: 384/391 ] | Loss: 1.041 | Acc: 74.174% (36553/49280)/ 91.918% (45297/49280)
01/15/2023 20:52:22 - INFO - __main__ -   test: [batch: 385/391 ] | Loss: 1.039 | Acc: 74.186% (36654/49408)/ 91.930% (45421/49408)
01/15/2023 20:52:24 - INFO - __main__ -   test: [batch: 386/391 ] | Loss: 1.037 | Acc: 74.229% (36770/49536)/ 91.949% (45548/49536)
01/15/2023 20:52:26 - INFO - __main__ -   test: [batch: 387/391 ] | Loss: 1.036 | Acc: 74.273% (36887/49664)/ 91.960% (45671/49664)
01/15/2023 20:52:28 - INFO - __main__ -   test: [batch: 388/391 ] | Loss: 1.034 | Acc: 74.325% (37008/49792)/ 91.975% (45796/49792)
01/15/2023 20:52:30 - INFO - __main__ -   test: [batch: 389/391 ] | Loss: 1.033 | Acc: 74.327% (37104/49920)/ 91.981% (45917/49920)
01/15/2023 20:52:32 - INFO - __main__ -   test: [batch: 390/391 ] | Loss: 1.035 | Acc: 74.286% (37143/50000)/ 91.970% (45985/50000)
01/15/2023 20:52:32 - INFO - __main__ -   Final accuracy: 74.286
