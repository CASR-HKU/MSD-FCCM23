/home/jjc/miniconda3/envs/ant_quant/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
Namespace(a_low=75, a_up=150, abit=8, batch_size=128, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='csd_eb2', epoch=5, layer_4bit_l=None, layer_8bit_l=None, layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='mobilenet_v2', percent=100, ptq=False, resume=False, search=False, tag='', train=False, w_low=75, w_up=150, wbit=8)
01/12/2023 16:59:49 - INFO - __main__ -   output/mobilenet_v2_imagenet/int_W8A8_26191/gpu_0
01/12/2023 16:59:49 - INFO - __main__ -   Namespace(a_low=75, a_up=150, abit=8, batch_size=128, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='csd_eb2', epoch=5, layer_4bit_l=None, layer_8bit_l=None, layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='mobilenet_v2', percent=100, ptq=False, resume=False, search=False, tag='', train=False, w_low=75, w_up=150, wbit=8)
01/12/2023 16:59:49 - INFO - __main__ -   ==> Preparing data..
01/12/2023 16:59:52 - INFO - __main__ -   ==> Setting quantizer..
Namespace(a_low=75, a_up=150, abit=8, batch_size=128, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='csd_eb2', epoch=5, layer_4bit_l=None, layer_8bit_l=None, layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='mobilenet_v2', percent=100, ptq=False, resume=False, search=False, tag='', train=False, w_low=75, w_up=150, wbit=8)
01/12/2023 16:59:52 - INFO - __main__ -   Namespace(a_low=75, a_up=150, abit=8, batch_size=128, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='csd_eb2', epoch=5, layer_4bit_l=None, layer_8bit_l=None, layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='mobilenet_v2', percent=100, ptq=False, resume=False, search=False, tag='', train=False, w_low=75, w_up=150, wbit=8)
01/12/2023 16:59:52 - INFO - __main__ -   ==> Building model..
MobileNetV2(
  (features): Sequential(
    (0): Sequential(
      (0): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (1): SyncBatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
    )
    (1): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (2): SyncBatchNorm(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (3): SyncBatchNorm(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (3): SyncBatchNorm(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (3): SyncBatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (3): SyncBatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (3): SyncBatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (7): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (3): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (8): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (3): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (9): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (3): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (10): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (3): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (11): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (3): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (12): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (3): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (13): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (3): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (14): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (3): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (15): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (3): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (16): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (3): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (17): InvertedResidual(
      (conv): Sequential(
        (0): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): Sequential(
          (0): Conv2dQuantizer(
            (quant_weight): TensorQuantizer()
            (quant_input): TensorQuantizer()
          )
          (1): SyncBatchNorm(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (3): SyncBatchNorm(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (18): Sequential(
      (0): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (1): SyncBatchNorm(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
    )
  )
  (classifier): Sequential(
    (0): Dropout(p=0.2, inplace=False)
    (1): LinearQuantizer(
      (quant_weight): TensorQuantizer()
      (quant_input): TensorQuantizer()
    )
  )
)
Layer quant EB csd_eb2
int	8-bit 	 features.0.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.0.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.1.conv.0.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.1.conv.0.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.1.conv.1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.1.conv.1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.2.conv.0.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.2.conv.0.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.2.conv.1.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.2.conv.1.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.2.conv.2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.2.conv.2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.3.conv.0.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.3.conv.0.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.3.conv.1.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.3.conv.1.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.3.conv.2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.3.conv.2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.4.conv.0.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.4.conv.0.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.4.conv.1.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.4.conv.1.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.4.conv.2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.4.conv.2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.5.conv.0.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.5.conv.0.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.5.conv.1.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.5.conv.1.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.5.conv.2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.5.conv.2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.6.conv.0.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.6.conv.0.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.6.conv.1.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.6.conv.1.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.6.conv.2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.6.conv.2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.7.conv.0.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.7.conv.0.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.7.conv.1.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.7.conv.1.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.7.conv.2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.7.conv.2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.8.conv.0.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.8.conv.0.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.8.conv.1.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.8.conv.1.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.8.conv.2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.8.conv.2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.9.conv.0.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.9.conv.0.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.9.conv.1.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.9.conv.1.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.9.conv.2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.9.conv.2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.10.conv.0.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.10.conv.0.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.10.conv.1.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.10.conv.1.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.10.conv.2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.10.conv.2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.11.conv.0.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.11.conv.0.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.11.conv.1.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.11.conv.1.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.11.conv.2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.11.conv.2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.12.conv.0.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.12.conv.0.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.12.conv.1.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.12.conv.1.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.12.conv.2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.12.conv.2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.13.conv.0.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.13.conv.0.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.13.conv.1.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.13.conv.1.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.13.conv.2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.13.conv.2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.14.conv.0.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.14.conv.0.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.14.conv.1.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.14.conv.1.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.14.conv.2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.14.conv.2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.15.conv.0.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.15.conv.0.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.15.conv.1.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.15.conv.1.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.15.conv.2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.15.conv.2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.16.conv.0.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.16.conv.0.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.16.conv.1.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.16.conv.1.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.16.conv.2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.16.conv.2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.17.conv.0.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.17.conv.0.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.17.conv.1.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.17.conv.1.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.17.conv.2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.17.conv.2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 features.18.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 features.18.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 classifier.1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 classifier.1.quant_input,
set init to 1
01/12/2023 17:00:03 - INFO - __main__ -   test: [batch: 0/391 ] | Loss: 0.544 | Acc: 83.594% (107/128)/ 96.094% (123/128)
01/12/2023 17:00:04 - INFO - __main__ -   test: [batch: 1/391 ] | Loss: 0.711 | Acc: 77.734% (199/256)/ 96.094% (246/256)
01/12/2023 17:00:04 - INFO - __main__ -   test: [batch: 2/391 ] | Loss: 0.949 | Acc: 72.917% (280/384)/ 93.229% (358/384)
01/12/2023 17:00:04 - INFO - __main__ -   test: [batch: 3/391 ] | Loss: 0.835 | Acc: 76.562% (392/512)/ 93.945% (481/512)
01/12/2023 17:00:04 - INFO - __main__ -   test: [batch: 4/391 ] | Loss: 0.745 | Acc: 79.844% (511/640)/ 94.688% (606/640)
01/12/2023 17:00:04 - INFO - __main__ -   test: [batch: 5/391 ] | Loss: 0.665 | Acc: 81.901% (629/768)/ 95.443% (733/768)
01/12/2023 17:00:05 - INFO - __main__ -   test: [batch: 6/391 ] | Loss: 0.661 | Acc: 82.366% (738/896)/ 95.424% (855/896)
01/12/2023 17:00:05 - INFO - __main__ -   test: [batch: 7/391 ] | Loss: 0.648 | Acc: 83.301% (853/1024)/ 95.508% (978/1024)
01/12/2023 17:00:05 - INFO - __main__ -   test: [batch: 8/391 ] | Loss: 0.661 | Acc: 82.899% (955/1152)/ 95.399% (1099/1152)
01/12/2023 17:00:05 - INFO - __main__ -   test: [batch: 9/391 ] | Loss: 0.638 | Acc: 83.672% (1071/1280)/ 95.469% (1222/1280)
01/12/2023 17:00:05 - INFO - __main__ -   test: [batch: 10/391 ] | Loss: 0.682 | Acc: 82.670% (1164/1408)/ 95.170% (1340/1408)
01/12/2023 17:00:06 - INFO - __main__ -   test: [batch: 11/391 ] | Loss: 0.686 | Acc: 83.008% (1275/1536)/ 94.987% (1459/1536)
01/12/2023 17:00:06 - INFO - __main__ -   test: [batch: 12/391 ] | Loss: 0.755 | Acc: 81.250% (1352/1664)/ 94.231% (1568/1664)
01/12/2023 17:00:06 - INFO - __main__ -   test: [batch: 13/391 ] | Loss: 0.836 | Acc: 79.129% (1418/1792)/ 93.359% (1673/1792)
01/12/2023 17:00:06 - INFO - __main__ -   test: [batch: 14/391 ] | Loss: 0.856 | Acc: 78.229% (1502/1920)/ 93.490% (1795/1920)
01/12/2023 17:00:06 - INFO - __main__ -   test: [batch: 15/391 ] | Loss: 0.872 | Acc: 77.295% (1583/2048)/ 93.408% (1913/2048)
01/12/2023 17:00:07 - INFO - __main__ -   test: [batch: 16/391 ] | Loss: 0.889 | Acc: 76.930% (1674/2176)/ 93.153% (2027/2176)
01/12/2023 17:00:07 - INFO - __main__ -   test: [batch: 17/391 ] | Loss: 0.925 | Acc: 76.519% (1763/2304)/ 92.578% (2133/2304)
01/12/2023 17:00:07 - INFO - __main__ -   test: [batch: 18/391 ] | Loss: 0.942 | Acc: 75.863% (1845/2432)/ 92.516% (2250/2432)
01/12/2023 17:00:07 - INFO - __main__ -   test: [batch: 19/391 ] | Loss: 0.969 | Acc: 75.391% (1930/2560)/ 92.305% (2363/2560)
01/12/2023 17:00:07 - INFO - __main__ -   test: [batch: 20/391 ] | Loss: 0.963 | Acc: 75.558% (2031/2688)/ 92.411% (2484/2688)
01/12/2023 17:00:08 - INFO - __main__ -   test: [batch: 21/391 ] | Loss: 0.985 | Acc: 75.000% (2112/2816)/ 92.188% (2596/2816)
01/12/2023 17:00:08 - INFO - __main__ -   test: [batch: 22/391 ] | Loss: 0.979 | Acc: 75.102% (2211/2944)/ 92.221% (2715/2944)
01/12/2023 17:00:08 - INFO - __main__ -   test: [batch: 23/391 ] | Loss: 1.040 | Acc: 73.926% (2271/3072)/ 91.764% (2819/3072)
01/12/2023 17:00:08 - INFO - __main__ -   test: [batch: 24/391 ] | Loss: 1.052 | Acc: 73.469% (2351/3200)/ 91.656% (2933/3200)
01/12/2023 17:00:08 - INFO - __main__ -   test: [batch: 25/391 ] | Loss: 1.074 | Acc: 73.107% (2433/3328)/ 91.346% (3040/3328)
01/12/2023 17:00:09 - INFO - __main__ -   test: [batch: 26/391 ] | Loss: 1.100 | Acc: 72.338% (2500/3456)/ 91.088% (3148/3456)
01/12/2023 17:00:09 - INFO - __main__ -   test: [batch: 27/391 ] | Loss: 1.081 | Acc: 72.879% (2612/3584)/ 91.211% (3269/3584)
01/12/2023 17:00:09 - INFO - __main__ -   test: [batch: 28/391 ] | Loss: 1.087 | Acc: 72.225% (2681/3712)/ 91.325% (3390/3712)
01/12/2023 17:00:09 - INFO - __main__ -   test: [batch: 29/391 ] | Loss: 1.072 | Acc: 72.734% (2793/3840)/ 91.458% (3512/3840)
01/12/2023 17:00:09 - INFO - __main__ -   test: [batch: 30/391 ] | Loss: 1.081 | Acc: 72.757% (2887/3968)/ 91.356% (3625/3968)
01/12/2023 17:00:10 - INFO - __main__ -   test: [batch: 31/391 ] | Loss: 1.073 | Acc: 73.022% (2991/4096)/ 91.309% (3740/4096)
01/12/2023 17:00:10 - INFO - __main__ -   test: [batch: 32/391 ] | Loss: 1.055 | Acc: 73.509% (3105/4224)/ 91.454% (3863/4224)
01/12/2023 17:00:10 - INFO - __main__ -   test: [batch: 33/391 ] | Loss: 1.044 | Acc: 73.828% (3213/4352)/ 91.590% (3986/4352)
01/12/2023 17:00:10 - INFO - __main__ -   test: [batch: 34/391 ] | Loss: 1.025 | Acc: 74.375% (3332/4480)/ 91.763% (4111/4480)
01/12/2023 17:00:10 - INFO - __main__ -   test: [batch: 35/391 ] | Loss: 1.009 | Acc: 74.848% (3449/4608)/ 91.884% (4234/4608)
01/12/2023 17:00:11 - INFO - __main__ -   test: [batch: 36/391 ] | Loss: 0.994 | Acc: 75.296% (3566/4736)/ 91.976% (4356/4736)
01/12/2023 17:00:11 - INFO - __main__ -   test: [batch: 37/391 ] | Loss: 0.983 | Acc: 75.596% (3677/4864)/ 92.044% (4477/4864)
01/12/2023 17:00:11 - INFO - __main__ -   test: [batch: 38/391 ] | Loss: 0.980 | Acc: 75.641% (3776/4992)/ 92.027% (4594/4992)
01/12/2023 17:00:11 - INFO - __main__ -   test: [batch: 39/391 ] | Loss: 0.974 | Acc: 75.723% (3877/5120)/ 92.090% (4715/5120)
01/12/2023 17:00:11 - INFO - __main__ -   test: [batch: 40/391 ] | Loss: 0.981 | Acc: 75.572% (3966/5248)/ 91.959% (4826/5248)
01/12/2023 17:00:12 - INFO - __main__ -   test: [batch: 41/391 ] | Loss: 0.982 | Acc: 75.688% (4069/5376)/ 91.927% (4942/5376)
01/12/2023 17:00:12 - INFO - __main__ -   test: [batch: 42/391 ] | Loss: 0.976 | Acc: 75.799% (4172/5504)/ 92.042% (5066/5504)
01/12/2023 17:00:12 - INFO - __main__ -   test: [batch: 43/391 ] | Loss: 0.975 | Acc: 75.870% (4273/5632)/ 92.045% (5184/5632)
01/12/2023 17:00:12 - INFO - __main__ -   test: [batch: 44/391 ] | Loss: 0.978 | Acc: 75.851% (4369/5760)/ 91.979% (5298/5760)
01/12/2023 17:00:12 - INFO - __main__ -   test: [batch: 45/391 ] | Loss: 0.974 | Acc: 76.002% (4475/5888)/ 92.035% (5419/5888)
01/12/2023 17:00:13 - INFO - __main__ -   test: [batch: 46/391 ] | Loss: 0.980 | Acc: 75.898% (4566/6016)/ 92.021% (5536/6016)
01/12/2023 17:00:13 - INFO - __main__ -   test: [batch: 47/391 ] | Loss: 0.981 | Acc: 75.911% (4664/6144)/ 92.041% (5655/6144)
01/12/2023 17:00:13 - INFO - __main__ -   test: [batch: 48/391 ] | Loss: 0.990 | Acc: 75.686% (4747/6272)/ 91.996% (5770/6272)
01/12/2023 17:00:13 - INFO - __main__ -   test: [batch: 49/391 ] | Loss: 0.996 | Acc: 75.719% (4846/6400)/ 91.844% (5878/6400)
01/12/2023 17:00:13 - INFO - __main__ -   test: [batch: 50/391 ] | Loss: 0.988 | Acc: 75.843% (4951/6528)/ 91.896% (5999/6528)
01/12/2023 17:00:13 - INFO - __main__ -   test: [batch: 51/391 ] | Loss: 0.981 | Acc: 76.052% (5062/6656)/ 91.977% (6122/6656)
01/12/2023 17:00:14 - INFO - __main__ -   test: [batch: 52/391 ] | Loss: 0.976 | Acc: 76.150% (5166/6784)/ 92.011% (6242/6784)
01/12/2023 17:00:14 - INFO - __main__ -   test: [batch: 53/391 ] | Loss: 0.966 | Acc: 76.403% (5281/6912)/ 92.072% (6364/6912)
01/12/2023 17:00:14 - INFO - __main__ -   test: [batch: 54/391 ] | Loss: 0.956 | Acc: 76.648% (5396/7040)/ 92.159% (6488/7040)
01/12/2023 17:00:14 - INFO - __main__ -   test: [batch: 55/391 ] | Loss: 0.951 | Acc: 76.744% (5501/7168)/ 92.146% (6605/7168)
01/12/2023 17:00:15 - INFO - __main__ -   test: [batch: 56/391 ] | Loss: 0.940 | Acc: 76.987% (5617/7296)/ 92.229% (6729/7296)
01/12/2023 17:00:15 - INFO - __main__ -   test: [batch: 57/391 ] | Loss: 0.932 | Acc: 77.196% (5731/7424)/ 92.282% (6851/7424)
01/12/2023 17:00:15 - INFO - __main__ -   test: [batch: 58/391 ] | Loss: 0.927 | Acc: 77.278% (5836/7552)/ 92.307% (6971/7552)
01/12/2023 17:00:15 - INFO - __main__ -   test: [batch: 59/391 ] | Loss: 0.930 | Acc: 77.044% (5917/7680)/ 92.318% (7090/7680)
01/12/2023 17:00:15 - INFO - __main__ -   test: [batch: 60/391 ] | Loss: 0.931 | Acc: 77.011% (6013/7808)/ 92.341% (7210/7808)
01/12/2023 17:00:16 - INFO - __main__ -   test: [batch: 61/391 ] | Loss: 0.930 | Acc: 77.041% (6114/7936)/ 92.402% (7333/7936)
01/12/2023 17:00:16 - INFO - __main__ -   test: [batch: 62/391 ] | Loss: 0.931 | Acc: 76.972% (6207/8064)/ 92.423% (7453/8064)
01/12/2023 17:00:16 - INFO - __main__ -   test: [batch: 63/391 ] | Loss: 0.940 | Acc: 76.843% (6295/8192)/ 92.346% (7565/8192)
01/12/2023 17:00:16 - INFO - __main__ -   test: [batch: 64/391 ] | Loss: 0.953 | Acc: 76.418% (6358/8320)/ 92.248% (7675/8320)
01/12/2023 17:00:16 - INFO - __main__ -   test: [batch: 65/391 ] | Loss: 0.967 | Acc: 75.900% (6412/8448)/ 92.176% (7787/8448)
01/12/2023 17:00:17 - INFO - __main__ -   test: [batch: 66/391 ] | Loss: 0.973 | Acc: 75.886% (6508/8576)/ 92.129% (7901/8576)
01/12/2023 17:00:17 - INFO - __main__ -   test: [batch: 67/391 ] | Loss: 0.975 | Acc: 75.793% (6597/8704)/ 92.107% (8017/8704)
01/12/2023 17:00:17 - INFO - __main__ -   test: [batch: 68/391 ] | Loss: 0.982 | Acc: 75.679% (6684/8832)/ 92.086% (8133/8832)
01/12/2023 17:00:17 - INFO - __main__ -   test: [batch: 69/391 ] | Loss: 0.979 | Acc: 75.714% (6784/8960)/ 92.098% (8252/8960)
01/12/2023 17:00:17 - INFO - __main__ -   test: [batch: 70/391 ] | Loss: 0.979 | Acc: 75.594% (6870/9088)/ 92.154% (8375/9088)
01/12/2023 17:00:18 - INFO - __main__ -   test: [batch: 71/391 ] | Loss: 0.981 | Acc: 75.532% (6961/9216)/ 92.122% (8490/9216)
01/12/2023 17:00:18 - INFO - __main__ -   test: [batch: 72/391 ] | Loss: 0.985 | Acc: 75.342% (7040/9344)/ 92.145% (8610/9344)
01/12/2023 17:00:18 - INFO - __main__ -   test: [batch: 73/391 ] | Loss: 0.991 | Acc: 75.169% (7120/9472)/ 92.103% (8724/9472)
01/12/2023 17:00:18 - INFO - __main__ -   test: [batch: 74/391 ] | Loss: 0.990 | Acc: 75.208% (7220/9600)/ 92.115% (8843/9600)
01/12/2023 17:00:18 - INFO - __main__ -   test: [batch: 75/391 ] | Loss: 0.995 | Acc: 75.010% (7297/9728)/ 92.002% (8950/9728)
01/12/2023 17:00:19 - INFO - __main__ -   test: [batch: 76/391 ] | Loss: 0.995 | Acc: 75.061% (7398/9856)/ 92.005% (9068/9856)
01/12/2023 17:00:19 - INFO - __main__ -   test: [batch: 77/391 ] | Loss: 0.997 | Acc: 74.900% (7478/9984)/ 92.027% (9188/9984)
01/12/2023 17:00:19 - INFO - __main__ -   test: [batch: 78/391 ] | Loss: 1.000 | Acc: 74.733% (7557/10112)/ 92.069% (9310/10112)
01/12/2023 17:00:19 - INFO - __main__ -   test: [batch: 79/391 ] | Loss: 0.999 | Acc: 74.648% (7644/10240)/ 92.158% (9437/10240)
01/12/2023 17:00:20 - INFO - __main__ -   test: [batch: 80/391 ] | Loss: 0.999 | Acc: 74.585% (7733/10368)/ 92.159% (9555/10368)
01/12/2023 17:00:20 - INFO - __main__ -   test: [batch: 81/391 ] | Loss: 0.996 | Acc: 74.695% (7840/10496)/ 92.207% (9678/10496)
01/12/2023 17:00:20 - INFO - __main__ -   test: [batch: 82/391 ] | Loss: 0.995 | Acc: 74.708% (7937/10624)/ 92.188% (9794/10624)
01/12/2023 17:00:20 - INFO - __main__ -   test: [batch: 83/391 ] | Loss: 0.997 | Acc: 74.693% (8031/10752)/ 92.160% (9909/10752)
01/12/2023 17:00:20 - INFO - __main__ -   test: [batch: 84/391 ] | Loss: 0.991 | Acc: 74.844% (8143/10880)/ 92.224% (10034/10880)
01/12/2023 17:00:21 - INFO - __main__ -   test: [batch: 85/391 ] | Loss: 0.990 | Acc: 74.827% (8237/11008)/ 92.260% (10156/11008)
01/12/2023 17:00:21 - INFO - __main__ -   test: [batch: 86/391 ] | Loss: 0.994 | Acc: 74.758% (8325/11136)/ 92.205% (10268/11136)
01/12/2023 17:00:21 - INFO - __main__ -   test: [batch: 87/391 ] | Loss: 0.993 | Acc: 74.716% (8416/11264)/ 92.214% (10387/11264)
01/12/2023 17:00:21 - INFO - __main__ -   test: [batch: 88/391 ] | Loss: 1.002 | Acc: 74.570% (8495/11392)/ 92.170% (10500/11392)
01/12/2023 17:00:22 - INFO - __main__ -   test: [batch: 89/391 ] | Loss: 1.000 | Acc: 74.601% (8594/11520)/ 92.196% (10621/11520)
01/12/2023 17:00:22 - INFO - __main__ -   test: [batch: 90/391 ] | Loss: 1.000 | Acc: 74.511% (8679/11648)/ 92.213% (10741/11648)
01/12/2023 17:00:22 - INFO - __main__ -   test: [batch: 91/391 ] | Loss: 0.999 | Acc: 74.541% (8778/11776)/ 92.230% (10861/11776)
01/12/2023 17:00:22 - INFO - __main__ -   test: [batch: 92/391 ] | Loss: 1.001 | Acc: 74.454% (8863/11904)/ 92.213% (10977/11904)
01/12/2023 17:00:22 - INFO - __main__ -   test: [batch: 93/391 ] | Loss: 1.003 | Acc: 74.294% (8939/12032)/ 92.254% (11100/12032)
01/12/2023 17:00:23 - INFO - __main__ -   test: [batch: 94/391 ] | Loss: 1.004 | Acc: 74.194% (9022/12160)/ 92.270% (11220/12160)
01/12/2023 17:00:23 - INFO - __main__ -   test: [batch: 95/391 ] | Loss: 0.999 | Acc: 74.284% (9128/12288)/ 92.310% (11343/12288)
01/12/2023 17:00:23 - INFO - __main__ -   test: [batch: 96/391 ] | Loss: 1.000 | Acc: 74.243% (9218/12416)/ 92.308% (11461/12416)
01/12/2023 17:00:23 - INFO - __main__ -   test: [batch: 97/391 ] | Loss: 1.001 | Acc: 74.075% (9292/12544)/ 92.355% (11585/12544)
01/12/2023 17:00:23 - INFO - __main__ -   test: [batch: 98/391 ] | Loss: 0.998 | Acc: 74.140% (9395/12672)/ 92.393% (11708/12672)
01/12/2023 17:00:24 - INFO - __main__ -   test: [batch: 99/391 ] | Loss: 0.991 | Acc: 74.312% (9512/12800)/ 92.453% (11834/12800)
01/12/2023 17:00:24 - INFO - __main__ -   test: [batch: 100/391 ] | Loss: 0.989 | Acc: 74.327% (9609/12928)/ 92.481% (11956/12928)
01/12/2023 17:00:24 - INFO - __main__ -   test: [batch: 101/391 ] | Loss: 0.983 | Acc: 74.479% (9724/13056)/ 92.540% (12082/13056)
01/12/2023 17:00:24 - INFO - __main__ -   test: [batch: 102/391 ] | Loss: 0.980 | Acc: 74.537% (9827/13184)/ 92.590% (12207/13184)
01/12/2023 17:00:25 - INFO - __main__ -   test: [batch: 103/391 ] | Loss: 0.982 | Acc: 74.437% (9909/13312)/ 92.586% (12325/13312)
01/12/2023 17:00:25 - INFO - __main__ -   test: [batch: 104/391 ] | Loss: 0.980 | Acc: 74.390% (9998/13440)/ 92.604% (12446/13440)
01/12/2023 17:00:25 - INFO - __main__ -   test: [batch: 105/391 ] | Loss: 0.982 | Acc: 74.388% (10093/13568)/ 92.593% (12563/13568)
01/12/2023 17:00:25 - INFO - __main__ -   test: [batch: 106/391 ] | Loss: 0.990 | Acc: 74.263% (10171/13696)/ 92.501% (12669/13696)
01/12/2023 17:00:25 - INFO - __main__ -   test: [batch: 107/391 ] | Loss: 0.986 | Acc: 74.385% (10283/13824)/ 92.535% (12792/13824)
01/12/2023 17:00:26 - INFO - __main__ -   test: [batch: 108/391 ] | Loss: 0.987 | Acc: 74.305% (10367/13952)/ 92.546% (12912/13952)
01/12/2023 17:00:26 - INFO - __main__ -   test: [batch: 109/391 ] | Loss: 0.986 | Acc: 74.318% (10464/14080)/ 92.578% (13035/14080)
01/12/2023 17:00:26 - INFO - __main__ -   test: [batch: 110/391 ] | Loss: 0.989 | Acc: 74.106% (10529/14208)/ 92.596% (13156/14208)
01/12/2023 17:00:26 - INFO - __main__ -   test: [batch: 111/391 ] | Loss: 0.988 | Acc: 74.170% (10633/14336)/ 92.599% (13275/14336)
01/12/2023 17:00:26 - INFO - __main__ -   test: [batch: 112/391 ] | Loss: 0.988 | Acc: 74.219% (10735/14464)/ 92.616% (13396/14464)
01/12/2023 17:00:27 - INFO - __main__ -   test: [batch: 113/391 ] | Loss: 0.986 | Acc: 74.246% (10834/14592)/ 92.633% (13517/14592)
01/12/2023 17:00:27 - INFO - __main__ -   test: [batch: 114/391 ] | Loss: 0.981 | Acc: 74.389% (10950/14720)/ 92.677% (13642/14720)
01/12/2023 17:00:27 - INFO - __main__ -   test: [batch: 115/391 ] | Loss: 0.977 | Acc: 74.481% (11059/14848)/ 92.713% (13766/14848)
01/12/2023 17:00:27 - INFO - __main__ -   test: [batch: 116/391 ] | Loss: 0.981 | Acc: 74.452% (11150/14976)/ 92.682% (13880/14976)
01/12/2023 17:00:28 - INFO - __main__ -   test: [batch: 117/391 ] | Loss: 0.978 | Acc: 74.517% (11255/15104)/ 92.704% (14002/15104)
01/12/2023 17:00:28 - INFO - __main__ -   test: [batch: 118/391 ] | Loss: 0.982 | Acc: 74.350% (11325/15232)/ 92.706% (14121/15232)
01/12/2023 17:00:28 - INFO - __main__ -   test: [batch: 119/391 ] | Loss: 0.980 | Acc: 74.414% (11430/15360)/ 92.741% (14245/15360)
01/12/2023 17:00:28 - INFO - __main__ -   test: [batch: 120/391 ] | Loss: 0.979 | Acc: 74.432% (11528/15488)/ 92.769% (14368/15488)
01/12/2023 17:00:28 - INFO - __main__ -   test: [batch: 121/391 ] | Loss: 0.983 | Acc: 74.321% (11606/15616)/ 92.732% (14481/15616)
01/12/2023 17:00:29 - INFO - __main__ -   test: [batch: 122/391 ] | Loss: 0.988 | Acc: 74.200% (11682/15744)/ 92.677% (14591/15744)
01/12/2023 17:00:29 - INFO - __main__ -   test: [batch: 123/391 ] | Loss: 0.989 | Acc: 74.225% (11781/15872)/ 92.635% (14703/15872)
01/12/2023 17:00:29 - INFO - __main__ -   test: [batch: 124/391 ] | Loss: 0.988 | Acc: 74.231% (11877/16000)/ 92.638% (14822/16000)
01/12/2023 17:00:29 - INFO - __main__ -   test: [batch: 125/391 ] | Loss: 0.983 | Acc: 74.349% (11991/16128)/ 92.671% (14946/16128)
01/12/2023 17:00:30 - INFO - __main__ -   test: [batch: 126/391 ] | Loss: 0.979 | Acc: 74.453% (12103/16256)/ 92.692% (15068/16256)
01/12/2023 17:00:30 - INFO - __main__ -   test: [batch: 127/391 ] | Loss: 0.975 | Acc: 74.561% (12216/16384)/ 92.719% (15191/16384)
01/12/2023 17:00:30 - INFO - __main__ -   test: [batch: 128/391 ] | Loss: 0.976 | Acc: 74.570% (12313/16512)/ 92.720% (15310/16512)
01/12/2023 17:00:30 - INFO - __main__ -   test: [batch: 129/391 ] | Loss: 0.973 | Acc: 74.609% (12415/16640)/ 92.746% (15433/16640)
01/12/2023 17:00:30 - INFO - __main__ -   test: [batch: 130/391 ] | Loss: 0.970 | Acc: 74.678% (12522/16768)/ 92.772% (15556/16768)
01/12/2023 17:00:31 - INFO - __main__ -   test: [batch: 131/391 ] | Loss: 0.970 | Acc: 74.704% (12622/16896)/ 92.785% (15677/16896)
01/12/2023 17:00:31 - INFO - __main__ -   test: [batch: 132/391 ] | Loss: 0.967 | Acc: 74.794% (12733/17024)/ 92.810% (15800/17024)
01/12/2023 17:00:31 - INFO - __main__ -   test: [batch: 133/391 ] | Loss: 0.967 | Acc: 74.796% (12829/17152)/ 92.800% (15917/17152)
01/12/2023 17:00:31 - INFO - __main__ -   test: [batch: 134/391 ] | Loss: 0.965 | Acc: 74.850% (12934/17280)/ 92.830% (16041/17280)
01/12/2023 17:00:32 - INFO - __main__ -   test: [batch: 135/391 ] | Loss: 0.965 | Acc: 74.839% (13028/17408)/ 92.842% (16162/17408)
01/12/2023 17:00:32 - INFO - __main__ -   test: [batch: 136/391 ] | Loss: 0.963 | Acc: 74.749% (13108/17536)/ 92.872% (16286/17536)
01/12/2023 17:00:32 - INFO - __main__ -   test: [batch: 137/391 ] | Loss: 0.960 | Acc: 74.825% (13217/17664)/ 92.912% (16412/17664)
01/12/2023 17:00:32 - INFO - __main__ -   test: [batch: 138/391 ] | Loss: 0.959 | Acc: 74.848% (13317/17792)/ 92.907% (16530/17792)
01/12/2023 17:00:32 - INFO - __main__ -   test: [batch: 139/391 ] | Loss: 0.965 | Acc: 74.721% (13390/17920)/ 92.885% (16645/17920)
01/12/2023 17:00:33 - INFO - __main__ -   test: [batch: 140/391 ] | Loss: 0.966 | Acc: 74.612% (13466/18048)/ 92.891% (16765/18048)
01/12/2023 17:00:33 - INFO - __main__ -   test: [batch: 141/391 ] | Loss: 0.966 | Acc: 74.670% (13572/18176)/ 92.870% (16880/18176)
01/12/2023 17:00:33 - INFO - __main__ -   test: [batch: 142/391 ] | Loss: 0.963 | Acc: 74.781% (13688/18304)/ 92.887% (17002/18304)
01/12/2023 17:00:33 - INFO - __main__ -   test: [batch: 143/391 ] | Loss: 0.963 | Acc: 74.799% (13787/18432)/ 92.871% (17118/18432)
01/12/2023 17:00:34 - INFO - __main__ -   test: [batch: 144/391 ] | Loss: 0.967 | Acc: 74.774% (13878/18560)/ 92.829% (17229/18560)
01/12/2023 17:00:34 - INFO - __main__ -   test: [batch: 145/391 ] | Loss: 0.970 | Acc: 74.754% (13970/18688)/ 92.781% (17339/18688)
01/12/2023 17:00:34 - INFO - __main__ -   test: [batch: 146/391 ] | Loss: 0.972 | Acc: 74.750% (14065/18816)/ 92.740% (17450/18816)
01/12/2023 17:00:34 - INFO - __main__ -   test: [batch: 147/391 ] | Loss: 0.971 | Acc: 74.778% (14166/18944)/ 92.731% (17567/18944)
01/12/2023 17:00:34 - INFO - __main__ -   test: [batch: 148/391 ] | Loss: 0.973 | Acc: 74.706% (14248/19072)/ 92.738% (17687/19072)
01/12/2023 17:00:35 - INFO - __main__ -   test: [batch: 149/391 ] | Loss: 0.976 | Acc: 74.651% (14333/19200)/ 92.708% (17800/19200)
01/12/2023 17:00:35 - INFO - __main__ -   test: [batch: 150/391 ] | Loss: 0.976 | Acc: 74.612% (14421/19328)/ 92.710% (17919/19328)
01/12/2023 17:00:35 - INFO - __main__ -   test: [batch: 151/391 ] | Loss: 0.975 | Acc: 74.656% (14525/19456)/ 92.722% (18040/19456)
01/12/2023 17:00:35 - INFO - __main__ -   test: [batch: 152/391 ] | Loss: 0.976 | Acc: 74.643% (14618/19584)/ 92.724% (18159/19584)
01/12/2023 17:00:35 - INFO - __main__ -   test: [batch: 153/391 ] | Loss: 0.974 | Acc: 74.680% (14721/19712)/ 92.751% (18283/19712)
01/12/2023 17:00:36 - INFO - __main__ -   test: [batch: 154/391 ] | Loss: 0.975 | Acc: 74.652% (14811/19840)/ 92.727% (18397/19840)
01/12/2023 17:00:36 - INFO - __main__ -   test: [batch: 155/391 ] | Loss: 0.976 | Acc: 74.679% (14912/19968)/ 92.708% (18512/19968)
01/12/2023 17:00:36 - INFO - __main__ -   test: [batch: 156/391 ] | Loss: 0.981 | Acc: 74.547% (14981/20096)/ 92.630% (18615/20096)
01/12/2023 17:00:36 - INFO - __main__ -   test: [batch: 157/391 ] | Loss: 0.984 | Acc: 74.486% (15064/20224)/ 92.608% (18729/20224)
01/12/2023 17:00:37 - INFO - __main__ -   test: [batch: 158/391 ] | Loss: 0.985 | Acc: 74.460% (15154/20352)/ 92.600% (18846/20352)
01/12/2023 17:00:37 - INFO - __main__ -   test: [batch: 159/391 ] | Loss: 0.985 | Acc: 74.443% (15246/20480)/ 92.603% (18965/20480)
01/12/2023 17:00:37 - INFO - __main__ -   test: [batch: 160/391 ] | Loss: 0.987 | Acc: 74.408% (15334/20608)/ 92.571% (19077/20608)
01/12/2023 17:00:37 - INFO - __main__ -   test: [batch: 161/391 ] | Loss: 0.997 | Acc: 74.175% (15381/20736)/ 92.462% (19173/20736)
01/12/2023 17:00:38 - INFO - __main__ -   test: [batch: 162/391 ] | Loss: 1.002 | Acc: 74.070% (15454/20864)/ 92.384% (19275/20864)
01/12/2023 17:00:38 - INFO - __main__ -   test: [batch: 163/391 ] | Loss: 1.004 | Acc: 74.014% (15537/20992)/ 92.388% (19394/20992)
01/12/2023 17:00:38 - INFO - __main__ -   test: [batch: 164/391 ] | Loss: 1.004 | Acc: 74.010% (15631/21120)/ 92.382% (19511/21120)
01/12/2023 17:00:38 - INFO - __main__ -   test: [batch: 165/391 ] | Loss: 1.008 | Acc: 73.946% (15712/21248)/ 92.348% (19622/21248)
01/12/2023 17:00:38 - INFO - __main__ -   test: [batch: 166/391 ] | Loss: 1.007 | Acc: 73.971% (15812/21376)/ 92.351% (19741/21376)
01/12/2023 17:00:39 - INFO - __main__ -   test: [batch: 167/391 ] | Loss: 1.010 | Acc: 73.907% (15893/21504)/ 92.318% (19852/21504)
01/12/2023 17:00:39 - INFO - __main__ -   test: [batch: 168/391 ] | Loss: 1.009 | Acc: 73.928% (15992/21632)/ 92.312% (19969/21632)
01/12/2023 17:00:39 - INFO - __main__ -   test: [batch: 169/391 ] | Loss: 1.014 | Acc: 73.837% (16067/21760)/ 92.238% (20071/21760)
01/12/2023 17:00:39 - INFO - __main__ -   test: [batch: 170/391 ] | Loss: 1.019 | Acc: 73.721% (16136/21888)/ 92.206% (20182/21888)
01/12/2023 17:00:40 - INFO - __main__ -   test: [batch: 171/391 ] | Loss: 1.022 | Acc: 73.651% (16215/22016)/ 92.142% (20286/22016)
01/12/2023 17:00:40 - INFO - __main__ -   test: [batch: 172/391 ] | Loss: 1.024 | Acc: 73.564% (16290/22144)/ 92.115% (20398/22144)
01/12/2023 17:00:40 - INFO - __main__ -   test: [batch: 173/391 ] | Loss: 1.028 | Acc: 73.527% (16376/22272)/ 92.084% (20509/22272)
01/12/2023 17:00:40 - INFO - __main__ -   test: [batch: 174/391 ] | Loss: 1.035 | Acc: 73.362% (16433/22400)/ 91.987% (20605/22400)
01/12/2023 17:00:41 - INFO - __main__ -   test: [batch: 175/391 ] | Loss: 1.036 | Acc: 73.327% (16519/22528)/ 91.974% (20720/22528)
01/12/2023 17:00:41 - INFO - __main__ -   test: [batch: 176/391 ] | Loss: 1.038 | Acc: 73.318% (16611/22656)/ 91.936% (20829/22656)
01/12/2023 17:00:41 - INFO - __main__ -   test: [batch: 177/391 ] | Loss: 1.042 | Acc: 73.209% (16680/22784)/ 91.885% (20935/22784)
01/12/2023 17:00:41 - INFO - __main__ -   test: [batch: 178/391 ] | Loss: 1.044 | Acc: 73.167% (16764/22912)/ 91.869% (21049/22912)
01/12/2023 17:00:41 - INFO - __main__ -   test: [batch: 179/391 ] | Loss: 1.048 | Acc: 73.121% (16847/23040)/ 91.801% (21151/23040)
01/12/2023 17:00:42 - INFO - __main__ -   test: [batch: 180/391 ] | Loss: 1.055 | Acc: 72.984% (16909/23168)/ 91.752% (21257/23168)
01/12/2023 17:00:42 - INFO - __main__ -   test: [batch: 181/391 ] | Loss: 1.062 | Acc: 72.837% (16968/23296)/ 91.621% (21344/23296)
01/12/2023 17:00:42 - INFO - __main__ -   test: [batch: 182/391 ] | Loss: 1.061 | Acc: 72.874% (17070/23424)/ 91.620% (21461/23424)
01/12/2023 17:00:42 - INFO - __main__ -   test: [batch: 183/391 ] | Loss: 1.067 | Acc: 72.754% (17135/23552)/ 91.525% (21556/23552)
01/12/2023 17:00:43 - INFO - __main__ -   test: [batch: 184/391 ] | Loss: 1.067 | Acc: 72.787% (17236/23680)/ 91.516% (21671/23680)
01/12/2023 17:00:43 - INFO - __main__ -   test: [batch: 185/391 ] | Loss: 1.068 | Acc: 72.812% (17335/23808)/ 91.507% (21786/23808)
01/12/2023 17:00:43 - INFO - __main__ -   test: [batch: 186/391 ] | Loss: 1.072 | Acc: 72.761% (17416/23936)/ 91.440% (21887/23936)
01/12/2023 17:00:43 - INFO - __main__ -   test: [batch: 187/391 ] | Loss: 1.078 | Acc: 72.627% (17477/24064)/ 91.340% (21980/24064)
01/12/2023 17:00:43 - INFO - __main__ -   test: [batch: 188/391 ] | Loss: 1.082 | Acc: 72.495% (17538/24192)/ 91.319% (22092/24192)
01/12/2023 17:00:44 - INFO - __main__ -   test: [batch: 189/391 ] | Loss: 1.082 | Acc: 72.463% (17623/24320)/ 91.324% (22210/24320)
01/12/2023 17:00:44 - INFO - __main__ -   test: [batch: 190/391 ] | Loss: 1.088 | Acc: 72.366% (17692/24448)/ 91.271% (22314/24448)
01/12/2023 17:00:44 - INFO - __main__ -   test: [batch: 191/391 ] | Loss: 1.092 | Acc: 72.310% (17771/24576)/ 91.235% (22422/24576)
01/12/2023 17:00:44 - INFO - __main__ -   test: [batch: 192/391 ] | Loss: 1.097 | Acc: 72.235% (17845/24704)/ 91.159% (22520/24704)
01/12/2023 17:00:44 - INFO - __main__ -   test: [batch: 193/391 ] | Loss: 1.099 | Acc: 72.193% (17927/24832)/ 91.124% (22628/24832)
01/12/2023 17:00:45 - INFO - __main__ -   test: [batch: 194/391 ] | Loss: 1.101 | Acc: 72.155% (18010/24960)/ 91.110% (22741/24960)
01/12/2023 17:00:45 - INFO - __main__ -   test: [batch: 195/391 ] | Loss: 1.108 | Acc: 72.042% (18074/25088)/ 91.004% (22831/25088)
01/12/2023 17:00:45 - INFO - __main__ -   test: [batch: 196/391 ] | Loss: 1.113 | Acc: 71.927% (18137/25216)/ 90.926% (22928/25216)
01/12/2023 17:00:45 - INFO - __main__ -   test: [batch: 197/391 ] | Loss: 1.118 | Acc: 71.867% (18214/25344)/ 90.893% (23036/25344)
01/12/2023 17:00:46 - INFO - __main__ -   test: [batch: 198/391 ] | Loss: 1.119 | Acc: 71.816% (18293/25472)/ 90.868% (23146/25472)
01/12/2023 17:00:46 - INFO - __main__ -   test: [batch: 199/391 ] | Loss: 1.118 | Acc: 71.828% (18388/25600)/ 90.887% (23267/25600)
01/12/2023 17:00:46 - INFO - __main__ -   test: [batch: 200/391 ] | Loss: 1.119 | Acc: 71.813% (18476/25728)/ 90.882% (23382/25728)
01/12/2023 17:00:46 - INFO - __main__ -   test: [batch: 201/391 ] | Loss: 1.123 | Acc: 71.728% (18546/25856)/ 90.826% (23484/25856)
01/12/2023 17:00:47 - INFO - __main__ -   test: [batch: 202/391 ] | Loss: 1.124 | Acc: 71.698% (18630/25984)/ 90.817% (23598/25984)
01/12/2023 17:00:47 - INFO - __main__ -   test: [batch: 203/391 ] | Loss: 1.125 | Acc: 71.680% (18717/26112)/ 90.809% (23712/26112)
01/12/2023 17:00:47 - INFO - __main__ -   test: [batch: 204/391 ] | Loss: 1.129 | Acc: 71.601% (18788/26240)/ 90.758% (23815/26240)
01/12/2023 17:00:47 - INFO - __main__ -   test: [batch: 205/391 ] | Loss: 1.131 | Acc: 71.534% (18862/26368)/ 90.743% (23927/26368)
01/12/2023 17:00:47 - INFO - __main__ -   test: [batch: 206/391 ] | Loss: 1.133 | Acc: 71.509% (18947/26496)/ 90.734% (24041/26496)
01/12/2023 17:00:48 - INFO - __main__ -   test: [batch: 207/391 ] | Loss: 1.135 | Acc: 71.462% (19026/26624)/ 90.715% (24152/26624)
01/12/2023 17:00:48 - INFO - __main__ -   test: [batch: 208/391 ] | Loss: 1.137 | Acc: 71.441% (19112/26752)/ 90.692% (24262/26752)
01/12/2023 17:00:48 - INFO - __main__ -   test: [batch: 209/391 ] | Loss: 1.136 | Acc: 71.440% (19203/26880)/ 90.722% (24386/26880)
01/12/2023 17:00:48 - INFO - __main__ -   test: [batch: 210/391 ] | Loss: 1.138 | Acc: 71.405% (19285/27008)/ 90.677% (24490/27008)
01/12/2023 17:00:48 - INFO - __main__ -   test: [batch: 211/391 ] | Loss: 1.142 | Acc: 71.337% (19358/27136)/ 90.614% (24589/27136)
01/12/2023 17:00:49 - INFO - __main__ -   test: [batch: 212/391 ] | Loss: 1.145 | Acc: 71.240% (19423/27264)/ 90.570% (24693/27264)
01/12/2023 17:00:49 - INFO - __main__ -   test: [batch: 213/391 ] | Loss: 1.146 | Acc: 71.258% (19519/27392)/ 90.567% (24808/27392)
01/12/2023 17:00:49 - INFO - __main__ -   test: [batch: 214/391 ] | Loss: 1.147 | Acc: 71.243% (19606/27520)/ 90.549% (24919/27520)
01/12/2023 17:00:49 - INFO - __main__ -   test: [batch: 215/391 ] | Loss: 1.149 | Acc: 71.224% (19692/27648)/ 90.520% (25027/27648)
01/12/2023 17:00:49 - INFO - __main__ -   test: [batch: 216/391 ] | Loss: 1.147 | Acc: 71.263% (19794/27776)/ 90.524% (25144/27776)
01/12/2023 17:00:50 - INFO - __main__ -   test: [batch: 217/391 ] | Loss: 1.151 | Acc: 71.208% (19870/27904)/ 90.460% (25242/27904)
01/12/2023 17:00:50 - INFO - __main__ -   test: [batch: 218/391 ] | Loss: 1.155 | Acc: 71.133% (19940/28032)/ 90.400% (25341/28032)
01/12/2023 17:00:50 - INFO - __main__ -   test: [batch: 219/391 ] | Loss: 1.153 | Acc: 71.204% (20051/28160)/ 90.415% (25461/28160)
01/12/2023 17:00:50 - INFO - __main__ -   test: [batch: 220/391 ] | Loss: 1.151 | Acc: 71.232% (20150/28288)/ 90.424% (25579/28288)
01/12/2023 17:00:50 - INFO - __main__ -   test: [batch: 221/391 ] | Loss: 1.154 | Acc: 71.185% (20228/28416)/ 90.386% (25684/28416)
01/12/2023 17:00:51 - INFO - __main__ -   test: [batch: 222/391 ] | Loss: 1.153 | Acc: 71.213% (20327/28544)/ 90.390% (25801/28544)
01/12/2023 17:00:51 - INFO - __main__ -   test: [batch: 223/391 ] | Loss: 1.152 | Acc: 71.223% (20421/28672)/ 90.391% (25917/28672)
01/12/2023 17:00:51 - INFO - __main__ -   test: [batch: 224/391 ] | Loss: 1.152 | Acc: 71.260% (20523/28800)/ 90.389% (26032/28800)
01/12/2023 17:00:51 - INFO - __main__ -   test: [batch: 225/391 ] | Loss: 1.150 | Acc: 71.301% (20626/28928)/ 90.407% (26153/28928)
01/12/2023 17:00:52 - INFO - __main__ -   test: [batch: 226/391 ] | Loss: 1.151 | Acc: 71.286% (20713/29056)/ 90.401% (26267/29056)
01/12/2023 17:00:52 - INFO - __main__ -   test: [batch: 227/391 ] | Loss: 1.154 | Acc: 71.251% (20794/29184)/ 90.389% (26379/29184)
01/12/2023 17:00:52 - INFO - __main__ -   test: [batch: 228/391 ] | Loss: 1.160 | Acc: 71.138% (20852/29312)/ 90.318% (26474/29312)
01/12/2023 17:00:52 - INFO - __main__ -   test: [batch: 229/391 ] | Loss: 1.164 | Acc: 71.050% (20917/29440)/ 90.262% (26573/29440)
01/12/2023 17:00:52 - INFO - __main__ -   test: [batch: 230/391 ] | Loss: 1.169 | Acc: 70.928% (20972/29568)/ 90.185% (26666/29568)
01/12/2023 17:00:53 - INFO - __main__ -   test: [batch: 231/391 ] | Loss: 1.170 | Acc: 70.892% (21052/29696)/ 90.160% (26774/29696)
01/12/2023 17:00:53 - INFO - __main__ -   test: [batch: 232/391 ] | Loss: 1.170 | Acc: 70.896% (21144/29824)/ 90.146% (26885/29824)
01/12/2023 17:00:53 - INFO - __main__ -   test: [batch: 233/391 ] | Loss: 1.173 | Acc: 70.850% (21221/29952)/ 90.114% (26991/29952)
01/12/2023 17:00:53 - INFO - __main__ -   test: [batch: 234/391 ] | Loss: 1.180 | Acc: 70.728% (21275/30080)/ 90.030% (27081/30080)
01/12/2023 17:00:53 - INFO - __main__ -   test: [batch: 235/391 ] | Loss: 1.181 | Acc: 70.703% (21358/30208)/ 90.019% (27193/30208)
01/12/2023 17:00:54 - INFO - __main__ -   test: [batch: 236/391 ] | Loss: 1.181 | Acc: 70.731% (21457/30336)/ 90.015% (27307/30336)
01/12/2023 17:00:54 - INFO - __main__ -   test: [batch: 237/391 ] | Loss: 1.181 | Acc: 70.749% (21553/30464)/ 89.991% (27415/30464)
01/12/2023 17:00:54 - INFO - __main__ -   test: [batch: 238/391 ] | Loss: 1.181 | Acc: 70.734% (21639/30592)/ 89.997% (27532/30592)
01/12/2023 17:00:54 - INFO - __main__ -   test: [batch: 239/391 ] | Loss: 1.180 | Acc: 70.775% (21742/30720)/ 90.016% (27653/30720)
01/12/2023 17:00:55 - INFO - __main__ -   test: [batch: 240/391 ] | Loss: 1.181 | Acc: 70.760% (21828/30848)/ 89.983% (27758/30848)
01/12/2023 17:00:55 - INFO - __main__ -   test: [batch: 241/391 ] | Loss: 1.186 | Acc: 70.658% (21887/30976)/ 89.934% (27858/30976)
01/12/2023 17:00:55 - INFO - __main__ -   test: [batch: 242/391 ] | Loss: 1.188 | Acc: 70.586% (21955/31104)/ 89.918% (27968/31104)
01/12/2023 17:00:55 - INFO - __main__ -   test: [batch: 243/391 ] | Loss: 1.196 | Acc: 70.441% (22000/31232)/ 89.805% (28048/31232)
01/12/2023 17:00:55 - INFO - __main__ -   test: [batch: 244/391 ] | Loss: 1.197 | Acc: 70.427% (22086/31360)/ 89.774% (28153/31360)
01/12/2023 17:00:56 - INFO - __main__ -   test: [batch: 245/391 ] | Loss: 1.196 | Acc: 70.452% (22184/31488)/ 89.780% (28270/31488)
01/12/2023 17:00:56 - INFO - __main__ -   test: [batch: 246/391 ] | Loss: 1.197 | Acc: 70.433% (22268/31616)/ 89.768% (28381/31616)
01/12/2023 17:00:56 - INFO - __main__ -   test: [batch: 247/391 ] | Loss: 1.205 | Acc: 70.331% (22326/31744)/ 89.661% (28462/31744)
01/12/2023 17:00:56 - INFO - __main__ -   test: [batch: 248/391 ] | Loss: 1.208 | Acc: 70.262% (22394/31872)/ 89.621% (28564/31872)
01/12/2023 17:00:57 - INFO - __main__ -   test: [batch: 249/391 ] | Loss: 1.211 | Acc: 70.138% (22444/32000)/ 89.603% (28673/32000)
01/12/2023 17:00:57 - INFO - __main__ -   test: [batch: 250/391 ] | Loss: 1.210 | Acc: 70.160% (22541/32128)/ 89.607% (28789/32128)
01/12/2023 17:00:57 - INFO - __main__ -   test: [batch: 251/391 ] | Loss: 1.212 | Acc: 70.133% (22622/32256)/ 89.577% (28894/32256)
01/12/2023 17:00:57 - INFO - __main__ -   test: [batch: 252/391 ] | Loss: 1.210 | Acc: 70.174% (22725/32384)/ 89.594% (29014/32384)
01/12/2023 17:00:57 - INFO - __main__ -   test: [batch: 253/391 ] | Loss: 1.212 | Acc: 70.116% (22796/32512)/ 89.561% (29118/32512)
01/12/2023 17:00:58 - INFO - __main__ -   test: [batch: 254/391 ] | Loss: 1.218 | Acc: 70.028% (22857/32640)/ 89.473% (29204/32640)
01/12/2023 17:00:58 - INFO - __main__ -   test: [batch: 255/391 ] | Loss: 1.220 | Acc: 69.992% (22935/32768)/ 89.441% (29308/32768)
01/12/2023 17:00:58 - INFO - __main__ -   test: [batch: 256/391 ] | Loss: 1.224 | Acc: 69.887% (22990/32896)/ 89.406% (29411/32896)
01/12/2023 17:00:58 - INFO - __main__ -   test: [batch: 257/391 ] | Loss: 1.225 | Acc: 69.858% (23070/33024)/ 89.399% (29523/33024)
01/12/2023 17:00:58 - INFO - __main__ -   test: [batch: 258/391 ] | Loss: 1.227 | Acc: 69.824% (23148/33152)/ 89.364% (29626/33152)
01/12/2023 17:00:59 - INFO - __main__ -   test: [batch: 259/391 ] | Loss: 1.229 | Acc: 69.742% (23210/33280)/ 89.363% (29740/33280)
01/12/2023 17:00:59 - INFO - __main__ -   test: [batch: 260/391 ] | Loss: 1.231 | Acc: 69.690% (23282/33408)/ 89.344% (29848/33408)
01/12/2023 17:00:59 - INFO - __main__ -   test: [batch: 261/391 ] | Loss: 1.229 | Acc: 69.743% (23389/33536)/ 89.373% (29972/33536)
01/12/2023 17:00:59 - INFO - __main__ -   test: [batch: 262/391 ] | Loss: 1.229 | Acc: 69.751% (23481/33664)/ 89.374% (30087/33664)
01/12/2023 17:01:00 - INFO - __main__ -   test: [batch: 263/391 ] | Loss: 1.232 | Acc: 69.664% (23541/33792)/ 89.338% (30189/33792)
01/12/2023 17:01:00 - INFO - __main__ -   test: [batch: 264/391 ] | Loss: 1.237 | Acc: 69.575% (23600/33920)/ 89.257% (30276/33920)
01/12/2023 17:01:00 - INFO - __main__ -   test: [batch: 265/391 ] | Loss: 1.236 | Acc: 69.605% (23699/34048)/ 89.271% (30395/34048)
01/12/2023 17:01:00 - INFO - __main__ -   test: [batch: 266/391 ] | Loss: 1.239 | Acc: 69.537% (23765/34176)/ 89.238% (30498/34176)
01/12/2023 17:01:00 - INFO - __main__ -   test: [batch: 267/391 ] | Loss: 1.237 | Acc: 69.584% (23870/34304)/ 89.258% (30619/34304)
01/12/2023 17:01:01 - INFO - __main__ -   test: [batch: 268/391 ] | Loss: 1.238 | Acc: 69.586% (23960/34432)/ 89.248% (30730/34432)
01/12/2023 17:01:01 - INFO - __main__ -   test: [batch: 269/391 ] | Loss: 1.241 | Acc: 69.508% (24022/34560)/ 89.196% (30826/34560)
01/12/2023 17:01:01 - INFO - __main__ -   test: [batch: 270/391 ] | Loss: 1.244 | Acc: 69.430% (24084/34688)/ 89.155% (30926/34688)
01/12/2023 17:01:01 - INFO - __main__ -   test: [batch: 271/391 ] | Loss: 1.245 | Acc: 69.434% (24174/34816)/ 89.129% (31031/34816)
01/12/2023 17:01:02 - INFO - __main__ -   test: [batch: 272/391 ] | Loss: 1.246 | Acc: 69.391% (24248/34944)/ 89.131% (31146/34944)
01/12/2023 17:01:02 - INFO - __main__ -   test: [batch: 273/391 ] | Loss: 1.246 | Acc: 69.389% (24336/35072)/ 89.120% (31256/35072)
01/12/2023 17:01:02 - INFO - __main__ -   test: [batch: 274/391 ] | Loss: 1.246 | Acc: 69.392% (24426/35200)/ 89.122% (31371/35200)
01/12/2023 17:01:02 - INFO - __main__ -   test: [batch: 275/391 ] | Loss: 1.249 | Acc: 69.356% (24502/35328)/ 89.096% (31476/35328)
01/12/2023 17:01:02 - INFO - __main__ -   test: [batch: 276/391 ] | Loss: 1.251 | Acc: 69.311% (24575/35456)/ 89.082% (31585/35456)
01/12/2023 17:01:03 - INFO - __main__ -   test: [batch: 277/391 ] | Loss: 1.253 | Acc: 69.253% (24643/35584)/ 89.051% (31688/35584)
01/12/2023 17:01:03 - INFO - __main__ -   test: [batch: 278/391 ] | Loss: 1.254 | Acc: 69.229% (24723/35712)/ 89.040% (31798/35712)
01/12/2023 17:01:03 - INFO - __main__ -   test: [batch: 279/391 ] | Loss: 1.254 | Acc: 69.247% (24818/35840)/ 89.037% (31911/35840)
01/12/2023 17:01:03 - INFO - __main__ -   test: [batch: 280/391 ] | Loss: 1.256 | Acc: 69.195% (24888/35968)/ 89.026% (32021/35968)
01/12/2023 17:01:03 - INFO - __main__ -   test: [batch: 281/391 ] | Loss: 1.256 | Acc: 69.210% (24982/36096)/ 89.015% (32131/36096)
01/12/2023 17:01:04 - INFO - __main__ -   test: [batch: 282/391 ] | Loss: 1.256 | Acc: 69.230% (25078/36224)/ 89.016% (32245/36224)
01/12/2023 17:01:04 - INFO - __main__ -   test: [batch: 283/391 ] | Loss: 1.256 | Acc: 69.220% (25163/36352)/ 89.013% (32358/36352)
01/12/2023 17:01:04 - INFO - __main__ -   test: [batch: 284/391 ] | Loss: 1.261 | Acc: 69.169% (25233/36480)/ 88.942% (32446/36480)
01/12/2023 17:01:04 - INFO - __main__ -   test: [batch: 285/391 ] | Loss: 1.264 | Acc: 69.124% (25305/36608)/ 88.899% (32544/36608)
01/12/2023 17:01:04 - INFO - __main__ -   test: [batch: 286/391 ] | Loss: 1.265 | Acc: 69.098% (25384/36736)/ 88.883% (32652/36736)
01/12/2023 17:01:05 - INFO - __main__ -   test: [batch: 287/391 ] | Loss: 1.265 | Acc: 69.095% (25471/36864)/ 88.881% (32765/36864)
01/12/2023 17:01:05 - INFO - __main__ -   test: [batch: 288/391 ] | Loss: 1.264 | Acc: 69.123% (25570/36992)/ 88.900% (32886/36992)
01/12/2023 17:01:05 - INFO - __main__ -   test: [batch: 289/391 ] | Loss: 1.267 | Acc: 69.062% (25636/37120)/ 88.860% (32985/37120)
01/12/2023 17:01:05 - INFO - __main__ -   test: [batch: 290/391 ] | Loss: 1.268 | Acc: 68.986% (25696/37248)/ 88.856% (33097/37248)
01/12/2023 17:01:05 - INFO - __main__ -   test: [batch: 291/391 ] | Loss: 1.269 | Acc: 68.969% (25778/37376)/ 88.824% (33199/37376)
01/12/2023 17:01:06 - INFO - __main__ -   test: [batch: 292/391 ] | Loss: 1.272 | Acc: 68.923% (25849/37504)/ 88.793% (33301/37504)
01/12/2023 17:01:06 - INFO - __main__ -   test: [batch: 293/391 ] | Loss: 1.272 | Acc: 68.915% (25934/37632)/ 88.789% (33413/37632)
01/12/2023 17:01:06 - INFO - __main__ -   test: [batch: 294/391 ] | Loss: 1.273 | Acc: 68.880% (26009/37760)/ 88.776% (33522/37760)
01/12/2023 17:01:06 - INFO - __main__ -   test: [batch: 295/391 ] | Loss: 1.272 | Acc: 68.900% (26105/37888)/ 88.780% (33637/37888)
01/12/2023 17:01:06 - INFO - __main__ -   test: [batch: 296/391 ] | Loss: 1.273 | Acc: 68.884% (26187/38016)/ 88.757% (33742/38016)
01/12/2023 17:01:07 - INFO - __main__ -   test: [batch: 297/391 ] | Loss: 1.275 | Acc: 68.873% (26271/38144)/ 88.727% (33844/38144)
01/12/2023 17:01:07 - INFO - __main__ -   test: [batch: 298/391 ] | Loss: 1.276 | Acc: 68.847% (26349/38272)/ 88.723% (33956/38272)
01/12/2023 17:01:07 - INFO - __main__ -   test: [batch: 299/391 ] | Loss: 1.277 | Acc: 68.836% (26433/38400)/ 88.706% (34063/38400)
01/12/2023 17:01:07 - INFO - __main__ -   test: [batch: 300/391 ] | Loss: 1.277 | Acc: 68.854% (26528/38528)/ 88.699% (34174/38528)
01/12/2023 17:01:07 - INFO - __main__ -   test: [batch: 301/391 ] | Loss: 1.279 | Acc: 68.859% (26618/38656)/ 88.677% (34279/38656)
01/12/2023 17:01:08 - INFO - __main__ -   test: [batch: 302/391 ] | Loss: 1.280 | Acc: 68.827% (26694/38784)/ 88.647% (34381/38784)
01/12/2023 17:01:08 - INFO - __main__ -   test: [batch: 303/391 ] | Loss: 1.281 | Acc: 68.783% (26765/38912)/ 88.623% (34485/38912)
01/12/2023 17:01:08 - INFO - __main__ -   test: [batch: 304/391 ] | Loss: 1.281 | Acc: 68.801% (26860/39040)/ 88.627% (34600/39040)
01/12/2023 17:01:08 - INFO - __main__ -   test: [batch: 305/391 ] | Loss: 1.281 | Acc: 68.758% (26931/39168)/ 88.618% (34710/39168)
01/12/2023 17:01:08 - INFO - __main__ -   test: [batch: 306/391 ] | Loss: 1.283 | Acc: 68.740% (27012/39296)/ 88.587% (34811/39296)
01/12/2023 17:01:09 - INFO - __main__ -   test: [batch: 307/391 ] | Loss: 1.284 | Acc: 68.732% (27097/39424)/ 88.581% (34922/39424)
01/12/2023 17:01:09 - INFO - __main__ -   test: [batch: 308/391 ] | Loss: 1.285 | Acc: 68.707% (27175/39552)/ 88.547% (35022/39552)
01/12/2023 17:01:09 - INFO - __main__ -   test: [batch: 309/391 ] | Loss: 1.287 | Acc: 68.690% (27256/39680)/ 88.521% (35125/39680)
01/12/2023 17:01:09 - INFO - __main__ -   test: [batch: 310/391 ] | Loss: 1.287 | Acc: 68.680% (27340/39808)/ 88.520% (35238/39808)
01/12/2023 17:01:09 - INFO - __main__ -   test: [batch: 311/391 ] | Loss: 1.289 | Acc: 68.655% (27418/39936)/ 88.484% (35337/39936)
01/12/2023 17:01:10 - INFO - __main__ -   test: [batch: 312/391 ] | Loss: 1.290 | Acc: 68.643% (27501/40064)/ 88.468% (35444/40064)
01/12/2023 17:01:10 - INFO - __main__ -   test: [batch: 313/391 ] | Loss: 1.288 | Acc: 68.675% (27602/40192)/ 88.500% (35570/40192)
01/12/2023 17:01:10 - INFO - __main__ -   test: [batch: 314/391 ] | Loss: 1.288 | Acc: 68.683% (27693/40320)/ 88.504% (35685/40320)
01/12/2023 17:01:10 - INFO - __main__ -   test: [batch: 315/391 ] | Loss: 1.290 | Acc: 68.673% (27777/40448)/ 88.486% (35791/40448)
01/12/2023 17:01:10 - INFO - __main__ -   test: [batch: 316/391 ] | Loss: 1.292 | Acc: 68.602% (27836/40576)/ 88.473% (35899/40576)
01/12/2023 17:01:11 - INFO - __main__ -   test: [batch: 317/391 ] | Loss: 1.295 | Acc: 68.544% (27900/40704)/ 88.431% (35995/40704)
01/12/2023 17:01:11 - INFO - __main__ -   test: [batch: 318/391 ] | Loss: 1.293 | Acc: 68.579% (28002/40832)/ 88.460% (36120/40832)
01/12/2023 17:01:11 - INFO - __main__ -   test: [batch: 319/391 ] | Loss: 1.297 | Acc: 68.499% (28057/40960)/ 88.403% (36210/40960)
01/12/2023 17:01:11 - INFO - __main__ -   test: [batch: 320/391 ] | Loss: 1.296 | Acc: 68.536% (28160/41088)/ 88.413% (36327/41088)
01/12/2023 17:01:12 - INFO - __main__ -   test: [batch: 321/391 ] | Loss: 1.297 | Acc: 68.524% (28243/41216)/ 88.398% (36434/41216)
01/12/2023 17:01:12 - INFO - __main__ -   test: [batch: 322/391 ] | Loss: 1.298 | Acc: 68.489% (28316/41344)/ 88.383% (36541/41344)
01/12/2023 17:01:12 - INFO - __main__ -   test: [batch: 323/391 ] | Loss: 1.300 | Acc: 68.453% (28389/41472)/ 88.361% (36645/41472)
01/12/2023 17:01:12 - INFO - __main__ -   test: [batch: 324/391 ] | Loss: 1.300 | Acc: 68.457% (28478/41600)/ 88.363% (36759/41600)
01/12/2023 17:01:12 - INFO - __main__ -   test: [batch: 325/391 ] | Loss: 1.300 | Acc: 68.482% (28576/41728)/ 88.365% (36873/41728)
01/12/2023 17:01:13 - INFO - __main__ -   test: [batch: 326/391 ] | Loss: 1.304 | Acc: 68.392% (28626/41856)/ 88.298% (36958/41856)
01/12/2023 17:01:13 - INFO - __main__ -   test: [batch: 327/391 ] | Loss: 1.308 | Acc: 68.288% (28670/41984)/ 88.246% (37049/41984)
01/12/2023 17:01:13 - INFO - __main__ -   test: [batch: 328/391 ] | Loss: 1.310 | Acc: 68.230% (28733/42112)/ 88.198% (37142/42112)
01/12/2023 17:01:13 - INFO - __main__ -   test: [batch: 329/391 ] | Loss: 1.311 | Acc: 68.213% (28813/42240)/ 88.191% (37252/42240)
01/12/2023 17:01:14 - INFO - __main__ -   test: [batch: 330/391 ] | Loss: 1.313 | Acc: 68.179% (28886/42368)/ 88.168% (37355/42368)
01/12/2023 17:01:14 - INFO - __main__ -   test: [batch: 331/391 ] | Loss: 1.313 | Acc: 68.159% (28965/42496)/ 88.175% (37471/42496)
01/12/2023 17:01:14 - INFO - __main__ -   test: [batch: 332/391 ] | Loss: 1.312 | Acc: 68.171% (29057/42624)/ 88.185% (37588/42624)
01/12/2023 17:01:14 - INFO - __main__ -   test: [batch: 333/391 ] | Loss: 1.311 | Acc: 68.191% (29153/42752)/ 88.195% (37705/42752)
01/12/2023 17:01:14 - INFO - __main__ -   test: [batch: 334/391 ] | Loss: 1.313 | Acc: 68.165% (29229/42880)/ 88.181% (37812/42880)
01/12/2023 17:01:15 - INFO - __main__ -   test: [batch: 335/391 ] | Loss: 1.314 | Acc: 68.141% (29306/43008)/ 88.151% (37912/43008)
01/12/2023 17:01:15 - INFO - __main__ -   test: [batch: 336/391 ] | Loss: 1.316 | Acc: 68.085% (29369/43136)/ 88.126% (38014/43136)
01/12/2023 17:01:15 - INFO - __main__ -   test: [batch: 337/391 ] | Loss: 1.316 | Acc: 68.084% (29456/43264)/ 88.124% (38126/43264)
01/12/2023 17:01:15 - INFO - __main__ -   test: [batch: 338/391 ] | Loss: 1.315 | Acc: 68.100% (29550/43392)/ 88.138% (38245/43392)
01/12/2023 17:01:15 - INFO - __main__ -   test: [batch: 339/391 ] | Loss: 1.318 | Acc: 68.054% (29617/43520)/ 88.086% (38335/43520)
01/12/2023 17:01:16 - INFO - __main__ -   test: [batch: 340/391 ] | Loss: 1.317 | Acc: 68.060% (29707/43648)/ 88.096% (38452/43648)
01/12/2023 17:01:16 - INFO - __main__ -   test: [batch: 341/391 ] | Loss: 1.315 | Acc: 68.090% (29807/43776)/ 88.121% (38576/43776)
01/12/2023 17:01:16 - INFO - __main__ -   test: [batch: 342/391 ] | Loss: 1.317 | Acc: 68.046% (29875/43904)/ 88.106% (38682/43904)
01/12/2023 17:01:16 - INFO - __main__ -   test: [batch: 343/391 ] | Loss: 1.317 | Acc: 68.041% (29960/44032)/ 88.090% (38788/44032)
01/12/2023 17:01:17 - INFO - __main__ -   test: [batch: 344/391 ] | Loss: 1.318 | Acc: 68.039% (30046/44160)/ 88.084% (38898/44160)
01/12/2023 17:01:17 - INFO - __main__ -   test: [batch: 345/391 ] | Loss: 1.320 | Acc: 67.989% (30111/44288)/ 88.049% (38995/44288)
01/12/2023 17:01:17 - INFO - __main__ -   test: [batch: 346/391 ] | Loss: 1.321 | Acc: 67.964% (30187/44416)/ 88.047% (39107/44416)
01/12/2023 17:01:17 - INFO - __main__ -   test: [batch: 347/391 ] | Loss: 1.321 | Acc: 67.958% (30271/44544)/ 88.055% (39223/44544)
01/12/2023 17:01:17 - INFO - __main__ -   test: [batch: 348/391 ] | Loss: 1.322 | Acc: 67.937% (30349/44672)/ 88.031% (39325/44672)
01/12/2023 17:01:18 - INFO - __main__ -   test: [batch: 349/391 ] | Loss: 1.322 | Acc: 67.924% (30430/44800)/ 88.029% (39437/44800)
01/12/2023 17:01:18 - INFO - __main__ -   test: [batch: 350/391 ] | Loss: 1.322 | Acc: 67.922% (30516/44928)/ 88.030% (39550/44928)
01/12/2023 17:01:18 - INFO - __main__ -   test: [batch: 351/391 ] | Loss: 1.325 | Acc: 67.858% (30574/45056)/ 87.986% (39643/45056)
01/12/2023 17:01:18 - INFO - __main__ -   test: [batch: 352/391 ] | Loss: 1.325 | Acc: 67.849% (30657/45184)/ 87.974% (39750/45184)
01/12/2023 17:01:18 - INFO - __main__ -   test: [batch: 353/391 ] | Loss: 1.328 | Acc: 67.803% (30723/45312)/ 87.933% (39844/45312)
01/12/2023 17:01:19 - INFO - __main__ -   test: [batch: 354/391 ] | Loss: 1.331 | Acc: 67.733% (30778/45440)/ 87.912% (39947/45440)
01/12/2023 17:01:19 - INFO - __main__ -   test: [batch: 355/391 ] | Loss: 1.333 | Acc: 67.653% (30828/45568)/ 87.897% (40053/45568)
01/12/2023 17:01:19 - INFO - __main__ -   test: [batch: 356/391 ] | Loss: 1.333 | Acc: 67.651% (30914/45696)/ 87.903% (40168/45696)
01/12/2023 17:01:19 - INFO - __main__ -   test: [batch: 357/391 ] | Loss: 1.331 | Acc: 67.687% (31017/45824)/ 87.919% (40288/45824)
01/12/2023 17:01:20 - INFO - __main__ -   test: [batch: 358/391 ] | Loss: 1.331 | Acc: 67.699% (31109/45952)/ 87.933% (40407/45952)
01/12/2023 17:01:20 - INFO - __main__ -   test: [batch: 359/391 ] | Loss: 1.332 | Acc: 67.689% (31191/46080)/ 87.923% (40515/46080)
01/12/2023 17:01:20 - INFO - __main__ -   test: [batch: 360/391 ] | Loss: 1.333 | Acc: 67.646% (31258/46208)/ 87.920% (40626/46208)
01/12/2023 17:01:20 - INFO - __main__ -   test: [batch: 361/391 ] | Loss: 1.332 | Acc: 67.647% (31345/46336)/ 87.938% (40747/46336)
01/12/2023 17:01:20 - INFO - __main__ -   test: [batch: 362/391 ] | Loss: 1.331 | Acc: 67.674% (31444/46464)/ 87.961% (40870/46464)
01/12/2023 17:01:21 - INFO - __main__ -   test: [batch: 363/391 ] | Loss: 1.332 | Acc: 67.638% (31514/46592)/ 87.938% (40972/46592)
01/12/2023 17:01:21 - INFO - __main__ -   test: [batch: 364/391 ] | Loss: 1.331 | Acc: 67.658% (31610/46720)/ 87.943% (41087/46720)
01/12/2023 17:01:21 - INFO - __main__ -   test: [batch: 365/391 ] | Loss: 1.330 | Acc: 67.691% (31712/46848)/ 87.963% (41209/46848)
01/12/2023 17:01:21 - INFO - __main__ -   test: [batch: 366/391 ] | Loss: 1.327 | Acc: 67.739% (31821/46976)/ 87.987% (41333/46976)
01/12/2023 17:01:21 - INFO - __main__ -   test: [batch: 367/391 ] | Loss: 1.326 | Acc: 67.761% (31918/47104)/ 88.010% (41456/47104)
01/12/2023 17:01:22 - INFO - __main__ -   test: [batch: 368/391 ] | Loss: 1.325 | Acc: 67.765% (32007/47232)/ 88.025% (41576/47232)
01/12/2023 17:01:22 - INFO - __main__ -   test: [batch: 369/391 ] | Loss: 1.324 | Acc: 67.806% (32113/47360)/ 88.043% (41697/47360)
01/12/2023 17:01:22 - INFO - __main__ -   test: [batch: 370/391 ] | Loss: 1.323 | Acc: 67.813% (32203/47488)/ 88.064% (41820/47488)
01/12/2023 17:01:22 - INFO - __main__ -   test: [batch: 371/391 ] | Loss: 1.322 | Acc: 67.828% (32297/47616)/ 88.071% (41936/47616)
01/12/2023 17:01:23 - INFO - __main__ -   test: [batch: 372/391 ] | Loss: 1.320 | Acc: 67.877% (32407/47744)/ 88.095% (42060/47744)
01/12/2023 17:01:23 - INFO - __main__ -   test: [batch: 373/391 ] | Loss: 1.318 | Acc: 67.931% (32520/47872)/ 88.114% (42182/47872)
01/12/2023 17:01:23 - INFO - __main__ -   test: [batch: 374/391 ] | Loss: 1.316 | Acc: 67.971% (32626/48000)/ 88.127% (42301/48000)
01/12/2023 17:01:23 - INFO - __main__ -   test: [batch: 375/391 ] | Loss: 1.318 | Acc: 67.915% (32686/48128)/ 88.100% (42401/48128)
01/12/2023 17:01:23 - INFO - __main__ -   test: [batch: 376/391 ] | Loss: 1.319 | Acc: 67.917% (32774/48256)/ 88.097% (42512/48256)
01/12/2023 17:01:24 - INFO - __main__ -   test: [batch: 377/391 ] | Loss: 1.319 | Acc: 67.898% (32852/48384)/ 88.095% (42624/48384)
01/12/2023 17:01:24 - INFO - __main__ -   test: [batch: 378/391 ] | Loss: 1.323 | Acc: 67.810% (32896/48512)/ 88.040% (42710/48512)
01/12/2023 17:01:24 - INFO - __main__ -   test: [batch: 379/391 ] | Loss: 1.323 | Acc: 67.780% (32968/48640)/ 88.053% (42829/48640)
01/12/2023 17:01:24 - INFO - __main__ -   test: [batch: 380/391 ] | Loss: 1.322 | Acc: 67.803% (33066/48768)/ 88.066% (42948/48768)
01/12/2023 17:01:24 - INFO - __main__ -   test: [batch: 381/391 ] | Loss: 1.323 | Acc: 67.764% (33134/48896)/ 88.062% (43059/48896)
01/12/2023 17:01:25 - INFO - __main__ -   test: [batch: 382/391 ] | Loss: 1.323 | Acc: 67.765% (33221/49024)/ 88.069% (43175/49024)
01/12/2023 17:01:25 - INFO - __main__ -   test: [batch: 383/391 ] | Loss: 1.323 | Acc: 67.778% (33314/49152)/ 88.064% (43285/49152)
01/12/2023 17:01:25 - INFO - __main__ -   test: [batch: 384/391 ] | Loss: 1.321 | Acc: 67.835% (33429/49280)/ 88.093% (43412/49280)
01/12/2023 17:01:25 - INFO - __main__ -   test: [batch: 385/391 ] | Loss: 1.319 | Acc: 67.853% (33525/49408)/ 88.105% (43531/49408)
01/12/2023 17:01:25 - INFO - __main__ -   test: [batch: 386/391 ] | Loss: 1.317 | Acc: 67.910% (33640/49536)/ 88.124% (43653/49536)
01/12/2023 17:01:26 - INFO - __main__ -   test: [batch: 387/391 ] | Loss: 1.314 | Acc: 67.971% (33757/49664)/ 88.146% (43777/49664)
01/12/2023 17:01:26 - INFO - __main__ -   test: [batch: 388/391 ] | Loss: 1.312 | Acc: 68.029% (33873/49792)/ 88.163% (43898/49792)
01/12/2023 17:01:26 - INFO - __main__ -   test: [batch: 389/391 ] | Loss: 1.311 | Acc: 68.033% (33962/49920)/ 88.167% (44013/49920)
01/12/2023 17:01:27 - INFO - __main__ -   test: [batch: 390/391 ] | Loss: 1.313 | Acc: 67.986% (33993/50000)/ 88.160% (44080/50000)
01/12/2023 17:01:27 - INFO - __main__ -   Final accuracy: 67.986
