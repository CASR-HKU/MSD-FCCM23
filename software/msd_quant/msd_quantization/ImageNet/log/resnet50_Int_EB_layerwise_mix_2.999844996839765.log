/home/jjc/miniconda3/envs/ant_quant/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
Namespace(a_low=75, a_up=150, abit=8, batch_size=128, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='csd_eb2', epoch=3, layer_4bit_l='28,40,18,0,39,27,33,40,27,33,39,13,43,24,45,48,2,3,9,25,12,16,22,25,49,47,41,35,32,4,1,10,8,7,41,32,35,47,23,14,49,31', layer_8bit_l=None, layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='resnet50', percent=100, ptq=False, resume=False, search=False, tag='', train=True, w_low=75, w_up=150, wbit=8)
01/15/2023 02:48:52 - INFO - __main__ -   output/resnet50_imagenet/int_W8A8_24936/gpu_0
01/15/2023 02:48:52 - INFO - __main__ -   Namespace(a_low=75, a_up=150, abit=8, batch_size=128, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='csd_eb2', epoch=3, layer_4bit_l='28,40,18,0,39,27,33,40,27,33,39,13,43,24,45,48,2,3,9,25,12,16,22,25,49,47,41,35,32,4,1,10,8,7,41,32,35,47,23,14,49,31', layer_8bit_l=None, layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='resnet50', percent=100, ptq=False, resume=False, search=False, tag='', train=True, w_low=75, w_up=150, wbit=8)
01/15/2023 02:48:52 - INFO - __main__ -   ==> Preparing data..
01/15/2023 02:48:55 - INFO - __main__ -   ==> Setting quantizer..
Namespace(a_low=75, a_up=150, abit=8, batch_size=128, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='csd_eb2', epoch=3, layer_4bit_l='28,40,18,0,39,27,33,40,27,33,39,13,43,24,45,48,2,3,9,25,12,16,22,25,49,47,41,35,32,4,1,10,8,7,41,32,35,47,23,14,49,31', layer_8bit_l=None, layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='resnet50', percent=100, ptq=False, resume=False, search=False, tag='', train=True, w_low=75, w_up=150, wbit=8)
01/15/2023 02:48:55 - INFO - __main__ -   Namespace(a_low=75, a_up=150, abit=8, batch_size=128, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='csd_eb2', epoch=3, layer_4bit_l='28,40,18,0,39,27,33,40,27,33,39,13,43,24,45,48,2,3,9,25,12,16,22,25,49,47,41,35,32,4,1,10,8,7,41,32,35,47,23,14,49,31', layer_8bit_l=None, layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='resnet50', percent=100, ptq=False, resume=False, search=False, tag='', train=True, w_low=75, w_up=150, wbit=8)
01/15/2023 02:48:55 - INFO - __main__ -   ==> Building model..
ResNet(
  (conv1): Conv2dQuantizer(
    (quant_weight): TensorQuantizer()
    (quant_input): TensorQuantizer()
  )
  (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (1): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (1): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): LinearQuantizer(
    (quant_weight): TensorQuantizer()
    (quant_input): TensorQuantizer()
  )
)
01/15/2023 02:48:55 - INFO - __main__ -   {'milestones': Counter({2: 1, 3: 1, 4: 1}), 'gamma': 0.1, 'base_lrs': [0.00025], 'last_epoch': 0, '_step_count': 1, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.00025]}
01/15/2023 02:48:55 - INFO - __main__ -   
Epoch: 0
Layer quant EB csd_eb2
int	8-bit 	 conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.3.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.3.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.3.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.3.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.3.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.3.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.4.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.4.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.4.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.5.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.5.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.5.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 fc.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 fc.quant_input,
set init to 1
------------- 8-bit EB1 Re-SET -------------
42
conv1.quant_weight 0
conv1.quant_input 0
layer1.0.conv1.quant_weight 1
layer1.0.conv1.quant_input 1
layer1.0.conv2.quant_weight 2
layer1.0.conv2.quant_input 2
layer1.0.conv3.quant_weight 3
layer1.0.conv3.quant_input 3
layer1.0.downsample.0.quant_weight 4
layer1.0.downsample.0.quant_input 4
layer1.1.conv3.quant_weight 7
layer1.1.conv3.quant_input 7
layer1.2.conv1.quant_weight 8
layer1.2.conv1.quant_input 8
layer1.2.conv2.quant_weight 9
layer1.2.conv2.quant_input 9
layer1.2.conv3.quant_weight 10
layer1.2.conv3.quant_input 10
layer2.0.conv2.quant_weight 12
layer2.0.conv2.quant_input 12
layer2.0.conv3.quant_weight 13
layer2.0.conv3.quant_input 13
layer2.0.downsample.0.quant_weight 14
layer2.0.downsample.0.quant_input 14
layer2.1.conv2.quant_weight 16
layer2.1.conv2.quant_input 16
layer2.2.conv1.quant_weight 18
layer2.2.conv1.quant_input 18
layer2.3.conv2.quant_weight 22
layer2.3.conv2.quant_input 22
layer2.3.conv3.quant_weight 23
layer2.3.conv3.quant_input 23
layer3.0.conv1.quant_weight 24
layer3.0.conv1.quant_input 24
layer3.0.conv2.quant_weight 25
layer3.0.conv2.quant_input 25
layer3.0.downsample.0.quant_weight 27
layer3.0.downsample.0.quant_input 27
layer3.1.conv1.quant_weight 28
layer3.1.conv1.quant_input 28
layer3.2.conv1.quant_weight 31
layer3.2.conv1.quant_input 31
layer3.2.conv2.quant_weight 32
layer3.2.conv2.quant_input 32
layer3.2.conv3.quant_weight 33
layer3.2.conv3.quant_input 33
layer3.3.conv2.quant_weight 35
layer3.3.conv2.quant_input 35
layer3.4.conv3.quant_weight 39
layer3.4.conv3.quant_input 39
layer3.5.conv1.quant_weight 40
layer3.5.conv1.quant_input 40
layer3.5.conv2.quant_weight 41
layer3.5.conv2.quant_input 41
layer4.0.conv1.quant_weight 43
layer4.0.conv1.quant_input 43
layer4.0.conv3.quant_weight 45
layer4.0.conv3.quant_input 45
layer4.1.conv1.quant_weight 47
layer4.1.conv1.quant_input 47
layer4.1.conv2.quant_weight 48
layer4.1.conv2.quant_input 48
layer4.1.conv3.quant_weight 49
layer4.1.conv3.quant_input 49
------------- 8-bit EB1 Re-SET -------------
Layer quant EB csd_eb1
int	8-bit 	 conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 conv1.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer1.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer1.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer1.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer1.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer1.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer1.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer1.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer1.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer2.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer2.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer2.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer2.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer2.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.3.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv1.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer2.3.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv2.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer2.3.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv3.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer3.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer3.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer3.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer3.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer3.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer3.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer3.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.3.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv1.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer3.3.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.3.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.4.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.4.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv2.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer3.4.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv3.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer3.5.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv1.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer3.5.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.5.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv3.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer4.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer4.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer4.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer4.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb1
int	8-bit 	 layer4.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 fc.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 fc.quant_input,
set init to 1
01/15/2023 02:49:28 - INFO - __main__ -   test: [epoch: 0 | batch: 0/10010 ] | Loss: 0.753 | Acc: 79.688% (102/128)
01/15/2023 02:50:37 - INFO - __main__ -   test: [epoch: 0 | batch: 100/10010 ] | Loss: 0.939 | Acc: 77.104% (9968/12928)
01/15/2023 02:54:02 - INFO - __main__ -   test: [epoch: 0 | batch: 200/10010 ] | Loss: 0.925 | Acc: 77.161% (19852/25728)
01/15/2023 02:57:33 - INFO - __main__ -   test: [epoch: 0 | batch: 300/10010 ] | Loss: 0.942 | Acc: 76.801% (29590/38528)
01/15/2023 03:01:05 - INFO - __main__ -   test: [epoch: 0 | batch: 400/10010 ] | Loss: 0.952 | Acc: 76.510% (39271/51328)
01/15/2023 03:04:37 - INFO - __main__ -   test: [epoch: 0 | batch: 500/10010 ] | Loss: 0.955 | Acc: 76.413% (49002/64128)
01/15/2023 03:08:07 - INFO - __main__ -   test: [epoch: 0 | batch: 600/10010 ] | Loss: 0.956 | Acc: 76.361% (58743/76928)
01/15/2023 03:11:39 - INFO - __main__ -   test: [epoch: 0 | batch: 700/10010 ] | Loss: 0.957 | Acc: 76.385% (68539/89728)
01/15/2023 03:15:11 - INFO - __main__ -   test: [epoch: 0 | batch: 800/10010 ] | Loss: 0.959 | Acc: 76.300% (78229/102528)
01/15/2023 03:18:42 - INFO - __main__ -   test: [epoch: 0 | batch: 900/10010 ] | Loss: 0.960 | Acc: 76.265% (87955/115328)
01/15/2023 03:22:14 - INFO - __main__ -   test: [epoch: 0 | batch: 1000/10010 ] | Loss: 0.960 | Acc: 76.260% (97710/128128)
01/15/2023 03:25:46 - INFO - __main__ -   test: [epoch: 0 | batch: 1100/10010 ] | Loss: 0.962 | Acc: 76.269% (107485/140928)
01/15/2023 03:29:19 - INFO - __main__ -   test: [epoch: 0 | batch: 1200/10010 ] | Loss: 0.964 | Acc: 76.245% (117210/153728)
01/15/2023 03:32:52 - INFO - __main__ -   test: [epoch: 0 | batch: 1300/10010 ] | Loss: 0.962 | Acc: 76.284% (127035/166528)
01/15/2023 03:36:25 - INFO - __main__ -   test: [epoch: 0 | batch: 1400/10010 ] | Loss: 0.963 | Acc: 76.290% (136810/179328)
01/15/2023 03:39:56 - INFO - __main__ -   test: [epoch: 0 | batch: 1500/10010 ] | Loss: 0.961 | Acc: 76.316% (146624/192128)
01/15/2023 03:43:26 - INFO - __main__ -   test: [epoch: 0 | batch: 1600/10010 ] | Loss: 0.962 | Acc: 76.300% (156361/204928)
01/15/2023 03:46:57 - INFO - __main__ -   test: [epoch: 0 | batch: 1700/10010 ] | Loss: 0.962 | Acc: 76.309% (166147/217728)
01/15/2023 03:50:28 - INFO - __main__ -   test: [epoch: 0 | batch: 1800/10010 ] | Loss: 0.960 | Acc: 76.349% (176005/230528)
01/15/2023 03:54:00 - INFO - __main__ -   test: [epoch: 0 | batch: 1900/10010 ] | Loss: 0.959 | Acc: 76.359% (185802/243328)
01/15/2023 03:57:31 - INFO - __main__ -   test: [epoch: 0 | batch: 2000/10010 ] | Loss: 0.959 | Acc: 76.359% (195577/256128)
01/15/2023 04:01:03 - INFO - __main__ -   test: [epoch: 0 | batch: 2100/10010 ] | Loss: 0.960 | Acc: 76.351% (205330/268928)
01/15/2023 04:04:36 - INFO - __main__ -   test: [epoch: 0 | batch: 2200/10010 ] | Loss: 0.960 | Acc: 76.334% (215054/281728)
01/15/2023 04:08:07 - INFO - __main__ -   test: [epoch: 0 | batch: 2300/10010 ] | Loss: 0.960 | Acc: 76.324% (224795/294528)
01/15/2023 04:11:37 - INFO - __main__ -   test: [epoch: 0 | batch: 2400/10010 ] | Loss: 0.961 | Acc: 76.322% (234558/307328)
01/15/2023 04:15:09 - INFO - __main__ -   test: [epoch: 0 | batch: 2500/10010 ] | Loss: 0.961 | Acc: 76.328% (244346/320128)
01/15/2023 04:18:41 - INFO - __main__ -   test: [epoch: 0 | batch: 2600/10010 ] | Loss: 0.962 | Acc: 76.305% (254040/332928)
01/15/2023 04:22:13 - INFO - __main__ -   test: [epoch: 0 | batch: 2700/10010 ] | Loss: 0.961 | Acc: 76.318% (263854/345728)
01/15/2023 04:25:44 - INFO - __main__ -   test: [epoch: 0 | batch: 2800/10010 ] | Loss: 0.961 | Acc: 76.310% (273591/358528)
01/15/2023 04:29:14 - INFO - __main__ -   test: [epoch: 0 | batch: 2900/10010 ] | Loss: 0.961 | Acc: 76.321% (283403/371328)
01/15/2023 04:32:45 - INFO - __main__ -   test: [epoch: 0 | batch: 3000/10010 ] | Loss: 0.960 | Acc: 76.322% (293175/384128)
01/15/2023 04:36:18 - INFO - __main__ -   test: [epoch: 0 | batch: 3100/10010 ] | Loss: 0.960 | Acc: 76.315% (302914/396928)
01/15/2023 04:39:48 - INFO - __main__ -   test: [epoch: 0 | batch: 3200/10010 ] | Loss: 0.961 | Acc: 76.314% (312678/409728)
01/15/2023 04:43:19 - INFO - __main__ -   test: [epoch: 0 | batch: 3300/10010 ] | Loss: 0.961 | Acc: 76.298% (322382/422528)
01/15/2023 04:46:51 - INFO - __main__ -   test: [epoch: 0 | batch: 3400/10010 ] | Loss: 0.961 | Acc: 76.300% (332156/435328)
01/15/2023 04:50:22 - INFO - __main__ -   test: [epoch: 0 | batch: 3500/10010 ] | Loss: 0.961 | Acc: 76.301% (341926/448128)
01/15/2023 04:53:52 - INFO - __main__ -   test: [epoch: 0 | batch: 3600/10010 ] | Loss: 0.961 | Acc: 76.288% (351631/460928)
01/15/2023 04:57:25 - INFO - __main__ -   test: [epoch: 0 | batch: 3700/10010 ] | Loss: 0.962 | Acc: 76.269% (361306/473728)
01/15/2023 05:00:57 - INFO - __main__ -   test: [epoch: 0 | batch: 3800/10010 ] | Loss: 0.962 | Acc: 76.269% (371072/486528)
01/15/2023 05:04:29 - INFO - __main__ -   test: [epoch: 0 | batch: 3900/10010 ] | Loss: 0.962 | Acc: 76.262% (380798/499328)
01/15/2023 05:07:59 - INFO - __main__ -   test: [epoch: 0 | batch: 4000/10010 ] | Loss: 0.962 | Acc: 76.250% (390496/512128)
01/15/2023 05:11:31 - INFO - __main__ -   test: [epoch: 0 | batch: 4100/10010 ] | Loss: 0.962 | Acc: 76.249% (400251/524928)
01/15/2023 05:15:03 - INFO - __main__ -   test: [epoch: 0 | batch: 4200/10010 ] | Loss: 0.962 | Acc: 76.247% (410003/537728)
01/15/2023 05:18:33 - INFO - __main__ -   test: [epoch: 0 | batch: 4300/10010 ] | Loss: 0.962 | Acc: 76.250% (419779/550528)
01/15/2023 05:22:07 - INFO - __main__ -   test: [epoch: 0 | batch: 4400/10010 ] | Loss: 0.962 | Acc: 76.248% (429528/563328)
01/15/2023 05:25:38 - INFO - __main__ -   test: [epoch: 0 | batch: 4500/10010 ] | Loss: 0.962 | Acc: 76.238% (439228/576128)
01/15/2023 05:29:10 - INFO - __main__ -   test: [epoch: 0 | batch: 4600/10010 ] | Loss: 0.962 | Acc: 76.246% (449033/588928)
01/15/2023 05:32:42 - INFO - __main__ -   test: [epoch: 0 | batch: 4700/10010 ] | Loss: 0.962 | Acc: 76.247% (458800/601728)
01/15/2023 05:36:13 - INFO - __main__ -   test: [epoch: 0 | batch: 4800/10010 ] | Loss: 0.962 | Acc: 76.248% (468565/614528)
01/15/2023 05:39:45 - INFO - __main__ -   test: [epoch: 0 | batch: 4900/10010 ] | Loss: 0.962 | Acc: 76.250% (478337/627328)
01/15/2023 05:43:16 - INFO - __main__ -   test: [epoch: 0 | batch: 5000/10010 ] | Loss: 0.962 | Acc: 76.261% (488171/640128)
01/15/2023 05:46:47 - INFO - __main__ -   test: [epoch: 0 | batch: 5100/10010 ] | Loss: 0.962 | Acc: 76.267% (497971/652928)
01/15/2023 05:50:20 - INFO - __main__ -   test: [epoch: 0 | batch: 5200/10010 ] | Loss: 0.962 | Acc: 76.266% (507722/665728)
01/15/2023 05:53:51 - INFO - __main__ -   test: [epoch: 0 | batch: 5300/10010 ] | Loss: 0.962 | Acc: 76.265% (517476/678528)
01/15/2023 05:57:23 - INFO - __main__ -   test: [epoch: 0 | batch: 5400/10010 ] | Loss: 0.962 | Acc: 76.258% (527195/691328)
01/15/2023 06:00:56 - INFO - __main__ -   test: [epoch: 0 | batch: 5500/10010 ] | Loss: 0.963 | Acc: 76.254% (536923/704128)
01/15/2023 06:04:27 - INFO - __main__ -   test: [epoch: 0 | batch: 5600/10010 ] | Loss: 0.962 | Acc: 76.264% (546756/716928)
01/15/2023 06:07:59 - INFO - __main__ -   test: [epoch: 0 | batch: 5700/10010 ] | Loss: 0.962 | Acc: 76.266% (556533/729728)
01/15/2023 06:11:31 - INFO - __main__ -   test: [epoch: 0 | batch: 5800/10010 ] | Loss: 0.962 | Acc: 76.268% (566312/742528)
01/15/2023 06:15:03 - INFO - __main__ -   test: [epoch: 0 | batch: 5900/10010 ] | Loss: 0.962 | Acc: 76.270% (576085/755328)
01/15/2023 06:18:34 - INFO - __main__ -   test: [epoch: 0 | batch: 6000/10010 ] | Loss: 0.962 | Acc: 76.264% (585805/768128)
01/15/2023 06:22:06 - INFO - __main__ -   test: [epoch: 0 | batch: 6100/10010 ] | Loss: 0.962 | Acc: 76.259% (595525/780928)
01/15/2023 06:25:37 - INFO - __main__ -   test: [epoch: 0 | batch: 6200/10010 ] | Loss: 0.963 | Acc: 76.254% (605247/793728)
01/15/2023 06:29:11 - INFO - __main__ -   test: [epoch: 0 | batch: 6300/10010 ] | Loss: 0.963 | Acc: 76.252% (614997/806528)
01/15/2023 06:32:41 - INFO - __main__ -   test: [epoch: 0 | batch: 6400/10010 ] | Loss: 0.963 | Acc: 76.252% (624755/819328)
01/15/2023 06:36:14 - INFO - __main__ -   test: [epoch: 0 | batch: 6500/10010 ] | Loss: 0.963 | Acc: 76.251% (634508/832128)
01/15/2023 06:39:46 - INFO - __main__ -   test: [epoch: 0 | batch: 6600/10010 ] | Loss: 0.963 | Acc: 76.250% (644260/844928)
01/15/2023 06:43:19 - INFO - __main__ -   test: [epoch: 0 | batch: 6700/10010 ] | Loss: 0.963 | Acc: 76.252% (654031/857728)
01/15/2023 06:46:51 - INFO - __main__ -   test: [epoch: 0 | batch: 6800/10010 ] | Loss: 0.963 | Acc: 76.246% (663746/870528)
01/15/2023 06:50:21 - INFO - __main__ -   test: [epoch: 0 | batch: 6900/10010 ] | Loss: 0.962 | Acc: 76.248% (673524/883328)
01/15/2023 06:53:52 - INFO - __main__ -   test: [epoch: 0 | batch: 7000/10010 ] | Loss: 0.963 | Acc: 76.240% (683212/896128)
01/15/2023 06:57:24 - INFO - __main__ -   test: [epoch: 0 | batch: 7100/10010 ] | Loss: 0.962 | Acc: 76.249% (693045/908928)
01/15/2023 07:00:56 - INFO - __main__ -   test: [epoch: 0 | batch: 7200/10010 ] | Loss: 0.962 | Acc: 76.258% (702892/921728)
01/15/2023 07:04:28 - INFO - __main__ -   test: [epoch: 0 | batch: 7300/10010 ] | Loss: 0.962 | Acc: 76.249% (712569/934528)
01/15/2023 07:08:00 - INFO - __main__ -   test: [epoch: 0 | batch: 7400/10010 ] | Loss: 0.962 | Acc: 76.242% (722263/947328)
01/15/2023 07:11:34 - INFO - __main__ -   test: [epoch: 0 | batch: 7500/10010 ] | Loss: 0.962 | Acc: 76.247% (732071/960128)
01/15/2023 07:15:06 - INFO - __main__ -   test: [epoch: 0 | batch: 7600/10010 ] | Loss: 0.962 | Acc: 76.247% (741826/972928)
01/15/2023 07:18:39 - INFO - __main__ -   test: [epoch: 0 | batch: 7700/10010 ] | Loss: 0.963 | Acc: 76.238% (751497/985728)
01/15/2023 07:22:10 - INFO - __main__ -   test: [epoch: 0 | batch: 7800/10010 ] | Loss: 0.963 | Acc: 76.233% (761205/998528)
01/15/2023 07:25:42 - INFO - __main__ -   test: [epoch: 0 | batch: 7900/10010 ] | Loss: 0.963 | Acc: 76.238% (771018/1011328)
01/15/2023 07:29:13 - INFO - __main__ -   test: [epoch: 0 | batch: 8000/10010 ] | Loss: 0.963 | Acc: 76.232% (780713/1024128)
01/15/2023 07:32:46 - INFO - __main__ -   test: [epoch: 0 | batch: 8100/10010 ] | Loss: 0.963 | Acc: 76.238% (790531/1036928)
01/15/2023 07:36:18 - INFO - __main__ -   test: [epoch: 0 | batch: 8200/10010 ] | Loss: 0.963 | Acc: 76.234% (800250/1049728)
01/15/2023 07:39:49 - INFO - __main__ -   test: [epoch: 0 | batch: 8300/10010 ] | Loss: 0.963 | Acc: 76.235% (810021/1062528)
01/15/2023 07:43:20 - INFO - __main__ -   test: [epoch: 0 | batch: 8400/10010 ] | Loss: 0.963 | Acc: 76.240% (819827/1075328)
01/15/2023 07:46:52 - INFO - __main__ -   test: [epoch: 0 | batch: 8500/10010 ] | Loss: 0.963 | Acc: 76.245% (829647/1088128)
01/15/2023 07:50:24 - INFO - __main__ -   test: [epoch: 0 | batch: 8600/10010 ] | Loss: 0.963 | Acc: 76.240% (839345/1100928)
01/15/2023 07:53:55 - INFO - __main__ -   test: [epoch: 0 | batch: 8700/10010 ] | Loss: 0.963 | Acc: 76.240% (849111/1113728)
01/15/2023 07:57:26 - INFO - __main__ -   test: [epoch: 0 | batch: 8800/10010 ] | Loss: 0.963 | Acc: 76.243% (858902/1126528)
01/15/2023 08:00:58 - INFO - __main__ -   test: [epoch: 0 | batch: 8900/10010 ] | Loss: 0.963 | Acc: 76.243% (868660/1139328)
01/15/2023 08:04:28 - INFO - __main__ -   test: [epoch: 0 | batch: 9000/10010 ] | Loss: 0.963 | Acc: 76.246% (878457/1152128)
01/15/2023 08:07:59 - INFO - __main__ -   test: [epoch: 0 | batch: 9100/10010 ] | Loss: 0.963 | Acc: 76.243% (888175/1164928)
01/15/2023 08:11:33 - INFO - __main__ -   test: [epoch: 0 | batch: 9200/10010 ] | Loss: 0.963 | Acc: 76.239% (897893/1177728)
01/15/2023 08:15:03 - INFO - __main__ -   test: [epoch: 0 | batch: 9300/10010 ] | Loss: 0.963 | Acc: 76.243% (907695/1190528)
01/15/2023 08:18:33 - INFO - __main__ -   test: [epoch: 0 | batch: 9400/10010 ] | Loss: 0.963 | Acc: 76.241% (917427/1203328)
01/15/2023 08:22:05 - INFO - __main__ -   test: [epoch: 0 | batch: 9500/10010 ] | Loss: 0.963 | Acc: 76.245% (927237/1216128)
01/15/2023 08:25:35 - INFO - __main__ -   test: [epoch: 0 | batch: 9600/10010 ] | Loss: 0.963 | Acc: 76.242% (936958/1228928)
01/15/2023 08:29:07 - INFO - __main__ -   test: [epoch: 0 | batch: 9700/10010 ] | Loss: 0.963 | Acc: 76.241% (946700/1241728)
01/15/2023 08:32:38 - INFO - __main__ -   test: [epoch: 0 | batch: 9800/10010 ] | Loss: 0.963 | Acc: 76.238% (956421/1254528)
01/15/2023 08:36:11 - INFO - __main__ -   test: [epoch: 0 | batch: 9900/10010 ] | Loss: 0.963 | Acc: 76.243% (966243/1267328)
01/15/2023 08:39:43 - INFO - __main__ -   test: [epoch: 0 | batch: 10000/10010 ] | Loss: 0.963 | Acc: 76.239% (975962/1280128)
01/15/2023 08:40:03 - INFO - __main__ -   Saving Checkpoint
01/15/2023 08:40:05 - INFO - __main__ -   test: [batch: 0/391 ] | Loss: 0.537 | Acc: 88.281% (113/128)/ 95.312% (122/128)
01/15/2023 08:40:07 - INFO - __main__ -   test: [batch: 1/391 ] | Loss: 0.574 | Acc: 85.547% (219/256)/ 96.875% (248/256)
01/15/2023 08:40:09 - INFO - __main__ -   test: [batch: 2/391 ] | Loss: 0.780 | Acc: 79.688% (306/384)/ 94.271% (362/384)
01/15/2023 08:40:11 - INFO - __main__ -   test: [batch: 3/391 ] | Loss: 0.714 | Acc: 82.422% (422/512)/ 94.922% (486/512)
01/15/2023 08:40:14 - INFO - __main__ -   test: [batch: 4/391 ] | Loss: 0.613 | Acc: 85.000% (544/640)/ 95.625% (612/640)
01/15/2023 08:40:16 - INFO - __main__ -   test: [batch: 5/391 ] | Loss: 0.540 | Acc: 86.328% (663/768)/ 96.354% (740/768)
01/15/2023 08:40:18 - INFO - __main__ -   test: [batch: 6/391 ] | Loss: 0.524 | Acc: 86.830% (778/896)/ 96.429% (864/896)
01/15/2023 08:40:20 - INFO - __main__ -   test: [batch: 7/391 ] | Loss: 0.500 | Acc: 87.793% (899/1024)/ 96.582% (989/1024)
01/15/2023 08:40:22 - INFO - __main__ -   test: [batch: 8/391 ] | Loss: 0.505 | Acc: 87.760% (1011/1152)/ 96.615% (1113/1152)
01/15/2023 08:40:24 - INFO - __main__ -   test: [batch: 9/391 ] | Loss: 0.480 | Acc: 88.359% (1131/1280)/ 96.797% (1239/1280)
01/15/2023 08:40:26 - INFO - __main__ -   test: [batch: 10/391 ] | Loss: 0.525 | Acc: 87.074% (1226/1408)/ 96.733% (1362/1408)
01/15/2023 08:40:28 - INFO - __main__ -   test: [batch: 11/391 ] | Loss: 0.530 | Acc: 86.979% (1336/1536)/ 96.549% (1483/1536)
01/15/2023 08:40:30 - INFO - __main__ -   test: [batch: 12/391 ] | Loss: 0.583 | Acc: 85.577% (1424/1664)/ 96.094% (1599/1664)
01/15/2023 08:40:33 - INFO - __main__ -   test: [batch: 13/391 ] | Loss: 0.639 | Acc: 84.040% (1506/1792)/ 95.312% (1708/1792)
01/15/2023 08:40:35 - INFO - __main__ -   test: [batch: 14/391 ] | Loss: 0.655 | Acc: 83.229% (1598/1920)/ 95.469% (1833/1920)
01/15/2023 08:40:37 - INFO - __main__ -   test: [batch: 15/391 ] | Loss: 0.663 | Acc: 82.666% (1693/2048)/ 95.557% (1957/2048)
01/15/2023 08:40:39 - INFO - __main__ -   test: [batch: 16/391 ] | Loss: 0.670 | Acc: 82.583% (1797/2176)/ 95.404% (2076/2176)
01/15/2023 08:40:41 - INFO - __main__ -   test: [batch: 17/391 ] | Loss: 0.694 | Acc: 82.031% (1890/2304)/ 94.965% (2188/2304)
01/15/2023 08:40:43 - INFO - __main__ -   test: [batch: 18/391 ] | Loss: 0.719 | Acc: 81.497% (1982/2432)/ 94.819% (2306/2432)
01/15/2023 08:40:45 - INFO - __main__ -   test: [batch: 19/391 ] | Loss: 0.726 | Acc: 81.289% (2081/2560)/ 94.648% (2423/2560)
01/15/2023 08:40:47 - INFO - __main__ -   test: [batch: 20/391 ] | Loss: 0.720 | Acc: 81.399% (2188/2688)/ 94.643% (2544/2688)
01/15/2023 08:40:50 - INFO - __main__ -   test: [batch: 21/391 ] | Loss: 0.755 | Acc: 80.540% (2268/2816)/ 94.460% (2660/2816)
01/15/2023 08:40:52 - INFO - __main__ -   test: [batch: 22/391 ] | Loss: 0.755 | Acc: 80.401% (2367/2944)/ 94.497% (2782/2944)
01/15/2023 08:40:54 - INFO - __main__ -   test: [batch: 23/391 ] | Loss: 0.797 | Acc: 79.525% (2443/3072)/ 94.173% (2893/3072)
01/15/2023 08:40:56 - INFO - __main__ -   test: [batch: 24/391 ] | Loss: 0.813 | Acc: 79.062% (2530/3200)/ 94.062% (3010/3200)
01/15/2023 08:40:58 - INFO - __main__ -   test: [batch: 25/391 ] | Loss: 0.839 | Acc: 78.395% (2609/3328)/ 93.870% (3124/3328)
01/15/2023 08:41:00 - INFO - __main__ -   test: [batch: 26/391 ] | Loss: 0.855 | Acc: 77.691% (2685/3456)/ 93.837% (3243/3456)
01/15/2023 08:41:02 - INFO - __main__ -   test: [batch: 27/391 ] | Loss: 0.837 | Acc: 78.209% (2803/3584)/ 93.890% (3365/3584)
01/15/2023 08:41:04 - INFO - __main__ -   test: [batch: 28/391 ] | Loss: 0.842 | Acc: 77.694% (2884/3712)/ 94.019% (3490/3712)
01/15/2023 08:41:07 - INFO - __main__ -   test: [batch: 29/391 ] | Loss: 0.833 | Acc: 77.812% (2988/3840)/ 94.167% (3616/3840)
01/15/2023 08:41:09 - INFO - __main__ -   test: [batch: 30/391 ] | Loss: 0.846 | Acc: 77.646% (3081/3968)/ 94.002% (3730/3968)
01/15/2023 08:41:11 - INFO - __main__ -   test: [batch: 31/391 ] | Loss: 0.840 | Acc: 77.930% (3192/4096)/ 94.092% (3854/4096)
01/15/2023 08:41:13 - INFO - __main__ -   test: [batch: 32/391 ] | Loss: 0.825 | Acc: 78.243% (3305/4224)/ 94.223% (3980/4224)
01/15/2023 08:41:15 - INFO - __main__ -   test: [batch: 33/391 ] | Loss: 0.818 | Acc: 78.447% (3414/4352)/ 94.278% (4103/4352)
01/15/2023 08:41:17 - INFO - __main__ -   test: [batch: 34/391 ] | Loss: 0.804 | Acc: 78.884% (3534/4480)/ 94.353% (4227/4480)
01/15/2023 08:41:19 - INFO - __main__ -   test: [batch: 35/391 ] | Loss: 0.791 | Acc: 79.232% (3651/4608)/ 94.401% (4350/4608)
01/15/2023 08:41:21 - INFO - __main__ -   test: [batch: 36/391 ] | Loss: 0.775 | Acc: 79.709% (3775/4736)/ 94.531% (4477/4736)
01/15/2023 08:41:23 - INFO - __main__ -   test: [batch: 37/391 ] | Loss: 0.765 | Acc: 80.016% (3892/4864)/ 94.613% (4602/4864)
01/15/2023 08:41:25 - INFO - __main__ -   test: [batch: 38/391 ] | Loss: 0.756 | Acc: 80.248% (4006/4992)/ 94.692% (4727/4992)
01/15/2023 08:41:27 - INFO - __main__ -   test: [batch: 39/391 ] | Loss: 0.750 | Acc: 80.430% (4118/5120)/ 94.766% (4852/5120)
01/15/2023 08:41:30 - INFO - __main__ -   test: [batch: 40/391 ] | Loss: 0.748 | Acc: 80.545% (4227/5248)/ 94.722% (4971/5248)
01/15/2023 08:41:32 - INFO - __main__ -   test: [batch: 41/391 ] | Loss: 0.750 | Acc: 80.673% (4337/5376)/ 94.624% (5087/5376)
01/15/2023 08:41:34 - INFO - __main__ -   test: [batch: 42/391 ] | Loss: 0.750 | Acc: 80.687% (4441/5504)/ 94.695% (5212/5504)
01/15/2023 08:41:36 - INFO - __main__ -   test: [batch: 43/391 ] | Loss: 0.749 | Acc: 80.700% (4545/5632)/ 94.656% (5331/5632)
01/15/2023 08:41:38 - INFO - __main__ -   test: [batch: 44/391 ] | Loss: 0.754 | Acc: 80.694% (4648/5760)/ 94.514% (5444/5760)
01/15/2023 08:41:40 - INFO - __main__ -   test: [batch: 45/391 ] | Loss: 0.752 | Acc: 80.859% (4761/5888)/ 94.565% (5568/5888)
01/15/2023 08:41:42 - INFO - __main__ -   test: [batch: 46/391 ] | Loss: 0.755 | Acc: 80.801% (4861/6016)/ 94.614% (5692/6016)
01/15/2023 08:41:44 - INFO - __main__ -   test: [batch: 47/391 ] | Loss: 0.757 | Acc: 80.697% (4958/6144)/ 94.645% (5815/6144)
01/15/2023 08:41:46 - INFO - __main__ -   test: [batch: 48/391 ] | Loss: 0.763 | Acc: 80.564% (5053/6272)/ 94.643% (5936/6272)
01/15/2023 08:41:48 - INFO - __main__ -   test: [batch: 49/391 ] | Loss: 0.768 | Acc: 80.547% (5155/6400)/ 94.562% (6052/6400)
01/15/2023 08:41:50 - INFO - __main__ -   test: [batch: 50/391 ] | Loss: 0.759 | Acc: 80.744% (5271/6528)/ 94.638% (6178/6528)
01/15/2023 08:41:52 - INFO - __main__ -   test: [batch: 51/391 ] | Loss: 0.750 | Acc: 80.950% (5388/6656)/ 94.742% (6306/6656)
01/15/2023 08:41:54 - INFO - __main__ -   test: [batch: 52/391 ] | Loss: 0.747 | Acc: 81.014% (5496/6784)/ 94.782% (6430/6784)
01/15/2023 08:41:57 - INFO - __main__ -   test: [batch: 53/391 ] | Loss: 0.737 | Acc: 81.236% (5615/6912)/ 94.864% (6557/6912)
01/15/2023 08:41:59 - INFO - __main__ -   test: [batch: 54/391 ] | Loss: 0.731 | Acc: 81.378% (5729/7040)/ 94.886% (6680/7040)
01/15/2023 08:42:01 - INFO - __main__ -   test: [batch: 55/391 ] | Loss: 0.724 | Acc: 81.515% (5843/7168)/ 94.936% (6805/7168)
01/15/2023 08:42:03 - INFO - __main__ -   test: [batch: 56/391 ] | Loss: 0.715 | Acc: 81.785% (5967/7296)/ 95.011% (6932/7296)
01/15/2023 08:42:05 - INFO - __main__ -   test: [batch: 57/391 ] | Loss: 0.708 | Acc: 81.964% (6085/7424)/ 95.057% (7057/7424)
01/15/2023 08:42:07 - INFO - __main__ -   test: [batch: 58/391 ] | Loss: 0.704 | Acc: 82.058% (6197/7552)/ 95.061% (7179/7552)
01/15/2023 08:42:09 - INFO - __main__ -   test: [batch: 59/391 ] | Loss: 0.706 | Acc: 82.005% (6298/7680)/ 95.052% (7300/7680)
01/15/2023 08:42:11 - INFO - __main__ -   test: [batch: 60/391 ] | Loss: 0.709 | Acc: 81.942% (6398/7808)/ 95.031% (7420/7808)
01/15/2023 08:42:13 - INFO - __main__ -   test: [batch: 61/391 ] | Loss: 0.709 | Acc: 81.943% (6503/7936)/ 95.048% (7543/7936)
01/15/2023 08:42:16 - INFO - __main__ -   test: [batch: 62/391 ] | Loss: 0.706 | Acc: 81.907% (6605/8064)/ 95.102% (7669/8064)
01/15/2023 08:42:18 - INFO - __main__ -   test: [batch: 63/391 ] | Loss: 0.714 | Acc: 81.763% (6698/8192)/ 95.032% (7785/8192)
01/15/2023 08:42:20 - INFO - __main__ -   test: [batch: 64/391 ] | Loss: 0.722 | Acc: 81.587% (6788/8320)/ 94.988% (7903/8320)
01/15/2023 08:42:22 - INFO - __main__ -   test: [batch: 65/391 ] | Loss: 0.734 | Acc: 81.108% (6852/8448)/ 94.898% (8017/8448)
01/15/2023 08:42:24 - INFO - __main__ -   test: [batch: 66/391 ] | Loss: 0.739 | Acc: 81.098% (6955/8576)/ 94.858% (8135/8576)
01/15/2023 08:42:26 - INFO - __main__ -   test: [batch: 67/391 ] | Loss: 0.741 | Acc: 81.066% (7056/8704)/ 94.876% (8258/8704)
01/15/2023 08:42:28 - INFO - __main__ -   test: [batch: 68/391 ] | Loss: 0.742 | Acc: 81.001% (7154/8832)/ 94.882% (8380/8832)
01/15/2023 08:42:30 - INFO - __main__ -   test: [batch: 69/391 ] | Loss: 0.738 | Acc: 81.105% (7267/8960)/ 94.933% (8506/8960)
01/15/2023 08:42:32 - INFO - __main__ -   test: [batch: 70/391 ] | Loss: 0.738 | Acc: 81.019% (7363/9088)/ 94.971% (8631/9088)
01/15/2023 08:42:34 - INFO - __main__ -   test: [batch: 71/391 ] | Loss: 0.736 | Acc: 81.087% (7473/9216)/ 94.965% (8752/9216)
01/15/2023 08:42:36 - INFO - __main__ -   test: [batch: 72/391 ] | Loss: 0.739 | Acc: 80.982% (7567/9344)/ 94.991% (8876/9344)
01/15/2023 08:42:39 - INFO - __main__ -   test: [batch: 73/391 ] | Loss: 0.743 | Acc: 80.859% (7659/9472)/ 94.985% (8997/9472)
01/15/2023 08:42:41 - INFO - __main__ -   test: [batch: 74/391 ] | Loss: 0.743 | Acc: 80.865% (7763/9600)/ 94.969% (9117/9600)
01/15/2023 08:42:43 - INFO - __main__ -   test: [batch: 75/391 ] | Loss: 0.749 | Acc: 80.705% (7851/9728)/ 94.953% (9237/9728)
01/15/2023 08:42:45 - INFO - __main__ -   test: [batch: 76/391 ] | Loss: 0.748 | Acc: 80.702% (7954/9856)/ 94.978% (9361/9856)
01/15/2023 08:42:47 - INFO - __main__ -   test: [batch: 77/391 ] | Loss: 0.748 | Acc: 80.619% (8049/9984)/ 95.012% (9486/9984)
01/15/2023 08:42:49 - INFO - __main__ -   test: [batch: 78/391 ] | Loss: 0.747 | Acc: 80.578% (8148/10112)/ 95.055% (9612/10112)
01/15/2023 08:42:51 - INFO - __main__ -   test: [batch: 79/391 ] | Loss: 0.746 | Acc: 80.547% (8248/10240)/ 95.068% (9735/10240)
01/15/2023 08:42:53 - INFO - __main__ -   test: [batch: 80/391 ] | Loss: 0.746 | Acc: 80.527% (8349/10368)/ 95.052% (9855/10368)
01/15/2023 08:42:55 - INFO - __main__ -   test: [batch: 81/391 ] | Loss: 0.745 | Acc: 80.535% (8453/10496)/ 95.065% (9978/10496)
01/15/2023 08:42:58 - INFO - __main__ -   test: [batch: 82/391 ] | Loss: 0.744 | Acc: 80.553% (8558/10624)/ 95.077% (10101/10624)
01/15/2023 08:43:00 - INFO - __main__ -   test: [batch: 83/391 ] | Loss: 0.744 | Acc: 80.599% (8666/10752)/ 95.061% (10221/10752)
01/15/2023 08:43:02 - INFO - __main__ -   test: [batch: 84/391 ] | Loss: 0.740 | Acc: 80.735% (8784/10880)/ 95.092% (10346/10880)
01/15/2023 08:43:04 - INFO - __main__ -   test: [batch: 85/391 ] | Loss: 0.738 | Acc: 80.723% (8886/11008)/ 95.131% (10472/11008)
01/15/2023 08:43:06 - INFO - __main__ -   test: [batch: 86/391 ] | Loss: 0.742 | Acc: 80.639% (8980/11136)/ 95.088% (10589/11136)
01/15/2023 08:43:08 - INFO - __main__ -   test: [batch: 87/391 ] | Loss: 0.741 | Acc: 80.682% (9088/11264)/ 95.073% (10709/11264)
01/15/2023 08:43:10 - INFO - __main__ -   test: [batch: 88/391 ] | Loss: 0.748 | Acc: 80.627% (9185/11392)/ 95.014% (10824/11392)
01/15/2023 08:43:13 - INFO - __main__ -   test: [batch: 89/391 ] | Loss: 0.746 | Acc: 80.668% (9293/11520)/ 95.026% (10947/11520)
01/15/2023 08:43:15 - INFO - __main__ -   test: [batch: 90/391 ] | Loss: 0.747 | Acc: 80.589% (9387/11648)/ 95.038% (11070/11648)
01/15/2023 08:43:17 - INFO - __main__ -   test: [batch: 91/391 ] | Loss: 0.745 | Acc: 80.639% (9496/11776)/ 95.049% (11193/11776)
01/15/2023 08:43:19 - INFO - __main__ -   test: [batch: 92/391 ] | Loss: 0.747 | Acc: 80.586% (9593/11904)/ 95.027% (11312/11904)
01/15/2023 08:43:21 - INFO - __main__ -   test: [batch: 93/391 ] | Loss: 0.749 | Acc: 80.386% (9672/12032)/ 95.063% (11438/12032)
01/15/2023 08:43:23 - INFO - __main__ -   test: [batch: 94/391 ] | Loss: 0.752 | Acc: 80.238% (9757/12160)/ 95.066% (11560/12160)
01/15/2023 08:43:25 - INFO - __main__ -   test: [batch: 95/391 ] | Loss: 0.750 | Acc: 80.298% (9867/12288)/ 95.085% (11684/12288)
01/15/2023 08:43:27 - INFO - __main__ -   test: [batch: 96/391 ] | Loss: 0.753 | Acc: 80.171% (9954/12416)/ 95.079% (11805/12416)
01/15/2023 08:43:29 - INFO - __main__ -   test: [batch: 97/391 ] | Loss: 0.756 | Acc: 79.998% (10035/12544)/ 95.097% (11929/12544)
01/15/2023 08:43:31 - INFO - __main__ -   test: [batch: 98/391 ] | Loss: 0.752 | Acc: 80.106% (10151/12672)/ 95.131% (12055/12672)
01/15/2023 08:43:33 - INFO - __main__ -   test: [batch: 99/391 ] | Loss: 0.746 | Acc: 80.273% (10275/12800)/ 95.172% (12182/12800)
01/15/2023 08:43:36 - INFO - __main__ -   test: [batch: 100/391 ] | Loss: 0.745 | Acc: 80.330% (10385/12928)/ 95.181% (12305/12928)
01/15/2023 08:43:38 - INFO - __main__ -   test: [batch: 101/391 ] | Loss: 0.741 | Acc: 80.446% (10503/13056)/ 95.213% (12431/13056)
01/15/2023 08:43:40 - INFO - __main__ -   test: [batch: 102/391 ] | Loss: 0.738 | Acc: 80.514% (10615/13184)/ 95.244% (12557/13184)
01/15/2023 08:43:42 - INFO - __main__ -   test: [batch: 103/391 ] | Loss: 0.739 | Acc: 80.379% (10700/13312)/ 95.245% (12679/13312)
01/15/2023 08:43:44 - INFO - __main__ -   test: [batch: 104/391 ] | Loss: 0.739 | Acc: 80.327% (10796/13440)/ 95.253% (12802/13440)
01/15/2023 08:43:46 - INFO - __main__ -   test: [batch: 105/391 ] | Loss: 0.740 | Acc: 80.314% (10897/13568)/ 95.254% (12924/13568)
01/15/2023 08:43:48 - INFO - __main__ -   test: [batch: 106/391 ] | Loss: 0.747 | Acc: 80.250% (10991/13696)/ 95.188% (13037/13696)
01/15/2023 08:43:50 - INFO - __main__ -   test: [batch: 107/391 ] | Loss: 0.743 | Acc: 80.375% (11111/13824)/ 95.226% (13164/13824)
01/15/2023 08:43:52 - INFO - __main__ -   test: [batch: 108/391 ] | Loss: 0.746 | Acc: 80.225% (11193/13952)/ 95.234% (13287/13952)
01/15/2023 08:43:55 - INFO - __main__ -   test: [batch: 109/391 ] | Loss: 0.747 | Acc: 80.227% (11296/14080)/ 95.227% (13408/14080)
01/15/2023 08:43:57 - INFO - __main__ -   test: [batch: 110/391 ] | Loss: 0.749 | Acc: 80.046% (11373/14208)/ 95.249% (13533/14208)
01/15/2023 08:43:59 - INFO - __main__ -   test: [batch: 111/391 ] | Loss: 0.753 | Acc: 79.987% (11467/14336)/ 95.208% (13649/14336)
01/15/2023 08:44:01 - INFO - __main__ -   test: [batch: 112/391 ] | Loss: 0.753 | Acc: 80.012% (11573/14464)/ 95.230% (13774/14464)
01/15/2023 08:44:03 - INFO - __main__ -   test: [batch: 113/391 ] | Loss: 0.751 | Acc: 80.051% (11681/14592)/ 95.237% (13897/14592)
01/15/2023 08:44:05 - INFO - __main__ -   test: [batch: 114/391 ] | Loss: 0.748 | Acc: 80.149% (11798/14720)/ 95.265% (14023/14720)
01/15/2023 08:44:08 - INFO - __main__ -   test: [batch: 115/391 ] | Loss: 0.745 | Acc: 80.246% (11915/14848)/ 95.286% (14148/14848)
01/15/2023 08:44:10 - INFO - __main__ -   test: [batch: 116/391 ] | Loss: 0.748 | Acc: 80.215% (12013/14976)/ 95.246% (14264/14976)
01/15/2023 08:44:12 - INFO - __main__ -   test: [batch: 117/391 ] | Loss: 0.747 | Acc: 80.244% (12120/15104)/ 95.266% (14389/15104)
01/15/2023 08:44:14 - INFO - __main__ -   test: [batch: 118/391 ] | Loss: 0.751 | Acc: 80.075% (12197/15232)/ 95.273% (14512/15232)
01/15/2023 08:44:16 - INFO - __main__ -   test: [batch: 119/391 ] | Loss: 0.749 | Acc: 80.111% (12305/15360)/ 95.299% (14638/15360)
01/15/2023 08:44:18 - INFO - __main__ -   test: [batch: 120/391 ] | Loss: 0.748 | Acc: 80.172% (12417/15488)/ 95.319% (14763/15488)
01/15/2023 08:44:20 - INFO - __main__ -   test: [batch: 121/391 ] | Loss: 0.753 | Acc: 80.033% (12498/15616)/ 95.287% (14880/15616)
01/15/2023 08:44:22 - INFO - __main__ -   test: [batch: 122/391 ] | Loss: 0.756 | Acc: 79.980% (12592/15744)/ 95.255% (14997/15744)
01/15/2023 08:44:24 - INFO - __main__ -   test: [batch: 123/391 ] | Loss: 0.757 | Acc: 79.990% (12696/15872)/ 95.231% (15115/15872)
01/15/2023 08:44:26 - INFO - __main__ -   test: [batch: 124/391 ] | Loss: 0.756 | Acc: 80.013% (12802/16000)/ 95.244% (15239/16000)
01/15/2023 08:44:29 - INFO - __main__ -   test: [batch: 125/391 ] | Loss: 0.753 | Acc: 80.115% (12921/16128)/ 95.269% (15365/16128)
01/15/2023 08:44:31 - INFO - __main__ -   test: [batch: 126/391 ] | Loss: 0.749 | Acc: 80.210% (13039/16256)/ 95.276% (15488/16256)
01/15/2023 08:44:33 - INFO - __main__ -   test: [batch: 127/391 ] | Loss: 0.747 | Acc: 80.286% (13154/16384)/ 95.288% (15612/16384)
01/15/2023 08:44:35 - INFO - __main__ -   test: [batch: 128/391 ] | Loss: 0.748 | Acc: 80.275% (13255/16512)/ 95.264% (15730/16512)
01/15/2023 08:44:37 - INFO - __main__ -   test: [batch: 129/391 ] | Loss: 0.746 | Acc: 80.312% (13364/16640)/ 95.288% (15856/16640)
01/15/2023 08:44:40 - INFO - __main__ -   test: [batch: 130/391 ] | Loss: 0.742 | Acc: 80.421% (13485/16768)/ 95.312% (15982/16768)
01/15/2023 08:44:42 - INFO - __main__ -   test: [batch: 131/391 ] | Loss: 0.742 | Acc: 80.463% (13595/16896)/ 95.324% (16106/16896)
01/15/2023 08:44:44 - INFO - __main__ -   test: [batch: 132/391 ] | Loss: 0.740 | Acc: 80.545% (13712/17024)/ 95.336% (16230/17024)
01/15/2023 08:44:46 - INFO - __main__ -   test: [batch: 133/391 ] | Loss: 0.742 | Acc: 80.498% (13807/17152)/ 95.330% (16351/17152)
01/15/2023 08:44:48 - INFO - __main__ -   test: [batch: 134/391 ] | Loss: 0.740 | Acc: 80.532% (13916/17280)/ 95.347% (16476/17280)
01/15/2023 08:44:50 - INFO - __main__ -   test: [batch: 135/391 ] | Loss: 0.740 | Acc: 80.515% (14016/17408)/ 95.353% (16599/17408)
01/15/2023 08:44:52 - INFO - __main__ -   test: [batch: 136/391 ] | Loss: 0.738 | Acc: 80.509% (14118/17536)/ 95.375% (16725/17536)
01/15/2023 08:44:54 - INFO - __main__ -   test: [batch: 137/391 ] | Loss: 0.736 | Acc: 80.548% (14228/17664)/ 95.409% (16853/17664)
01/15/2023 08:44:56 - INFO - __main__ -   test: [batch: 138/391 ] | Loss: 0.735 | Acc: 80.570% (14335/17792)/ 95.425% (16978/17792)
01/15/2023 08:44:58 - INFO - __main__ -   test: [batch: 139/391 ] | Loss: 0.740 | Acc: 80.419% (14411/17920)/ 95.419% (17099/17920)
01/15/2023 08:45:01 - INFO - __main__ -   test: [batch: 140/391 ] | Loss: 0.742 | Acc: 80.336% (14499/18048)/ 95.423% (17222/18048)
01/15/2023 08:45:03 - INFO - __main__ -   test: [batch: 141/391 ] | Loss: 0.740 | Acc: 80.386% (14611/18176)/ 95.428% (17345/18176)
01/15/2023 08:45:05 - INFO - __main__ -   test: [batch: 142/391 ] | Loss: 0.738 | Acc: 80.463% (14728/18304)/ 95.444% (17470/18304)
01/15/2023 08:45:07 - INFO - __main__ -   test: [batch: 143/391 ] | Loss: 0.738 | Acc: 80.490% (14836/18432)/ 95.437% (17591/18432)
01/15/2023 08:45:09 - INFO - __main__ -   test: [batch: 144/391 ] | Loss: 0.740 | Acc: 80.469% (14935/18560)/ 95.409% (17708/18560)
01/15/2023 08:45:11 - INFO - __main__ -   test: [batch: 145/391 ] | Loss: 0.743 | Acc: 80.458% (15036/18688)/ 95.393% (17827/18688)
01/15/2023 08:45:13 - INFO - __main__ -   test: [batch: 146/391 ] | Loss: 0.744 | Acc: 80.421% (15132/18816)/ 95.371% (17945/18816)
01/15/2023 08:45:15 - INFO - __main__ -   test: [batch: 147/391 ] | Loss: 0.744 | Acc: 80.427% (15236/18944)/ 95.349% (18063/18944)
01/15/2023 08:45:17 - INFO - __main__ -   test: [batch: 148/391 ] | Loss: 0.746 | Acc: 80.343% (15323/19072)/ 95.349% (18185/19072)
01/15/2023 08:45:20 - INFO - __main__ -   test: [batch: 149/391 ] | Loss: 0.750 | Acc: 80.255% (15409/19200)/ 95.307% (18299/19200)
01/15/2023 08:45:22 - INFO - __main__ -   test: [batch: 150/391 ] | Loss: 0.750 | Acc: 80.210% (15503/19328)/ 95.318% (18423/19328)
01/15/2023 08:45:24 - INFO - __main__ -   test: [batch: 151/391 ] | Loss: 0.750 | Acc: 80.253% (15614/19456)/ 95.312% (18544/19456)
01/15/2023 08:45:26 - INFO - __main__ -   test: [batch: 152/391 ] | Loss: 0.751 | Acc: 80.219% (15710/19584)/ 95.297% (18663/19584)
01/15/2023 08:45:28 - INFO - __main__ -   test: [batch: 153/391 ] | Loss: 0.750 | Acc: 80.281% (15825/19712)/ 95.307% (18787/19712)
01/15/2023 08:45:30 - INFO - __main__ -   test: [batch: 154/391 ] | Loss: 0.751 | Acc: 80.252% (15922/19840)/ 95.257% (18899/19840)
01/15/2023 08:45:32 - INFO - __main__ -   test: [batch: 155/391 ] | Loss: 0.752 | Acc: 80.263% (16027/19968)/ 95.242% (19018/19968)
01/15/2023 08:45:34 - INFO - __main__ -   test: [batch: 156/391 ] | Loss: 0.754 | Acc: 80.190% (16115/20096)/ 95.218% (19135/20096)
01/15/2023 08:45:36 - INFO - __main__ -   test: [batch: 157/391 ] | Loss: 0.757 | Acc: 80.147% (16209/20224)/ 95.199% (19253/20224)
01/15/2023 08:45:38 - INFO - __main__ -   test: [batch: 158/391 ] | Loss: 0.758 | Acc: 80.120% (16306/20352)/ 95.175% (19370/20352)
01/15/2023 08:45:40 - INFO - __main__ -   test: [batch: 159/391 ] | Loss: 0.759 | Acc: 80.107% (16406/20480)/ 95.181% (19493/20480)
01/15/2023 08:45:42 - INFO - __main__ -   test: [batch: 160/391 ] | Loss: 0.760 | Acc: 80.085% (16504/20608)/ 95.152% (19609/20608)
01/15/2023 08:45:44 - INFO - __main__ -   test: [batch: 161/391 ] | Loss: 0.770 | Acc: 79.832% (16554/20736)/ 95.047% (19709/20736)
01/15/2023 08:45:46 - INFO - __main__ -   test: [batch: 162/391 ] | Loss: 0.776 | Acc: 79.716% (16632/20864)/ 94.967% (19814/20864)
01/15/2023 08:45:49 - INFO - __main__ -   test: [batch: 163/391 ] | Loss: 0.778 | Acc: 79.654% (16721/20992)/ 94.960% (19934/20992)
01/15/2023 08:45:51 - INFO - __main__ -   test: [batch: 164/391 ] | Loss: 0.778 | Acc: 79.645% (16821/21120)/ 94.967% (20057/21120)
01/15/2023 08:45:53 - INFO - __main__ -   test: [batch: 165/391 ] | Loss: 0.780 | Acc: 79.575% (16908/21248)/ 94.964% (20178/21248)
01/15/2023 08:45:55 - INFO - __main__ -   test: [batch: 166/391 ] | Loss: 0.780 | Acc: 79.603% (17016/21376)/ 94.952% (20297/21376)
01/15/2023 08:45:57 - INFO - __main__ -   test: [batch: 167/391 ] | Loss: 0.782 | Acc: 79.557% (17108/21504)/ 94.927% (20413/21504)
01/15/2023 08:45:59 - INFO - __main__ -   test: [batch: 168/391 ] | Loss: 0.782 | Acc: 79.572% (17213/21632)/ 94.915% (20532/21632)
01/15/2023 08:46:01 - INFO - __main__ -   test: [batch: 169/391 ] | Loss: 0.784 | Acc: 79.517% (17303/21760)/ 94.881% (20646/21760)
01/15/2023 08:46:03 - INFO - __main__ -   test: [batch: 170/391 ] | Loss: 0.789 | Acc: 79.418% (17383/21888)/ 94.846% (20760/21888)
01/15/2023 08:46:06 - INFO - __main__ -   test: [batch: 171/391 ] | Loss: 0.792 | Acc: 79.379% (17476/22016)/ 94.817% (20875/22016)
01/15/2023 08:46:08 - INFO - __main__ -   test: [batch: 172/391 ] | Loss: 0.793 | Acc: 79.335% (17568/22144)/ 94.816% (20996/22144)
01/15/2023 08:46:10 - INFO - __main__ -   test: [batch: 173/391 ] | Loss: 0.795 | Acc: 79.274% (17656/22272)/ 94.783% (21110/22272)
01/15/2023 08:46:12 - INFO - __main__ -   test: [batch: 174/391 ] | Loss: 0.800 | Acc: 79.174% (17735/22400)/ 94.741% (21222/22400)
01/15/2023 08:46:14 - INFO - __main__ -   test: [batch: 175/391 ] | Loss: 0.798 | Acc: 79.230% (17849/22528)/ 94.753% (21346/22528)
01/15/2023 08:46:16 - INFO - __main__ -   test: [batch: 176/391 ] | Loss: 0.799 | Acc: 79.220% (17948/22656)/ 94.725% (21461/22656)
01/15/2023 08:46:18 - INFO - __main__ -   test: [batch: 177/391 ] | Loss: 0.803 | Acc: 79.139% (18031/22784)/ 94.685% (21573/22784)
01/15/2023 08:46:20 - INFO - __main__ -   test: [batch: 178/391 ] | Loss: 0.803 | Acc: 79.155% (18136/22912)/ 94.671% (21691/22912)
01/15/2023 08:46:22 - INFO - __main__ -   test: [batch: 179/391 ] | Loss: 0.807 | Acc: 79.102% (18225/23040)/ 94.622% (21801/23040)
01/15/2023 08:46:24 - INFO - __main__ -   test: [batch: 180/391 ] | Loss: 0.813 | Acc: 78.967% (18295/23168)/ 94.579% (21912/23168)
01/15/2023 08:46:26 - INFO - __main__ -   test: [batch: 181/391 ] | Loss: 0.819 | Acc: 78.850% (18369/23296)/ 94.510% (22017/23296)
01/15/2023 08:46:29 - INFO - __main__ -   test: [batch: 182/391 ] | Loss: 0.818 | Acc: 78.868% (18474/23424)/ 94.514% (22139/23424)
01/15/2023 08:46:31 - INFO - __main__ -   test: [batch: 183/391 ] | Loss: 0.825 | Acc: 78.745% (18546/23552)/ 94.408% (22235/23552)
01/15/2023 08:46:33 - INFO - __main__ -   test: [batch: 184/391 ] | Loss: 0.825 | Acc: 78.771% (18653/23680)/ 94.405% (22355/23680)
01/15/2023 08:46:35 - INFO - __main__ -   test: [batch: 185/391 ] | Loss: 0.825 | Acc: 78.784% (18757/23808)/ 94.393% (22473/23808)
01/15/2023 08:46:37 - INFO - __main__ -   test: [batch: 186/391 ] | Loss: 0.827 | Acc: 78.747% (18849/23936)/ 94.352% (22584/23936)
01/15/2023 08:46:40 - INFO - __main__ -   test: [batch: 187/391 ] | Loss: 0.832 | Acc: 78.644% (18925/24064)/ 94.336% (22701/24064)
01/15/2023 08:46:42 - INFO - __main__ -   test: [batch: 188/391 ] | Loss: 0.835 | Acc: 78.526% (18997/24192)/ 94.308% (22815/24192)
01/15/2023 08:46:44 - INFO - __main__ -   test: [batch: 189/391 ] | Loss: 0.837 | Acc: 78.458% (19081/24320)/ 94.309% (22936/24320)
01/15/2023 08:46:46 - INFO - __main__ -   test: [batch: 190/391 ] | Loss: 0.841 | Acc: 78.366% (19159/24448)/ 94.282% (23050/24448)
01/15/2023 08:46:48 - INFO - __main__ -   test: [batch: 191/391 ] | Loss: 0.844 | Acc: 78.341% (19253/24576)/ 94.255% (23164/24576)
01/15/2023 08:46:50 - INFO - __main__ -   test: [batch: 192/391 ] | Loss: 0.851 | Acc: 78.210% (19321/24704)/ 94.151% (23259/24704)
01/15/2023 08:46:52 - INFO - __main__ -   test: [batch: 193/391 ] | Loss: 0.851 | Acc: 78.230% (19426/24832)/ 94.145% (23378/24832)
01/15/2023 08:46:55 - INFO - __main__ -   test: [batch: 194/391 ] | Loss: 0.854 | Acc: 78.161% (19509/24960)/ 94.111% (23490/24960)
01/15/2023 08:46:57 - INFO - __main__ -   test: [batch: 195/391 ] | Loss: 0.860 | Acc: 78.065% (19585/25088)/ 94.049% (23595/25088)
01/15/2023 08:46:59 - INFO - __main__ -   test: [batch: 196/391 ] | Loss: 0.864 | Acc: 77.974% (19662/25216)/ 94.004% (23704/25216)
01/15/2023 08:47:01 - INFO - __main__ -   test: [batch: 197/391 ] | Loss: 0.868 | Acc: 77.880% (19738/25344)/ 93.983% (23819/25344)
01/15/2023 08:47:03 - INFO - __main__ -   test: [batch: 198/391 ] | Loss: 0.870 | Acc: 77.838% (19827/25472)/ 93.931% (23926/25472)
01/15/2023 08:47:05 - INFO - __main__ -   test: [batch: 199/391 ] | Loss: 0.870 | Acc: 77.801% (19917/25600)/ 93.945% (24050/25600)
01/15/2023 08:47:08 - INFO - __main__ -   test: [batch: 200/391 ] | Loss: 0.870 | Acc: 77.767% (20008/25728)/ 93.952% (24172/25728)
01/15/2023 08:47:10 - INFO - __main__ -   test: [batch: 201/391 ] | Loss: 0.873 | Acc: 77.672% (20083/25856)/ 93.920% (24284/25856)
01/15/2023 08:47:12 - INFO - __main__ -   test: [batch: 202/391 ] | Loss: 0.874 | Acc: 77.655% (20178/25984)/ 93.915% (24403/25984)
01/15/2023 08:47:14 - INFO - __main__ -   test: [batch: 203/391 ] | Loss: 0.875 | Acc: 77.646% (20275/26112)/ 93.903% (24520/26112)
01/15/2023 08:47:16 - INFO - __main__ -   test: [batch: 204/391 ] | Loss: 0.878 | Acc: 77.557% (20351/26240)/ 93.872% (24632/26240)
01/15/2023 08:47:18 - INFO - __main__ -   test: [batch: 205/391 ] | Loss: 0.881 | Acc: 77.484% (20431/26368)/ 93.833% (24742/26368)
01/15/2023 08:47:20 - INFO - __main__ -   test: [batch: 206/391 ] | Loss: 0.883 | Acc: 77.457% (20523/26496)/ 93.829% (24861/26496)
01/15/2023 08:47:22 - INFO - __main__ -   test: [batch: 207/391 ] | Loss: 0.886 | Acc: 77.381% (20602/26624)/ 93.795% (24972/26624)
01/15/2023 08:47:25 - INFO - __main__ -   test: [batch: 208/391 ] | Loss: 0.888 | Acc: 77.340% (20690/26752)/ 93.769% (25085/26752)
01/15/2023 08:47:27 - INFO - __main__ -   test: [batch: 209/391 ] | Loss: 0.888 | Acc: 77.340% (20789/26880)/ 93.791% (25211/26880)
01/15/2023 08:47:29 - INFO - __main__ -   test: [batch: 210/391 ] | Loss: 0.889 | Acc: 77.307% (20879/27008)/ 93.772% (25326/27008)
01/15/2023 08:47:31 - INFO - __main__ -   test: [batch: 211/391 ] | Loss: 0.892 | Acc: 77.244% (20961/27136)/ 93.735% (25436/27136)
01/15/2023 08:47:33 - INFO - __main__ -   test: [batch: 212/391 ] | Loss: 0.894 | Acc: 77.168% (21039/27264)/ 93.724% (25553/27264)
01/15/2023 08:47:35 - INFO - __main__ -   test: [batch: 213/391 ] | Loss: 0.893 | Acc: 77.172% (21139/27392)/ 93.732% (25675/27392)
01/15/2023 08:47:37 - INFO - __main__ -   test: [batch: 214/391 ] | Loss: 0.894 | Acc: 77.166% (21236/27520)/ 93.735% (25796/27520)
01/15/2023 08:47:39 - INFO - __main__ -   test: [batch: 215/391 ] | Loss: 0.895 | Acc: 77.138% (21327/27648)/ 93.710% (25909/27648)
01/15/2023 08:47:41 - INFO - __main__ -   test: [batch: 216/391 ] | Loss: 0.893 | Acc: 77.189% (21440/27776)/ 93.732% (26035/27776)
01/15/2023 08:47:44 - INFO - __main__ -   test: [batch: 217/391 ] | Loss: 0.898 | Acc: 77.129% (21522/27904)/ 93.671% (26138/27904)
01/15/2023 08:47:46 - INFO - __main__ -   test: [batch: 218/391 ] | Loss: 0.901 | Acc: 77.055% (21600/28032)/ 93.632% (26247/28032)
01/15/2023 08:47:48 - INFO - __main__ -   test: [batch: 219/391 ] | Loss: 0.899 | Acc: 77.092% (21709/28160)/ 93.647% (26371/28160)
01/15/2023 08:47:50 - INFO - __main__ -   test: [batch: 220/391 ] | Loss: 0.897 | Acc: 77.135% (21820/28288)/ 93.662% (26495/28288)
01/15/2023 08:47:52 - INFO - __main__ -   test: [batch: 221/391 ] | Loss: 0.899 | Acc: 77.104% (21910/28416)/ 93.637% (26608/28416)
01/15/2023 08:47:54 - INFO - __main__ -   test: [batch: 222/391 ] | Loss: 0.897 | Acc: 77.155% (22023/28544)/ 93.655% (26733/28544)
01/15/2023 08:47:56 - INFO - __main__ -   test: [batch: 223/391 ] | Loss: 0.898 | Acc: 77.145% (22119/28672)/ 93.652% (26852/28672)
01/15/2023 08:47:58 - INFO - __main__ -   test: [batch: 224/391 ] | Loss: 0.897 | Acc: 77.170% (22225/28800)/ 93.656% (26973/28800)
01/15/2023 08:48:00 - INFO - __main__ -   test: [batch: 225/391 ] | Loss: 0.897 | Acc: 77.174% (22325/28928)/ 93.660% (27094/28928)
01/15/2023 08:48:03 - INFO - __main__ -   test: [batch: 226/391 ] | Loss: 0.897 | Acc: 77.148% (22416/29056)/ 93.661% (27214/29056)
01/15/2023 08:48:05 - INFO - __main__ -   test: [batch: 227/391 ] | Loss: 0.899 | Acc: 77.138% (22512/29184)/ 93.647% (27330/29184)
01/15/2023 08:48:07 - INFO - __main__ -   test: [batch: 228/391 ] | Loss: 0.903 | Acc: 77.057% (22587/29312)/ 93.576% (27429/29312)
01/15/2023 08:48:09 - INFO - __main__ -   test: [batch: 229/391 ] | Loss: 0.907 | Acc: 76.970% (22660/29440)/ 93.526% (27534/29440)
01/15/2023 08:48:11 - INFO - __main__ -   test: [batch: 230/391 ] | Loss: 0.910 | Acc: 76.897% (22737/29568)/ 93.476% (27639/29568)
01/15/2023 08:48:13 - INFO - __main__ -   test: [batch: 231/391 ] | Loss: 0.910 | Acc: 76.879% (22830/29696)/ 93.474% (27758/29696)
01/15/2023 08:48:15 - INFO - __main__ -   test: [batch: 232/391 ] | Loss: 0.909 | Acc: 76.908% (22937/29824)/ 93.485% (27881/29824)
01/15/2023 08:48:18 - INFO - __main__ -   test: [batch: 233/391 ] | Loss: 0.911 | Acc: 76.866% (23023/29952)/ 93.460% (27993/29952)
01/15/2023 08:48:20 - INFO - __main__ -   test: [batch: 234/391 ] | Loss: 0.917 | Acc: 76.755% (23088/30080)/ 93.398% (28094/30080)
01/15/2023 08:48:22 - INFO - __main__ -   test: [batch: 235/391 ] | Loss: 0.917 | Acc: 76.751% (23185/30208)/ 93.396% (28213/30208)
01/15/2023 08:48:24 - INFO - __main__ -   test: [batch: 236/391 ] | Loss: 0.915 | Acc: 76.806% (23300/30336)/ 93.407% (28336/30336)
01/15/2023 08:48:26 - INFO - __main__ -   test: [batch: 237/391 ] | Loss: 0.916 | Acc: 76.809% (23399/30464)/ 93.379% (28447/30464)
01/15/2023 08:48:28 - INFO - __main__ -   test: [batch: 238/391 ] | Loss: 0.915 | Acc: 76.844% (23508/30592)/ 93.387% (28569/30592)
01/15/2023 08:48:30 - INFO - __main__ -   test: [batch: 239/391 ] | Loss: 0.914 | Acc: 76.872% (23615/30720)/ 93.389% (28689/30720)
01/15/2023 08:48:32 - INFO - __main__ -   test: [batch: 240/391 ] | Loss: 0.914 | Acc: 76.883% (23717/30848)/ 93.374% (28804/30848)
01/15/2023 08:48:35 - INFO - __main__ -   test: [batch: 241/391 ] | Loss: 0.917 | Acc: 76.798% (23789/30976)/ 93.337% (28912/30976)
01/15/2023 08:48:37 - INFO - __main__ -   test: [batch: 242/391 ] | Loss: 0.920 | Acc: 76.691% (23854/31104)/ 93.326% (29028/31104)
01/15/2023 08:48:39 - INFO - __main__ -   test: [batch: 243/391 ] | Loss: 0.925 | Acc: 76.579% (23917/31232)/ 93.244% (29122/31232)
01/15/2023 08:48:41 - INFO - __main__ -   test: [batch: 244/391 ] | Loss: 0.926 | Acc: 76.572% (24013/31360)/ 93.237% (29239/31360)
01/15/2023 08:48:43 - INFO - __main__ -   test: [batch: 245/391 ] | Loss: 0.925 | Acc: 76.575% (24112/31488)/ 93.232% (29357/31488)
01/15/2023 08:48:45 - INFO - __main__ -   test: [batch: 246/391 ] | Loss: 0.926 | Acc: 76.556% (24204/31616)/ 93.225% (29474/31616)
01/15/2023 08:48:48 - INFO - __main__ -   test: [batch: 247/391 ] | Loss: 0.932 | Acc: 76.468% (24274/31744)/ 93.161% (29573/31744)
01/15/2023 08:48:50 - INFO - __main__ -   test: [batch: 248/391 ] | Loss: 0.933 | Acc: 76.431% (24360/31872)/ 93.160% (29692/31872)
01/15/2023 08:48:52 - INFO - __main__ -   test: [batch: 249/391 ] | Loss: 0.935 | Acc: 76.278% (24409/32000)/ 93.153% (29809/32000)
01/15/2023 08:48:54 - INFO - __main__ -   test: [batch: 250/391 ] | Loss: 0.934 | Acc: 76.310% (24517/32128)/ 93.162% (29931/32128)
01/15/2023 08:48:56 - INFO - __main__ -   test: [batch: 251/391 ] | Loss: 0.936 | Acc: 76.271% (24602/32256)/ 93.136% (30042/32256)
01/15/2023 08:48:58 - INFO - __main__ -   test: [batch: 252/391 ] | Loss: 0.935 | Acc: 76.297% (24708/32384)/ 93.126% (30158/32384)
01/15/2023 08:49:00 - INFO - __main__ -   test: [batch: 253/391 ] | Loss: 0.936 | Acc: 76.261% (24794/32512)/ 93.107% (30271/32512)
01/15/2023 08:49:03 - INFO - __main__ -   test: [batch: 254/391 ] | Loss: 0.941 | Acc: 76.183% (24866/32640)/ 93.055% (30373/32640)
01/15/2023 08:49:05 - INFO - __main__ -   test: [batch: 255/391 ] | Loss: 0.943 | Acc: 76.160% (24956/32768)/ 93.045% (30489/32768)
01/15/2023 08:49:07 - INFO - __main__ -   test: [batch: 256/391 ] | Loss: 0.948 | Acc: 76.040% (25014/32896)/ 92.999% (30593/32896)
01/15/2023 08:49:09 - INFO - __main__ -   test: [batch: 257/391 ] | Loss: 0.948 | Acc: 76.020% (25105/33024)/ 92.999% (30712/33024)
01/15/2023 08:49:11 - INFO - __main__ -   test: [batch: 258/391 ] | Loss: 0.949 | Acc: 76.017% (25201/33152)/ 92.987% (30827/33152)
01/15/2023 08:49:13 - INFO - __main__ -   test: [batch: 259/391 ] | Loss: 0.953 | Acc: 75.895% (25258/33280)/ 92.984% (30945/33280)
01/15/2023 08:49:15 - INFO - __main__ -   test: [batch: 260/391 ] | Loss: 0.953 | Acc: 75.877% (25349/33408)/ 92.981% (31063/33408)
01/15/2023 08:49:17 - INFO - __main__ -   test: [batch: 261/391 ] | Loss: 0.951 | Acc: 75.918% (25460/33536)/ 93.005% (31190/33536)
01/15/2023 08:49:19 - INFO - __main__ -   test: [batch: 262/391 ] | Loss: 0.950 | Acc: 75.942% (25565/33664)/ 93.013% (31312/33664)
01/15/2023 08:49:21 - INFO - __main__ -   test: [batch: 263/391 ] | Loss: 0.953 | Acc: 75.879% (25641/33792)/ 92.989% (31423/33792)
01/15/2023 08:49:23 - INFO - __main__ -   test: [batch: 264/391 ] | Loss: 0.958 | Acc: 75.808% (25714/33920)/ 92.930% (31522/33920)
01/15/2023 08:49:25 - INFO - __main__ -   test: [batch: 265/391 ] | Loss: 0.957 | Acc: 75.843% (25823/34048)/ 92.931% (31641/34048)
01/15/2023 08:49:27 - INFO - __main__ -   test: [batch: 266/391 ] | Loss: 0.960 | Acc: 75.787% (25901/34176)/ 92.916% (31755/34176)
01/15/2023 08:49:30 - INFO - __main__ -   test: [batch: 267/391 ] | Loss: 0.959 | Acc: 75.825% (26011/34304)/ 92.916% (31874/34304)
01/15/2023 08:49:32 - INFO - __main__ -   test: [batch: 268/391 ] | Loss: 0.958 | Acc: 75.836% (26112/34432)/ 92.911% (31991/34432)
01/15/2023 08:49:34 - INFO - __main__ -   test: [batch: 269/391 ] | Loss: 0.960 | Acc: 75.781% (26190/34560)/ 92.894% (32104/34560)
01/15/2023 08:49:36 - INFO - __main__ -   test: [batch: 270/391 ] | Loss: 0.963 | Acc: 75.735% (26271/34688)/ 92.868% (32214/34688)
01/15/2023 08:49:38 - INFO - __main__ -   test: [batch: 271/391 ] | Loss: 0.963 | Acc: 75.750% (26373/34816)/ 92.862% (32331/34816)
01/15/2023 08:49:40 - INFO - __main__ -   test: [batch: 272/391 ] | Loss: 0.964 | Acc: 75.718% (26459/34944)/ 92.857% (32448/34944)
01/15/2023 08:49:42 - INFO - __main__ -   test: [batch: 273/391 ] | Loss: 0.964 | Acc: 75.733% (26561/35072)/ 92.835% (32559/35072)
01/15/2023 08:49:44 - INFO - __main__ -   test: [batch: 274/391 ] | Loss: 0.964 | Acc: 75.733% (26658/35200)/ 92.841% (32680/35200)
01/15/2023 08:49:47 - INFO - __main__ -   test: [batch: 275/391 ] | Loss: 0.966 | Acc: 75.694% (26741/35328)/ 92.833% (32796/35328)
01/15/2023 08:49:49 - INFO - __main__ -   test: [batch: 276/391 ] | Loss: 0.967 | Acc: 75.654% (26824/35456)/ 92.819% (32910/35456)
01/15/2023 08:49:51 - INFO - __main__ -   test: [batch: 277/391 ] | Loss: 0.967 | Acc: 75.627% (26911/35584)/ 92.800% (33022/35584)
01/15/2023 08:49:53 - INFO - __main__ -   test: [batch: 278/391 ] | Loss: 0.967 | Acc: 75.641% (27013/35712)/ 92.798% (33140/35712)
01/15/2023 08:49:55 - INFO - __main__ -   test: [batch: 279/391 ] | Loss: 0.967 | Acc: 75.667% (27119/35840)/ 92.796% (33258/35840)
01/15/2023 08:49:57 - INFO - __main__ -   test: [batch: 280/391 ] | Loss: 0.968 | Acc: 75.648% (27209/35968)/ 92.785% (33373/35968)
01/15/2023 08:50:00 - INFO - __main__ -   test: [batch: 281/391 ] | Loss: 0.969 | Acc: 75.648% (27306/36096)/ 92.772% (33487/36096)
01/15/2023 08:50:02 - INFO - __main__ -   test: [batch: 282/391 ] | Loss: 0.968 | Acc: 75.682% (27415/36224)/ 92.770% (33605/36224)
01/15/2023 08:50:04 - INFO - __main__ -   test: [batch: 283/391 ] | Loss: 0.968 | Acc: 75.674% (27509/36352)/ 92.760% (33720/36352)
01/15/2023 08:50:06 - INFO - __main__ -   test: [batch: 284/391 ] | Loss: 0.972 | Acc: 75.622% (27587/36480)/ 92.708% (33820/36480)
01/15/2023 08:50:08 - INFO - __main__ -   test: [batch: 285/391 ] | Loss: 0.975 | Acc: 75.595% (27674/36608)/ 92.660% (33921/36608)
01/15/2023 08:50:10 - INFO - __main__ -   test: [batch: 286/391 ] | Loss: 0.976 | Acc: 75.558% (27757/36736)/ 92.634% (34030/36736)
01/15/2023 08:50:12 - INFO - __main__ -   test: [batch: 287/391 ] | Loss: 0.977 | Acc: 75.548% (27850/36864)/ 92.630% (34147/36864)
01/15/2023 08:50:14 - INFO - __main__ -   test: [batch: 288/391 ] | Loss: 0.976 | Acc: 75.570% (27955/36992)/ 92.634% (34267/36992)
01/15/2023 08:50:16 - INFO - __main__ -   test: [batch: 289/391 ] | Loss: 0.978 | Acc: 75.541% (28041/37120)/ 92.597% (34372/37120)
01/15/2023 08:50:18 - INFO - __main__ -   test: [batch: 290/391 ] | Loss: 0.979 | Acc: 75.459% (28107/37248)/ 92.588% (34487/37248)
01/15/2023 08:50:20 - INFO - __main__ -   test: [batch: 291/391 ] | Loss: 0.979 | Acc: 75.474% (28209/37376)/ 92.589% (34606/37376)
01/15/2023 08:50:23 - INFO - __main__ -   test: [batch: 292/391 ] | Loss: 0.981 | Acc: 75.416% (28284/37504)/ 92.571% (34718/37504)
01/15/2023 08:50:25 - INFO - __main__ -   test: [batch: 293/391 ] | Loss: 0.982 | Acc: 75.396% (28373/37632)/ 92.567% (34835/37632)
01/15/2023 08:50:27 - INFO - __main__ -   test: [batch: 294/391 ] | Loss: 0.983 | Acc: 75.371% (28460/37760)/ 92.553% (34948/37760)
01/15/2023 08:50:29 - INFO - __main__ -   test: [batch: 295/391 ] | Loss: 0.982 | Acc: 75.393% (28565/37888)/ 92.549% (35065/37888)
01/15/2023 08:50:31 - INFO - __main__ -   test: [batch: 296/391 ] | Loss: 0.983 | Acc: 75.381% (28657/38016)/ 92.543% (35181/38016)
01/15/2023 08:50:33 - INFO - __main__ -   test: [batch: 297/391 ] | Loss: 0.985 | Acc: 75.359% (28745/38144)/ 92.520% (35291/38144)
01/15/2023 08:50:35 - INFO - __main__ -   test: [batch: 298/391 ] | Loss: 0.987 | Acc: 75.311% (28823/38272)/ 92.496% (35400/38272)
01/15/2023 08:50:37 - INFO - __main__ -   test: [batch: 299/391 ] | Loss: 0.988 | Acc: 75.297% (28914/38400)/ 92.469% (35508/38400)
01/15/2023 08:50:40 - INFO - __main__ -   test: [batch: 300/391 ] | Loss: 0.989 | Acc: 75.314% (29017/38528)/ 92.468% (35626/38528)
01/15/2023 08:50:42 - INFO - __main__ -   test: [batch: 301/391 ] | Loss: 0.989 | Acc: 75.297% (29107/38656)/ 92.459% (35741/38656)
01/15/2023 08:50:44 - INFO - __main__ -   test: [batch: 302/391 ] | Loss: 0.991 | Acc: 75.266% (29191/38784)/ 92.443% (35853/38784)
01/15/2023 08:50:46 - INFO - __main__ -   test: [batch: 303/391 ] | Loss: 0.992 | Acc: 75.242% (29278/38912)/ 92.432% (35967/38912)
01/15/2023 08:50:48 - INFO - __main__ -   test: [batch: 304/391 ] | Loss: 0.991 | Acc: 75.256% (29380/39040)/ 92.436% (36087/39040)
01/15/2023 08:50:50 - INFO - __main__ -   test: [batch: 305/391 ] | Loss: 0.992 | Acc: 75.220% (29462/39168)/ 92.433% (36204/39168)
01/15/2023 08:50:53 - INFO - __main__ -   test: [batch: 306/391 ] | Loss: 0.993 | Acc: 75.211% (29555/39296)/ 92.409% (36313/39296)
01/15/2023 08:50:55 - INFO - __main__ -   test: [batch: 307/391 ] | Loss: 0.995 | Acc: 75.185% (29641/39424)/ 92.398% (36427/39424)
01/15/2023 08:50:57 - INFO - __main__ -   test: [batch: 308/391 ] | Loss: 0.995 | Acc: 75.172% (29732/39552)/ 92.390% (36542/39552)
01/15/2023 08:50:59 - INFO - __main__ -   test: [batch: 309/391 ] | Loss: 0.996 | Acc: 75.164% (29825/39680)/ 92.369% (36652/39680)
01/15/2023 08:51:01 - INFO - __main__ -   test: [batch: 310/391 ] | Loss: 0.997 | Acc: 75.153% (29917/39808)/ 92.353% (36764/39808)
01/15/2023 08:51:03 - INFO - __main__ -   test: [batch: 311/391 ] | Loss: 0.998 | Acc: 75.138% (30007/39936)/ 92.340% (36877/39936)
01/15/2023 08:51:06 - INFO - __main__ -   test: [batch: 312/391 ] | Loss: 0.999 | Acc: 75.125% (30098/40064)/ 92.325% (36989/40064)
01/15/2023 08:51:08 - INFO - __main__ -   test: [batch: 313/391 ] | Loss: 0.997 | Acc: 75.172% (30213/40192)/ 92.349% (37117/40192)
01/15/2023 08:51:10 - INFO - __main__ -   test: [batch: 314/391 ] | Loss: 0.998 | Acc: 75.156% (30303/40320)/ 92.344% (37233/40320)
01/15/2023 08:51:12 - INFO - __main__ -   test: [batch: 315/391 ] | Loss: 0.999 | Acc: 75.143% (30394/40448)/ 92.326% (37344/40448)
01/15/2023 08:51:14 - INFO - __main__ -   test: [batch: 316/391 ] | Loss: 1.001 | Acc: 75.052% (30453/40576)/ 92.301% (37452/40576)
01/15/2023 08:51:16 - INFO - __main__ -   test: [batch: 317/391 ] | Loss: 1.004 | Acc: 75.010% (30532/40704)/ 92.269% (37557/40704)
01/15/2023 08:51:18 - INFO - __main__ -   test: [batch: 318/391 ] | Loss: 1.002 | Acc: 75.039% (30640/40832)/ 92.288% (37683/40832)
01/15/2023 08:51:20 - INFO - __main__ -   test: [batch: 319/391 ] | Loss: 1.005 | Acc: 74.988% (30715/40960)/ 92.256% (37788/40960)
01/15/2023 08:51:22 - INFO - __main__ -   test: [batch: 320/391 ] | Loss: 1.004 | Acc: 75.029% (30828/41088)/ 92.265% (37910/41088)
01/15/2023 08:51:24 - INFO - __main__ -   test: [batch: 321/391 ] | Loss: 1.003 | Acc: 75.049% (30932/41216)/ 92.260% (38026/41216)
01/15/2023 08:51:26 - INFO - __main__ -   test: [batch: 322/391 ] | Loss: 1.005 | Acc: 75.017% (31015/41344)/ 92.248% (38139/41344)
01/15/2023 08:51:28 - INFO - __main__ -   test: [batch: 323/391 ] | Loss: 1.007 | Acc: 74.978% (31095/41472)/ 92.219% (38245/41472)
01/15/2023 08:51:31 - INFO - __main__ -   test: [batch: 324/391 ] | Loss: 1.007 | Acc: 74.978% (31191/41600)/ 92.212% (38360/41600)
01/15/2023 08:51:33 - INFO - __main__ -   test: [batch: 325/391 ] | Loss: 1.007 | Acc: 74.983% (31289/41728)/ 92.219% (38481/41728)
01/15/2023 08:51:35 - INFO - __main__ -   test: [batch: 326/391 ] | Loss: 1.011 | Acc: 74.897% (31349/41856)/ 92.173% (38580/41856)
01/15/2023 08:51:37 - INFO - __main__ -   test: [batch: 327/391 ] | Loss: 1.014 | Acc: 74.814% (31410/41984)/ 92.133% (38681/41984)
01/15/2023 08:51:39 - INFO - __main__ -   test: [batch: 328/391 ] | Loss: 1.016 | Acc: 74.763% (31484/42112)/ 92.100% (38785/42112)
01/15/2023 08:51:41 - INFO - __main__ -   test: [batch: 329/391 ] | Loss: 1.016 | Acc: 74.766% (31581/42240)/ 92.109% (38907/42240)
01/15/2023 08:51:43 - INFO - __main__ -   test: [batch: 330/391 ] | Loss: 1.018 | Acc: 74.731% (31662/42368)/ 92.093% (39018/42368)
01/15/2023 08:51:45 - INFO - __main__ -   test: [batch: 331/391 ] | Loss: 1.018 | Acc: 74.696% (31743/42496)/ 92.107% (39142/42496)
01/15/2023 08:51:48 - INFO - __main__ -   test: [batch: 332/391 ] | Loss: 1.019 | Acc: 74.669% (31827/42624)/ 92.103% (39258/42624)
01/15/2023 08:51:50 - INFO - __main__ -   test: [batch: 333/391 ] | Loss: 1.017 | Acc: 74.701% (31936/42752)/ 92.110% (39379/42752)
01/15/2023 08:51:52 - INFO - __main__ -   test: [batch: 334/391 ] | Loss: 1.019 | Acc: 74.662% (32015/42880)/ 92.101% (39493/42880)
01/15/2023 08:51:54 - INFO - __main__ -   test: [batch: 335/391 ] | Loss: 1.020 | Acc: 74.637% (32100/43008)/ 92.081% (39602/43008)
01/15/2023 08:51:56 - INFO - __main__ -   test: [batch: 336/391 ] | Loss: 1.022 | Acc: 74.597% (32178/43136)/ 92.065% (39713/43136)
01/15/2023 08:51:58 - INFO - __main__ -   test: [batch: 337/391 ] | Loss: 1.022 | Acc: 74.593% (32272/43264)/ 92.056% (39827/43264)
01/15/2023 08:52:00 - INFO - __main__ -   test: [batch: 338/391 ] | Loss: 1.021 | Acc: 74.594% (32368/43392)/ 92.070% (39951/43392)
01/15/2023 08:52:03 - INFO - __main__ -   test: [batch: 339/391 ] | Loss: 1.024 | Acc: 74.554% (32446/43520)/ 92.031% (40052/43520)
01/15/2023 08:52:05 - INFO - __main__ -   test: [batch: 340/391 ] | Loss: 1.024 | Acc: 74.553% (32541/43648)/ 92.043% (40175/43648)
01/15/2023 08:52:07 - INFO - __main__ -   test: [batch: 341/391 ] | Loss: 1.022 | Acc: 74.593% (32654/43776)/ 92.062% (40301/43776)
01/15/2023 08:52:09 - INFO - __main__ -   test: [batch: 342/391 ] | Loss: 1.023 | Acc: 74.547% (32729/43904)/ 92.051% (40414/43904)
01/15/2023 08:52:11 - INFO - __main__ -   test: [batch: 343/391 ] | Loss: 1.023 | Acc: 74.546% (32824/44032)/ 92.049% (40531/44032)
01/15/2023 08:52:13 - INFO - __main__ -   test: [batch: 344/391 ] | Loss: 1.023 | Acc: 74.547% (32920/44160)/ 92.040% (40645/44160)
01/15/2023 08:52:15 - INFO - __main__ -   test: [batch: 345/391 ] | Loss: 1.026 | Acc: 74.478% (32985/44288)/ 92.011% (40750/44288)
01/15/2023 08:52:18 - INFO - __main__ -   test: [batch: 346/391 ] | Loss: 1.028 | Acc: 74.462% (33073/44416)/ 92.005% (40865/44416)
01/15/2023 08:52:20 - INFO - __main__ -   test: [batch: 347/391 ] | Loss: 1.028 | Acc: 74.472% (33173/44544)/ 92.006% (40983/44544)
01/15/2023 08:52:22 - INFO - __main__ -   test: [batch: 348/391 ] | Loss: 1.029 | Acc: 74.445% (33256/44672)/ 91.986% (41092/44672)
01/15/2023 08:52:24 - INFO - __main__ -   test: [batch: 349/391 ] | Loss: 1.028 | Acc: 74.444% (33351/44800)/ 91.996% (41214/44800)
01/15/2023 08:52:26 - INFO - __main__ -   test: [batch: 350/391 ] | Loss: 1.028 | Acc: 74.452% (33450/44928)/ 91.994% (41331/44928)
01/15/2023 08:52:28 - INFO - __main__ -   test: [batch: 351/391 ] | Loss: 1.031 | Acc: 74.394% (33519/45056)/ 91.977% (41441/45056)
01/15/2023 08:52:31 - INFO - __main__ -   test: [batch: 352/391 ] | Loss: 1.031 | Acc: 74.394% (33614/45184)/ 91.975% (41558/45184)
01/15/2023 08:52:33 - INFO - __main__ -   test: [batch: 353/391 ] | Loss: 1.034 | Acc: 74.345% (33687/45312)/ 91.929% (41655/45312)
01/15/2023 08:52:35 - INFO - __main__ -   test: [batch: 354/391 ] | Loss: 1.036 | Acc: 74.287% (33756/45440)/ 91.910% (41764/45440)
01/15/2023 08:52:37 - INFO - __main__ -   test: [batch: 355/391 ] | Loss: 1.039 | Acc: 74.223% (33822/45568)/ 91.898% (41876/45568)
01/15/2023 08:52:39 - INFO - __main__ -   test: [batch: 356/391 ] | Loss: 1.039 | Acc: 74.214% (33913/45696)/ 91.899% (41994/45696)
01/15/2023 08:52:41 - INFO - __main__ -   test: [batch: 357/391 ] | Loss: 1.038 | Acc: 74.249% (34024/45824)/ 91.913% (42118/45824)
01/15/2023 08:52:43 - INFO - __main__ -   test: [batch: 358/391 ] | Loss: 1.037 | Acc: 74.278% (34132/45952)/ 91.918% (42238/45952)
01/15/2023 08:52:45 - INFO - __main__ -   test: [batch: 359/391 ] | Loss: 1.037 | Acc: 74.282% (34229/46080)/ 91.914% (42354/46080)
01/15/2023 08:52:48 - INFO - __main__ -   test: [batch: 360/391 ] | Loss: 1.039 | Acc: 74.245% (34307/46208)/ 91.913% (42471/46208)
01/15/2023 08:52:50 - INFO - __main__ -   test: [batch: 361/391 ] | Loss: 1.039 | Acc: 74.247% (34403/46336)/ 91.913% (42589/46336)
01/15/2023 08:52:52 - INFO - __main__ -   test: [batch: 362/391 ] | Loss: 1.039 | Acc: 74.242% (34496/46464)/ 91.925% (42712/46464)
01/15/2023 08:52:54 - INFO - __main__ -   test: [batch: 363/391 ] | Loss: 1.039 | Acc: 74.225% (34583/46592)/ 91.913% (42824/46592)
01/15/2023 08:52:56 - INFO - __main__ -   test: [batch: 364/391 ] | Loss: 1.038 | Acc: 74.251% (34690/46720)/ 91.922% (42946/46720)
01/15/2023 08:52:58 - INFO - __main__ -   test: [batch: 365/391 ] | Loss: 1.038 | Acc: 74.251% (34785/46848)/ 91.921% (43063/46848)
01/15/2023 08:53:01 - INFO - __main__ -   test: [batch: 366/391 ] | Loss: 1.037 | Acc: 74.287% (34897/46976)/ 91.938% (43189/46976)
01/15/2023 08:53:03 - INFO - __main__ -   test: [batch: 367/391 ] | Loss: 1.036 | Acc: 74.302% (34999/47104)/ 91.948% (43311/47104)
01/15/2023 08:53:05 - INFO - __main__ -   test: [batch: 368/391 ] | Loss: 1.035 | Acc: 74.295% (35091/47232)/ 91.959% (43434/47232)
01/15/2023 08:53:07 - INFO - __main__ -   test: [batch: 369/391 ] | Loss: 1.034 | Acc: 74.318% (35197/47360)/ 91.970% (43557/47360)
01/15/2023 08:53:09 - INFO - __main__ -   test: [batch: 370/391 ] | Loss: 1.035 | Acc: 74.292% (35280/47488)/ 91.973% (43676/47488)
01/15/2023 08:53:11 - INFO - __main__ -   test: [batch: 371/391 ] | Loss: 1.035 | Acc: 74.305% (35381/47616)/ 91.971% (43793/47616)
01/15/2023 08:53:14 - INFO - __main__ -   test: [batch: 372/391 ] | Loss: 1.033 | Acc: 74.353% (35499/47744)/ 91.989% (43919/47744)
01/15/2023 08:53:16 - INFO - __main__ -   test: [batch: 373/391 ] | Loss: 1.032 | Acc: 74.386% (35610/47872)/ 92.002% (44043/47872)
01/15/2023 08:53:18 - INFO - __main__ -   test: [batch: 374/391 ] | Loss: 1.031 | Acc: 74.408% (35716/48000)/ 92.000% (44160/48000)
01/15/2023 08:53:20 - INFO - __main__ -   test: [batch: 375/391 ] | Loss: 1.034 | Acc: 74.352% (35784/48128)/ 91.961% (44259/48128)
01/15/2023 08:53:22 - INFO - __main__ -   test: [batch: 376/391 ] | Loss: 1.035 | Acc: 74.339% (35873/48256)/ 91.945% (44369/48256)
01/15/2023 08:53:24 - INFO - __main__ -   test: [batch: 377/391 ] | Loss: 1.035 | Acc: 74.337% (35967/48384)/ 91.937% (44483/48384)
01/15/2023 08:53:26 - INFO - __main__ -   test: [batch: 378/391 ] | Loss: 1.038 | Acc: 74.270% (36030/48512)/ 91.899% (44582/48512)
01/15/2023 08:53:28 - INFO - __main__ -   test: [batch: 379/391 ] | Loss: 1.038 | Acc: 74.260% (36120/48640)/ 91.910% (44705/48640)
01/15/2023 08:53:30 - INFO - __main__ -   test: [batch: 380/391 ] | Loss: 1.038 | Acc: 74.268% (36219/48768)/ 91.925% (44830/48768)
01/15/2023 08:53:32 - INFO - __main__ -   test: [batch: 381/391 ] | Loss: 1.039 | Acc: 74.223% (36292/48896)/ 91.928% (44949/48896)
01/15/2023 08:53:34 - INFO - __main__ -   test: [batch: 382/391 ] | Loss: 1.041 | Acc: 74.192% (36372/49024)/ 91.916% (45061/49024)
01/15/2023 08:53:37 - INFO - __main__ -   test: [batch: 383/391 ] | Loss: 1.041 | Acc: 74.198% (36470/49152)/ 91.911% (45176/49152)
01/15/2023 08:53:39 - INFO - __main__ -   test: [batch: 384/391 ] | Loss: 1.039 | Acc: 74.235% (36583/49280)/ 91.926% (45301/49280)
01/15/2023 08:53:41 - INFO - __main__ -   test: [batch: 385/391 ] | Loss: 1.038 | Acc: 74.249% (36685/49408)/ 91.939% (45425/49408)
01/15/2023 08:53:43 - INFO - __main__ -   test: [batch: 386/391 ] | Loss: 1.036 | Acc: 74.287% (36799/49536)/ 91.957% (45552/49536)
01/15/2023 08:53:45 - INFO - __main__ -   test: [batch: 387/391 ] | Loss: 1.034 | Acc: 74.332% (36916/49664)/ 91.970% (45676/49664)
01/15/2023 08:53:47 - INFO - __main__ -   test: [batch: 388/391 ] | Loss: 1.032 | Acc: 74.383% (37037/49792)/ 91.985% (45801/49792)
01/15/2023 08:53:49 - INFO - __main__ -   test: [batch: 389/391 ] | Loss: 1.032 | Acc: 74.385% (37133/49920)/ 91.991% (45922/49920)
01/15/2023 08:53:52 - INFO - __main__ -   test: [batch: 390/391 ] | Loss: 1.034 | Acc: 74.346% (37173/50000)/ 91.982% (45991/50000)
01/15/2023 08:53:52 - INFO - __main__ -   Final accuracy: 74.346
01/15/2023 08:53:52 - INFO - __main__ -   {'milestones': Counter({2: 1, 3: 1, 4: 1}), 'gamma': 0.1, 'base_lrs': [0.00025], 'last_epoch': 1, '_step_count': 2, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.00025]}
01/15/2023 08:53:52 - INFO - __main__ -   
Epoch: 1
01/15/2023 08:53:54 - INFO - __main__ -   test: [epoch: 1 | batch: 0/10010 ] | Loss: 0.702 | Acc: 84.375% (108/128)
01/15/2023 08:57:29 - INFO - __main__ -   test: [epoch: 1 | batch: 100/10010 ] | Loss: 0.933 | Acc: 77.205% (9981/12928)
01/15/2023 09:01:01 - INFO - __main__ -   test: [epoch: 1 | batch: 200/10010 ] | Loss: 0.930 | Acc: 77.017% (19815/25728)
01/15/2023 09:04:34 - INFO - __main__ -   test: [epoch: 1 | batch: 300/10010 ] | Loss: 0.945 | Acc: 76.661% (29536/38528)
01/15/2023 09:08:05 - INFO - __main__ -   test: [epoch: 1 | batch: 400/10010 ] | Loss: 0.956 | Acc: 76.379% (39204/51328)
01/15/2023 09:11:37 - INFO - __main__ -   test: [epoch: 1 | batch: 500/10010 ] | Loss: 0.957 | Acc: 76.315% (48939/64128)
01/15/2023 09:15:07 - INFO - __main__ -   test: [epoch: 1 | batch: 600/10010 ] | Loss: 0.960 | Acc: 76.303% (58698/76928)
01/15/2023 09:18:39 - INFO - __main__ -   test: [epoch: 1 | batch: 700/10010 ] | Loss: 0.960 | Acc: 76.273% (68438/89728)
01/15/2023 09:22:11 - INFO - __main__ -   test: [epoch: 1 | batch: 800/10010 ] | Loss: 0.959 | Acc: 76.292% (78221/102528)
01/15/2023 09:25:41 - INFO - __main__ -   test: [epoch: 1 | batch: 900/10010 ] | Loss: 0.959 | Acc: 76.290% (87984/115328)
01/15/2023 09:29:12 - INFO - __main__ -   test: [epoch: 1 | batch: 1000/10010 ] | Loss: 0.959 | Acc: 76.271% (97724/128128)
01/15/2023 09:32:42 - INFO - __main__ -   test: [epoch: 1 | batch: 1100/10010 ] | Loss: 0.961 | Acc: 76.231% (107431/140928)
01/15/2023 09:36:12 - INFO - __main__ -   test: [epoch: 1 | batch: 1200/10010 ] | Loss: 0.962 | Acc: 76.246% (117211/153728)
01/15/2023 09:39:43 - INFO - __main__ -   test: [epoch: 1 | batch: 1300/10010 ] | Loss: 0.958 | Acc: 76.332% (127114/166528)
01/15/2023 09:43:14 - INFO - __main__ -   test: [epoch: 1 | batch: 1400/10010 ] | Loss: 0.958 | Acc: 76.346% (136909/179328)
01/15/2023 09:46:46 - INFO - __main__ -   test: [epoch: 1 | batch: 1500/10010 ] | Loss: 0.958 | Acc: 76.330% (146652/192128)
01/15/2023 09:50:17 - INFO - __main__ -   test: [epoch: 1 | batch: 1600/10010 ] | Loss: 0.961 | Acc: 76.298% (156356/204928)
01/15/2023 09:53:46 - INFO - __main__ -   test: [epoch: 1 | batch: 1700/10010 ] | Loss: 0.960 | Acc: 76.331% (166193/217728)
01/15/2023 09:57:17 - INFO - __main__ -   test: [epoch: 1 | batch: 1800/10010 ] | Loss: 0.960 | Acc: 76.330% (175961/230528)
01/15/2023 10:00:48 - INFO - __main__ -   test: [epoch: 1 | batch: 1900/10010 ] | Loss: 0.959 | Acc: 76.346% (185770/243328)
01/15/2023 10:04:20 - INFO - __main__ -   test: [epoch: 1 | batch: 2000/10010 ] | Loss: 0.959 | Acc: 76.356% (195569/256128)
01/15/2023 10:07:51 - INFO - __main__ -   test: [epoch: 1 | batch: 2100/10010 ] | Loss: 0.958 | Acc: 76.354% (205336/268928)
01/15/2023 10:11:22 - INFO - __main__ -   test: [epoch: 1 | batch: 2200/10010 ] | Loss: 0.960 | Acc: 76.333% (215051/281728)
01/15/2023 10:14:53 - INFO - __main__ -   test: [epoch: 1 | batch: 2300/10010 ] | Loss: 0.961 | Acc: 76.338% (224836/294528)
01/15/2023 10:18:24 - INFO - __main__ -   test: [epoch: 1 | batch: 2400/10010 ] | Loss: 0.960 | Acc: 76.371% (234709/307328)
01/15/2023 10:21:56 - INFO - __main__ -   test: [epoch: 1 | batch: 2500/10010 ] | Loss: 0.961 | Acc: 76.343% (244394/320128)
01/15/2023 10:25:29 - INFO - __main__ -   test: [epoch: 1 | batch: 2600/10010 ] | Loss: 0.962 | Acc: 76.326% (254111/332928)
01/15/2023 10:29:00 - INFO - __main__ -   test: [epoch: 1 | batch: 2700/10010 ] | Loss: 0.962 | Acc: 76.326% (263879/345728)
01/15/2023 10:32:31 - INFO - __main__ -   test: [epoch: 1 | batch: 2800/10010 ] | Loss: 0.962 | Acc: 76.315% (273612/358528)
01/15/2023 10:36:03 - INFO - __main__ -   test: [epoch: 1 | batch: 2900/10010 ] | Loss: 0.962 | Acc: 76.320% (283397/371328)
01/15/2023 10:39:36 - INFO - __main__ -   test: [epoch: 1 | batch: 3000/10010 ] | Loss: 0.961 | Acc: 76.339% (293239/384128)
01/15/2023 10:43:08 - INFO - __main__ -   test: [epoch: 1 | batch: 3100/10010 ] | Loss: 0.961 | Acc: 76.340% (303014/396928)
01/15/2023 10:46:40 - INFO - __main__ -   test: [epoch: 1 | batch: 3200/10010 ] | Loss: 0.961 | Acc: 76.320% (312704/409728)
01/15/2023 10:50:12 - INFO - __main__ -   test: [epoch: 1 | batch: 3300/10010 ] | Loss: 0.962 | Acc: 76.311% (322436/422528)
01/15/2023 10:53:44 - INFO - __main__ -   test: [epoch: 1 | batch: 3400/10010 ] | Loss: 0.962 | Acc: 76.319% (332236/435328)
01/15/2023 10:57:15 - INFO - __main__ -   test: [epoch: 1 | batch: 3500/10010 ] | Loss: 0.962 | Acc: 76.312% (341977/448128)
01/15/2023 11:00:47 - INFO - __main__ -   test: [epoch: 1 | batch: 3600/10010 ] | Loss: 0.962 | Acc: 76.308% (351726/460928)
01/15/2023 11:04:18 - INFO - __main__ -   test: [epoch: 1 | batch: 3700/10010 ] | Loss: 0.963 | Acc: 76.290% (361409/473728)
01/15/2023 11:07:51 - INFO - __main__ -   test: [epoch: 1 | batch: 3800/10010 ] | Loss: 0.963 | Acc: 76.284% (371143/486528)
01/15/2023 11:11:22 - INFO - __main__ -   test: [epoch: 1 | batch: 3900/10010 ] | Loss: 0.962 | Acc: 76.291% (380941/499328)
01/15/2023 11:14:54 - INFO - __main__ -   test: [epoch: 1 | batch: 4000/10010 ] | Loss: 0.963 | Acc: 76.285% (390677/512128)
01/15/2023 11:18:24 - INFO - __main__ -   test: [epoch: 1 | batch: 4100/10010 ] | Loss: 0.962 | Acc: 76.297% (400505/524928)
01/15/2023 11:21:56 - INFO - __main__ -   test: [epoch: 1 | batch: 4200/10010 ] | Loss: 0.962 | Acc: 76.295% (410260/537728)
01/15/2023 11:25:27 - INFO - __main__ -   test: [epoch: 1 | batch: 4300/10010 ] | Loss: 0.962 | Acc: 76.303% (420068/550528)
01/15/2023 11:28:58 - INFO - __main__ -   test: [epoch: 1 | batch: 4400/10010 ] | Loss: 0.962 | Acc: 76.305% (429849/563328)
01/15/2023 11:32:29 - INFO - __main__ -   test: [epoch: 1 | batch: 4500/10010 ] | Loss: 0.962 | Acc: 76.294% (439551/576128)
01/15/2023 11:36:00 - INFO - __main__ -   test: [epoch: 1 | batch: 4600/10010 ] | Loss: 0.962 | Acc: 76.288% (449283/588928)
01/15/2023 11:39:33 - INFO - __main__ -   test: [epoch: 1 | batch: 4700/10010 ] | Loss: 0.962 | Acc: 76.303% (459135/601728)
01/15/2023 11:43:05 - INFO - __main__ -   test: [epoch: 1 | batch: 4800/10010 ] | Loss: 0.961 | Acc: 76.315% (468980/614528)
01/15/2023 11:46:38 - INFO - __main__ -   test: [epoch: 1 | batch: 4900/10010 ] | Loss: 0.961 | Acc: 76.314% (478736/627328)
01/15/2023 11:50:09 - INFO - __main__ -   test: [epoch: 1 | batch: 5000/10010 ] | Loss: 0.961 | Acc: 76.328% (488594/640128)
01/15/2023 11:53:40 - INFO - __main__ -   test: [epoch: 1 | batch: 5100/10010 ] | Loss: 0.961 | Acc: 76.326% (498356/652928)
01/15/2023 11:57:13 - INFO - __main__ -   test: [epoch: 1 | batch: 5200/10010 ] | Loss: 0.961 | Acc: 76.316% (508059/665728)
01/15/2023 12:00:45 - INFO - __main__ -   test: [epoch: 1 | batch: 5300/10010 ] | Loss: 0.961 | Acc: 76.319% (517849/678528)
01/15/2023 12:04:18 - INFO - __main__ -   test: [epoch: 1 | batch: 5400/10010 ] | Loss: 0.961 | Acc: 76.320% (527624/691328)
01/15/2023 12:07:50 - INFO - __main__ -   test: [epoch: 1 | batch: 5500/10010 ] | Loss: 0.961 | Acc: 76.315% (537354/704128)
01/15/2023 12:11:23 - INFO - __main__ -   test: [epoch: 1 | batch: 5600/10010 ] | Loss: 0.961 | Acc: 76.326% (547201/716928)
01/15/2023 12:14:54 - INFO - __main__ -   test: [epoch: 1 | batch: 5700/10010 ] | Loss: 0.961 | Acc: 76.328% (556989/729728)
01/15/2023 12:18:26 - INFO - __main__ -   test: [epoch: 1 | batch: 5800/10010 ] | Loss: 0.961 | Acc: 76.336% (566816/742528)
01/15/2023 12:21:58 - INFO - __main__ -   test: [epoch: 1 | batch: 5900/10010 ] | Loss: 0.961 | Acc: 76.333% (576562/755328)
01/15/2023 12:25:29 - INFO - __main__ -   test: [epoch: 1 | batch: 6000/10010 ] | Loss: 0.960 | Acc: 76.336% (586362/768128)
01/15/2023 12:29:02 - INFO - __main__ -   test: [epoch: 1 | batch: 6100/10010 ] | Loss: 0.961 | Acc: 76.327% (596061/780928)
01/15/2023 12:32:34 - INFO - __main__ -   test: [epoch: 1 | batch: 6200/10010 ] | Loss: 0.961 | Acc: 76.334% (605886/793728)
01/15/2023 12:36:08 - INFO - __main__ -   test: [epoch: 1 | batch: 6300/10010 ] | Loss: 0.961 | Acc: 76.329% (615612/806528)
01/15/2023 12:39:40 - INFO - __main__ -   test: [epoch: 1 | batch: 6400/10010 ] | Loss: 0.961 | Acc: 76.315% (625272/819328)
01/15/2023 12:43:11 - INFO - __main__ -   test: [epoch: 1 | batch: 6500/10010 ] | Loss: 0.961 | Acc: 76.323% (635106/832128)
01/15/2023 12:46:45 - INFO - __main__ -   test: [epoch: 1 | batch: 6600/10010 ] | Loss: 0.960 | Acc: 76.330% (644932/844928)
01/15/2023 12:50:17 - INFO - __main__ -   test: [epoch: 1 | batch: 6700/10010 ] | Loss: 0.961 | Acc: 76.330% (654701/857728)
01/15/2023 12:53:49 - INFO - __main__ -   test: [epoch: 1 | batch: 6800/10010 ] | Loss: 0.961 | Acc: 76.323% (664414/870528)
01/15/2023 12:57:20 - INFO - __main__ -   test: [epoch: 1 | batch: 6900/10010 ] | Loss: 0.961 | Acc: 76.330% (674243/883328)
01/15/2023 13:00:52 - INFO - __main__ -   test: [epoch: 1 | batch: 7000/10010 ] | Loss: 0.961 | Acc: 76.321% (683938/896128)
01/15/2023 13:04:23 - INFO - __main__ -   test: [epoch: 1 | batch: 7100/10010 ] | Loss: 0.960 | Acc: 76.328% (693768/908928)
01/15/2023 13:07:56 - INFO - __main__ -   test: [epoch: 1 | batch: 7200/10010 ] | Loss: 0.960 | Acc: 76.330% (703555/921728)
01/15/2023 13:11:30 - INFO - __main__ -   test: [epoch: 1 | batch: 7300/10010 ] | Loss: 0.961 | Acc: 76.325% (713280/934528)
01/15/2023 13:15:02 - INFO - __main__ -   test: [epoch: 1 | batch: 7400/10010 ] | Loss: 0.961 | Acc: 76.314% (722946/947328)
01/15/2023 13:18:34 - INFO - __main__ -   test: [epoch: 1 | batch: 7500/10010 ] | Loss: 0.961 | Acc: 76.316% (732731/960128)
01/15/2023 13:22:05 - INFO - __main__ -   test: [epoch: 1 | batch: 7600/10010 ] | Loss: 0.961 | Acc: 76.311% (742454/972928)
01/15/2023 13:25:37 - INFO - __main__ -   test: [epoch: 1 | batch: 7700/10010 ] | Loss: 0.961 | Acc: 76.309% (752204/985728)
01/15/2023 13:29:09 - INFO - __main__ -   test: [epoch: 1 | batch: 7800/10010 ] | Loss: 0.961 | Acc: 76.303% (761905/998528)
01/15/2023 13:32:39 - INFO - __main__ -   test: [epoch: 1 | batch: 7900/10010 ] | Loss: 0.961 | Acc: 76.307% (771717/1011328)
01/15/2023 13:36:11 - INFO - __main__ -   test: [epoch: 1 | batch: 8000/10010 ] | Loss: 0.961 | Acc: 76.311% (781524/1024128)
01/15/2023 13:39:43 - INFO - __main__ -   test: [epoch: 1 | batch: 8100/10010 ] | Loss: 0.961 | Acc: 76.320% (791388/1036928)
01/15/2023 13:43:16 - INFO - __main__ -   test: [epoch: 1 | batch: 8200/10010 ] | Loss: 0.961 | Acc: 76.315% (801096/1049728)
01/15/2023 13:46:49 - INFO - __main__ -   test: [epoch: 1 | batch: 8300/10010 ] | Loss: 0.961 | Acc: 76.312% (810839/1062528)
01/15/2023 13:50:20 - INFO - __main__ -   test: [epoch: 1 | batch: 8400/10010 ] | Loss: 0.961 | Acc: 76.312% (820604/1075328)
01/15/2023 13:53:51 - INFO - __main__ -   test: [epoch: 1 | batch: 8500/10010 ] | Loss: 0.961 | Acc: 76.313% (830388/1088128)
01/15/2023 13:57:22 - INFO - __main__ -   test: [epoch: 1 | batch: 8600/10010 ] | Loss: 0.961 | Acc: 76.309% (840107/1100928)
01/15/2023 14:00:54 - INFO - __main__ -   test: [epoch: 1 | batch: 8700/10010 ] | Loss: 0.961 | Acc: 76.320% (849995/1113728)
01/15/2023 14:04:27 - INFO - __main__ -   test: [epoch: 1 | batch: 8800/10010 ] | Loss: 0.961 | Acc: 76.323% (859797/1126528)
01/15/2023 14:07:59 - INFO - __main__ -   test: [epoch: 1 | batch: 8900/10010 ] | Loss: 0.961 | Acc: 76.326% (869606/1139328)
01/15/2023 14:11:29 - INFO - __main__ -   test: [epoch: 1 | batch: 9000/10010 ] | Loss: 0.961 | Acc: 76.328% (879397/1152128)
01/15/2023 14:15:01 - INFO - __main__ -   test: [epoch: 1 | batch: 9100/10010 ] | Loss: 0.960 | Acc: 76.330% (889184/1164928)
01/15/2023 14:18:33 - INFO - __main__ -   test: [epoch: 1 | batch: 9200/10010 ] | Loss: 0.960 | Acc: 76.334% (899002/1177728)
01/15/2023 14:22:03 - INFO - __main__ -   test: [epoch: 1 | batch: 9300/10010 ] | Loss: 0.960 | Acc: 76.336% (908807/1190528)
01/15/2023 14:25:33 - INFO - __main__ -   test: [epoch: 1 | batch: 9400/10010 ] | Loss: 0.961 | Acc: 76.332% (918528/1203328)
01/15/2023 14:29:05 - INFO - __main__ -   test: [epoch: 1 | batch: 9500/10010 ] | Loss: 0.960 | Acc: 76.335% (928331/1216128)
01/15/2023 14:32:35 - INFO - __main__ -   test: [epoch: 1 | batch: 9600/10010 ] | Loss: 0.960 | Acc: 76.333% (938081/1228928)
01/15/2023 14:36:08 - INFO - __main__ -   test: [epoch: 1 | batch: 9700/10010 ] | Loss: 0.961 | Acc: 76.331% (947821/1241728)
01/15/2023 14:39:40 - INFO - __main__ -   test: [epoch: 1 | batch: 9800/10010 ] | Loss: 0.961 | Acc: 76.327% (957542/1254528)
01/15/2023 14:43:12 - INFO - __main__ -   test: [epoch: 1 | batch: 9900/10010 ] | Loss: 0.961 | Acc: 76.327% (967308/1267328)
01/15/2023 14:46:46 - INFO - __main__ -   test: [epoch: 1 | batch: 10000/10010 ] | Loss: 0.961 | Acc: 76.322% (977018/1280128)
01/15/2023 14:47:05 - INFO - __main__ -   Saving Checkpoint
01/15/2023 14:47:08 - INFO - __main__ -   test: [batch: 0/391 ] | Loss: 0.514 | Acc: 88.281% (113/128)/ 95.312% (122/128)
01/15/2023 14:47:10 - INFO - __main__ -   test: [batch: 1/391 ] | Loss: 0.555 | Acc: 84.375% (216/256)/ 96.875% (248/256)
01/15/2023 14:47:12 - INFO - __main__ -   test: [batch: 2/391 ] | Loss: 0.765 | Acc: 78.906% (303/384)/ 94.531% (363/384)
01/15/2023 14:47:14 - INFO - __main__ -   test: [batch: 3/391 ] | Loss: 0.700 | Acc: 82.031% (420/512)/ 95.117% (487/512)
01/15/2023 14:47:16 - INFO - __main__ -   test: [batch: 4/391 ] | Loss: 0.604 | Acc: 84.688% (542/640)/ 95.781% (613/640)
01/15/2023 14:47:18 - INFO - __main__ -   test: [batch: 5/391 ] | Loss: 0.532 | Acc: 86.068% (661/768)/ 96.484% (741/768)
01/15/2023 14:47:21 - INFO - __main__ -   test: [batch: 6/391 ] | Loss: 0.517 | Acc: 86.607% (776/896)/ 96.540% (865/896)
01/15/2023 14:47:23 - INFO - __main__ -   test: [batch: 7/391 ] | Loss: 0.494 | Acc: 87.500% (896/1024)/ 96.680% (990/1024)
01/15/2023 14:47:25 - INFO - __main__ -   test: [batch: 8/391 ] | Loss: 0.499 | Acc: 87.587% (1009/1152)/ 96.701% (1114/1152)
01/15/2023 14:47:27 - INFO - __main__ -   test: [batch: 9/391 ] | Loss: 0.474 | Acc: 88.203% (1129/1280)/ 96.875% (1240/1280)
01/15/2023 14:47:29 - INFO - __main__ -   test: [batch: 10/391 ] | Loss: 0.516 | Acc: 87.003% (1225/1408)/ 96.875% (1364/1408)
01/15/2023 14:47:31 - INFO - __main__ -   test: [batch: 11/391 ] | Loss: 0.521 | Acc: 87.174% (1339/1536)/ 96.680% (1485/1536)
01/15/2023 14:47:33 - INFO - __main__ -   test: [batch: 12/391 ] | Loss: 0.573 | Acc: 85.877% (1429/1664)/ 96.274% (1602/1664)
01/15/2023 14:47:35 - INFO - __main__ -   test: [batch: 13/391 ] | Loss: 0.633 | Acc: 84.263% (1510/1792)/ 95.536% (1712/1792)
01/15/2023 14:47:37 - INFO - __main__ -   test: [batch: 14/391 ] | Loss: 0.649 | Acc: 83.438% (1602/1920)/ 95.677% (1837/1920)
01/15/2023 14:47:40 - INFO - __main__ -   test: [batch: 15/391 ] | Loss: 0.657 | Acc: 82.861% (1697/2048)/ 95.752% (1961/2048)
01/15/2023 14:47:42 - INFO - __main__ -   test: [batch: 16/391 ] | Loss: 0.665 | Acc: 82.767% (1801/2176)/ 95.588% (2080/2176)
01/15/2023 14:47:44 - INFO - __main__ -   test: [batch: 17/391 ] | Loss: 0.690 | Acc: 82.248% (1895/2304)/ 95.139% (2192/2304)
01/15/2023 14:47:46 - INFO - __main__ -   test: [batch: 18/391 ] | Loss: 0.714 | Acc: 81.785% (1989/2432)/ 94.942% (2309/2432)
01/15/2023 14:47:48 - INFO - __main__ -   test: [batch: 19/391 ] | Loss: 0.723 | Acc: 81.562% (2088/2560)/ 94.766% (2426/2560)
01/15/2023 14:47:50 - INFO - __main__ -   test: [batch: 20/391 ] | Loss: 0.718 | Acc: 81.659% (2195/2688)/ 94.754% (2547/2688)
01/15/2023 14:47:52 - INFO - __main__ -   test: [batch: 21/391 ] | Loss: 0.753 | Acc: 80.788% (2275/2816)/ 94.567% (2663/2816)
01/15/2023 14:47:55 - INFO - __main__ -   test: [batch: 22/391 ] | Loss: 0.752 | Acc: 80.673% (2375/2944)/ 94.565% (2784/2944)
01/15/2023 14:47:57 - INFO - __main__ -   test: [batch: 23/391 ] | Loss: 0.794 | Acc: 79.915% (2455/3072)/ 94.238% (2895/3072)
01/15/2023 14:47:59 - INFO - __main__ -   test: [batch: 24/391 ] | Loss: 0.811 | Acc: 79.344% (2539/3200)/ 94.062% (3010/3200)
01/15/2023 14:48:01 - INFO - __main__ -   test: [batch: 25/391 ] | Loss: 0.838 | Acc: 78.636% (2617/3328)/ 93.810% (3122/3328)
01/15/2023 14:48:03 - INFO - __main__ -   test: [batch: 26/391 ] | Loss: 0.857 | Acc: 77.836% (2690/3456)/ 93.837% (3243/3456)
01/15/2023 14:48:05 - INFO - __main__ -   test: [batch: 27/391 ] | Loss: 0.838 | Acc: 78.348% (2808/3584)/ 93.890% (3365/3584)
01/15/2023 14:48:07 - INFO - __main__ -   test: [batch: 28/391 ] | Loss: 0.844 | Acc: 77.829% (2889/3712)/ 94.019% (3490/3712)
01/15/2023 14:48:10 - INFO - __main__ -   test: [batch: 29/391 ] | Loss: 0.834 | Acc: 78.021% (2996/3840)/ 94.167% (3616/3840)
01/15/2023 14:48:12 - INFO - __main__ -   test: [batch: 30/391 ] | Loss: 0.846 | Acc: 77.873% (3090/3968)/ 94.002% (3730/3968)
01/15/2023 14:48:14 - INFO - __main__ -   test: [batch: 31/391 ] | Loss: 0.841 | Acc: 78.149% (3201/4096)/ 94.116% (3855/4096)
01/15/2023 14:48:16 - INFO - __main__ -   test: [batch: 32/391 ] | Loss: 0.826 | Acc: 78.480% (3315/4224)/ 94.200% (3979/4224)
01/15/2023 14:48:18 - INFO - __main__ -   test: [batch: 33/391 ] | Loss: 0.820 | Acc: 78.631% (3422/4352)/ 94.256% (4102/4352)
01/15/2023 14:48:20 - INFO - __main__ -   test: [batch: 34/391 ] | Loss: 0.806 | Acc: 79.040% (3541/4480)/ 94.330% (4226/4480)
01/15/2023 14:48:23 - INFO - __main__ -   test: [batch: 35/391 ] | Loss: 0.792 | Acc: 79.427% (3660/4608)/ 94.379% (4349/4608)
01/15/2023 14:48:25 - INFO - __main__ -   test: [batch: 36/391 ] | Loss: 0.777 | Acc: 79.899% (3784/4736)/ 94.510% (4476/4736)
01/15/2023 14:48:27 - INFO - __main__ -   test: [batch: 37/391 ] | Loss: 0.767 | Acc: 80.181% (3900/4864)/ 94.593% (4601/4864)
01/15/2023 14:48:29 - INFO - __main__ -   test: [batch: 38/391 ] | Loss: 0.758 | Acc: 80.409% (4014/4992)/ 94.671% (4726/4992)
01/15/2023 14:48:31 - INFO - __main__ -   test: [batch: 39/391 ] | Loss: 0.752 | Acc: 80.586% (4126/5120)/ 94.746% (4851/5120)
01/15/2023 14:48:33 - INFO - __main__ -   test: [batch: 40/391 ] | Loss: 0.750 | Acc: 80.678% (4234/5248)/ 94.665% (4968/5248)
01/15/2023 14:48:35 - INFO - __main__ -   test: [batch: 41/391 ] | Loss: 0.751 | Acc: 80.822% (4345/5376)/ 94.587% (5085/5376)
01/15/2023 14:48:37 - INFO - __main__ -   test: [batch: 42/391 ] | Loss: 0.751 | Acc: 80.814% (4448/5504)/ 94.658% (5210/5504)
01/15/2023 14:48:40 - INFO - __main__ -   test: [batch: 43/391 ] | Loss: 0.750 | Acc: 80.788% (4550/5632)/ 94.638% (5330/5632)
01/15/2023 14:48:42 - INFO - __main__ -   test: [batch: 44/391 ] | Loss: 0.755 | Acc: 80.781% (4653/5760)/ 94.514% (5444/5760)
01/15/2023 14:48:44 - INFO - __main__ -   test: [batch: 45/391 ] | Loss: 0.753 | Acc: 80.910% (4764/5888)/ 94.565% (5568/5888)
01/15/2023 14:48:46 - INFO - __main__ -   test: [batch: 46/391 ] | Loss: 0.756 | Acc: 80.834% (4863/6016)/ 94.614% (5692/6016)
01/15/2023 14:48:48 - INFO - __main__ -   test: [batch: 47/391 ] | Loss: 0.759 | Acc: 80.697% (4958/6144)/ 94.645% (5815/6144)
01/15/2023 14:48:50 - INFO - __main__ -   test: [batch: 48/391 ] | Loss: 0.765 | Acc: 80.564% (5053/6272)/ 94.611% (5934/6272)
01/15/2023 14:48:52 - INFO - __main__ -   test: [batch: 49/391 ] | Loss: 0.770 | Acc: 80.562% (5156/6400)/ 94.531% (6050/6400)
01/15/2023 14:48:54 - INFO - __main__ -   test: [batch: 50/391 ] | Loss: 0.761 | Acc: 80.775% (5273/6528)/ 94.608% (6176/6528)
01/15/2023 14:48:56 - INFO - __main__ -   test: [batch: 51/391 ] | Loss: 0.753 | Acc: 80.980% (5390/6656)/ 94.697% (6303/6656)
01/15/2023 14:48:58 - INFO - __main__ -   test: [batch: 52/391 ] | Loss: 0.749 | Acc: 81.058% (5499/6784)/ 94.738% (6427/6784)
01/15/2023 14:49:00 - INFO - __main__ -   test: [batch: 53/391 ] | Loss: 0.739 | Acc: 81.293% (5619/6912)/ 94.821% (6554/6912)
01/15/2023 14:49:03 - INFO - __main__ -   test: [batch: 54/391 ] | Loss: 0.732 | Acc: 81.463% (5735/7040)/ 94.844% (6677/7040)
01/15/2023 14:49:05 - INFO - __main__ -   test: [batch: 55/391 ] | Loss: 0.725 | Acc: 81.599% (5849/7168)/ 94.894% (6802/7168)
01/15/2023 14:49:07 - INFO - __main__ -   test: [batch: 56/391 ] | Loss: 0.717 | Acc: 81.853% (5972/7296)/ 94.970% (6929/7296)
01/15/2023 14:49:09 - INFO - __main__ -   test: [batch: 57/391 ] | Loss: 0.710 | Acc: 82.031% (6090/7424)/ 95.016% (7054/7424)
01/15/2023 14:49:11 - INFO - __main__ -   test: [batch: 58/391 ] | Loss: 0.705 | Acc: 82.124% (6202/7552)/ 95.008% (7175/7552)
01/15/2023 14:49:13 - INFO - __main__ -   test: [batch: 59/391 ] | Loss: 0.707 | Acc: 82.070% (6303/7680)/ 95.000% (7296/7680)
01/15/2023 14:49:15 - INFO - __main__ -   test: [batch: 60/391 ] | Loss: 0.710 | Acc: 82.006% (6403/7808)/ 94.980% (7416/7808)
01/15/2023 14:49:17 - INFO - __main__ -   test: [batch: 61/391 ] | Loss: 0.709 | Acc: 82.006% (6508/7936)/ 94.997% (7539/7936)
01/15/2023 14:49:19 - INFO - __main__ -   test: [batch: 62/391 ] | Loss: 0.708 | Acc: 81.969% (6610/8064)/ 95.027% (7663/8064)
01/15/2023 14:49:21 - INFO - __main__ -   test: [batch: 63/391 ] | Loss: 0.715 | Acc: 81.848% (6705/8192)/ 94.971% (7780/8192)
01/15/2023 14:49:24 - INFO - __main__ -   test: [batch: 64/391 ] | Loss: 0.723 | Acc: 81.635% (6792/8320)/ 94.928% (7898/8320)
01/15/2023 14:49:26 - INFO - __main__ -   test: [batch: 65/391 ] | Loss: 0.735 | Acc: 81.143% (6855/8448)/ 94.839% (8012/8448)
01/15/2023 14:49:28 - INFO - __main__ -   test: [batch: 66/391 ] | Loss: 0.740 | Acc: 81.157% (6960/8576)/ 94.799% (8130/8576)
01/15/2023 14:49:30 - INFO - __main__ -   test: [batch: 67/391 ] | Loss: 0.742 | Acc: 81.112% (7060/8704)/ 94.807% (8252/8704)
01/15/2023 14:49:32 - INFO - __main__ -   test: [batch: 68/391 ] | Loss: 0.744 | Acc: 81.046% (7158/8832)/ 94.826% (8375/8832)
01/15/2023 14:49:34 - INFO - __main__ -   test: [batch: 69/391 ] | Loss: 0.740 | Acc: 81.127% (7269/8960)/ 94.877% (8501/8960)
01/15/2023 14:49:36 - INFO - __main__ -   test: [batch: 70/391 ] | Loss: 0.740 | Acc: 81.052% (7366/9088)/ 94.905% (8625/9088)
01/15/2023 14:49:38 - INFO - __main__ -   test: [batch: 71/391 ] | Loss: 0.738 | Acc: 81.120% (7476/9216)/ 94.889% (8745/9216)
01/15/2023 14:49:40 - INFO - __main__ -   test: [batch: 72/391 ] | Loss: 0.741 | Acc: 80.982% (7567/9344)/ 94.906% (8868/9344)
01/15/2023 14:49:43 - INFO - __main__ -   test: [batch: 73/391 ] | Loss: 0.744 | Acc: 80.859% (7659/9472)/ 94.901% (8989/9472)
01/15/2023 14:49:45 - INFO - __main__ -   test: [batch: 74/391 ] | Loss: 0.745 | Acc: 80.854% (7762/9600)/ 94.885% (9109/9600)
01/15/2023 14:49:47 - INFO - __main__ -   test: [batch: 75/391 ] | Loss: 0.751 | Acc: 80.685% (7849/9728)/ 94.870% (9229/9728)
01/15/2023 14:49:49 - INFO - __main__ -   test: [batch: 76/391 ] | Loss: 0.750 | Acc: 80.692% (7953/9856)/ 94.897% (9353/9856)
01/15/2023 14:49:51 - INFO - __main__ -   test: [batch: 77/391 ] | Loss: 0.750 | Acc: 80.619% (8049/9984)/ 94.932% (9478/9984)
01/15/2023 14:49:54 - INFO - __main__ -   test: [batch: 78/391 ] | Loss: 0.749 | Acc: 80.587% (8149/10112)/ 94.976% (9604/10112)
01/15/2023 14:49:56 - INFO - __main__ -   test: [batch: 79/391 ] | Loss: 0.747 | Acc: 80.566% (8250/10240)/ 95.000% (9728/10240)
01/15/2023 14:49:58 - INFO - __main__ -   test: [batch: 80/391 ] | Loss: 0.748 | Acc: 80.536% (8350/10368)/ 94.985% (9848/10368)
01/15/2023 14:50:00 - INFO - __main__ -   test: [batch: 81/391 ] | Loss: 0.747 | Acc: 80.545% (8454/10496)/ 95.008% (9972/10496)
01/15/2023 14:50:02 - INFO - __main__ -   test: [batch: 82/391 ] | Loss: 0.746 | Acc: 80.563% (8559/10624)/ 95.021% (10095/10624)
01/15/2023 14:50:04 - INFO - __main__ -   test: [batch: 83/391 ] | Loss: 0.746 | Acc: 80.599% (8666/10752)/ 94.996% (10214/10752)
01/15/2023 14:50:06 - INFO - __main__ -   test: [batch: 84/391 ] | Loss: 0.742 | Acc: 80.726% (8783/10880)/ 95.028% (10339/10880)
01/15/2023 14:50:08 - INFO - __main__ -   test: [batch: 85/391 ] | Loss: 0.740 | Acc: 80.732% (8887/11008)/ 95.076% (10466/11008)
01/15/2023 14:50:10 - INFO - __main__ -   test: [batch: 86/391 ] | Loss: 0.744 | Acc: 80.648% (8981/11136)/ 95.034% (10583/11136)
01/15/2023 14:50:12 - INFO - __main__ -   test: [batch: 87/391 ] | Loss: 0.743 | Acc: 80.691% (9089/11264)/ 95.028% (10704/11264)
01/15/2023 14:50:15 - INFO - __main__ -   test: [batch: 88/391 ] | Loss: 0.750 | Acc: 80.609% (9183/11392)/ 94.970% (10819/11392)
01/15/2023 14:50:17 - INFO - __main__ -   test: [batch: 89/391 ] | Loss: 0.748 | Acc: 80.660% (9292/11520)/ 94.983% (10942/11520)
01/15/2023 14:50:19 - INFO - __main__ -   test: [batch: 90/391 ] | Loss: 0.749 | Acc: 80.563% (9384/11648)/ 94.995% (11065/11648)
01/15/2023 14:50:21 - INFO - __main__ -   test: [batch: 91/391 ] | Loss: 0.747 | Acc: 80.596% (9491/11776)/ 94.998% (11187/11776)
01/15/2023 14:50:23 - INFO - __main__ -   test: [batch: 92/391 ] | Loss: 0.749 | Acc: 80.536% (9587/11904)/ 94.985% (11307/11904)
01/15/2023 14:50:25 - INFO - __main__ -   test: [batch: 93/391 ] | Loss: 0.751 | Acc: 80.344% (9667/12032)/ 95.022% (11433/12032)
01/15/2023 14:50:27 - INFO - __main__ -   test: [batch: 94/391 ] | Loss: 0.754 | Acc: 80.222% (9755/12160)/ 95.041% (11557/12160)
01/15/2023 14:50:29 - INFO - __main__ -   test: [batch: 95/391 ] | Loss: 0.752 | Acc: 80.290% (9866/12288)/ 95.060% (11681/12288)
01/15/2023 14:50:32 - INFO - __main__ -   test: [batch: 96/391 ] | Loss: 0.756 | Acc: 80.139% (9950/12416)/ 95.063% (11803/12416)
01/15/2023 14:50:34 - INFO - __main__ -   test: [batch: 97/391 ] | Loss: 0.758 | Acc: 79.959% (10030/12544)/ 95.081% (11927/12544)
01/15/2023 14:50:36 - INFO - __main__ -   test: [batch: 98/391 ] | Loss: 0.755 | Acc: 80.058% (10145/12672)/ 95.084% (12049/12672)
01/15/2023 14:50:38 - INFO - __main__ -   test: [batch: 99/391 ] | Loss: 0.749 | Acc: 80.227% (10269/12800)/ 95.125% (12176/12800)
01/15/2023 14:50:40 - INFO - __main__ -   test: [batch: 100/391 ] | Loss: 0.748 | Acc: 80.275% (10378/12928)/ 95.142% (12300/12928)
01/15/2023 14:50:42 - INFO - __main__ -   test: [batch: 101/391 ] | Loss: 0.743 | Acc: 80.392% (10496/13056)/ 95.175% (12426/13056)
01/15/2023 14:50:44 - INFO - __main__ -   test: [batch: 102/391 ] | Loss: 0.740 | Acc: 80.476% (10610/13184)/ 95.206% (12552/13184)
01/15/2023 14:50:47 - INFO - __main__ -   test: [batch: 103/391 ] | Loss: 0.742 | Acc: 80.341% (10695/13312)/ 95.207% (12674/13312)
01/15/2023 14:50:49 - INFO - __main__ -   test: [batch: 104/391 ] | Loss: 0.741 | Acc: 80.283% (10790/13440)/ 95.216% (12797/13440)
01/15/2023 14:50:51 - INFO - __main__ -   test: [batch: 105/391 ] | Loss: 0.742 | Acc: 80.277% (10892/13568)/ 95.217% (12919/13568)
01/15/2023 14:50:53 - INFO - __main__ -   test: [batch: 106/391 ] | Loss: 0.750 | Acc: 80.206% (10985/13696)/ 95.159% (13033/13696)
01/15/2023 14:50:55 - INFO - __main__ -   test: [batch: 107/391 ] | Loss: 0.746 | Acc: 80.331% (11105/13824)/ 95.197% (13160/13824)
01/15/2023 14:50:57 - INFO - __main__ -   test: [batch: 108/391 ] | Loss: 0.749 | Acc: 80.204% (11190/13952)/ 95.205% (13283/13952)
01/15/2023 14:50:59 - INFO - __main__ -   test: [batch: 109/391 ] | Loss: 0.749 | Acc: 80.199% (11292/14080)/ 95.206% (13405/14080)
01/15/2023 14:51:01 - INFO - __main__ -   test: [batch: 110/391 ] | Loss: 0.752 | Acc: 80.025% (11370/14208)/ 95.221% (13529/14208)
01/15/2023 14:51:03 - INFO - __main__ -   test: [batch: 111/391 ] | Loss: 0.755 | Acc: 79.980% (11466/14336)/ 95.187% (13646/14336)
01/15/2023 14:51:05 - INFO - __main__ -   test: [batch: 112/391 ] | Loss: 0.755 | Acc: 80.012% (11573/14464)/ 95.209% (13771/14464)
01/15/2023 14:51:07 - INFO - __main__ -   test: [batch: 113/391 ] | Loss: 0.754 | Acc: 80.051% (11681/14592)/ 95.223% (13895/14592)
01/15/2023 14:51:10 - INFO - __main__ -   test: [batch: 114/391 ] | Loss: 0.750 | Acc: 80.149% (11798/14720)/ 95.251% (14021/14720)
01/15/2023 14:51:12 - INFO - __main__ -   test: [batch: 115/391 ] | Loss: 0.747 | Acc: 80.240% (11914/14848)/ 95.272% (14146/14848)
01/15/2023 14:51:14 - INFO - __main__ -   test: [batch: 116/391 ] | Loss: 0.750 | Acc: 80.208% (12012/14976)/ 95.239% (14263/14976)
01/15/2023 14:51:16 - INFO - __main__ -   test: [batch: 117/391 ] | Loss: 0.749 | Acc: 80.244% (12120/15104)/ 95.260% (14388/15104)
01/15/2023 14:51:18 - INFO - __main__ -   test: [batch: 118/391 ] | Loss: 0.753 | Acc: 80.081% (12198/15232)/ 95.267% (14511/15232)
01/15/2023 14:51:20 - INFO - __main__ -   test: [batch: 119/391 ] | Loss: 0.751 | Acc: 80.130% (12308/15360)/ 95.293% (14637/15360)
01/15/2023 14:51:22 - INFO - __main__ -   test: [batch: 120/391 ] | Loss: 0.750 | Acc: 80.178% (12418/15488)/ 95.306% (14761/15488)
01/15/2023 14:51:24 - INFO - __main__ -   test: [batch: 121/391 ] | Loss: 0.755 | Acc: 80.040% (12499/15616)/ 95.268% (14877/15616)
01/15/2023 14:51:26 - INFO - __main__ -   test: [batch: 122/391 ] | Loss: 0.758 | Acc: 79.980% (12592/15744)/ 95.243% (14995/15744)
01/15/2023 14:51:28 - INFO - __main__ -   test: [batch: 123/391 ] | Loss: 0.759 | Acc: 80.003% (12698/15872)/ 95.237% (15116/15872)
01/15/2023 14:51:30 - INFO - __main__ -   test: [batch: 124/391 ] | Loss: 0.758 | Acc: 80.019% (12803/16000)/ 95.250% (15240/16000)
01/15/2023 14:51:32 - INFO - __main__ -   test: [batch: 125/391 ] | Loss: 0.754 | Acc: 80.115% (12921/16128)/ 95.275% (15366/16128)
01/15/2023 14:51:35 - INFO - __main__ -   test: [batch: 126/391 ] | Loss: 0.751 | Acc: 80.204% (13038/16256)/ 95.282% (15489/16256)
01/15/2023 14:51:36 - INFO - __main__ -   test: [batch: 127/391 ] | Loss: 0.749 | Acc: 80.255% (13149/16384)/ 95.294% (15613/16384)
01/15/2023 14:51:39 - INFO - __main__ -   test: [batch: 128/391 ] | Loss: 0.749 | Acc: 80.263% (13253/16512)/ 95.276% (15732/16512)
01/15/2023 14:51:41 - INFO - __main__ -   test: [batch: 129/391 ] | Loss: 0.748 | Acc: 80.306% (13363/16640)/ 95.294% (15857/16640)
01/15/2023 14:51:43 - INFO - __main__ -   test: [batch: 130/391 ] | Loss: 0.744 | Acc: 80.415% (13484/16768)/ 95.318% (15983/16768)
01/15/2023 14:51:45 - INFO - __main__ -   test: [batch: 131/391 ] | Loss: 0.744 | Acc: 80.463% (13595/16896)/ 95.330% (16107/16896)
01/15/2023 14:51:47 - INFO - __main__ -   test: [batch: 132/391 ] | Loss: 0.741 | Acc: 80.533% (13710/17024)/ 95.348% (16232/17024)
01/15/2023 14:51:49 - INFO - __main__ -   test: [batch: 133/391 ] | Loss: 0.743 | Acc: 80.498% (13807/17152)/ 95.342% (16353/17152)
01/15/2023 14:51:51 - INFO - __main__ -   test: [batch: 134/391 ] | Loss: 0.741 | Acc: 80.527% (13915/17280)/ 95.359% (16478/17280)
01/15/2023 14:51:53 - INFO - __main__ -   test: [batch: 135/391 ] | Loss: 0.742 | Acc: 80.515% (14016/17408)/ 95.364% (16601/17408)
01/15/2023 14:51:56 - INFO - __main__ -   test: [batch: 136/391 ] | Loss: 0.740 | Acc: 80.520% (14120/17536)/ 95.387% (16727/17536)
01/15/2023 14:51:58 - INFO - __main__ -   test: [batch: 137/391 ] | Loss: 0.738 | Acc: 80.554% (14229/17664)/ 95.420% (16855/17664)
01/15/2023 14:52:00 - INFO - __main__ -   test: [batch: 138/391 ] | Loss: 0.736 | Acc: 80.587% (14338/17792)/ 95.431% (16979/17792)
01/15/2023 14:52:02 - INFO - __main__ -   test: [batch: 139/391 ] | Loss: 0.741 | Acc: 80.441% (14415/17920)/ 95.424% (17100/17920)
01/15/2023 14:52:04 - INFO - __main__ -   test: [batch: 140/391 ] | Loss: 0.743 | Acc: 80.363% (14504/18048)/ 95.418% (17221/18048)
01/15/2023 14:52:06 - INFO - __main__ -   test: [batch: 141/391 ] | Loss: 0.742 | Acc: 80.408% (14615/18176)/ 95.423% (17344/18176)
01/15/2023 14:52:08 - INFO - __main__ -   test: [batch: 142/391 ] | Loss: 0.739 | Acc: 80.496% (14734/18304)/ 95.433% (17468/18304)
01/15/2023 14:52:11 - INFO - __main__ -   test: [batch: 143/391 ] | Loss: 0.739 | Acc: 80.528% (14843/18432)/ 95.426% (17589/18432)
01/15/2023 14:52:13 - INFO - __main__ -   test: [batch: 144/391 ] | Loss: 0.741 | Acc: 80.501% (14941/18560)/ 95.399% (17706/18560)
01/15/2023 14:52:15 - INFO - __main__ -   test: [batch: 145/391 ] | Loss: 0.744 | Acc: 80.490% (15042/18688)/ 95.382% (17825/18688)
01/15/2023 14:52:17 - INFO - __main__ -   test: [batch: 146/391 ] | Loss: 0.746 | Acc: 80.458% (15139/18816)/ 95.360% (17943/18816)
01/15/2023 14:52:19 - INFO - __main__ -   test: [batch: 147/391 ] | Loss: 0.746 | Acc: 80.469% (15244/18944)/ 95.344% (18062/18944)
01/15/2023 14:52:21 - INFO - __main__ -   test: [batch: 148/391 ] | Loss: 0.747 | Acc: 80.385% (15331/19072)/ 95.339% (18183/19072)
01/15/2023 14:52:24 - INFO - __main__ -   test: [batch: 149/391 ] | Loss: 0.751 | Acc: 80.307% (15419/19200)/ 95.297% (18297/19200)
01/15/2023 14:52:26 - INFO - __main__ -   test: [batch: 150/391 ] | Loss: 0.751 | Acc: 80.257% (15512/19328)/ 95.302% (18420/19328)
01/15/2023 14:52:28 - INFO - __main__ -   test: [batch: 151/391 ] | Loss: 0.751 | Acc: 80.299% (15623/19456)/ 95.302% (18542/19456)
01/15/2023 14:52:30 - INFO - __main__ -   test: [batch: 152/391 ] | Loss: 0.752 | Acc: 80.275% (15721/19584)/ 95.282% (18660/19584)
01/15/2023 14:52:32 - INFO - __main__ -   test: [batch: 153/391 ] | Loss: 0.750 | Acc: 80.337% (15836/19712)/ 95.292% (18784/19712)
01/15/2023 14:52:34 - INFO - __main__ -   test: [batch: 154/391 ] | Loss: 0.752 | Acc: 80.318% (15935/19840)/ 95.252% (18898/19840)
01/15/2023 14:52:36 - INFO - __main__ -   test: [batch: 155/391 ] | Loss: 0.753 | Acc: 80.329% (16040/19968)/ 95.242% (19018/19968)
01/15/2023 14:52:38 - INFO - __main__ -   test: [batch: 156/391 ] | Loss: 0.755 | Acc: 80.250% (16127/20096)/ 95.218% (19135/20096)
01/15/2023 14:52:41 - INFO - __main__ -   test: [batch: 157/391 ] | Loss: 0.758 | Acc: 80.212% (16222/20224)/ 95.199% (19253/20224)
01/15/2023 14:52:43 - INFO - __main__ -   test: [batch: 158/391 ] | Loss: 0.759 | Acc: 80.184% (16319/20352)/ 95.175% (19370/20352)
01/15/2023 14:52:45 - INFO - __main__ -   test: [batch: 159/391 ] | Loss: 0.760 | Acc: 80.181% (16421/20480)/ 95.186% (19494/20480)
01/15/2023 14:52:47 - INFO - __main__ -   test: [batch: 160/391 ] | Loss: 0.761 | Acc: 80.163% (16520/20608)/ 95.157% (19610/20608)
01/15/2023 14:52:50 - INFO - __main__ -   test: [batch: 161/391 ] | Loss: 0.771 | Acc: 79.919% (16572/20736)/ 95.052% (19710/20736)
01/15/2023 14:52:52 - INFO - __main__ -   test: [batch: 162/391 ] | Loss: 0.776 | Acc: 79.822% (16654/20864)/ 94.982% (19817/20864)
01/15/2023 14:52:54 - INFO - __main__ -   test: [batch: 163/391 ] | Loss: 0.779 | Acc: 79.764% (16744/20992)/ 94.970% (19936/20992)
01/15/2023 14:52:56 - INFO - __main__ -   test: [batch: 164/391 ] | Loss: 0.779 | Acc: 79.749% (16843/21120)/ 94.976% (20059/21120)
01/15/2023 14:52:58 - INFO - __main__ -   test: [batch: 165/391 ] | Loss: 0.781 | Acc: 79.688% (16932/21248)/ 94.964% (20178/21248)
01/15/2023 14:53:00 - INFO - __main__ -   test: [batch: 166/391 ] | Loss: 0.781 | Acc: 79.716% (17040/21376)/ 94.952% (20297/21376)
01/15/2023 14:53:02 - INFO - __main__ -   test: [batch: 167/391 ] | Loss: 0.783 | Acc: 79.688% (17136/21504)/ 94.922% (20412/21504)
01/15/2023 14:53:04 - INFO - __main__ -   test: [batch: 168/391 ] | Loss: 0.783 | Acc: 79.701% (17241/21632)/ 94.929% (20535/21632)
01/15/2023 14:53:06 - INFO - __main__ -   test: [batch: 169/391 ] | Loss: 0.785 | Acc: 79.646% (17331/21760)/ 94.890% (20648/21760)
01/15/2023 14:53:09 - INFO - __main__ -   test: [batch: 170/391 ] | Loss: 0.789 | Acc: 79.550% (17412/21888)/ 94.860% (20763/21888)
01/15/2023 14:53:11 - INFO - __main__ -   test: [batch: 171/391 ] | Loss: 0.793 | Acc: 79.501% (17503/22016)/ 94.831% (20878/22016)
01/15/2023 14:53:13 - INFO - __main__ -   test: [batch: 172/391 ] | Loss: 0.793 | Acc: 79.462% (17596/22144)/ 94.825% (20998/22144)
01/15/2023 14:53:15 - INFO - __main__ -   test: [batch: 173/391 ] | Loss: 0.796 | Acc: 79.418% (17688/22272)/ 94.796% (21113/22272)
01/15/2023 14:53:17 - INFO - __main__ -   test: [batch: 174/391 ] | Loss: 0.800 | Acc: 79.312% (17766/22400)/ 94.754% (21225/22400)
01/15/2023 14:53:19 - INFO - __main__ -   test: [batch: 175/391 ] | Loss: 0.799 | Acc: 79.363% (17879/22528)/ 94.767% (21349/22528)
01/15/2023 14:53:21 - INFO - __main__ -   test: [batch: 176/391 ] | Loss: 0.800 | Acc: 79.352% (17978/22656)/ 94.739% (21464/22656)
01/15/2023 14:53:24 - INFO - __main__ -   test: [batch: 177/391 ] | Loss: 0.804 | Acc: 79.275% (18062/22784)/ 94.694% (21575/22784)
01/15/2023 14:53:26 - INFO - __main__ -   test: [batch: 178/391 ] | Loss: 0.805 | Acc: 79.290% (18167/22912)/ 94.680% (21693/22912)
01/15/2023 14:53:28 - INFO - __main__ -   test: [batch: 179/391 ] | Loss: 0.808 | Acc: 79.232% (18255/23040)/ 94.631% (21803/23040)
01/15/2023 14:53:30 - INFO - __main__ -   test: [batch: 180/391 ] | Loss: 0.814 | Acc: 79.088% (18323/23168)/ 94.587% (21914/23168)
01/15/2023 14:53:32 - INFO - __main__ -   test: [batch: 181/391 ] | Loss: 0.821 | Acc: 78.966% (18396/23296)/ 94.505% (22016/23296)
01/15/2023 14:53:34 - INFO - __main__ -   test: [batch: 182/391 ] | Loss: 0.820 | Acc: 78.987% (18502/23424)/ 94.506% (22137/23424)
01/15/2023 14:53:36 - INFO - __main__ -   test: [batch: 183/391 ] | Loss: 0.827 | Acc: 78.860% (18573/23552)/ 94.395% (22232/23552)
01/15/2023 14:53:38 - INFO - __main__ -   test: [batch: 184/391 ] | Loss: 0.826 | Acc: 78.881% (18679/23680)/ 94.388% (22351/23680)
01/15/2023 14:53:40 - INFO - __main__ -   test: [batch: 185/391 ] | Loss: 0.826 | Acc: 78.889% (18782/23808)/ 94.384% (22471/23808)
01/15/2023 14:53:42 - INFO - __main__ -   test: [batch: 186/391 ] | Loss: 0.829 | Acc: 78.848% (18873/23936)/ 94.347% (22583/23936)
01/15/2023 14:53:45 - INFO - __main__ -   test: [batch: 187/391 ] | Loss: 0.833 | Acc: 78.732% (18946/24064)/ 94.332% (22700/24064)
01/15/2023 14:53:47 - INFO - __main__ -   test: [batch: 188/391 ] | Loss: 0.837 | Acc: 78.609% (19017/24192)/ 94.304% (22814/24192)
01/15/2023 14:53:49 - INFO - __main__ -   test: [batch: 189/391 ] | Loss: 0.838 | Acc: 78.540% (19101/24320)/ 94.309% (22936/24320)
01/15/2023 14:53:51 - INFO - __main__ -   test: [batch: 190/391 ] | Loss: 0.843 | Acc: 78.440% (19177/24448)/ 94.282% (23050/24448)
01/15/2023 14:53:53 - INFO - __main__ -   test: [batch: 191/391 ] | Loss: 0.846 | Acc: 78.410% (19270/24576)/ 94.255% (23164/24576)
01/15/2023 14:53:55 - INFO - __main__ -   test: [batch: 192/391 ] | Loss: 0.852 | Acc: 78.279% (19338/24704)/ 94.151% (23259/24704)
01/15/2023 14:53:57 - INFO - __main__ -   test: [batch: 193/391 ] | Loss: 0.852 | Acc: 78.294% (19442/24832)/ 94.145% (23378/24832)
01/15/2023 14:53:59 - INFO - __main__ -   test: [batch: 194/391 ] | Loss: 0.856 | Acc: 78.225% (19525/24960)/ 94.111% (23490/24960)
01/15/2023 14:54:01 - INFO - __main__ -   test: [batch: 195/391 ] | Loss: 0.861 | Acc: 78.125% (19600/25088)/ 94.045% (23594/25088)
01/15/2023 14:54:04 - INFO - __main__ -   test: [batch: 196/391 ] | Loss: 0.865 | Acc: 78.026% (19675/25216)/ 94.004% (23704/25216)
01/15/2023 14:54:06 - INFO - __main__ -   test: [batch: 197/391 ] | Loss: 0.869 | Acc: 77.932% (19751/25344)/ 93.987% (23820/25344)
01/15/2023 14:54:08 - INFO - __main__ -   test: [batch: 198/391 ] | Loss: 0.871 | Acc: 77.889% (19840/25472)/ 93.931% (23926/25472)
01/15/2023 14:54:10 - INFO - __main__ -   test: [batch: 199/391 ] | Loss: 0.871 | Acc: 77.859% (19932/25600)/ 93.945% (24050/25600)
01/15/2023 14:54:12 - INFO - __main__ -   test: [batch: 200/391 ] | Loss: 0.871 | Acc: 77.833% (20025/25728)/ 93.952% (24172/25728)
01/15/2023 14:54:14 - INFO - __main__ -   test: [batch: 201/391 ] | Loss: 0.875 | Acc: 77.734% (20099/25856)/ 93.924% (24285/25856)
01/15/2023 14:54:16 - INFO - __main__ -   test: [batch: 202/391 ] | Loss: 0.876 | Acc: 77.721% (20195/25984)/ 93.915% (24403/25984)
01/15/2023 14:54:19 - INFO - __main__ -   test: [batch: 203/391 ] | Loss: 0.876 | Acc: 77.715% (20293/26112)/ 93.907% (24521/26112)
01/15/2023 14:54:21 - INFO - __main__ -   test: [batch: 204/391 ] | Loss: 0.879 | Acc: 77.626% (20369/26240)/ 93.876% (24633/26240)
01/15/2023 14:54:23 - INFO - __main__ -   test: [batch: 205/391 ] | Loss: 0.882 | Acc: 77.545% (20447/26368)/ 93.845% (24745/26368)
01/15/2023 14:54:25 - INFO - __main__ -   test: [batch: 206/391 ] | Loss: 0.884 | Acc: 77.521% (20540/26496)/ 93.841% (24864/26496)
01/15/2023 14:54:27 - INFO - __main__ -   test: [batch: 207/391 ] | Loss: 0.887 | Acc: 77.445% (20619/26624)/ 93.806% (24975/26624)
01/15/2023 14:54:29 - INFO - __main__ -   test: [batch: 208/391 ] | Loss: 0.889 | Acc: 77.407% (20708/26752)/ 93.784% (25089/26752)
01/15/2023 14:54:31 - INFO - __main__ -   test: [batch: 209/391 ] | Loss: 0.888 | Acc: 77.400% (20805/26880)/ 93.806% (25215/26880)
01/15/2023 14:54:34 - INFO - __main__ -   test: [batch: 210/391 ] | Loss: 0.890 | Acc: 77.362% (20894/27008)/ 93.787% (25330/27008)
01/15/2023 14:54:36 - INFO - __main__ -   test: [batch: 211/391 ] | Loss: 0.893 | Acc: 77.307% (20978/27136)/ 93.743% (25438/27136)
01/15/2023 14:54:38 - INFO - __main__ -   test: [batch: 212/391 ] | Loss: 0.895 | Acc: 77.245% (21060/27264)/ 93.739% (25557/27264)
01/15/2023 14:54:40 - INFO - __main__ -   test: [batch: 213/391 ] | Loss: 0.894 | Acc: 77.252% (21161/27392)/ 93.746% (25679/27392)
01/15/2023 14:54:42 - INFO - __main__ -   test: [batch: 214/391 ] | Loss: 0.894 | Acc: 77.238% (21256/27520)/ 93.750% (25800/27520)
01/15/2023 14:54:44 - INFO - __main__ -   test: [batch: 215/391 ] | Loss: 0.896 | Acc: 77.214% (21348/27648)/ 93.728% (25914/27648)
01/15/2023 14:54:46 - INFO - __main__ -   test: [batch: 216/391 ] | Loss: 0.893 | Acc: 77.265% (21461/27776)/ 93.743% (26038/27776)
01/15/2023 14:54:48 - INFO - __main__ -   test: [batch: 217/391 ] | Loss: 0.898 | Acc: 77.204% (21543/27904)/ 93.678% (26140/27904)
01/15/2023 14:54:51 - INFO - __main__ -   test: [batch: 218/391 ] | Loss: 0.901 | Acc: 77.140% (21624/28032)/ 93.639% (26249/28032)
01/15/2023 14:54:53 - INFO - __main__ -   test: [batch: 219/391 ] | Loss: 0.900 | Acc: 77.177% (21733/28160)/ 93.654% (26373/28160)
01/15/2023 14:54:55 - INFO - __main__ -   test: [batch: 220/391 ] | Loss: 0.898 | Acc: 77.213% (21842/28288)/ 93.665% (26496/28288)
01/15/2023 14:54:57 - INFO - __main__ -   test: [batch: 221/391 ] | Loss: 0.900 | Acc: 77.178% (21931/28416)/ 93.644% (26610/28416)
01/15/2023 14:54:59 - INFO - __main__ -   test: [batch: 222/391 ] | Loss: 0.898 | Acc: 77.228% (22044/28544)/ 93.659% (26734/28544)
01/15/2023 14:55:01 - INFO - __main__ -   test: [batch: 223/391 ] | Loss: 0.899 | Acc: 77.225% (22142/28672)/ 93.652% (26852/28672)
01/15/2023 14:55:03 - INFO - __main__ -   test: [batch: 224/391 ] | Loss: 0.898 | Acc: 77.243% (22246/28800)/ 93.653% (26972/28800)
01/15/2023 14:55:05 - INFO - __main__ -   test: [batch: 225/391 ] | Loss: 0.898 | Acc: 77.237% (22343/28928)/ 93.657% (27093/28928)
01/15/2023 14:55:07 - INFO - __main__ -   test: [batch: 226/391 ] | Loss: 0.898 | Acc: 77.206% (22433/29056)/ 93.657% (27213/29056)
01/15/2023 14:55:09 - INFO - __main__ -   test: [batch: 227/391 ] | Loss: 0.900 | Acc: 77.203% (22531/29184)/ 93.651% (27331/29184)
01/15/2023 14:55:11 - INFO - __main__ -   test: [batch: 228/391 ] | Loss: 0.904 | Acc: 77.115% (22604/29312)/ 93.579% (27430/29312)
01/15/2023 14:55:13 - INFO - __main__ -   test: [batch: 229/391 ] | Loss: 0.908 | Acc: 77.031% (22678/29440)/ 93.533% (27536/29440)
01/15/2023 14:55:15 - INFO - __main__ -   test: [batch: 230/391 ] | Loss: 0.912 | Acc: 76.951% (22753/29568)/ 93.486% (27642/29568)
01/15/2023 14:55:18 - INFO - __main__ -   test: [batch: 231/391 ] | Loss: 0.911 | Acc: 76.946% (22850/29696)/ 93.477% (27759/29696)
01/15/2023 14:55:20 - INFO - __main__ -   test: [batch: 232/391 ] | Loss: 0.910 | Acc: 76.978% (22958/29824)/ 93.492% (27883/29824)
01/15/2023 14:55:22 - INFO - __main__ -   test: [batch: 233/391 ] | Loss: 0.912 | Acc: 76.936% (23044/29952)/ 93.466% (27995/29952)
01/15/2023 14:55:24 - INFO - __main__ -   test: [batch: 234/391 ] | Loss: 0.918 | Acc: 76.815% (23106/30080)/ 93.401% (28095/30080)
01/15/2023 14:55:26 - INFO - __main__ -   test: [batch: 235/391 ] | Loss: 0.919 | Acc: 76.807% (23202/30208)/ 93.396% (28213/30208)
01/15/2023 14:55:28 - INFO - __main__ -   test: [batch: 236/391 ] | Loss: 0.917 | Acc: 76.856% (23315/30336)/ 93.407% (28336/30336)
01/15/2023 14:55:30 - INFO - __main__ -   test: [batch: 237/391 ] | Loss: 0.918 | Acc: 76.855% (23413/30464)/ 93.379% (28447/30464)
01/15/2023 14:55:32 - INFO - __main__ -   test: [batch: 238/391 ] | Loss: 0.917 | Acc: 76.883% (23520/30592)/ 93.387% (28569/30592)
01/15/2023 14:55:34 - INFO - __main__ -   test: [batch: 239/391 ] | Loss: 0.916 | Acc: 76.908% (23626/30720)/ 93.392% (28690/30720)
01/15/2023 14:55:37 - INFO - __main__ -   test: [batch: 240/391 ] | Loss: 0.916 | Acc: 76.919% (23728/30848)/ 93.384% (28807/30848)
01/15/2023 14:55:39 - INFO - __main__ -   test: [batch: 241/391 ] | Loss: 0.919 | Acc: 76.834% (23800/30976)/ 93.334% (28911/30976)
01/15/2023 14:55:41 - INFO - __main__ -   test: [batch: 242/391 ] | Loss: 0.921 | Acc: 76.733% (23867/31104)/ 93.322% (29027/31104)
01/15/2023 14:55:43 - INFO - __main__ -   test: [batch: 243/391 ] | Loss: 0.927 | Acc: 76.617% (23929/31232)/ 93.244% (29122/31232)
01/15/2023 14:55:45 - INFO - __main__ -   test: [batch: 244/391 ] | Loss: 0.927 | Acc: 76.614% (24026/31360)/ 93.233% (29238/31360)
01/15/2023 14:55:47 - INFO - __main__ -   test: [batch: 245/391 ] | Loss: 0.927 | Acc: 76.616% (24125/31488)/ 93.232% (29357/31488)
01/15/2023 14:55:49 - INFO - __main__ -   test: [batch: 246/391 ] | Loss: 0.928 | Acc: 76.594% (24216/31616)/ 93.225% (29474/31616)
01/15/2023 14:55:51 - INFO - __main__ -   test: [batch: 247/391 ] | Loss: 0.933 | Acc: 76.509% (24287/31744)/ 93.161% (29573/31744)
01/15/2023 14:55:53 - INFO - __main__ -   test: [batch: 248/391 ] | Loss: 0.934 | Acc: 76.472% (24373/31872)/ 93.160% (29692/31872)
01/15/2023 14:55:55 - INFO - __main__ -   test: [batch: 249/391 ] | Loss: 0.937 | Acc: 76.316% (24421/32000)/ 93.156% (29810/32000)
01/15/2023 14:55:57 - INFO - __main__ -   test: [batch: 250/391 ] | Loss: 0.935 | Acc: 76.345% (24528/32128)/ 93.168% (29933/32128)
01/15/2023 14:56:00 - INFO - __main__ -   test: [batch: 251/391 ] | Loss: 0.937 | Acc: 76.308% (24614/32256)/ 93.142% (30044/32256)
01/15/2023 14:56:02 - INFO - __main__ -   test: [batch: 252/391 ] | Loss: 0.936 | Acc: 76.334% (24720/32384)/ 93.135% (30161/32384)
01/15/2023 14:56:04 - INFO - __main__ -   test: [batch: 253/391 ] | Loss: 0.938 | Acc: 76.295% (24805/32512)/ 93.113% (30273/32512)
01/15/2023 14:56:06 - INFO - __main__ -   test: [batch: 254/391 ] | Loss: 0.942 | Acc: 76.219% (24878/32640)/ 93.064% (30376/32640)
01/15/2023 14:56:08 - INFO - __main__ -   test: [batch: 255/391 ] | Loss: 0.944 | Acc: 76.193% (24967/32768)/ 93.057% (30493/32768)
01/15/2023 14:56:10 - INFO - __main__ -   test: [batch: 256/391 ] | Loss: 0.949 | Acc: 76.064% (25022/32896)/ 93.008% (30596/32896)
01/15/2023 14:56:13 - INFO - __main__ -   test: [batch: 257/391 ] | Loss: 0.950 | Acc: 76.054% (25116/33024)/ 93.008% (30715/33024)
01/15/2023 14:56:15 - INFO - __main__ -   test: [batch: 258/391 ] | Loss: 0.951 | Acc: 76.047% (25211/33152)/ 92.996% (30830/33152)
01/15/2023 14:56:17 - INFO - __main__ -   test: [batch: 259/391 ] | Loss: 0.954 | Acc: 75.925% (25268/33280)/ 92.990% (30947/33280)
01/15/2023 14:56:19 - INFO - __main__ -   test: [batch: 260/391 ] | Loss: 0.955 | Acc: 75.910% (25360/33408)/ 92.990% (31066/33408)
01/15/2023 14:56:21 - INFO - __main__ -   test: [batch: 261/391 ] | Loss: 0.953 | Acc: 75.954% (25472/33536)/ 93.013% (31193/33536)
01/15/2023 14:56:23 - INFO - __main__ -   test: [batch: 262/391 ] | Loss: 0.952 | Acc: 75.977% (25577/33664)/ 93.025% (31316/33664)
01/15/2023 14:56:25 - INFO - __main__ -   test: [batch: 263/391 ] | Loss: 0.955 | Acc: 75.911% (25652/33792)/ 93.001% (31427/33792)
01/15/2023 14:56:28 - INFO - __main__ -   test: [batch: 264/391 ] | Loss: 0.960 | Acc: 75.840% (25725/33920)/ 92.936% (31524/33920)
01/15/2023 14:56:30 - INFO - __main__ -   test: [batch: 265/391 ] | Loss: 0.959 | Acc: 75.872% (25833/34048)/ 92.936% (31643/34048)
01/15/2023 14:56:32 - INFO - __main__ -   test: [batch: 266/391 ] | Loss: 0.961 | Acc: 75.816% (25911/34176)/ 92.922% (31757/34176)
01/15/2023 14:56:34 - INFO - __main__ -   test: [batch: 267/391 ] | Loss: 0.961 | Acc: 75.845% (26018/34304)/ 92.925% (31877/34304)
01/15/2023 14:56:36 - INFO - __main__ -   test: [batch: 268/391 ] | Loss: 0.960 | Acc: 75.851% (26117/34432)/ 92.919% (31994/34432)
01/15/2023 14:56:38 - INFO - __main__ -   test: [batch: 269/391 ] | Loss: 0.962 | Acc: 75.802% (26197/34560)/ 92.899% (32106/34560)
01/15/2023 14:56:40 - INFO - __main__ -   test: [batch: 270/391 ] | Loss: 0.965 | Acc: 75.752% (26277/34688)/ 92.876% (32217/34688)
01/15/2023 14:56:42 - INFO - __main__ -   test: [batch: 271/391 ] | Loss: 0.965 | Acc: 75.767% (26379/34816)/ 92.871% (32334/34816)
01/15/2023 14:56:44 - INFO - __main__ -   test: [batch: 272/391 ] | Loss: 0.966 | Acc: 75.738% (26466/34944)/ 92.863% (32450/34944)
01/15/2023 14:56:46 - INFO - __main__ -   test: [batch: 273/391 ] | Loss: 0.966 | Acc: 75.747% (26566/35072)/ 92.846% (32563/35072)
01/15/2023 14:56:48 - INFO - __main__ -   test: [batch: 274/391 ] | Loss: 0.966 | Acc: 75.750% (26664/35200)/ 92.852% (32684/35200)
01/15/2023 14:56:50 - INFO - __main__ -   test: [batch: 275/391 ] | Loss: 0.968 | Acc: 75.710% (26747/35328)/ 92.844% (32800/35328)
01/15/2023 14:56:52 - INFO - __main__ -   test: [batch: 276/391 ] | Loss: 0.969 | Acc: 75.663% (26827/35456)/ 92.836% (32916/35456)
01/15/2023 14:56:54 - INFO - __main__ -   test: [batch: 277/391 ] | Loss: 0.969 | Acc: 75.638% (26915/35584)/ 92.817% (33028/35584)
01/15/2023 14:56:57 - INFO - __main__ -   test: [batch: 278/391 ] | Loss: 0.969 | Acc: 75.652% (27017/35712)/ 92.815% (33146/35712)
01/15/2023 14:56:59 - INFO - __main__ -   test: [batch: 279/391 ] | Loss: 0.969 | Acc: 75.678% (27123/35840)/ 92.810% (33263/35840)
01/15/2023 14:57:01 - INFO - __main__ -   test: [batch: 280/391 ] | Loss: 0.970 | Acc: 75.653% (27211/35968)/ 92.805% (33380/35968)
01/15/2023 14:57:03 - INFO - __main__ -   test: [batch: 281/391 ] | Loss: 0.971 | Acc: 75.654% (27308/36096)/ 92.791% (33494/36096)
01/15/2023 14:57:05 - INFO - __main__ -   test: [batch: 282/391 ] | Loss: 0.970 | Acc: 75.690% (27418/36224)/ 92.795% (33614/36224)
01/15/2023 14:57:07 - INFO - __main__ -   test: [batch: 283/391 ] | Loss: 0.970 | Acc: 75.674% (27509/36352)/ 92.787% (33730/36352)
01/15/2023 14:57:09 - INFO - __main__ -   test: [batch: 284/391 ] | Loss: 0.974 | Acc: 75.620% (27586/36480)/ 92.736% (33830/36480)
01/15/2023 14:57:12 - INFO - __main__ -   test: [batch: 285/391 ] | Loss: 0.977 | Acc: 75.582% (27669/36608)/ 92.693% (33933/36608)
01/15/2023 14:57:14 - INFO - __main__ -   test: [batch: 286/391 ] | Loss: 0.978 | Acc: 75.544% (27752/36736)/ 92.675% (34045/36736)
01/15/2023 14:57:16 - INFO - __main__ -   test: [batch: 287/391 ] | Loss: 0.979 | Acc: 75.534% (27845/36864)/ 92.670% (34162/36864)
01/15/2023 14:57:18 - INFO - __main__ -   test: [batch: 288/391 ] | Loss: 0.978 | Acc: 75.554% (27949/36992)/ 92.677% (34283/36992)
01/15/2023 14:57:20 - INFO - __main__ -   test: [batch: 289/391 ] | Loss: 0.979 | Acc: 75.523% (28034/37120)/ 92.643% (34389/37120)
01/15/2023 14:57:22 - INFO - __main__ -   test: [batch: 290/391 ] | Loss: 0.981 | Acc: 75.435% (28098/37248)/ 92.639% (34506/37248)
01/15/2023 14:57:24 - INFO - __main__ -   test: [batch: 291/391 ] | Loss: 0.981 | Acc: 75.444% (28198/37376)/ 92.640% (34625/37376)
01/15/2023 14:57:26 - INFO - __main__ -   test: [batch: 292/391 ] | Loss: 0.983 | Acc: 75.384% (28272/37504)/ 92.617% (34735/37504)
01/15/2023 14:57:28 - INFO - __main__ -   test: [batch: 293/391 ] | Loss: 0.984 | Acc: 75.356% (28358/37632)/ 92.613% (34852/37632)
01/15/2023 14:57:31 - INFO - __main__ -   test: [batch: 294/391 ] | Loss: 0.985 | Acc: 75.336% (28447/37760)/ 92.601% (34966/37760)
01/15/2023 14:57:33 - INFO - __main__ -   test: [batch: 295/391 ] | Loss: 0.984 | Acc: 75.359% (28552/37888)/ 92.597% (35083/37888)
01/15/2023 14:57:35 - INFO - __main__ -   test: [batch: 296/391 ] | Loss: 0.985 | Acc: 75.350% (28645/38016)/ 92.593% (35200/38016)
01/15/2023 14:57:37 - INFO - __main__ -   test: [batch: 297/391 ] | Loss: 0.986 | Acc: 75.333% (28735/38144)/ 92.568% (35309/38144)
01/15/2023 14:57:39 - INFO - __main__ -   test: [batch: 298/391 ] | Loss: 0.989 | Acc: 75.285% (28813/38272)/ 92.551% (35421/38272)
01/15/2023 14:57:41 - INFO - __main__ -   test: [batch: 299/391 ] | Loss: 0.990 | Acc: 75.271% (28904/38400)/ 92.523% (35529/38400)
01/15/2023 14:57:43 - INFO - __main__ -   test: [batch: 300/391 ] | Loss: 0.990 | Acc: 75.286% (29006/38528)/ 92.528% (35649/38528)
01/15/2023 14:57:45 - INFO - __main__ -   test: [batch: 301/391 ] | Loss: 0.991 | Acc: 75.274% (29098/38656)/ 92.516% (35763/38656)
01/15/2023 14:57:47 - INFO - __main__ -   test: [batch: 302/391 ] | Loss: 0.993 | Acc: 75.242% (29182/38784)/ 92.497% (35874/38784)
01/15/2023 14:57:49 - INFO - __main__ -   test: [batch: 303/391 ] | Loss: 0.994 | Acc: 75.213% (29267/38912)/ 92.486% (35988/38912)
01/15/2023 14:57:52 - INFO - __main__ -   test: [batch: 304/391 ] | Loss: 0.993 | Acc: 75.228% (29369/39040)/ 92.492% (36109/39040)
01/15/2023 14:57:54 - INFO - __main__ -   test: [batch: 305/391 ] | Loss: 0.994 | Acc: 75.189% (29450/39168)/ 92.486% (36225/39168)
01/15/2023 14:57:56 - INFO - __main__ -   test: [batch: 306/391 ] | Loss: 0.995 | Acc: 75.188% (29546/39296)/ 92.460% (36333/39296)
01/15/2023 14:57:58 - INFO - __main__ -   test: [batch: 307/391 ] | Loss: 0.996 | Acc: 75.162% (29632/39424)/ 92.449% (36447/39424)
01/15/2023 14:58:00 - INFO - __main__ -   test: [batch: 308/391 ] | Loss: 0.997 | Acc: 75.152% (29724/39552)/ 92.440% (36562/39552)
01/15/2023 14:58:02 - INFO - __main__ -   test: [batch: 309/391 ] | Loss: 0.998 | Acc: 75.139% (29815/39680)/ 92.419% (36672/39680)
01/15/2023 14:58:04 - INFO - __main__ -   test: [batch: 310/391 ] | Loss: 0.999 | Acc: 75.128% (29907/39808)/ 92.404% (36784/39808)
01/15/2023 14:58:06 - INFO - __main__ -   test: [batch: 311/391 ] | Loss: 1.000 | Acc: 75.115% (29998/39936)/ 92.390% (36897/39936)
01/15/2023 14:58:08 - INFO - __main__ -   test: [batch: 312/391 ] | Loss: 1.001 | Acc: 75.102% (30089/40064)/ 92.375% (37009/40064)
01/15/2023 14:58:11 - INFO - __main__ -   test: [batch: 313/391 ] | Loss: 0.999 | Acc: 75.154% (30206/40192)/ 92.399% (37137/40192)
01/15/2023 14:58:13 - INFO - __main__ -   test: [batch: 314/391 ] | Loss: 1.000 | Acc: 75.144% (30298/40320)/ 92.391% (37252/40320)
01/15/2023 14:58:15 - INFO - __main__ -   test: [batch: 315/391 ] | Loss: 1.000 | Acc: 75.134% (30390/40448)/ 92.375% (37364/40448)
01/15/2023 14:58:17 - INFO - __main__ -   test: [batch: 316/391 ] | Loss: 1.003 | Acc: 75.039% (30448/40576)/ 92.355% (37474/40576)
01/15/2023 14:58:19 - INFO - __main__ -   test: [batch: 317/391 ] | Loss: 1.005 | Acc: 74.998% (30527/40704)/ 92.328% (37581/40704)
01/15/2023 14:58:21 - INFO - __main__ -   test: [batch: 318/391 ] | Loss: 1.004 | Acc: 75.029% (30636/40832)/ 92.347% (37707/40832)
01/15/2023 14:58:23 - INFO - __main__ -   test: [batch: 319/391 ] | Loss: 1.006 | Acc: 74.978% (30711/40960)/ 92.317% (37813/40960)
01/15/2023 14:58:26 - INFO - __main__ -   test: [batch: 320/391 ] | Loss: 1.005 | Acc: 75.017% (30823/41088)/ 92.326% (37935/41088)
01/15/2023 14:58:28 - INFO - __main__ -   test: [batch: 321/391 ] | Loss: 1.005 | Acc: 75.034% (30926/41216)/ 92.319% (38050/41216)
01/15/2023 14:58:30 - INFO - __main__ -   test: [batch: 322/391 ] | Loss: 1.006 | Acc: 74.995% (31006/41344)/ 92.306% (38163/41344)
01/15/2023 14:58:32 - INFO - __main__ -   test: [batch: 323/391 ] | Loss: 1.008 | Acc: 74.964% (31089/41472)/ 92.279% (38270/41472)
01/15/2023 14:58:34 - INFO - __main__ -   test: [batch: 324/391 ] | Loss: 1.009 | Acc: 74.969% (31187/41600)/ 92.272% (38385/41600)
01/15/2023 14:58:36 - INFO - __main__ -   test: [batch: 325/391 ] | Loss: 1.008 | Acc: 74.969% (31283/41728)/ 92.279% (38506/41728)
01/15/2023 14:58:38 - INFO - __main__ -   test: [batch: 326/391 ] | Loss: 1.012 | Acc: 74.885% (31344/41856)/ 92.221% (38600/41856)
01/15/2023 14:58:40 - INFO - __main__ -   test: [batch: 327/391 ] | Loss: 1.015 | Acc: 74.805% (31406/41984)/ 92.180% (38701/41984)
01/15/2023 14:58:43 - INFO - __main__ -   test: [batch: 328/391 ] | Loss: 1.018 | Acc: 74.751% (31479/42112)/ 92.145% (38804/42112)
01/15/2023 14:58:45 - INFO - __main__ -   test: [batch: 329/391 ] | Loss: 1.018 | Acc: 74.756% (31577/42240)/ 92.152% (38925/42240)
01/15/2023 14:58:47 - INFO - __main__ -   test: [batch: 330/391 ] | Loss: 1.019 | Acc: 74.721% (31658/42368)/ 92.131% (39034/42368)
01/15/2023 14:58:49 - INFO - __main__ -   test: [batch: 331/391 ] | Loss: 1.019 | Acc: 74.687% (31739/42496)/ 92.143% (39157/42496)
01/15/2023 14:58:51 - INFO - __main__ -   test: [batch: 332/391 ] | Loss: 1.020 | Acc: 74.655% (31821/42624)/ 92.134% (39271/42624)
01/15/2023 14:58:54 - INFO - __main__ -   test: [batch: 333/391 ] | Loss: 1.019 | Acc: 74.689% (31931/42752)/ 92.141% (39392/42752)
01/15/2023 14:58:56 - INFO - __main__ -   test: [batch: 334/391 ] | Loss: 1.020 | Acc: 74.650% (32010/42880)/ 92.134% (39507/42880)
01/15/2023 14:58:58 - INFO - __main__ -   test: [batch: 335/391 ] | Loss: 1.022 | Acc: 74.621% (32093/43008)/ 92.113% (39616/43008)
01/15/2023 14:59:00 - INFO - __main__ -   test: [batch: 336/391 ] | Loss: 1.023 | Acc: 74.580% (32171/43136)/ 92.095% (39726/43136)
01/15/2023 14:59:02 - INFO - __main__ -   test: [batch: 337/391 ] | Loss: 1.024 | Acc: 74.577% (32265/43264)/ 92.088% (39841/43264)
01/15/2023 14:59:04 - INFO - __main__ -   test: [batch: 338/391 ] | Loss: 1.023 | Acc: 74.571% (32358/43392)/ 92.102% (39965/43392)
01/15/2023 14:59:06 - INFO - __main__ -   test: [batch: 339/391 ] | Loss: 1.026 | Acc: 74.534% (32437/43520)/ 92.070% (40069/43520)
01/15/2023 14:59:09 - INFO - __main__ -   test: [batch: 340/391 ] | Loss: 1.025 | Acc: 74.530% (32531/43648)/ 92.082% (40192/43648)
01/15/2023 14:59:11 - INFO - __main__ -   test: [batch: 341/391 ] | Loss: 1.024 | Acc: 74.573% (32645/43776)/ 92.101% (40318/43776)
01/15/2023 14:59:13 - INFO - __main__ -   test: [batch: 342/391 ] | Loss: 1.024 | Acc: 74.522% (32718/43904)/ 92.090% (40431/43904)
01/15/2023 14:59:15 - INFO - __main__ -   test: [batch: 343/391 ] | Loss: 1.024 | Acc: 74.514% (32810/44032)/ 92.092% (40550/44032)
01/15/2023 14:59:17 - INFO - __main__ -   test: [batch: 344/391 ] | Loss: 1.025 | Acc: 74.520% (32908/44160)/ 92.083% (40664/44160)
01/15/2023 14:59:19 - INFO - __main__ -   test: [batch: 345/391 ] | Loss: 1.028 | Acc: 74.451% (32973/44288)/ 92.043% (40764/44288)
01/15/2023 14:59:21 - INFO - __main__ -   test: [batch: 346/391 ] | Loss: 1.030 | Acc: 74.433% (33060/44416)/ 92.037% (40879/44416)
01/15/2023 14:59:24 - INFO - __main__ -   test: [batch: 347/391 ] | Loss: 1.029 | Acc: 74.443% (33160/44544)/ 92.039% (40998/44544)
01/15/2023 14:59:26 - INFO - __main__ -   test: [batch: 348/391 ] | Loss: 1.031 | Acc: 74.416% (33243/44672)/ 92.024% (41109/44672)
01/15/2023 14:59:28 - INFO - __main__ -   test: [batch: 349/391 ] | Loss: 1.030 | Acc: 74.415% (33338/44800)/ 92.038% (41233/44800)
01/15/2023 14:59:30 - INFO - __main__ -   test: [batch: 350/391 ] | Loss: 1.030 | Acc: 74.424% (33437/44928)/ 92.034% (41349/44928)
01/15/2023 14:59:32 - INFO - __main__ -   test: [batch: 351/391 ] | Loss: 1.033 | Acc: 74.363% (33505/45056)/ 92.012% (41457/45056)
01/15/2023 14:59:34 - INFO - __main__ -   test: [batch: 352/391 ] | Loss: 1.033 | Acc: 74.360% (33599/45184)/ 92.010% (41574/45184)
01/15/2023 14:59:36 - INFO - __main__ -   test: [batch: 353/391 ] | Loss: 1.036 | Acc: 74.314% (33673/45312)/ 91.967% (41672/45312)
01/15/2023 14:59:38 - INFO - __main__ -   test: [batch: 354/391 ] | Loss: 1.038 | Acc: 74.256% (33742/45440)/ 91.954% (41784/45440)
01/15/2023 14:59:40 - INFO - __main__ -   test: [batch: 355/391 ] | Loss: 1.041 | Acc: 74.197% (33810/45568)/ 91.942% (41896/45568)
01/15/2023 14:59:42 - INFO - __main__ -   test: [batch: 356/391 ] | Loss: 1.041 | Acc: 74.188% (33901/45696)/ 91.945% (42015/45696)
01/15/2023 14:59:44 - INFO - __main__ -   test: [batch: 357/391 ] | Loss: 1.039 | Acc: 74.221% (34011/45824)/ 91.958% (42139/45824)
01/15/2023 14:59:47 - INFO - __main__ -   test: [batch: 358/391 ] | Loss: 1.039 | Acc: 74.249% (34119/45952)/ 91.963% (42259/45952)
01/15/2023 14:59:49 - INFO - __main__ -   test: [batch: 359/391 ] | Loss: 1.039 | Acc: 74.256% (34217/46080)/ 91.960% (42375/46080)
01/15/2023 14:59:51 - INFO - __main__ -   test: [batch: 360/391 ] | Loss: 1.041 | Acc: 74.214% (34293/46208)/ 91.960% (42493/46208)
01/15/2023 14:59:53 - INFO - __main__ -   test: [batch: 361/391 ] | Loss: 1.041 | Acc: 74.217% (34389/46336)/ 91.961% (42611/46336)
01/15/2023 14:59:55 - INFO - __main__ -   test: [batch: 362/391 ] | Loss: 1.040 | Acc: 74.219% (34485/46464)/ 91.972% (42734/46464)
01/15/2023 14:59:57 - INFO - __main__ -   test: [batch: 363/391 ] | Loss: 1.041 | Acc: 74.199% (34571/46592)/ 91.960% (42846/46592)
01/15/2023 14:59:59 - INFO - __main__ -   test: [batch: 364/391 ] | Loss: 1.040 | Acc: 74.225% (34678/46720)/ 91.969% (42968/46720)
01/15/2023 15:00:01 - INFO - __main__ -   test: [batch: 365/391 ] | Loss: 1.040 | Acc: 74.229% (34775/46848)/ 91.970% (43086/46848)
01/15/2023 15:00:04 - INFO - __main__ -   test: [batch: 366/391 ] | Loss: 1.038 | Acc: 74.266% (34887/46976)/ 91.985% (43211/46976)
01/15/2023 15:00:06 - INFO - __main__ -   test: [batch: 367/391 ] | Loss: 1.037 | Acc: 74.278% (34988/47104)/ 91.996% (43334/47104)
01/15/2023 15:00:08 - INFO - __main__ -   test: [batch: 368/391 ] | Loss: 1.037 | Acc: 74.272% (35080/47232)/ 92.005% (43456/47232)
01/15/2023 15:00:09 - INFO - __main__ -   test: [batch: 369/391 ] | Loss: 1.036 | Acc: 74.293% (35185/47360)/ 92.016% (43579/47360)
01/15/2023 15:00:12 - INFO - __main__ -   test: [batch: 370/391 ] | Loss: 1.037 | Acc: 74.265% (35267/47488)/ 92.019% (43698/47488)
01/15/2023 15:00:14 - INFO - __main__ -   test: [batch: 371/391 ] | Loss: 1.037 | Acc: 74.280% (35369/47616)/ 92.017% (43815/47616)
01/15/2023 15:00:16 - INFO - __main__ -   test: [batch: 372/391 ] | Loss: 1.035 | Acc: 74.323% (35485/47744)/ 92.035% (43941/47744)
01/15/2023 15:00:18 - INFO - __main__ -   test: [batch: 373/391 ] | Loss: 1.033 | Acc: 74.361% (35598/47872)/ 92.050% (44066/47872)
01/15/2023 15:00:20 - INFO - __main__ -   test: [batch: 374/391 ] | Loss: 1.033 | Acc: 74.385% (35705/48000)/ 92.048% (44183/48000)
01/15/2023 15:00:22 - INFO - __main__ -   test: [batch: 375/391 ] | Loss: 1.035 | Acc: 74.331% (35774/48128)/ 92.015% (44285/48128)
01/15/2023 15:00:25 - INFO - __main__ -   test: [batch: 376/391 ] | Loss: 1.036 | Acc: 74.314% (35861/48256)/ 91.997% (44394/48256)
01/15/2023 15:00:27 - INFO - __main__ -   test: [batch: 377/391 ] | Loss: 1.037 | Acc: 74.310% (35954/48384)/ 91.989% (44508/48384)
01/15/2023 15:00:29 - INFO - __main__ -   test: [batch: 378/391 ] | Loss: 1.040 | Acc: 74.241% (36016/48512)/ 91.944% (44604/48512)
01/15/2023 15:00:31 - INFO - __main__ -   test: [batch: 379/391 ] | Loss: 1.040 | Acc: 74.229% (36105/48640)/ 91.955% (44727/48640)
01/15/2023 15:00:33 - INFO - __main__ -   test: [batch: 380/391 ] | Loss: 1.040 | Acc: 74.239% (36205/48768)/ 91.972% (44853/48768)
01/15/2023 15:00:35 - INFO - __main__ -   test: [batch: 381/391 ] | Loss: 1.041 | Acc: 74.196% (36279/48896)/ 91.971% (44970/48896)
01/15/2023 15:00:37 - INFO - __main__ -   test: [batch: 382/391 ] | Loss: 1.042 | Acc: 74.164% (36358/49024)/ 91.959% (45082/49024)
01/15/2023 15:00:39 - INFO - __main__ -   test: [batch: 383/391 ] | Loss: 1.043 | Acc: 74.170% (36456/49152)/ 91.949% (45195/49152)
01/15/2023 15:00:41 - INFO - __main__ -   test: [batch: 384/391 ] | Loss: 1.041 | Acc: 74.207% (36569/49280)/ 91.964% (45320/49280)
01/15/2023 15:00:44 - INFO - __main__ -   test: [batch: 385/391 ] | Loss: 1.039 | Acc: 74.221% (36671/49408)/ 91.979% (45445/49408)
01/15/2023 15:00:46 - INFO - __main__ -   test: [batch: 386/391 ] | Loss: 1.037 | Acc: 74.259% (36785/49536)/ 91.998% (45572/49536)
01/15/2023 15:00:48 - INFO - __main__ -   test: [batch: 387/391 ] | Loss: 1.036 | Acc: 74.303% (36902/49664)/ 92.008% (45695/49664)
01/15/2023 15:00:50 - INFO - __main__ -   test: [batch: 388/391 ] | Loss: 1.034 | Acc: 74.355% (37023/49792)/ 92.023% (45820/49792)
01/15/2023 15:00:52 - INFO - __main__ -   test: [batch: 389/391 ] | Loss: 1.033 | Acc: 74.355% (37118/49920)/ 92.029% (45941/49920)
01/15/2023 15:00:54 - INFO - __main__ -   test: [batch: 390/391 ] | Loss: 1.035 | Acc: 74.314% (37157/50000)/ 92.018% (46009/50000)
01/15/2023 15:00:54 - INFO - __main__ -   Final accuracy: 74.314
01/15/2023 15:00:54 - INFO - __main__ -   {'milestones': Counter({2: 1, 3: 1, 4: 1}), 'gamma': 0.1, 'base_lrs': [0.00025], 'last_epoch': 2, '_step_count': 3, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [2.5e-05]}
01/15/2023 15:00:54 - INFO - __main__ -   
Epoch: 2
01/15/2023 15:00:56 - INFO - __main__ -   test: [epoch: 2 | batch: 0/10010 ] | Loss: 1.100 | Acc: 75.000% (96/128)
01/15/2023 15:04:31 - INFO - __main__ -   test: [epoch: 2 | batch: 100/10010 ] | Loss: 0.931 | Acc: 77.429% (10010/12928)
01/15/2023 15:08:02 - INFO - __main__ -   test: [epoch: 2 | batch: 200/10010 ] | Loss: 0.938 | Acc: 76.873% (19778/25728)
01/15/2023 15:11:33 - INFO - __main__ -   test: [epoch: 2 | batch: 300/10010 ] | Loss: 0.943 | Acc: 76.783% (29583/38528)
01/15/2023 15:15:05 - INFO - __main__ -   test: [epoch: 2 | batch: 400/10010 ] | Loss: 0.950 | Acc: 76.631% (39333/51328)
01/15/2023 15:18:36 - INFO - __main__ -   test: [epoch: 2 | batch: 500/10010 ] | Loss: 0.956 | Acc: 76.502% (49059/64128)
01/15/2023 15:22:08 - INFO - __main__ -   test: [epoch: 2 | batch: 600/10010 ] | Loss: 0.958 | Acc: 76.414% (58784/76928)
01/15/2023 15:25:41 - INFO - __main__ -   test: [epoch: 2 | batch: 700/10010 ] | Loss: 0.959 | Acc: 76.369% (68524/89728)
01/15/2023 15:29:12 - INFO - __main__ -   test: [epoch: 2 | batch: 800/10010 ] | Loss: 0.961 | Acc: 76.325% (78254/102528)
01/15/2023 15:32:42 - INFO - __main__ -   test: [epoch: 2 | batch: 900/10010 ] | Loss: 0.959 | Acc: 76.360% (88065/115328)
01/15/2023 15:36:15 - INFO - __main__ -   test: [epoch: 2 | batch: 1000/10010 ] | Loss: 0.960 | Acc: 76.331% (97801/128128)
01/15/2023 15:39:45 - INFO - __main__ -   test: [epoch: 2 | batch: 1100/10010 ] | Loss: 0.962 | Acc: 76.287% (107510/140928)
01/15/2023 15:43:17 - INFO - __main__ -   test: [epoch: 2 | batch: 1200/10010 ] | Loss: 0.963 | Acc: 76.282% (117267/153728)
01/15/2023 15:46:48 - INFO - __main__ -   test: [epoch: 2 | batch: 1300/10010 ] | Loss: 0.962 | Acc: 76.313% (127083/166528)
01/15/2023 15:50:20 - INFO - __main__ -   test: [epoch: 2 | batch: 1400/10010 ] | Loss: 0.962 | Acc: 76.338% (136895/179328)
01/15/2023 15:53:51 - INFO - __main__ -   test: [epoch: 2 | batch: 1500/10010 ] | Loss: 0.962 | Acc: 76.316% (146625/192128)
01/15/2023 15:57:22 - INFO - __main__ -   test: [epoch: 2 | batch: 1600/10010 ] | Loss: 0.963 | Acc: 76.295% (156350/204928)
01/15/2023 16:00:53 - INFO - __main__ -   test: [epoch: 2 | batch: 1700/10010 ] | Loss: 0.963 | Acc: 76.297% (166121/217728)
01/15/2023 16:04:24 - INFO - __main__ -   test: [epoch: 2 | batch: 1800/10010 ] | Loss: 0.962 | Acc: 76.317% (175933/230528)
01/15/2023 16:07:57 - INFO - __main__ -   test: [epoch: 2 | batch: 1900/10010 ] | Loss: 0.961 | Acc: 76.336% (185748/243328)
01/15/2023 16:11:27 - INFO - __main__ -   test: [epoch: 2 | batch: 2000/10010 ] | Loss: 0.960 | Acc: 76.352% (195558/256128)
01/15/2023 16:14:59 - INFO - __main__ -   test: [epoch: 2 | batch: 2100/10010 ] | Loss: 0.960 | Acc: 76.346% (205315/268928)
01/15/2023 16:18:30 - INFO - __main__ -   test: [epoch: 2 | batch: 2200/10010 ] | Loss: 0.961 | Acc: 76.326% (215033/281728)
01/15/2023 16:22:02 - INFO - __main__ -   test: [epoch: 2 | batch: 2300/10010 ] | Loss: 0.962 | Acc: 76.306% (224742/294528)
01/15/2023 16:25:33 - INFO - __main__ -   test: [epoch: 2 | batch: 2400/10010 ] | Loss: 0.962 | Acc: 76.309% (234520/307328)
01/15/2023 16:29:03 - INFO - __main__ -   test: [epoch: 2 | batch: 2500/10010 ] | Loss: 0.962 | Acc: 76.300% (244258/320128)
01/15/2023 16:32:34 - INFO - __main__ -   test: [epoch: 2 | batch: 2600/10010 ] | Loss: 0.962 | Acc: 76.303% (254034/332928)
01/15/2023 16:36:05 - INFO - __main__ -   test: [epoch: 2 | batch: 2700/10010 ] | Loss: 0.961 | Acc: 76.334% (263908/345728)
01/15/2023 16:39:36 - INFO - __main__ -   test: [epoch: 2 | batch: 2800/10010 ] | Loss: 0.962 | Acc: 76.326% (273651/358528)
01/15/2023 16:43:09 - INFO - __main__ -   test: [epoch: 2 | batch: 2900/10010 ] | Loss: 0.961 | Acc: 76.323% (283408/371328)
01/15/2023 16:46:40 - INFO - __main__ -   test: [epoch: 2 | batch: 3000/10010 ] | Loss: 0.961 | Acc: 76.324% (293181/384128)
01/15/2023 16:50:11 - INFO - __main__ -   test: [epoch: 2 | batch: 3100/10010 ] | Loss: 0.961 | Acc: 76.339% (303009/396928)
01/15/2023 16:53:43 - INFO - __main__ -   test: [epoch: 2 | batch: 3200/10010 ] | Loss: 0.961 | Acc: 76.330% (312745/409728)
01/15/2023 16:57:14 - INFO - __main__ -   test: [epoch: 2 | batch: 3300/10010 ] | Loss: 0.962 | Acc: 76.317% (322459/422528)
01/15/2023 17:00:45 - INFO - __main__ -   test: [epoch: 2 | batch: 3400/10010 ] | Loss: 0.961 | Acc: 76.328% (332276/435328)
01/15/2023 17:04:17 - INFO - __main__ -   test: [epoch: 2 | batch: 3500/10010 ] | Loss: 0.961 | Acc: 76.328% (342048/448128)
01/15/2023 17:07:50 - INFO - __main__ -   test: [epoch: 2 | batch: 3600/10010 ] | Loss: 0.961 | Acc: 76.341% (351879/460928)
01/15/2023 17:11:22 - INFO - __main__ -   test: [epoch: 2 | batch: 3700/10010 ] | Loss: 0.961 | Acc: 76.332% (361608/473728)
01/15/2023 17:14:52 - INFO - __main__ -   test: [epoch: 2 | batch: 3800/10010 ] | Loss: 0.960 | Acc: 76.359% (371509/486528)
01/15/2023 17:18:25 - INFO - __main__ -   test: [epoch: 2 | batch: 3900/10010 ] | Loss: 0.960 | Acc: 76.346% (381217/499328)
01/15/2023 17:21:55 - INFO - __main__ -   test: [epoch: 2 | batch: 4000/10010 ] | Loss: 0.961 | Acc: 76.330% (390907/512128)
01/15/2023 17:25:26 - INFO - __main__ -   test: [epoch: 2 | batch: 4100/10010 ] | Loss: 0.961 | Acc: 76.332% (400689/524928)
01/15/2023 17:28:57 - INFO - __main__ -   test: [epoch: 2 | batch: 4200/10010 ] | Loss: 0.961 | Acc: 76.330% (410450/537728)
01/15/2023 17:32:28 - INFO - __main__ -   test: [epoch: 2 | batch: 4300/10010 ] | Loss: 0.961 | Acc: 76.331% (420221/550528)
01/15/2023 17:36:01 - INFO - __main__ -   test: [epoch: 2 | batch: 4400/10010 ] | Loss: 0.961 | Acc: 76.342% (430058/563328)
01/15/2023 17:39:33 - INFO - __main__ -   test: [epoch: 2 | batch: 4500/10010 ] | Loss: 0.961 | Acc: 76.336% (439791/576128)
01/15/2023 17:43:05 - INFO - __main__ -   test: [epoch: 2 | batch: 4600/10010 ] | Loss: 0.961 | Acc: 76.330% (449530/588928)
01/15/2023 17:46:39 - INFO - __main__ -   test: [epoch: 2 | batch: 4700/10010 ] | Loss: 0.961 | Acc: 76.332% (459314/601728)
01/15/2023 17:50:09 - INFO - __main__ -   test: [epoch: 2 | batch: 4800/10010 ] | Loss: 0.961 | Acc: 76.338% (469119/614528)
01/15/2023 17:53:41 - INFO - __main__ -   test: [epoch: 2 | batch: 4900/10010 ] | Loss: 0.961 | Acc: 76.338% (478888/627328)
01/15/2023 17:57:12 - INFO - __main__ -   test: [epoch: 2 | batch: 5000/10010 ] | Loss: 0.961 | Acc: 76.348% (488724/640128)
01/15/2023 18:00:44 - INFO - __main__ -   test: [epoch: 2 | batch: 5100/10010 ] | Loss: 0.961 | Acc: 76.340% (498443/652928)
01/15/2023 18:04:14 - INFO - __main__ -   test: [epoch: 2 | batch: 5200/10010 ] | Loss: 0.961 | Acc: 76.332% (508162/665728)
01/15/2023 18:07:46 - INFO - __main__ -   test: [epoch: 2 | batch: 5300/10010 ] | Loss: 0.961 | Acc: 76.327% (517902/678528)
01/15/2023 18:11:18 - INFO - __main__ -   test: [epoch: 2 | batch: 5400/10010 ] | Loss: 0.962 | Acc: 76.323% (527641/691328)
01/15/2023 18:14:49 - INFO - __main__ -   test: [epoch: 2 | batch: 5500/10010 ] | Loss: 0.962 | Acc: 76.327% (537438/704128)
01/15/2023 18:18:21 - INFO - __main__ -   test: [epoch: 2 | batch: 5600/10010 ] | Loss: 0.962 | Acc: 76.331% (547236/716928)
01/15/2023 18:21:53 - INFO - __main__ -   test: [epoch: 2 | batch: 5700/10010 ] | Loss: 0.962 | Acc: 76.332% (557017/729728)
01/15/2023 18:25:25 - INFO - __main__ -   test: [epoch: 2 | batch: 5800/10010 ] | Loss: 0.961 | Acc: 76.333% (566796/742528)
01/15/2023 18:28:56 - INFO - __main__ -   test: [epoch: 2 | batch: 5900/10010 ] | Loss: 0.961 | Acc: 76.339% (576609/755328)
01/15/2023 18:32:28 - INFO - __main__ -   test: [epoch: 2 | batch: 6000/10010 ] | Loss: 0.961 | Acc: 76.336% (586359/768128)
01/15/2023 18:36:00 - INFO - __main__ -   test: [epoch: 2 | batch: 6100/10010 ] | Loss: 0.961 | Acc: 76.323% (596031/780928)
01/15/2023 18:39:31 - INFO - __main__ -   test: [epoch: 2 | batch: 6200/10010 ] | Loss: 0.962 | Acc: 76.326% (605822/793728)
01/15/2023 18:43:02 - INFO - __main__ -   test: [epoch: 2 | batch: 6300/10010 ] | Loss: 0.961 | Acc: 76.327% (615597/806528)
01/15/2023 18:46:32 - INFO - __main__ -   test: [epoch: 2 | batch: 6400/10010 ] | Loss: 0.962 | Acc: 76.322% (625327/819328)
01/15/2023 18:50:03 - INFO - __main__ -   test: [epoch: 2 | batch: 6500/10010 ] | Loss: 0.961 | Acc: 76.332% (635176/832128)
01/15/2023 18:53:35 - INFO - __main__ -   test: [epoch: 2 | batch: 6600/10010 ] | Loss: 0.962 | Acc: 76.329% (644921/844928)
01/15/2023 18:57:08 - INFO - __main__ -   test: [epoch: 2 | batch: 6700/10010 ] | Loss: 0.962 | Acc: 76.323% (654640/857728)
01/15/2023 19:00:41 - INFO - __main__ -   test: [epoch: 2 | batch: 6800/10010 ] | Loss: 0.962 | Acc: 76.319% (664378/870528)
01/15/2023 19:04:12 - INFO - __main__ -   test: [epoch: 2 | batch: 6900/10010 ] | Loss: 0.961 | Acc: 76.316% (674122/883328)
01/15/2023 19:07:43 - INFO - __main__ -   test: [epoch: 2 | batch: 7000/10010 ] | Loss: 0.962 | Acc: 76.309% (683823/896128)
01/15/2023 19:11:14 - INFO - __main__ -   test: [epoch: 2 | batch: 7100/10010 ] | Loss: 0.961 | Acc: 76.310% (693607/908928)
01/15/2023 19:14:43 - INFO - __main__ -   test: [epoch: 2 | batch: 7200/10010 ] | Loss: 0.961 | Acc: 76.315% (703421/921728)
01/15/2023 19:18:15 - INFO - __main__ -   test: [epoch: 2 | batch: 7300/10010 ] | Loss: 0.961 | Acc: 76.312% (713160/934528)
01/15/2023 19:21:47 - INFO - __main__ -   test: [epoch: 2 | batch: 7400/10010 ] | Loss: 0.961 | Acc: 76.317% (722970/947328)
01/15/2023 19:25:17 - INFO - __main__ -   test: [epoch: 2 | batch: 7500/10010 ] | Loss: 0.961 | Acc: 76.320% (732769/960128)
01/15/2023 19:28:49 - INFO - __main__ -   test: [epoch: 2 | batch: 7600/10010 ] | Loss: 0.961 | Acc: 76.325% (742583/972928)
01/15/2023 19:32:22 - INFO - __main__ -   test: [epoch: 2 | batch: 7700/10010 ] | Loss: 0.961 | Acc: 76.321% (752316/985728)
01/15/2023 19:35:53 - INFO - __main__ -   test: [epoch: 2 | batch: 7800/10010 ] | Loss: 0.961 | Acc: 76.312% (762001/998528)
01/15/2023 19:39:24 - INFO - __main__ -   test: [epoch: 2 | batch: 7900/10010 ] | Loss: 0.961 | Acc: 76.316% (771802/1011328)
01/15/2023 19:42:56 - INFO - __main__ -   test: [epoch: 2 | batch: 8000/10010 ] | Loss: 0.962 | Acc: 76.312% (781536/1024128)
01/15/2023 19:46:27 - INFO - __main__ -   test: [epoch: 2 | batch: 8100/10010 ] | Loss: 0.961 | Acc: 76.319% (791371/1036928)
01/15/2023 19:50:00 - INFO - __main__ -   test: [epoch: 2 | batch: 8200/10010 ] | Loss: 0.961 | Acc: 76.318% (801132/1049728)
01/15/2023 19:53:30 - INFO - __main__ -   test: [epoch: 2 | batch: 8300/10010 ] | Loss: 0.961 | Acc: 76.314% (810854/1062528)
01/15/2023 19:57:01 - INFO - __main__ -   test: [epoch: 2 | batch: 8400/10010 ] | Loss: 0.961 | Acc: 76.314% (820629/1075328)
01/15/2023 20:00:33 - INFO - __main__ -   test: [epoch: 2 | batch: 8500/10010 ] | Loss: 0.961 | Acc: 76.320% (830458/1088128)
01/15/2023 20:04:05 - INFO - __main__ -   test: [epoch: 2 | batch: 8600/10010 ] | Loss: 0.961 | Acc: 76.323% (840257/1100928)
01/15/2023 20:07:37 - INFO - __main__ -   test: [epoch: 2 | batch: 8700/10010 ] | Loss: 0.961 | Acc: 76.322% (850019/1113728)
01/15/2023 20:11:09 - INFO - __main__ -   test: [epoch: 2 | batch: 8800/10010 ] | Loss: 0.961 | Acc: 76.331% (859885/1126528)
01/15/2023 20:14:42 - INFO - __main__ -   test: [epoch: 2 | batch: 8900/10010 ] | Loss: 0.961 | Acc: 76.331% (869666/1139328)
01/15/2023 20:18:12 - INFO - __main__ -   test: [epoch: 2 | batch: 9000/10010 ] | Loss: 0.961 | Acc: 76.325% (879363/1152128)
01/15/2023 20:21:43 - INFO - __main__ -   test: [epoch: 2 | batch: 9100/10010 ] | Loss: 0.961 | Acc: 76.320% (889077/1164928)
01/15/2023 20:25:15 - INFO - __main__ -   test: [epoch: 2 | batch: 9200/10010 ] | Loss: 0.961 | Acc: 76.323% (898883/1177728)
01/15/2023 20:28:46 - INFO - __main__ -   test: [epoch: 2 | batch: 9300/10010 ] | Loss: 0.961 | Acc: 76.318% (908586/1190528)
01/15/2023 20:32:17 - INFO - __main__ -   test: [epoch: 2 | batch: 9400/10010 ] | Loss: 0.961 | Acc: 76.317% (918347/1203328)
01/15/2023 20:35:49 - INFO - __main__ -   test: [epoch: 2 | batch: 9500/10010 ] | Loss: 0.961 | Acc: 76.318% (928123/1216128)
01/15/2023 20:39:21 - INFO - __main__ -   test: [epoch: 2 | batch: 9600/10010 ] | Loss: 0.961 | Acc: 76.317% (937885/1228928)
01/15/2023 20:42:54 - INFO - __main__ -   test: [epoch: 2 | batch: 9700/10010 ] | Loss: 0.961 | Acc: 76.313% (947597/1241728)
01/15/2023 20:46:27 - INFO - __main__ -   test: [epoch: 2 | batch: 9800/10010 ] | Loss: 0.961 | Acc: 76.310% (957335/1254528)
01/15/2023 20:49:59 - INFO - __main__ -   test: [epoch: 2 | batch: 9900/10010 ] | Loss: 0.961 | Acc: 76.313% (967137/1267328)
01/15/2023 20:53:17 - INFO - __main__ -   test: [epoch: 2 | batch: 10000/10010 ] | Loss: 0.962 | Acc: 76.310% (976860/1280128)
01/15/2023 20:53:30 - INFO - __main__ -   Saving Checkpoint
01/15/2023 20:53:32 - INFO - __main__ -   test: [batch: 0/391 ] | Loss: 0.547 | Acc: 86.719% (111/128)/ 95.312% (122/128)
01/15/2023 20:53:33 - INFO - __main__ -   test: [batch: 1/391 ] | Loss: 0.580 | Acc: 85.156% (218/256)/ 96.875% (248/256)
01/15/2023 20:53:35 - INFO - __main__ -   test: [batch: 2/391 ] | Loss: 0.781 | Acc: 79.427% (305/384)/ 94.010% (361/384)
01/15/2023 20:53:36 - INFO - __main__ -   test: [batch: 3/391 ] | Loss: 0.717 | Acc: 82.031% (420/512)/ 94.727% (485/512)
01/15/2023 20:53:38 - INFO - __main__ -   test: [batch: 4/391 ] | Loss: 0.618 | Acc: 84.688% (542/640)/ 95.469% (611/640)
01/15/2023 20:53:39 - INFO - __main__ -   test: [batch: 5/391 ] | Loss: 0.544 | Acc: 86.198% (662/768)/ 96.224% (739/768)
01/15/2023 20:53:41 - INFO - __main__ -   test: [batch: 6/391 ] | Loss: 0.529 | Acc: 86.719% (777/896)/ 96.317% (863/896)
01/15/2023 20:53:42 - INFO - __main__ -   test: [batch: 7/391 ] | Loss: 0.503 | Acc: 87.695% (898/1024)/ 96.484% (988/1024)
01/15/2023 20:53:44 - INFO - __main__ -   test: [batch: 8/391 ] | Loss: 0.509 | Acc: 87.760% (1011/1152)/ 96.528% (1112/1152)
01/15/2023 20:53:46 - INFO - __main__ -   test: [batch: 9/391 ] | Loss: 0.484 | Acc: 88.359% (1131/1280)/ 96.719% (1238/1280)
01/15/2023 20:53:47 - INFO - __main__ -   test: [batch: 10/391 ] | Loss: 0.526 | Acc: 87.074% (1226/1408)/ 96.733% (1362/1408)
01/15/2023 20:53:49 - INFO - __main__ -   test: [batch: 11/391 ] | Loss: 0.530 | Acc: 87.044% (1337/1536)/ 96.549% (1483/1536)
01/15/2023 20:53:50 - INFO - __main__ -   test: [batch: 12/391 ] | Loss: 0.578 | Acc: 85.817% (1428/1664)/ 96.094% (1599/1664)
01/15/2023 20:53:52 - INFO - __main__ -   test: [batch: 13/391 ] | Loss: 0.633 | Acc: 84.152% (1508/1792)/ 95.312% (1708/1792)
01/15/2023 20:53:53 - INFO - __main__ -   test: [batch: 14/391 ] | Loss: 0.649 | Acc: 83.333% (1600/1920)/ 95.521% (1834/1920)
01/15/2023 20:53:55 - INFO - __main__ -   test: [batch: 15/391 ] | Loss: 0.657 | Acc: 82.764% (1695/2048)/ 95.508% (1956/2048)
01/15/2023 20:53:56 - INFO - __main__ -   test: [batch: 16/391 ] | Loss: 0.662 | Acc: 82.675% (1799/2176)/ 95.358% (2075/2176)
01/15/2023 20:53:58 - INFO - __main__ -   test: [batch: 17/391 ] | Loss: 0.685 | Acc: 82.248% (1895/2304)/ 94.965% (2188/2304)
01/15/2023 20:53:59 - INFO - __main__ -   test: [batch: 18/391 ] | Loss: 0.709 | Acc: 81.702% (1987/2432)/ 94.819% (2306/2432)
01/15/2023 20:54:01 - INFO - __main__ -   test: [batch: 19/391 ] | Loss: 0.717 | Acc: 81.484% (2086/2560)/ 94.688% (2424/2560)
01/15/2023 20:54:02 - INFO - __main__ -   test: [batch: 20/391 ] | Loss: 0.712 | Acc: 81.585% (2193/2688)/ 94.680% (2545/2688)
01/15/2023 20:54:03 - INFO - __main__ -   test: [batch: 21/391 ] | Loss: 0.748 | Acc: 80.646% (2271/2816)/ 94.496% (2661/2816)
01/15/2023 20:54:04 - INFO - __main__ -   test: [batch: 22/391 ] | Loss: 0.748 | Acc: 80.503% (2370/2944)/ 94.497% (2782/2944)
01/15/2023 20:54:06 - INFO - __main__ -   test: [batch: 23/391 ] | Loss: 0.790 | Acc: 79.720% (2449/3072)/ 94.173% (2893/3072)
01/15/2023 20:54:08 - INFO - __main__ -   test: [batch: 24/391 ] | Loss: 0.808 | Acc: 79.219% (2535/3200)/ 94.062% (3010/3200)
01/15/2023 20:54:09 - INFO - __main__ -   test: [batch: 25/391 ] | Loss: 0.833 | Acc: 78.546% (2614/3328)/ 93.840% (3123/3328)
01/15/2023 20:54:11 - INFO - __main__ -   test: [batch: 26/391 ] | Loss: 0.849 | Acc: 77.865% (2691/3456)/ 93.866% (3244/3456)
01/15/2023 20:54:12 - INFO - __main__ -   test: [batch: 27/391 ] | Loss: 0.830 | Acc: 78.376% (2809/3584)/ 93.917% (3366/3584)
01/15/2023 20:54:14 - INFO - __main__ -   test: [batch: 28/391 ] | Loss: 0.837 | Acc: 77.829% (2889/3712)/ 94.046% (3491/3712)
01/15/2023 20:54:15 - INFO - __main__ -   test: [batch: 29/391 ] | Loss: 0.828 | Acc: 77.995% (2995/3840)/ 94.193% (3617/3840)
01/15/2023 20:54:17 - INFO - __main__ -   test: [batch: 30/391 ] | Loss: 0.839 | Acc: 77.898% (3091/3968)/ 94.052% (3732/3968)
01/15/2023 20:54:18 - INFO - __main__ -   test: [batch: 31/391 ] | Loss: 0.833 | Acc: 78.174% (3202/4096)/ 94.165% (3857/4096)
01/15/2023 20:54:19 - INFO - __main__ -   test: [batch: 32/391 ] | Loss: 0.818 | Acc: 78.504% (3316/4224)/ 94.271% (3982/4224)
01/15/2023 20:54:20 - INFO - __main__ -   test: [batch: 33/391 ] | Loss: 0.812 | Acc: 78.676% (3424/4352)/ 94.324% (4105/4352)
01/15/2023 20:54:22 - INFO - __main__ -   test: [batch: 34/391 ] | Loss: 0.799 | Acc: 79.107% (3544/4480)/ 94.375% (4228/4480)
01/15/2023 20:54:23 - INFO - __main__ -   test: [batch: 35/391 ] | Loss: 0.786 | Acc: 79.470% (3662/4608)/ 94.423% (4351/4608)
01/15/2023 20:54:25 - INFO - __main__ -   test: [batch: 36/391 ] | Loss: 0.770 | Acc: 79.941% (3786/4736)/ 94.531% (4477/4736)
01/15/2023 20:54:26 - INFO - __main__ -   test: [batch: 37/391 ] | Loss: 0.761 | Acc: 80.201% (3901/4864)/ 94.613% (4602/4864)
01/15/2023 20:54:28 - INFO - __main__ -   test: [batch: 38/391 ] | Loss: 0.752 | Acc: 80.489% (4018/4992)/ 94.671% (4726/4992)
01/15/2023 20:54:30 - INFO - __main__ -   test: [batch: 39/391 ] | Loss: 0.747 | Acc: 80.645% (4129/5120)/ 94.727% (4850/5120)
01/15/2023 20:54:31 - INFO - __main__ -   test: [batch: 40/391 ] | Loss: 0.744 | Acc: 80.736% (4237/5248)/ 94.646% (4967/5248)
01/15/2023 20:54:32 - INFO - __main__ -   test: [batch: 41/391 ] | Loss: 0.746 | Acc: 80.878% (4348/5376)/ 94.568% (5084/5376)
01/15/2023 20:54:34 - INFO - __main__ -   test: [batch: 42/391 ] | Loss: 0.747 | Acc: 80.887% (4452/5504)/ 94.640% (5209/5504)
01/15/2023 20:54:36 - INFO - __main__ -   test: [batch: 43/391 ] | Loss: 0.746 | Acc: 80.895% (4556/5632)/ 94.585% (5327/5632)
01/15/2023 20:54:37 - INFO - __main__ -   test: [batch: 44/391 ] | Loss: 0.751 | Acc: 80.885% (4659/5760)/ 94.462% (5441/5760)
01/15/2023 20:54:39 - INFO - __main__ -   test: [batch: 45/391 ] | Loss: 0.749 | Acc: 81.012% (4770/5888)/ 94.497% (5564/5888)
01/15/2023 20:54:40 - INFO - __main__ -   test: [batch: 46/391 ] | Loss: 0.752 | Acc: 81.001% (4873/6016)/ 94.548% (5688/6016)
01/15/2023 20:54:42 - INFO - __main__ -   test: [batch: 47/391 ] | Loss: 0.755 | Acc: 80.827% (4966/6144)/ 94.580% (5811/6144)
01/15/2023 20:54:43 - INFO - __main__ -   test: [batch: 48/391 ] | Loss: 0.761 | Acc: 80.708% (5062/6272)/ 94.531% (5929/6272)
01/15/2023 20:54:44 - INFO - __main__ -   test: [batch: 49/391 ] | Loss: 0.766 | Acc: 80.703% (5165/6400)/ 94.469% (6046/6400)
01/15/2023 20:54:46 - INFO - __main__ -   test: [batch: 50/391 ] | Loss: 0.758 | Acc: 80.913% (5282/6528)/ 94.547% (6172/6528)
01/15/2023 20:54:47 - INFO - __main__ -   test: [batch: 51/391 ] | Loss: 0.749 | Acc: 81.115% (5399/6656)/ 94.636% (6299/6656)
01/15/2023 20:54:48 - INFO - __main__ -   test: [batch: 52/391 ] | Loss: 0.746 | Acc: 81.162% (5506/6784)/ 94.679% (6423/6784)
01/15/2023 20:54:50 - INFO - __main__ -   test: [batch: 53/391 ] | Loss: 0.737 | Acc: 81.409% (5627/6912)/ 94.763% (6550/6912)
01/15/2023 20:54:51 - INFO - __main__ -   test: [batch: 54/391 ] | Loss: 0.730 | Acc: 81.548% (5741/7040)/ 94.787% (6673/7040)
01/15/2023 20:54:53 - INFO - __main__ -   test: [batch: 55/391 ] | Loss: 0.723 | Acc: 81.682% (5855/7168)/ 94.838% (6798/7168)
01/15/2023 20:54:54 - INFO - __main__ -   test: [batch: 56/391 ] | Loss: 0.714 | Acc: 81.935% (5978/7296)/ 94.915% (6925/7296)
01/15/2023 20:54:56 - INFO - __main__ -   test: [batch: 57/391 ] | Loss: 0.707 | Acc: 82.112% (6096/7424)/ 94.962% (7050/7424)
01/15/2023 20:54:57 - INFO - __main__ -   test: [batch: 58/391 ] | Loss: 0.704 | Acc: 82.190% (6207/7552)/ 94.955% (7171/7552)
01/15/2023 20:54:58 - INFO - __main__ -   test: [batch: 59/391 ] | Loss: 0.705 | Acc: 82.148% (6309/7680)/ 94.961% (7293/7680)
01/15/2023 20:55:00 - INFO - __main__ -   test: [batch: 60/391 ] | Loss: 0.708 | Acc: 82.082% (6409/7808)/ 94.941% (7413/7808)
01/15/2023 20:55:01 - INFO - __main__ -   test: [batch: 61/391 ] | Loss: 0.707 | Acc: 82.069% (6513/7936)/ 94.960% (7536/7936)
01/15/2023 20:55:03 - INFO - __main__ -   test: [batch: 62/391 ] | Loss: 0.705 | Acc: 82.031% (6615/8064)/ 95.002% (7661/8064)
01/15/2023 20:55:04 - INFO - __main__ -   test: [batch: 63/391 ] | Loss: 0.713 | Acc: 81.897% (6709/8192)/ 94.946% (7778/8192)
01/15/2023 20:55:06 - INFO - __main__ -   test: [batch: 64/391 ] | Loss: 0.721 | Acc: 81.707% (6798/8320)/ 94.904% (7896/8320)
01/15/2023 20:55:07 - INFO - __main__ -   test: [batch: 65/391 ] | Loss: 0.733 | Acc: 81.226% (6862/8448)/ 94.804% (8009/8448)
01/15/2023 20:55:09 - INFO - __main__ -   test: [batch: 66/391 ] | Loss: 0.738 | Acc: 81.215% (6965/8576)/ 94.776% (8128/8576)
01/15/2023 20:55:10 - INFO - __main__ -   test: [batch: 67/391 ] | Loss: 0.741 | Acc: 81.170% (7065/8704)/ 94.784% (8250/8704)
01/15/2023 20:55:12 - INFO - __main__ -   test: [batch: 68/391 ] | Loss: 0.742 | Acc: 81.103% (7163/8832)/ 94.792% (8372/8832)
01/15/2023 20:55:13 - INFO - __main__ -   test: [batch: 69/391 ] | Loss: 0.738 | Acc: 81.205% (7276/8960)/ 94.844% (8498/8960)
01/15/2023 20:55:14 - INFO - __main__ -   test: [batch: 70/391 ] | Loss: 0.738 | Acc: 81.118% (7372/9088)/ 94.883% (8623/9088)
01/15/2023 20:55:16 - INFO - __main__ -   test: [batch: 71/391 ] | Loss: 0.735 | Acc: 81.185% (7482/9216)/ 94.878% (8744/9216)
01/15/2023 20:55:17 - INFO - __main__ -   test: [batch: 72/391 ] | Loss: 0.738 | Acc: 81.036% (7572/9344)/ 94.906% (8868/9344)
01/15/2023 20:55:19 - INFO - __main__ -   test: [batch: 73/391 ] | Loss: 0.742 | Acc: 80.923% (7665/9472)/ 94.901% (8989/9472)
01/15/2023 20:55:20 - INFO - __main__ -   test: [batch: 74/391 ] | Loss: 0.742 | Acc: 80.927% (7769/9600)/ 94.885% (9109/9600)
01/15/2023 20:55:22 - INFO - __main__ -   test: [batch: 75/391 ] | Loss: 0.749 | Acc: 80.757% (7856/9728)/ 94.870% (9229/9728)
01/15/2023 20:55:23 - INFO - __main__ -   test: [batch: 76/391 ] | Loss: 0.748 | Acc: 80.783% (7962/9856)/ 94.897% (9353/9856)
01/15/2023 20:55:25 - INFO - __main__ -   test: [batch: 77/391 ] | Loss: 0.748 | Acc: 80.709% (8058/9984)/ 94.932% (9478/9984)
01/15/2023 20:55:26 - INFO - __main__ -   test: [batch: 78/391 ] | Loss: 0.747 | Acc: 80.647% (8155/10112)/ 94.976% (9604/10112)
01/15/2023 20:55:28 - INFO - __main__ -   test: [batch: 79/391 ] | Loss: 0.746 | Acc: 80.625% (8256/10240)/ 94.990% (9727/10240)
01/15/2023 20:55:29 - INFO - __main__ -   test: [batch: 80/391 ] | Loss: 0.746 | Acc: 80.594% (8356/10368)/ 94.975% (9847/10368)
01/15/2023 20:55:31 - INFO - __main__ -   test: [batch: 81/391 ] | Loss: 0.746 | Acc: 80.593% (8459/10496)/ 94.998% (9971/10496)
01/15/2023 20:55:32 - INFO - __main__ -   test: [batch: 82/391 ] | Loss: 0.745 | Acc: 80.610% (8564/10624)/ 95.011% (10094/10624)
01/15/2023 20:55:34 - INFO - __main__ -   test: [batch: 83/391 ] | Loss: 0.745 | Acc: 80.655% (8672/10752)/ 94.987% (10213/10752)
01/15/2023 20:55:35 - INFO - __main__ -   test: [batch: 84/391 ] | Loss: 0.741 | Acc: 80.790% (8790/10880)/ 95.018% (10338/10880)
01/15/2023 20:55:37 - INFO - __main__ -   test: [batch: 85/391 ] | Loss: 0.739 | Acc: 80.787% (8893/11008)/ 95.076% (10466/11008)
01/15/2023 20:55:38 - INFO - __main__ -   test: [batch: 86/391 ] | Loss: 0.743 | Acc: 80.711% (8988/11136)/ 95.034% (10583/11136)
01/15/2023 20:55:40 - INFO - __main__ -   test: [batch: 87/391 ] | Loss: 0.741 | Acc: 80.753% (9096/11264)/ 95.020% (10703/11264)
01/15/2023 20:55:41 - INFO - __main__ -   test: [batch: 88/391 ] | Loss: 0.748 | Acc: 80.679% (9191/11392)/ 94.953% (10817/11392)
01/15/2023 20:55:42 - INFO - __main__ -   test: [batch: 89/391 ] | Loss: 0.746 | Acc: 80.720% (9299/11520)/ 94.974% (10941/11520)
01/15/2023 20:55:43 - INFO - __main__ -   test: [batch: 90/391 ] | Loss: 0.747 | Acc: 80.615% (9390/11648)/ 94.986% (11064/11648)
01/15/2023 20:55:45 - INFO - __main__ -   test: [batch: 91/391 ] | Loss: 0.746 | Acc: 80.647% (9497/11776)/ 94.990% (11186/11776)
01/15/2023 20:55:46 - INFO - __main__ -   test: [batch: 92/391 ] | Loss: 0.747 | Acc: 80.603% (9595/11904)/ 94.976% (11306/11904)
01/15/2023 20:55:47 - INFO - __main__ -   test: [batch: 93/391 ] | Loss: 0.750 | Acc: 80.411% (9675/12032)/ 95.013% (11432/12032)
01/15/2023 20:55:49 - INFO - __main__ -   test: [batch: 94/391 ] | Loss: 0.753 | Acc: 80.288% (9763/12160)/ 95.016% (11554/12160)
01/15/2023 20:55:50 - INFO - __main__ -   test: [batch: 95/391 ] | Loss: 0.751 | Acc: 80.355% (9874/12288)/ 95.036% (11678/12288)
01/15/2023 20:55:52 - INFO - __main__ -   test: [batch: 96/391 ] | Loss: 0.755 | Acc: 80.219% (9960/12416)/ 95.031% (11799/12416)
01/15/2023 20:55:53 - INFO - __main__ -   test: [batch: 97/391 ] | Loss: 0.757 | Acc: 80.054% (10042/12544)/ 95.049% (11923/12544)
01/15/2023 20:55:55 - INFO - __main__ -   test: [batch: 98/391 ] | Loss: 0.753 | Acc: 80.161% (10158/12672)/ 95.076% (12048/12672)
01/15/2023 20:55:56 - INFO - __main__ -   test: [batch: 99/391 ] | Loss: 0.747 | Acc: 80.328% (10282/12800)/ 95.117% (12175/12800)
01/15/2023 20:55:57 - INFO - __main__ -   test: [batch: 100/391 ] | Loss: 0.746 | Acc: 80.384% (10392/12928)/ 95.135% (12299/12928)
01/15/2023 20:55:59 - INFO - __main__ -   test: [batch: 101/391 ] | Loss: 0.742 | Acc: 80.499% (10510/13056)/ 95.167% (12425/13056)
01/15/2023 20:56:01 - INFO - __main__ -   test: [batch: 102/391 ] | Loss: 0.739 | Acc: 80.583% (10624/13184)/ 95.199% (12551/13184)
01/15/2023 20:56:02 - INFO - __main__ -   test: [batch: 103/391 ] | Loss: 0.740 | Acc: 80.431% (10707/13312)/ 95.215% (12675/13312)
01/15/2023 20:56:04 - INFO - __main__ -   test: [batch: 104/391 ] | Loss: 0.739 | Acc: 80.365% (10801/13440)/ 95.223% (12798/13440)
01/15/2023 20:56:05 - INFO - __main__ -   test: [batch: 105/391 ] | Loss: 0.740 | Acc: 80.366% (10904/13568)/ 95.217% (12919/13568)
01/15/2023 20:56:06 - INFO - __main__ -   test: [batch: 106/391 ] | Loss: 0.748 | Acc: 80.301% (10998/13696)/ 95.152% (13032/13696)
01/15/2023 20:56:07 - INFO - __main__ -   test: [batch: 107/391 ] | Loss: 0.744 | Acc: 80.425% (11118/13824)/ 95.190% (13159/13824)
01/15/2023 20:56:09 - INFO - __main__ -   test: [batch: 108/391 ] | Loss: 0.747 | Acc: 80.275% (11200/13952)/ 95.198% (13282/13952)
01/15/2023 20:56:10 - INFO - __main__ -   test: [batch: 109/391 ] | Loss: 0.748 | Acc: 80.277% (11303/14080)/ 95.206% (13405/14080)
01/15/2023 20:56:12 - INFO - __main__ -   test: [batch: 110/391 ] | Loss: 0.750 | Acc: 80.103% (11381/14208)/ 95.221% (13529/14208)
01/15/2023 20:56:14 - INFO - __main__ -   test: [batch: 111/391 ] | Loss: 0.753 | Acc: 80.064% (11478/14336)/ 95.187% (13646/14336)
01/15/2023 20:56:15 - INFO - __main__ -   test: [batch: 112/391 ] | Loss: 0.753 | Acc: 80.095% (11585/14464)/ 95.209% (13771/14464)
01/15/2023 20:56:17 - INFO - __main__ -   test: [batch: 113/391 ] | Loss: 0.752 | Acc: 80.140% (11694/14592)/ 95.223% (13895/14592)
01/15/2023 20:56:18 - INFO - __main__ -   test: [batch: 114/391 ] | Loss: 0.748 | Acc: 80.238% (11811/14720)/ 95.251% (14021/14720)
01/15/2023 20:56:20 - INFO - __main__ -   test: [batch: 115/391 ] | Loss: 0.746 | Acc: 80.334% (11928/14848)/ 95.272% (14146/14848)
01/15/2023 20:56:22 - INFO - __main__ -   test: [batch: 116/391 ] | Loss: 0.749 | Acc: 80.308% (12027/14976)/ 95.239% (14263/14976)
01/15/2023 20:56:23 - INFO - __main__ -   test: [batch: 117/391 ] | Loss: 0.747 | Acc: 80.350% (12136/15104)/ 95.260% (14388/15104)
01/15/2023 20:56:25 - INFO - __main__ -   test: [batch: 118/391 ] | Loss: 0.751 | Acc: 80.186% (12214/15232)/ 95.267% (14511/15232)
01/15/2023 20:56:26 - INFO - __main__ -   test: [batch: 119/391 ] | Loss: 0.749 | Acc: 80.228% (12323/15360)/ 95.286% (14636/15360)
01/15/2023 20:56:28 - INFO - __main__ -   test: [batch: 120/391 ] | Loss: 0.748 | Acc: 80.275% (12433/15488)/ 95.306% (14761/15488)
01/15/2023 20:56:29 - INFO - __main__ -   test: [batch: 121/391 ] | Loss: 0.753 | Acc: 80.136% (12514/15616)/ 95.268% (14877/15616)
01/15/2023 20:56:31 - INFO - __main__ -   test: [batch: 122/391 ] | Loss: 0.756 | Acc: 80.081% (12608/15744)/ 95.255% (14997/15744)
01/15/2023 20:56:33 - INFO - __main__ -   test: [batch: 123/391 ] | Loss: 0.757 | Acc: 80.091% (12712/15872)/ 95.243% (15117/15872)
01/15/2023 20:56:34 - INFO - __main__ -   test: [batch: 124/391 ] | Loss: 0.756 | Acc: 80.100% (12816/16000)/ 95.256% (15241/16000)
01/15/2023 20:56:36 - INFO - __main__ -   test: [batch: 125/391 ] | Loss: 0.752 | Acc: 80.202% (12935/16128)/ 95.281% (15367/16128)
01/15/2023 20:56:37 - INFO - __main__ -   test: [batch: 126/391 ] | Loss: 0.749 | Acc: 80.297% (13053/16256)/ 95.282% (15489/16256)
01/15/2023 20:56:39 - INFO - __main__ -   test: [batch: 127/391 ] | Loss: 0.747 | Acc: 80.365% (13167/16384)/ 95.294% (15613/16384)
01/15/2023 20:56:40 - INFO - __main__ -   test: [batch: 128/391 ] | Loss: 0.747 | Acc: 80.366% (13270/16512)/ 95.264% (15730/16512)
01/15/2023 20:56:42 - INFO - __main__ -   test: [batch: 129/391 ] | Loss: 0.746 | Acc: 80.403% (13379/16640)/ 95.288% (15856/16640)
01/15/2023 20:56:43 - INFO - __main__ -   test: [batch: 130/391 ] | Loss: 0.742 | Acc: 80.510% (13500/16768)/ 95.312% (15982/16768)
01/15/2023 20:56:45 - INFO - __main__ -   test: [batch: 131/391 ] | Loss: 0.742 | Acc: 80.558% (13611/16896)/ 95.324% (16106/16896)
01/15/2023 20:56:46 - INFO - __main__ -   test: [batch: 132/391 ] | Loss: 0.739 | Acc: 80.633% (13727/17024)/ 95.336% (16230/17024)
01/15/2023 20:56:48 - INFO - __main__ -   test: [batch: 133/391 ] | Loss: 0.741 | Acc: 80.580% (13821/17152)/ 95.330% (16351/17152)
01/15/2023 20:56:49 - INFO - __main__ -   test: [batch: 134/391 ] | Loss: 0.739 | Acc: 80.608% (13929/17280)/ 95.347% (16476/17280)
01/15/2023 20:56:51 - INFO - __main__ -   test: [batch: 135/391 ] | Loss: 0.739 | Acc: 80.595% (14030/17408)/ 95.353% (16599/17408)
01/15/2023 20:56:52 - INFO - __main__ -   test: [batch: 136/391 ] | Loss: 0.738 | Acc: 80.589% (14132/17536)/ 95.375% (16725/17536)
01/15/2023 20:56:54 - INFO - __main__ -   test: [batch: 137/391 ] | Loss: 0.736 | Acc: 80.627% (14242/17664)/ 95.409% (16853/17664)
01/15/2023 20:56:55 - INFO - __main__ -   test: [batch: 138/391 ] | Loss: 0.734 | Acc: 80.660% (14351/17792)/ 95.419% (16977/17792)
01/15/2023 20:56:57 - INFO - __main__ -   test: [batch: 139/391 ] | Loss: 0.739 | Acc: 80.519% (14429/17920)/ 95.413% (17098/17920)
01/15/2023 20:56:59 - INFO - __main__ -   test: [batch: 140/391 ] | Loss: 0.741 | Acc: 80.424% (14515/18048)/ 95.418% (17221/18048)
01/15/2023 20:57:00 - INFO - __main__ -   test: [batch: 141/391 ] | Loss: 0.739 | Acc: 80.474% (14627/18176)/ 95.423% (17344/18176)
01/15/2023 20:57:02 - INFO - __main__ -   test: [batch: 142/391 ] | Loss: 0.737 | Acc: 80.551% (14744/18304)/ 95.433% (17468/18304)
01/15/2023 20:57:03 - INFO - __main__ -   test: [batch: 143/391 ] | Loss: 0.737 | Acc: 80.583% (14853/18432)/ 95.421% (17588/18432)
01/15/2023 20:57:05 - INFO - __main__ -   test: [batch: 144/391 ] | Loss: 0.739 | Acc: 80.555% (14951/18560)/ 95.388% (17704/18560)
01/15/2023 20:57:06 - INFO - __main__ -   test: [batch: 145/391 ] | Loss: 0.742 | Acc: 80.544% (15052/18688)/ 95.366% (17822/18688)
01/15/2023 20:57:08 - INFO - __main__ -   test: [batch: 146/391 ] | Loss: 0.744 | Acc: 80.511% (15149/18816)/ 95.344% (17940/18816)
01/15/2023 20:57:09 - INFO - __main__ -   test: [batch: 147/391 ] | Loss: 0.744 | Acc: 80.511% (15252/18944)/ 95.328% (18059/18944)
01/15/2023 20:57:11 - INFO - __main__ -   test: [batch: 148/391 ] | Loss: 0.745 | Acc: 80.432% (15340/19072)/ 95.328% (18181/19072)
01/15/2023 20:57:12 - INFO - __main__ -   test: [batch: 149/391 ] | Loss: 0.749 | Acc: 80.349% (15427/19200)/ 95.286% (18295/19200)
01/15/2023 20:57:14 - INFO - __main__ -   test: [batch: 150/391 ] | Loss: 0.749 | Acc: 80.303% (15521/19328)/ 95.297% (18419/19328)
01/15/2023 20:57:16 - INFO - __main__ -   test: [batch: 151/391 ] | Loss: 0.749 | Acc: 80.351% (15633/19456)/ 95.292% (18540/19456)
01/15/2023 20:57:17 - INFO - __main__ -   test: [batch: 152/391 ] | Loss: 0.751 | Acc: 80.321% (15730/19584)/ 95.277% (18659/19584)
01/15/2023 20:57:19 - INFO - __main__ -   test: [batch: 153/391 ] | Loss: 0.749 | Acc: 80.377% (15844/19712)/ 95.287% (18783/19712)
01/15/2023 20:57:20 - INFO - __main__ -   test: [batch: 154/391 ] | Loss: 0.750 | Acc: 80.353% (15942/19840)/ 95.237% (18895/19840)
01/15/2023 20:57:21 - INFO - __main__ -   test: [batch: 155/391 ] | Loss: 0.751 | Acc: 80.364% (16047/19968)/ 95.222% (19014/19968)
01/15/2023 20:57:23 - INFO - __main__ -   test: [batch: 156/391 ] | Loss: 0.754 | Acc: 80.285% (16134/20096)/ 95.198% (19131/20096)
01/15/2023 20:57:24 - INFO - __main__ -   test: [batch: 157/391 ] | Loss: 0.756 | Acc: 80.246% (16229/20224)/ 95.179% (19249/20224)
01/15/2023 20:57:26 - INFO - __main__ -   test: [batch: 158/391 ] | Loss: 0.757 | Acc: 80.228% (16328/20352)/ 95.150% (19365/20352)
01/15/2023 20:57:28 - INFO - __main__ -   test: [batch: 159/391 ] | Loss: 0.759 | Acc: 80.205% (16426/20480)/ 95.156% (19488/20480)
01/15/2023 20:57:29 - INFO - __main__ -   test: [batch: 160/391 ] | Loss: 0.759 | Acc: 80.187% (16525/20608)/ 95.128% (19604/20608)
01/15/2023 20:57:31 - INFO - __main__ -   test: [batch: 161/391 ] | Loss: 0.769 | Acc: 79.929% (16574/20736)/ 95.018% (19703/20736)
01/15/2023 20:57:32 - INFO - __main__ -   test: [batch: 162/391 ] | Loss: 0.775 | Acc: 79.822% (16654/20864)/ 94.939% (19808/20864)
01/15/2023 20:57:34 - INFO - __main__ -   test: [batch: 163/391 ] | Loss: 0.777 | Acc: 79.768% (16745/20992)/ 94.931% (19928/20992)
01/15/2023 20:57:36 - INFO - __main__ -   test: [batch: 164/391 ] | Loss: 0.777 | Acc: 79.759% (16845/21120)/ 94.938% (20051/21120)
01/15/2023 20:57:37 - INFO - __main__ -   test: [batch: 165/391 ] | Loss: 0.779 | Acc: 79.688% (16932/21248)/ 94.936% (20172/21248)
01/15/2023 20:57:39 - INFO - __main__ -   test: [batch: 166/391 ] | Loss: 0.779 | Acc: 79.716% (17040/21376)/ 94.924% (20291/21376)
01/15/2023 20:57:40 - INFO - __main__ -   test: [batch: 167/391 ] | Loss: 0.781 | Acc: 79.674% (17133/21504)/ 94.899% (20407/21504)
01/15/2023 20:57:42 - INFO - __main__ -   test: [batch: 168/391 ] | Loss: 0.781 | Acc: 79.683% (17237/21632)/ 94.896% (20528/21632)
01/15/2023 20:57:44 - INFO - __main__ -   test: [batch: 169/391 ] | Loss: 0.783 | Acc: 79.619% (17325/21760)/ 94.858% (20641/21760)
01/15/2023 20:57:45 - INFO - __main__ -   test: [batch: 170/391 ] | Loss: 0.788 | Acc: 79.518% (17405/21888)/ 94.824% (20755/21888)
01/15/2023 20:57:47 - INFO - __main__ -   test: [batch: 171/391 ] | Loss: 0.791 | Acc: 79.465% (17495/22016)/ 94.790% (20869/22016)
01/15/2023 20:57:48 - INFO - __main__ -   test: [batch: 172/391 ] | Loss: 0.792 | Acc: 79.408% (17584/22144)/ 94.784% (20989/22144)
01/15/2023 20:57:49 - INFO - __main__ -   test: [batch: 173/391 ] | Loss: 0.795 | Acc: 79.351% (17673/22272)/ 94.756% (21104/22272)
01/15/2023 20:57:51 - INFO - __main__ -   test: [batch: 174/391 ] | Loss: 0.799 | Acc: 79.254% (17753/22400)/ 94.705% (21214/22400)
01/15/2023 20:57:53 - INFO - __main__ -   test: [batch: 175/391 ] | Loss: 0.797 | Acc: 79.306% (17866/22528)/ 94.718% (21338/22528)
01/15/2023 20:57:54 - INFO - __main__ -   test: [batch: 176/391 ] | Loss: 0.798 | Acc: 79.286% (17963/22656)/ 94.686% (21452/22656)
01/15/2023 20:57:56 - INFO - __main__ -   test: [batch: 177/391 ] | Loss: 0.802 | Acc: 79.205% (18046/22784)/ 94.650% (21565/22784)
01/15/2023 20:57:58 - INFO - __main__ -   test: [batch: 178/391 ] | Loss: 0.803 | Acc: 79.220% (18151/22912)/ 94.632% (21682/22912)
01/15/2023 20:57:59 - INFO - __main__ -   test: [batch: 179/391 ] | Loss: 0.806 | Acc: 79.171% (18241/23040)/ 94.583% (21792/23040)
01/15/2023 20:58:01 - INFO - __main__ -   test: [batch: 180/391 ] | Loss: 0.812 | Acc: 79.031% (18310/23168)/ 94.536% (21902/23168)
01/15/2023 20:58:02 - INFO - __main__ -   test: [batch: 181/391 ] | Loss: 0.819 | Acc: 78.911% (18383/23296)/ 94.467% (22007/23296)
01/15/2023 20:58:04 - INFO - __main__ -   test: [batch: 182/391 ] | Loss: 0.818 | Acc: 78.932% (18489/23424)/ 94.467% (22128/23424)
01/15/2023 20:58:05 - INFO - __main__ -   test: [batch: 183/391 ] | Loss: 0.825 | Acc: 78.809% (18561/23552)/ 94.357% (22223/23552)
01/15/2023 20:58:07 - INFO - __main__ -   test: [batch: 184/391 ] | Loss: 0.824 | Acc: 78.830% (18667/23680)/ 94.345% (22341/23680)
01/15/2023 20:58:08 - INFO - __main__ -   test: [batch: 185/391 ] | Loss: 0.824 | Acc: 78.839% (18770/23808)/ 94.346% (22462/23808)
01/15/2023 20:58:10 - INFO - __main__ -   test: [batch: 186/391 ] | Loss: 0.827 | Acc: 78.798% (18861/23936)/ 94.306% (22573/23936)
01/15/2023 20:58:12 - INFO - __main__ -   test: [batch: 187/391 ] | Loss: 0.831 | Acc: 78.682% (18934/24064)/ 94.290% (22690/24064)
01/15/2023 20:58:13 - INFO - __main__ -   test: [batch: 188/391 ] | Loss: 0.835 | Acc: 78.563% (19006/24192)/ 94.263% (22804/24192)
01/15/2023 20:58:15 - INFO - __main__ -   test: [batch: 189/391 ] | Loss: 0.836 | Acc: 78.483% (19087/24320)/ 94.272% (22927/24320)
01/15/2023 20:58:16 - INFO - __main__ -   test: [batch: 190/391 ] | Loss: 0.841 | Acc: 78.383% (19163/24448)/ 94.245% (23041/24448)
01/15/2023 20:58:18 - INFO - __main__ -   test: [batch: 191/391 ] | Loss: 0.844 | Acc: 78.349% (19255/24576)/ 94.218% (23155/24576)
01/15/2023 20:58:19 - INFO - __main__ -   test: [batch: 192/391 ] | Loss: 0.850 | Acc: 78.222% (19324/24704)/ 94.122% (23252/24704)
01/15/2023 20:58:21 - INFO - __main__ -   test: [batch: 193/391 ] | Loss: 0.850 | Acc: 78.242% (19429/24832)/ 94.116% (23371/24832)
01/15/2023 20:58:22 - INFO - __main__ -   test: [batch: 194/391 ] | Loss: 0.853 | Acc: 78.173% (19512/24960)/ 94.091% (23485/24960)
01/15/2023 20:58:23 - INFO - __main__ -   test: [batch: 195/391 ] | Loss: 0.859 | Acc: 78.077% (19588/25088)/ 94.025% (23589/25088)
01/15/2023 20:58:25 - INFO - __main__ -   test: [batch: 196/391 ] | Loss: 0.863 | Acc: 77.982% (19664/25216)/ 93.984% (23699/25216)
01/15/2023 20:58:26 - INFO - __main__ -   test: [batch: 197/391 ] | Loss: 0.867 | Acc: 77.888% (19740/25344)/ 93.967% (23815/25344)
01/15/2023 20:58:28 - INFO - __main__ -   test: [batch: 198/391 ] | Loss: 0.869 | Acc: 77.846% (19829/25472)/ 93.915% (23922/25472)
01/15/2023 20:58:30 - INFO - __main__ -   test: [batch: 199/391 ] | Loss: 0.869 | Acc: 77.816% (19921/25600)/ 93.930% (24046/25600)
01/15/2023 20:58:31 - INFO - __main__ -   test: [batch: 200/391 ] | Loss: 0.869 | Acc: 77.791% (20014/25728)/ 93.937% (24168/25728)
01/15/2023 20:58:33 - INFO - __main__ -   test: [batch: 201/391 ] | Loss: 0.872 | Acc: 77.707% (20092/25856)/ 93.909% (24281/25856)
01/15/2023 20:58:34 - INFO - __main__ -   test: [batch: 202/391 ] | Loss: 0.873 | Acc: 77.682% (20185/25984)/ 93.896% (24398/25984)
01/15/2023 20:58:36 - INFO - __main__ -   test: [batch: 203/391 ] | Loss: 0.874 | Acc: 77.665% (20280/26112)/ 93.884% (24515/26112)
01/15/2023 20:58:37 - INFO - __main__ -   test: [batch: 204/391 ] | Loss: 0.877 | Acc: 77.572% (20355/26240)/ 93.849% (24626/26240)
01/15/2023 20:58:39 - INFO - __main__ -   test: [batch: 205/391 ] | Loss: 0.880 | Acc: 77.492% (20433/26368)/ 93.822% (24739/26368)
01/15/2023 20:58:41 - INFO - __main__ -   test: [batch: 206/391 ] | Loss: 0.881 | Acc: 77.461% (20524/26496)/ 93.814% (24857/26496)
01/15/2023 20:58:42 - INFO - __main__ -   test: [batch: 207/391 ] | Loss: 0.885 | Acc: 77.378% (20601/26624)/ 93.780% (24968/26624)
01/15/2023 20:58:43 - INFO - __main__ -   test: [batch: 208/391 ] | Loss: 0.887 | Acc: 77.344% (20691/26752)/ 93.754% (25081/26752)
01/15/2023 20:58:45 - INFO - __main__ -   test: [batch: 209/391 ] | Loss: 0.886 | Acc: 77.340% (20789/26880)/ 93.776% (25207/26880)
01/15/2023 20:58:47 - INFO - __main__ -   test: [batch: 210/391 ] | Loss: 0.888 | Acc: 77.310% (20880/27008)/ 93.754% (25321/27008)
01/15/2023 20:58:48 - INFO - __main__ -   test: [batch: 211/391 ] | Loss: 0.890 | Acc: 77.255% (20964/27136)/ 93.724% (25433/27136)
01/15/2023 20:58:49 - INFO - __main__ -   test: [batch: 212/391 ] | Loss: 0.892 | Acc: 77.186% (21044/27264)/ 93.717% (25551/27264)
01/15/2023 20:58:51 - INFO - __main__ -   test: [batch: 213/391 ] | Loss: 0.892 | Acc: 77.190% (21144/27392)/ 93.724% (25673/27392)
01/15/2023 20:58:52 - INFO - __main__ -   test: [batch: 214/391 ] | Loss: 0.892 | Acc: 77.173% (21238/27520)/ 93.728% (25794/27520)
01/15/2023 20:58:54 - INFO - __main__ -   test: [batch: 215/391 ] | Loss: 0.894 | Acc: 77.141% (21328/27648)/ 93.696% (25905/27648)
01/15/2023 20:58:55 - INFO - __main__ -   test: [batch: 216/391 ] | Loss: 0.892 | Acc: 77.193% (21441/27776)/ 93.707% (26028/27776)
01/15/2023 20:58:56 - INFO - __main__ -   test: [batch: 217/391 ] | Loss: 0.897 | Acc: 77.129% (21522/27904)/ 93.639% (26129/27904)
01/15/2023 20:58:58 - INFO - __main__ -   test: [batch: 218/391 ] | Loss: 0.900 | Acc: 77.069% (21604/28032)/ 93.607% (26240/28032)
01/15/2023 20:59:00 - INFO - __main__ -   test: [batch: 219/391 ] | Loss: 0.898 | Acc: 77.109% (21714/28160)/ 93.619% (26363/28160)
01/15/2023 20:59:01 - INFO - __main__ -   test: [batch: 220/391 ] | Loss: 0.896 | Acc: 77.153% (21825/28288)/ 93.633% (26487/28288)
01/15/2023 20:59:03 - INFO - __main__ -   test: [batch: 221/391 ] | Loss: 0.898 | Acc: 77.119% (21914/28416)/ 93.613% (26601/28416)
01/15/2023 20:59:04 - INFO - __main__ -   test: [batch: 222/391 ] | Loss: 0.896 | Acc: 77.172% (22028/28544)/ 93.627% (26725/28544)
01/15/2023 20:59:06 - INFO - __main__ -   test: [batch: 223/391 ] | Loss: 0.897 | Acc: 77.166% (22125/28672)/ 93.617% (26842/28672)
01/15/2023 20:59:08 - INFO - __main__ -   test: [batch: 224/391 ] | Loss: 0.896 | Acc: 77.184% (22229/28800)/ 93.618% (26962/28800)
01/15/2023 20:59:09 - INFO - __main__ -   test: [batch: 225/391 ] | Loss: 0.896 | Acc: 77.185% (22328/28928)/ 93.622% (27083/28928)
01/15/2023 20:59:11 - INFO - __main__ -   test: [batch: 226/391 ] | Loss: 0.896 | Acc: 77.158% (22419/29056)/ 93.623% (27203/29056)
01/15/2023 20:59:12 - INFO - __main__ -   test: [batch: 227/391 ] | Loss: 0.898 | Acc: 77.155% (22517/29184)/ 93.616% (27321/29184)
01/15/2023 20:59:14 - INFO - __main__ -   test: [batch: 228/391 ] | Loss: 0.902 | Acc: 77.067% (22590/29312)/ 93.542% (27419/29312)
01/15/2023 20:59:15 - INFO - __main__ -   test: [batch: 229/391 ] | Loss: 0.906 | Acc: 76.980% (22663/29440)/ 93.495% (27525/29440)
01/15/2023 20:59:17 - INFO - __main__ -   test: [batch: 230/391 ] | Loss: 0.910 | Acc: 76.901% (22738/29568)/ 93.446% (27630/29568)
01/15/2023 20:59:18 - INFO - __main__ -   test: [batch: 231/391 ] | Loss: 0.909 | Acc: 76.896% (22835/29696)/ 93.433% (27746/29696)
01/15/2023 20:59:20 - INFO - __main__ -   test: [batch: 232/391 ] | Loss: 0.909 | Acc: 76.921% (22941/29824)/ 93.445% (27869/29824)
01/15/2023 20:59:21 - INFO - __main__ -   test: [batch: 233/391 ] | Loss: 0.910 | Acc: 76.880% (23027/29952)/ 93.419% (27981/29952)
01/15/2023 20:59:23 - INFO - __main__ -   test: [batch: 234/391 ] | Loss: 0.917 | Acc: 76.759% (23089/30080)/ 93.358% (28082/30080)
01/15/2023 20:59:24 - INFO - __main__ -   test: [batch: 235/391 ] | Loss: 0.917 | Acc: 76.755% (23186/30208)/ 93.353% (28200/30208)
01/15/2023 20:59:26 - INFO - __main__ -   test: [batch: 236/391 ] | Loss: 0.915 | Acc: 76.800% (23298/30336)/ 93.364% (28323/30336)
01/15/2023 20:59:27 - INFO - __main__ -   test: [batch: 237/391 ] | Loss: 0.915 | Acc: 76.802% (23397/30464)/ 93.340% (28435/30464)
01/15/2023 20:59:29 - INFO - __main__ -   test: [batch: 238/391 ] | Loss: 0.914 | Acc: 76.840% (23507/30592)/ 93.351% (28558/30592)
01/15/2023 20:59:31 - INFO - __main__ -   test: [batch: 239/391 ] | Loss: 0.913 | Acc: 76.868% (23614/30720)/ 93.359% (28680/30720)
01/15/2023 20:59:32 - INFO - __main__ -   test: [batch: 240/391 ] | Loss: 0.914 | Acc: 76.877% (23715/30848)/ 93.351% (28797/30848)
01/15/2023 20:59:34 - INFO - __main__ -   test: [batch: 241/391 ] | Loss: 0.917 | Acc: 76.785% (23785/30976)/ 93.304% (28902/30976)
01/15/2023 20:59:35 - INFO - __main__ -   test: [batch: 242/391 ] | Loss: 0.919 | Acc: 76.678% (23850/31104)/ 93.293% (29018/31104)
01/15/2023 20:59:37 - INFO - __main__ -   test: [batch: 243/391 ] | Loss: 0.925 | Acc: 76.572% (23915/31232)/ 93.212% (29112/31232)
01/15/2023 20:59:38 - INFO - __main__ -   test: [batch: 244/391 ] | Loss: 0.925 | Acc: 76.569% (24012/31360)/ 93.211% (29231/31360)
01/15/2023 20:59:40 - INFO - __main__ -   test: [batch: 245/391 ] | Loss: 0.925 | Acc: 76.572% (24111/31488)/ 93.207% (29349/31488)
01/15/2023 20:59:41 - INFO - __main__ -   test: [batch: 246/391 ] | Loss: 0.926 | Acc: 76.550% (24202/31616)/ 93.200% (29466/31616)
01/15/2023 20:59:43 - INFO - __main__ -   test: [batch: 247/391 ] | Loss: 0.931 | Acc: 76.465% (24273/31744)/ 93.133% (29564/31744)
01/15/2023 20:59:44 - INFO - __main__ -   test: [batch: 248/391 ] | Loss: 0.932 | Acc: 76.428% (24359/31872)/ 93.132% (29683/31872)
01/15/2023 20:59:45 - INFO - __main__ -   test: [batch: 249/391 ] | Loss: 0.934 | Acc: 76.278% (24409/32000)/ 93.122% (29799/32000)
01/15/2023 20:59:47 - INFO - __main__ -   test: [batch: 250/391 ] | Loss: 0.933 | Acc: 76.310% (24517/32128)/ 93.134% (29922/32128)
01/15/2023 20:59:48 - INFO - __main__ -   test: [batch: 251/391 ] | Loss: 0.935 | Acc: 76.277% (24604/32256)/ 93.105% (30032/32256)
01/15/2023 20:59:50 - INFO - __main__ -   test: [batch: 252/391 ] | Loss: 0.934 | Acc: 76.303% (24710/32384)/ 93.095% (30148/32384)
01/15/2023 20:59:51 - INFO - __main__ -   test: [batch: 253/391 ] | Loss: 0.936 | Acc: 76.267% (24796/32512)/ 93.073% (30260/32512)
01/15/2023 20:59:52 - INFO - __main__ -   test: [batch: 254/391 ] | Loss: 0.940 | Acc: 76.186% (24867/32640)/ 93.021% (30362/32640)
01/15/2023 20:59:54 - INFO - __main__ -   test: [batch: 255/391 ] | Loss: 0.942 | Acc: 76.163% (24957/32768)/ 93.011% (30478/32768)
01/15/2023 20:59:55 - INFO - __main__ -   test: [batch: 256/391 ] | Loss: 0.947 | Acc: 76.037% (25013/32896)/ 92.969% (30583/32896)
01/15/2023 20:59:57 - INFO - __main__ -   test: [batch: 257/391 ] | Loss: 0.948 | Acc: 76.030% (25108/33024)/ 92.963% (30700/33024)
01/15/2023 20:59:59 - INFO - __main__ -   test: [batch: 258/391 ] | Loss: 0.949 | Acc: 76.023% (25203/33152)/ 92.945% (30813/33152)
01/15/2023 21:00:00 - INFO - __main__ -   test: [batch: 259/391 ] | Loss: 0.952 | Acc: 75.907% (25262/33280)/ 92.942% (30931/33280)
01/15/2023 21:00:02 - INFO - __main__ -   test: [batch: 260/391 ] | Loss: 0.952 | Acc: 75.895% (25355/33408)/ 92.939% (31049/33408)
01/15/2023 21:00:03 - INFO - __main__ -   test: [batch: 261/391 ] | Loss: 0.950 | Acc: 75.939% (25467/33536)/ 92.963% (31176/33536)
01/15/2023 21:00:05 - INFO - __main__ -   test: [batch: 262/391 ] | Loss: 0.950 | Acc: 75.957% (25570/33664)/ 92.975% (31299/33664)
01/15/2023 21:00:06 - INFO - __main__ -   test: [batch: 263/391 ] | Loss: 0.952 | Acc: 75.894% (25646/33792)/ 92.951% (31410/33792)
01/15/2023 21:00:08 - INFO - __main__ -   test: [batch: 264/391 ] | Loss: 0.957 | Acc: 75.820% (25718/33920)/ 92.880% (31505/33920)
01/15/2023 21:00:10 - INFO - __main__ -   test: [batch: 265/391 ] | Loss: 0.957 | Acc: 75.855% (25827/34048)/ 92.881% (31624/34048)
01/15/2023 21:00:11 - INFO - __main__ -   test: [batch: 266/391 ] | Loss: 0.959 | Acc: 75.796% (25904/34176)/ 92.866% (31738/34176)
01/15/2023 21:00:13 - INFO - __main__ -   test: [batch: 267/391 ] | Loss: 0.958 | Acc: 75.828% (26012/34304)/ 92.867% (31857/34304)
01/15/2023 21:00:15 - INFO - __main__ -   test: [batch: 268/391 ] | Loss: 0.958 | Acc: 75.831% (26110/34432)/ 92.858% (31973/34432)
01/15/2023 21:00:16 - INFO - __main__ -   test: [batch: 269/391 ] | Loss: 0.960 | Acc: 75.784% (26191/34560)/ 92.841% (32086/34560)
01/15/2023 21:00:18 - INFO - __main__ -   test: [batch: 270/391 ] | Loss: 0.963 | Acc: 75.735% (26271/34688)/ 92.819% (32197/34688)
01/15/2023 21:00:19 - INFO - __main__ -   test: [batch: 271/391 ] | Loss: 0.963 | Acc: 75.753% (26374/34816)/ 92.814% (32314/34816)
01/15/2023 21:00:20 - INFO - __main__ -   test: [batch: 272/391 ] | Loss: 0.964 | Acc: 75.721% (26460/34944)/ 92.806% (32430/34944)
01/15/2023 21:00:22 - INFO - __main__ -   test: [batch: 273/391 ] | Loss: 0.964 | Acc: 75.733% (26561/35072)/ 92.783% (32541/35072)
01/15/2023 21:00:23 - INFO - __main__ -   test: [batch: 274/391 ] | Loss: 0.963 | Acc: 75.736% (26659/35200)/ 92.790% (32662/35200)
01/15/2023 21:00:25 - INFO - __main__ -   test: [batch: 275/391 ] | Loss: 0.965 | Acc: 75.702% (26744/35328)/ 92.782% (32778/35328)
01/15/2023 21:00:26 - INFO - __main__ -   test: [batch: 276/391 ] | Loss: 0.967 | Acc: 75.660% (26826/35456)/ 92.774% (32894/35456)
01/15/2023 21:00:27 - INFO - __main__ -   test: [batch: 277/391 ] | Loss: 0.967 | Acc: 75.632% (26913/35584)/ 92.758% (33007/35584)
01/15/2023 21:00:29 - INFO - __main__ -   test: [batch: 278/391 ] | Loss: 0.967 | Acc: 75.650% (27016/35712)/ 92.759% (33126/35712)
01/15/2023 21:00:30 - INFO - __main__ -   test: [batch: 279/391 ] | Loss: 0.967 | Acc: 75.675% (27122/35840)/ 92.754% (33243/35840)
01/15/2023 21:00:32 - INFO - __main__ -   test: [batch: 280/391 ] | Loss: 0.968 | Acc: 75.645% (27208/35968)/ 92.746% (33359/35968)
01/15/2023 21:00:34 - INFO - __main__ -   test: [batch: 281/391 ] | Loss: 0.969 | Acc: 75.640% (27303/36096)/ 92.733% (33473/36096)
01/15/2023 21:00:35 - INFO - __main__ -   test: [batch: 282/391 ] | Loss: 0.968 | Acc: 75.674% (27412/36224)/ 92.731% (33591/36224)
01/15/2023 21:00:37 - INFO - __main__ -   test: [batch: 283/391 ] | Loss: 0.968 | Acc: 75.660% (27504/36352)/ 92.724% (33707/36352)
01/15/2023 21:00:38 - INFO - __main__ -   test: [batch: 284/391 ] | Loss: 0.972 | Acc: 75.606% (27581/36480)/ 92.667% (33805/36480)
01/15/2023 21:00:40 - INFO - __main__ -   test: [batch: 285/391 ] | Loss: 0.975 | Acc: 75.579% (27668/36608)/ 92.619% (33906/36608)
01/15/2023 21:00:41 - INFO - __main__ -   test: [batch: 286/391 ] | Loss: 0.976 | Acc: 75.542% (27751/36736)/ 92.601% (34018/36736)
01/15/2023 21:00:43 - INFO - __main__ -   test: [batch: 287/391 ] | Loss: 0.976 | Acc: 75.532% (27844/36864)/ 92.594% (34134/36864)
01/15/2023 21:00:45 - INFO - __main__ -   test: [batch: 288/391 ] | Loss: 0.976 | Acc: 75.554% (27949/36992)/ 92.598% (34254/36992)
01/15/2023 21:00:46 - INFO - __main__ -   test: [batch: 289/391 ] | Loss: 0.977 | Acc: 75.528% (28036/37120)/ 92.562% (34359/37120)
01/15/2023 21:00:48 - INFO - __main__ -   test: [batch: 290/391 ] | Loss: 0.979 | Acc: 75.443% (28101/37248)/ 92.555% (34475/37248)
01/15/2023 21:00:49 - INFO - __main__ -   test: [batch: 291/391 ] | Loss: 0.979 | Acc: 75.449% (28200/37376)/ 92.557% (34594/37376)
01/15/2023 21:00:51 - INFO - __main__ -   test: [batch: 292/391 ] | Loss: 0.981 | Acc: 75.392% (28275/37504)/ 92.537% (34705/37504)
01/15/2023 21:00:52 - INFO - __main__ -   test: [batch: 293/391 ] | Loss: 0.982 | Acc: 75.372% (28364/37632)/ 92.533% (34822/37632)
01/15/2023 21:00:54 - INFO - __main__ -   test: [batch: 294/391 ] | Loss: 0.983 | Acc: 75.347% (28451/37760)/ 92.519% (34935/37760)
01/15/2023 21:00:56 - INFO - __main__ -   test: [batch: 295/391 ] | Loss: 0.982 | Acc: 75.367% (28555/37888)/ 92.515% (35052/37888)
01/15/2023 21:00:57 - INFO - __main__ -   test: [batch: 296/391 ] | Loss: 0.983 | Acc: 75.355% (28647/38016)/ 92.508% (35168/38016)
01/15/2023 21:00:59 - INFO - __main__ -   test: [batch: 297/391 ] | Loss: 0.984 | Acc: 75.330% (28734/38144)/ 92.486% (35278/38144)
01/15/2023 21:01:00 - INFO - __main__ -   test: [batch: 298/391 ] | Loss: 0.987 | Acc: 75.293% (28816/38272)/ 92.467% (35389/38272)
01/15/2023 21:01:02 - INFO - __main__ -   test: [batch: 299/391 ] | Loss: 0.988 | Acc: 75.279% (28907/38400)/ 92.438% (35496/38400)
01/15/2023 21:01:04 - INFO - __main__ -   test: [batch: 300/391 ] | Loss: 0.988 | Acc: 75.293% (29009/38528)/ 92.442% (35616/38528)
01/15/2023 21:01:05 - INFO - __main__ -   test: [batch: 301/391 ] | Loss: 0.989 | Acc: 75.277% (29099/38656)/ 92.431% (35730/38656)
01/15/2023 21:01:07 - INFO - __main__ -   test: [batch: 302/391 ] | Loss: 0.991 | Acc: 75.242% (29182/38784)/ 92.412% (35841/38784)
01/15/2023 21:01:08 - INFO - __main__ -   test: [batch: 303/391 ] | Loss: 0.992 | Acc: 75.211% (29266/38912)/ 92.401% (35955/38912)
01/15/2023 21:01:10 - INFO - __main__ -   test: [batch: 304/391 ] | Loss: 0.991 | Acc: 75.231% (29370/39040)/ 92.408% (36076/39040)
01/15/2023 21:01:11 - INFO - __main__ -   test: [batch: 305/391 ] | Loss: 0.992 | Acc: 75.194% (29452/39168)/ 92.405% (36193/39168)
01/15/2023 21:01:13 - INFO - __main__ -   test: [batch: 306/391 ] | Loss: 0.993 | Acc: 75.188% (29546/39296)/ 92.381% (36302/39296)
01/15/2023 21:01:15 - INFO - __main__ -   test: [batch: 307/391 ] | Loss: 0.994 | Acc: 75.165% (29633/39424)/ 92.370% (36416/39424)
01/15/2023 21:01:16 - INFO - __main__ -   test: [batch: 308/391 ] | Loss: 0.995 | Acc: 75.154% (29725/39552)/ 92.359% (36530/39552)
01/15/2023 21:01:18 - INFO - __main__ -   test: [batch: 309/391 ] | Loss: 0.996 | Acc: 75.144% (29817/39680)/ 92.336% (36639/39680)
01/15/2023 21:01:19 - INFO - __main__ -   test: [batch: 310/391 ] | Loss: 0.997 | Acc: 75.128% (29907/39808)/ 92.318% (36750/39808)
01/15/2023 21:01:21 - INFO - __main__ -   test: [batch: 311/391 ] | Loss: 0.999 | Acc: 75.110% (29996/39936)/ 92.303% (36862/39936)
01/15/2023 21:01:22 - INFO - __main__ -   test: [batch: 312/391 ] | Loss: 1.000 | Acc: 75.097% (30087/40064)/ 92.287% (36974/40064)
01/15/2023 21:01:24 - INFO - __main__ -   test: [batch: 313/391 ] | Loss: 0.998 | Acc: 75.144% (30202/40192)/ 92.312% (37102/40192)
01/15/2023 21:01:25 - INFO - __main__ -   test: [batch: 314/391 ] | Loss: 0.998 | Acc: 75.134% (30294/40320)/ 92.304% (37217/40320)
01/15/2023 21:01:27 - INFO - __main__ -   test: [batch: 315/391 ] | Loss: 0.999 | Acc: 75.119% (30384/40448)/ 92.289% (37329/40448)
01/15/2023 21:01:28 - INFO - __main__ -   test: [batch: 316/391 ] | Loss: 1.001 | Acc: 75.027% (30443/40576)/ 92.269% (37439/40576)
01/15/2023 21:01:30 - INFO - __main__ -   test: [batch: 317/391 ] | Loss: 1.003 | Acc: 74.983% (30521/40704)/ 92.239% (37545/40704)
01/15/2023 21:01:32 - INFO - __main__ -   test: [batch: 318/391 ] | Loss: 1.002 | Acc: 75.015% (30630/40832)/ 92.259% (37671/40832)
01/15/2023 21:01:33 - INFO - __main__ -   test: [batch: 319/391 ] | Loss: 1.005 | Acc: 74.961% (30704/40960)/ 92.229% (37777/40960)
01/15/2023 21:01:35 - INFO - __main__ -   test: [batch: 320/391 ] | Loss: 1.003 | Acc: 75.002% (30817/41088)/ 92.239% (37899/41088)
01/15/2023 21:01:36 - INFO - __main__ -   test: [batch: 321/391 ] | Loss: 1.003 | Acc: 75.022% (30921/41216)/ 92.234% (38015/41216)
01/15/2023 21:01:38 - INFO - __main__ -   test: [batch: 322/391 ] | Loss: 1.005 | Acc: 74.985% (31002/41344)/ 92.224% (38129/41344)
01/15/2023 21:01:39 - INFO - __main__ -   test: [batch: 323/391 ] | Loss: 1.007 | Acc: 74.952% (31084/41472)/ 92.195% (38235/41472)
01/15/2023 21:01:41 - INFO - __main__ -   test: [batch: 324/391 ] | Loss: 1.007 | Acc: 74.954% (31181/41600)/ 92.188% (38350/41600)
01/15/2023 21:01:43 - INFO - __main__ -   test: [batch: 325/391 ] | Loss: 1.007 | Acc: 74.957% (31278/41728)/ 92.190% (38469/41728)
01/15/2023 21:01:44 - INFO - __main__ -   test: [batch: 326/391 ] | Loss: 1.010 | Acc: 74.873% (31339/41856)/ 92.135% (38564/41856)
01/15/2023 21:01:46 - INFO - __main__ -   test: [batch: 327/391 ] | Loss: 1.014 | Acc: 74.793% (31401/41984)/ 92.095% (38665/41984)
01/15/2023 21:01:47 - INFO - __main__ -   test: [batch: 328/391 ] | Loss: 1.016 | Acc: 74.744% (31476/42112)/ 92.062% (38769/42112)
01/15/2023 21:01:49 - INFO - __main__ -   test: [batch: 329/391 ] | Loss: 1.016 | Acc: 74.747% (31573/42240)/ 92.067% (38889/42240)
01/15/2023 21:01:51 - INFO - __main__ -   test: [batch: 330/391 ] | Loss: 1.017 | Acc: 74.714% (31655/42368)/ 92.044% (38997/42368)
01/15/2023 21:01:52 - INFO - __main__ -   test: [batch: 331/391 ] | Loss: 1.018 | Acc: 74.682% (31737/42496)/ 92.058% (39121/42496)
01/15/2023 21:01:54 - INFO - __main__ -   test: [batch: 332/391 ] | Loss: 1.018 | Acc: 74.665% (31825/42624)/ 92.049% (39235/42624)
01/15/2023 21:01:55 - INFO - __main__ -   test: [batch: 333/391 ] | Loss: 1.017 | Acc: 74.698% (31935/42752)/ 92.057% (39356/42752)
01/15/2023 21:01:57 - INFO - __main__ -   test: [batch: 334/391 ] | Loss: 1.018 | Acc: 74.660% (32014/42880)/ 92.050% (39471/42880)
01/15/2023 21:01:59 - INFO - __main__ -   test: [batch: 335/391 ] | Loss: 1.019 | Acc: 74.633% (32098/43008)/ 92.027% (39579/43008)
01/15/2023 21:02:00 - INFO - __main__ -   test: [batch: 336/391 ] | Loss: 1.021 | Acc: 74.585% (32173/43136)/ 92.004% (39687/43136)
01/15/2023 21:02:02 - INFO - __main__ -   test: [batch: 337/391 ] | Loss: 1.021 | Acc: 74.582% (32267/43264)/ 91.998% (39802/43264)
01/15/2023 21:02:03 - INFO - __main__ -   test: [batch: 338/391 ] | Loss: 1.021 | Acc: 74.581% (32362/43392)/ 92.008% (39924/43392)
01/15/2023 21:02:05 - INFO - __main__ -   test: [batch: 339/391 ] | Loss: 1.024 | Acc: 74.543% (32441/43520)/ 91.978% (40029/43520)
01/15/2023 21:02:06 - INFO - __main__ -   test: [batch: 340/391 ] | Loss: 1.023 | Acc: 74.539% (32535/43648)/ 91.990% (40152/43648)
01/15/2023 21:02:08 - INFO - __main__ -   test: [batch: 341/391 ] | Loss: 1.022 | Acc: 74.577% (32647/43776)/ 92.009% (40278/43776)
01/15/2023 21:02:09 - INFO - __main__ -   test: [batch: 342/391 ] | Loss: 1.022 | Acc: 74.526% (32720/43904)/ 91.996% (40390/43904)
01/15/2023 21:02:11 - INFO - __main__ -   test: [batch: 343/391 ] | Loss: 1.022 | Acc: 74.521% (32813/44032)/ 91.994% (40507/44032)
01/15/2023 21:02:12 - INFO - __main__ -   test: [batch: 344/391 ] | Loss: 1.023 | Acc: 74.520% (32908/44160)/ 91.986% (40621/44160)
01/15/2023 21:02:14 - INFO - __main__ -   test: [batch: 345/391 ] | Loss: 1.026 | Acc: 74.449% (32972/44288)/ 91.953% (40724/44288)
01/15/2023 21:02:16 - INFO - __main__ -   test: [batch: 346/391 ] | Loss: 1.028 | Acc: 74.430% (33059/44416)/ 91.944% (40838/44416)
01/15/2023 21:02:17 - INFO - __main__ -   test: [batch: 347/391 ] | Loss: 1.027 | Acc: 74.445% (33161/44544)/ 91.950% (40958/44544)
01/15/2023 21:02:19 - INFO - __main__ -   test: [batch: 348/391 ] | Loss: 1.029 | Acc: 74.422% (33246/44672)/ 91.932% (41068/44672)
01/15/2023 21:02:20 - INFO - __main__ -   test: [batch: 349/391 ] | Loss: 1.028 | Acc: 74.424% (33342/44800)/ 91.946% (41192/44800)
01/15/2023 21:02:22 - INFO - __main__ -   test: [batch: 350/391 ] | Loss: 1.028 | Acc: 74.437% (33443/44928)/ 91.943% (41308/44928)
01/15/2023 21:02:24 - INFO - __main__ -   test: [batch: 351/391 ] | Loss: 1.030 | Acc: 74.376% (33511/45056)/ 91.928% (41419/45056)
01/15/2023 21:02:25 - INFO - __main__ -   test: [batch: 352/391 ] | Loss: 1.031 | Acc: 74.374% (33605/45184)/ 91.924% (41535/45184)
01/15/2023 21:02:27 - INFO - __main__ -   test: [batch: 353/391 ] | Loss: 1.033 | Acc: 74.325% (33678/45312)/ 91.883% (41634/45312)
01/15/2023 21:02:29 - INFO - __main__ -   test: [batch: 354/391 ] | Loss: 1.036 | Acc: 74.267% (33747/45440)/ 91.868% (41745/45440)
01/15/2023 21:02:30 - INFO - __main__ -   test: [batch: 355/391 ] | Loss: 1.038 | Acc: 74.208% (33815/45568)/ 91.854% (41856/45568)
01/15/2023 21:02:32 - INFO - __main__ -   test: [batch: 356/391 ] | Loss: 1.039 | Acc: 74.197% (33905/45696)/ 91.859% (41976/45696)
01/15/2023 21:02:33 - INFO - __main__ -   test: [batch: 357/391 ] | Loss: 1.037 | Acc: 74.232% (34016/45824)/ 91.873% (42100/45824)
01/15/2023 21:02:35 - INFO - __main__ -   test: [batch: 358/391 ] | Loss: 1.037 | Acc: 74.258% (34123/45952)/ 91.876% (42219/45952)
01/15/2023 21:02:37 - INFO - __main__ -   test: [batch: 359/391 ] | Loss: 1.037 | Acc: 74.262% (34220/46080)/ 91.873% (42335/46080)
01/15/2023 21:02:38 - INFO - __main__ -   test: [batch: 360/391 ] | Loss: 1.039 | Acc: 74.221% (34296/46208)/ 91.872% (42452/46208)
01/15/2023 21:02:40 - INFO - __main__ -   test: [batch: 361/391 ] | Loss: 1.039 | Acc: 74.221% (34391/46336)/ 91.872% (42570/46336)
01/15/2023 21:02:41 - INFO - __main__ -   test: [batch: 362/391 ] | Loss: 1.038 | Acc: 74.223% (34487/46464)/ 91.884% (42693/46464)
01/15/2023 21:02:43 - INFO - __main__ -   test: [batch: 363/391 ] | Loss: 1.039 | Acc: 74.202% (34572/46592)/ 91.872% (42805/46592)
01/15/2023 21:02:45 - INFO - __main__ -   test: [batch: 364/391 ] | Loss: 1.038 | Acc: 74.227% (34679/46720)/ 91.884% (42928/46720)
01/15/2023 21:02:46 - INFO - __main__ -   test: [batch: 365/391 ] | Loss: 1.038 | Acc: 74.229% (34775/46848)/ 91.880% (43044/46848)
01/15/2023 21:02:48 - INFO - __main__ -   test: [batch: 366/391 ] | Loss: 1.037 | Acc: 74.263% (34886/46976)/ 91.898% (43170/46976)
01/15/2023 21:02:50 - INFO - __main__ -   test: [batch: 367/391 ] | Loss: 1.036 | Acc: 74.278% (34988/47104)/ 91.907% (43292/47104)
01/15/2023 21:02:51 - INFO - __main__ -   test: [batch: 368/391 ] | Loss: 1.036 | Acc: 74.274% (35081/47232)/ 91.914% (43413/47232)
01/15/2023 21:02:53 - INFO - __main__ -   test: [batch: 369/391 ] | Loss: 1.035 | Acc: 74.291% (35184/47360)/ 91.926% (43536/47360)
01/15/2023 21:02:54 - INFO - __main__ -   test: [batch: 370/391 ] | Loss: 1.035 | Acc: 74.267% (35268/47488)/ 91.931% (43656/47488)
01/15/2023 21:02:56 - INFO - __main__ -   test: [batch: 371/391 ] | Loss: 1.035 | Acc: 74.280% (35369/47616)/ 91.929% (43773/47616)
01/15/2023 21:02:57 - INFO - __main__ -   test: [batch: 372/391 ] | Loss: 1.033 | Acc: 74.330% (35488/47744)/ 91.947% (43899/47744)
01/15/2023 21:02:59 - INFO - __main__ -   test: [batch: 373/391 ] | Loss: 1.032 | Acc: 74.365% (35600/47872)/ 91.958% (44022/47872)
01/15/2023 21:03:01 - INFO - __main__ -   test: [batch: 374/391 ] | Loss: 1.031 | Acc: 74.388% (35706/48000)/ 91.958% (44140/48000)
01/15/2023 21:03:03 - INFO - __main__ -   test: [batch: 375/391 ] | Loss: 1.034 | Acc: 74.331% (35774/48128)/ 91.922% (44240/48128)
01/15/2023 21:03:04 - INFO - __main__ -   test: [batch: 376/391 ] | Loss: 1.035 | Acc: 74.316% (35862/48256)/ 91.904% (44349/48256)
01/15/2023 21:03:06 - INFO - __main__ -   test: [batch: 377/391 ] | Loss: 1.036 | Acc: 74.312% (35955/48384)/ 91.896% (44463/48384)
01/15/2023 21:03:07 - INFO - __main__ -   test: [batch: 378/391 ] | Loss: 1.039 | Acc: 74.246% (36018/48512)/ 91.860% (44563/48512)
01/15/2023 21:03:09 - INFO - __main__ -   test: [batch: 379/391 ] | Loss: 1.039 | Acc: 74.229% (36105/48640)/ 91.871% (44686/48640)
01/15/2023 21:03:10 - INFO - __main__ -   test: [batch: 380/391 ] | Loss: 1.038 | Acc: 74.239% (36205/48768)/ 91.886% (44811/48768)
01/15/2023 21:03:12 - INFO - __main__ -   test: [batch: 381/391 ] | Loss: 1.040 | Acc: 74.192% (36277/48896)/ 91.887% (44929/48896)
01/15/2023 21:03:13 - INFO - __main__ -   test: [batch: 382/391 ] | Loss: 1.041 | Acc: 74.160% (36356/49024)/ 91.879% (45043/49024)
01/15/2023 21:03:15 - INFO - __main__ -   test: [batch: 383/391 ] | Loss: 1.041 | Acc: 74.162% (36452/49152)/ 91.872% (45157/49152)
01/15/2023 21:03:16 - INFO - __main__ -   test: [batch: 384/391 ] | Loss: 1.039 | Acc: 74.198% (36565/49280)/ 91.887% (45282/49280)
01/15/2023 21:03:18 - INFO - __main__ -   test: [batch: 385/391 ] | Loss: 1.038 | Acc: 74.213% (36667/49408)/ 91.900% (45406/49408)
01/15/2023 21:03:19 - INFO - __main__ -   test: [batch: 386/391 ] | Loss: 1.036 | Acc: 74.253% (36782/49536)/ 91.919% (45533/49536)
01/15/2023 21:03:21 - INFO - __main__ -   test: [batch: 387/391 ] | Loss: 1.034 | Acc: 74.297% (36899/49664)/ 91.930% (45656/49664)
01/15/2023 21:03:23 - INFO - __main__ -   test: [batch: 388/391 ] | Loss: 1.032 | Acc: 74.349% (37020/49792)/ 91.944% (45781/49792)
01/15/2023 21:03:24 - INFO - __main__ -   test: [batch: 389/391 ] | Loss: 1.032 | Acc: 74.353% (37117/49920)/ 91.951% (45902/49920)
01/15/2023 21:03:26 - INFO - __main__ -   test: [batch: 390/391 ] | Loss: 1.034 | Acc: 74.316% (37158/50000)/ 91.942% (45971/50000)
01/15/2023 21:03:26 - INFO - __main__ -   Final accuracy: 74.316
