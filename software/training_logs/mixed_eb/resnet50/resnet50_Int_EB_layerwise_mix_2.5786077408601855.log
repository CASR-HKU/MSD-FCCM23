/home/jjc/miniconda3/envs/ant_quant/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
Namespace(a_low=75, a_up=150, abit=8, batch_size=128, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='csd_eb3', epoch=3, layer_4bit_l=None, layer_8bit_l='25,47,46,35,29,38,46,5,8,15,38,29,1,10,31,37,40,8,10,1,17,14,27,42,33,0,14,40,31,15,37,13,42,27,33,43,24,13,43,45,45,48,48,12,9,6,3,16,19', layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='resnet50', percent=100, ptq=False, resume=False, search=False, tag='', train=True, w_low=75, w_up=150, wbit=8)
01/15/2023 02:49:52 - INFO - __main__ -   output/resnet50_imagenet/int_W8A8_1365/gpu_0
01/15/2023 02:49:52 - INFO - __main__ -   Namespace(a_low=75, a_up=150, abit=8, batch_size=128, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='csd_eb3', epoch=3, layer_4bit_l=None, layer_8bit_l='25,47,46,35,29,38,46,5,8,15,38,29,1,10,31,37,40,8,10,1,17,14,27,42,33,0,14,40,31,15,37,13,42,27,33,43,24,13,43,45,45,48,48,12,9,6,3,16,19', layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='resnet50', percent=100, ptq=False, resume=False, search=False, tag='', train=True, w_low=75, w_up=150, wbit=8)
01/15/2023 02:49:52 - INFO - __main__ -   ==> Preparing data..
01/15/2023 02:49:56 - INFO - __main__ -   ==> Setting quantizer..
Namespace(a_low=75, a_up=150, abit=8, batch_size=128, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='csd_eb3', epoch=3, layer_4bit_l=None, layer_8bit_l='25,47,46,35,29,38,46,5,8,15,38,29,1,10,31,37,40,8,10,1,17,14,27,42,33,0,14,40,31,15,37,13,42,27,33,43,24,13,43,45,45,48,48,12,9,6,3,16,19', layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='resnet50', percent=100, ptq=False, resume=False, search=False, tag='', train=True, w_low=75, w_up=150, wbit=8)
01/15/2023 02:49:56 - INFO - __main__ -   Namespace(a_low=75, a_up=150, abit=8, batch_size=128, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='csd_eb3', epoch=3, layer_4bit_l=None, layer_8bit_l='25,47,46,35,29,38,46,5,8,15,38,29,1,10,31,37,40,8,10,1,17,14,27,42,33,0,14,40,31,15,37,13,42,27,33,43,24,13,43,45,45,48,48,12,9,6,3,16,19', layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='resnet50', percent=100, ptq=False, resume=False, search=False, tag='', train=True, w_low=75, w_up=150, wbit=8)
01/15/2023 02:49:56 - INFO - __main__ -   ==> Building model..
ResNet(
  (conv1): Conv2dQuantizer(
    (quant_weight): TensorQuantizer()
    (quant_input): TensorQuantizer()
  )
  (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (1): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (1): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): LinearQuantizer(
    (quant_weight): TensorQuantizer()
    (quant_input): TensorQuantizer()
  )
)
01/15/2023 02:49:58 - INFO - __main__ -   {'milestones': Counter({2: 1, 3: 1, 4: 1}), 'gamma': 0.1, 'base_lrs': [0.00025], 'last_epoch': 0, '_step_count': 1, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.00025]}
01/15/2023 02:49:58 - INFO - __main__ -   
Epoch: 0
Layer quant EB csd_eb3
int	8-bit 	 conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.3.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.3.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.3.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.3.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.3.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.3.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.4.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.4.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.4.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.5.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.5.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.5.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 fc.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 fc.quant_input,
set init to 1
------------- 8-bit EB2 Re-SET -------------
49
conv1.quant_weight 0
conv1.quant_input 0
layer1.0.conv1.quant_weight 1
layer1.0.conv1.quant_input 1
layer1.0.conv3.quant_weight 3
layer1.0.conv3.quant_input 3
layer1.1.conv1.quant_weight 5
layer1.1.conv1.quant_input 5
layer1.1.conv2.quant_weight 6
layer1.1.conv2.quant_input 6
layer1.2.conv1.quant_weight 8
layer1.2.conv1.quant_input 8
layer1.2.conv2.quant_weight 9
layer1.2.conv2.quant_input 9
layer1.2.conv3.quant_weight 10
layer1.2.conv3.quant_input 10
layer2.0.conv2.quant_weight 12
layer2.0.conv2.quant_input 12
layer2.0.conv3.quant_weight 13
layer2.0.conv3.quant_input 13
layer2.0.downsample.0.quant_weight 14
layer2.0.downsample.0.quant_input 14
layer2.1.conv1.quant_weight 15
layer2.1.conv1.quant_input 15
layer2.1.conv2.quant_weight 16
layer2.1.conv2.quant_input 16
layer2.1.conv3.quant_weight 17
layer2.1.conv3.quant_input 17
layer2.2.conv2.quant_weight 19
layer2.2.conv2.quant_input 19
layer3.0.conv1.quant_weight 24
layer3.0.conv1.quant_input 24
layer3.0.conv2.quant_weight 25
layer3.0.conv2.quant_input 25
layer3.0.downsample.0.quant_weight 27
layer3.0.downsample.0.quant_input 27
layer3.1.conv2.quant_weight 29
layer3.1.conv2.quant_input 29
layer3.2.conv1.quant_weight 31
layer3.2.conv1.quant_input 31
layer3.2.conv3.quant_weight 33
layer3.2.conv3.quant_input 33
layer3.3.conv2.quant_weight 35
layer3.3.conv2.quant_input 35
layer3.4.conv1.quant_weight 37
layer3.4.conv1.quant_input 37
layer3.4.conv2.quant_weight 38
layer3.4.conv2.quant_input 38
layer3.5.conv1.quant_weight 40
layer3.5.conv1.quant_input 40
layer3.5.conv3.quant_weight 42
layer3.5.conv3.quant_input 42
layer4.0.conv1.quant_weight 43
layer4.0.conv1.quant_input 43
layer4.0.conv3.quant_weight 45
layer4.0.conv3.quant_input 45
layer4.0.downsample.0.quant_weight 46
layer4.0.downsample.0.quant_input 46
layer4.1.conv1.quant_weight 47
layer4.1.conv1.quant_input 47
layer4.1.conv2.quant_weight 48
layer4.1.conv2.quant_input 48
------------- 8-bit EB2 Re-SET -------------
Layer quant EB csd_eb2
int	8-bit 	 conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.3.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.3.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.3.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.3.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.3.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.3.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.4.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.4.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.4.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.5.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.5.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.5.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 fc.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 fc.quant_input,
set init to 1
01/15/2023 02:50:56 - INFO - __main__ -   test: [epoch: 0 | batch: 0/10010 ] | Loss: 0.678 | Acc: 80.469% (103/128)
01/15/2023 02:54:27 - INFO - __main__ -   test: [epoch: 0 | batch: 100/10010 ] | Loss: 0.841 | Acc: 79.138% (10231/12928)
01/15/2023 02:57:58 - INFO - __main__ -   test: [epoch: 0 | batch: 200/10010 ] | Loss: 0.826 | Acc: 79.248% (20389/25728)
01/15/2023 03:01:28 - INFO - __main__ -   test: [epoch: 0 | batch: 300/10010 ] | Loss: 0.840 | Acc: 78.992% (30434/38528)
01/15/2023 03:05:01 - INFO - __main__ -   test: [epoch: 0 | batch: 400/10010 ] | Loss: 0.851 | Acc: 78.754% (40423/51328)
01/15/2023 03:08:32 - INFO - __main__ -   test: [epoch: 0 | batch: 500/10010 ] | Loss: 0.853 | Acc: 78.654% (50439/64128)
01/15/2023 03:12:05 - INFO - __main__ -   test: [epoch: 0 | batch: 600/10010 ] | Loss: 0.854 | Acc: 78.642% (60498/76928)
01/15/2023 03:15:37 - INFO - __main__ -   test: [epoch: 0 | batch: 700/10010 ] | Loss: 0.854 | Acc: 78.666% (70585/89728)
01/15/2023 03:19:08 - INFO - __main__ -   test: [epoch: 0 | batch: 800/10010 ] | Loss: 0.856 | Acc: 78.614% (80601/102528)
01/15/2023 03:22:40 - INFO - __main__ -   test: [epoch: 0 | batch: 900/10010 ] | Loss: 0.857 | Acc: 78.597% (90644/115328)
01/15/2023 03:26:11 - INFO - __main__ -   test: [epoch: 0 | batch: 1000/10010 ] | Loss: 0.858 | Acc: 78.575% (100676/128128)
01/15/2023 03:29:43 - INFO - __main__ -   test: [epoch: 0 | batch: 1100/10010 ] | Loss: 0.859 | Acc: 78.570% (110727/140928)
01/15/2023 03:33:13 - INFO - __main__ -   test: [epoch: 0 | batch: 1200/10010 ] | Loss: 0.861 | Acc: 78.535% (120730/153728)
01/15/2023 03:36:46 - INFO - __main__ -   test: [epoch: 0 | batch: 1300/10010 ] | Loss: 0.859 | Acc: 78.589% (130872/166528)
01/15/2023 03:40:18 - INFO - __main__ -   test: [epoch: 0 | batch: 1400/10010 ] | Loss: 0.860 | Acc: 78.596% (140945/179328)
01/15/2023 03:43:50 - INFO - __main__ -   test: [epoch: 0 | batch: 1500/10010 ] | Loss: 0.859 | Acc: 78.643% (151095/192128)
01/15/2023 03:47:23 - INFO - __main__ -   test: [epoch: 0 | batch: 1600/10010 ] | Loss: 0.859 | Acc: 78.628% (161131/204928)
01/15/2023 03:50:54 - INFO - __main__ -   test: [epoch: 0 | batch: 1700/10010 ] | Loss: 0.860 | Acc: 78.628% (171196/217728)
01/15/2023 03:54:26 - INFO - __main__ -   test: [epoch: 0 | batch: 1800/10010 ] | Loss: 0.858 | Acc: 78.665% (181346/230528)
01/15/2023 03:57:58 - INFO - __main__ -   test: [epoch: 0 | batch: 1900/10010 ] | Loss: 0.856 | Acc: 78.670% (191426/243328)
01/15/2023 04:01:29 - INFO - __main__ -   test: [epoch: 0 | batch: 2000/10010 ] | Loss: 0.856 | Acc: 78.677% (201515/256128)
01/15/2023 04:05:02 - INFO - __main__ -   test: [epoch: 0 | batch: 2100/10010 ] | Loss: 0.857 | Acc: 78.659% (211537/268928)
01/15/2023 04:08:33 - INFO - __main__ -   test: [epoch: 0 | batch: 2200/10010 ] | Loss: 0.857 | Acc: 78.646% (221568/281728)
01/15/2023 04:12:05 - INFO - __main__ -   test: [epoch: 0 | batch: 2300/10010 ] | Loss: 0.857 | Acc: 78.641% (231621/294528)
01/15/2023 04:15:38 - INFO - __main__ -   test: [epoch: 0 | batch: 2400/10010 ] | Loss: 0.858 | Acc: 78.645% (241698/307328)
01/15/2023 04:19:09 - INFO - __main__ -   test: [epoch: 0 | batch: 2500/10010 ] | Loss: 0.858 | Acc: 78.650% (251781/320128)
01/15/2023 04:22:42 - INFO - __main__ -   test: [epoch: 0 | batch: 2600/10010 ] | Loss: 0.858 | Acc: 78.653% (261859/332928)
01/15/2023 04:26:14 - INFO - __main__ -   test: [epoch: 0 | batch: 2700/10010 ] | Loss: 0.858 | Acc: 78.657% (271938/345728)
01/15/2023 04:29:47 - INFO - __main__ -   test: [epoch: 0 | batch: 2800/10010 ] | Loss: 0.858 | Acc: 78.647% (281972/358528)
01/15/2023 04:33:18 - INFO - __main__ -   test: [epoch: 0 | batch: 2900/10010 ] | Loss: 0.858 | Acc: 78.640% (292011/371328)
01/15/2023 04:36:50 - INFO - __main__ -   test: [epoch: 0 | batch: 3000/10010 ] | Loss: 0.857 | Acc: 78.637% (302067/384128)
01/15/2023 04:40:23 - INFO - __main__ -   test: [epoch: 0 | batch: 3100/10010 ] | Loss: 0.858 | Acc: 78.640% (312143/396928)
01/15/2023 04:43:55 - INFO - __main__ -   test: [epoch: 0 | batch: 3200/10010 ] | Loss: 0.858 | Acc: 78.638% (322202/409728)
01/15/2023 04:47:25 - INFO - __main__ -   test: [epoch: 0 | batch: 3300/10010 ] | Loss: 0.859 | Acc: 78.625% (332213/422528)
01/15/2023 04:50:57 - INFO - __main__ -   test: [epoch: 0 | batch: 3400/10010 ] | Loss: 0.858 | Acc: 78.635% (342322/435328)
01/15/2023 04:54:29 - INFO - __main__ -   test: [epoch: 0 | batch: 3500/10010 ] | Loss: 0.858 | Acc: 78.641% (352411/448128)
01/15/2023 04:57:59 - INFO - __main__ -   test: [epoch: 0 | batch: 3600/10010 ] | Loss: 0.858 | Acc: 78.631% (362433/460928)
01/15/2023 05:01:30 - INFO - __main__ -   test: [epoch: 0 | batch: 3700/10010 ] | Loss: 0.859 | Acc: 78.617% (372430/473728)
01/15/2023 05:05:03 - INFO - __main__ -   test: [epoch: 0 | batch: 3800/10010 ] | Loss: 0.859 | Acc: 78.618% (382499/486528)
01/15/2023 05:08:35 - INFO - __main__ -   test: [epoch: 0 | batch: 3900/10010 ] | Loss: 0.859 | Acc: 78.606% (392501/499328)
01/15/2023 05:12:06 - INFO - __main__ -   test: [epoch: 0 | batch: 4000/10010 ] | Loss: 0.859 | Acc: 78.602% (402544/512128)
01/15/2023 05:15:38 - INFO - __main__ -   test: [epoch: 0 | batch: 4100/10010 ] | Loss: 0.859 | Acc: 78.600% (412596/524928)
01/15/2023 05:19:09 - INFO - __main__ -   test: [epoch: 0 | batch: 4200/10010 ] | Loss: 0.859 | Acc: 78.599% (422651/537728)
01/15/2023 05:22:41 - INFO - __main__ -   test: [epoch: 0 | batch: 4300/10010 ] | Loss: 0.859 | Acc: 78.598% (432703/550528)
01/15/2023 05:26:11 - INFO - __main__ -   test: [epoch: 0 | batch: 4400/10010 ] | Loss: 0.859 | Acc: 78.607% (442814/563328)
01/15/2023 05:29:44 - INFO - __main__ -   test: [epoch: 0 | batch: 4500/10010 ] | Loss: 0.859 | Acc: 78.600% (452834/576128)
01/15/2023 05:33:15 - INFO - __main__ -   test: [epoch: 0 | batch: 4600/10010 ] | Loss: 0.859 | Acc: 78.605% (462928/588928)
01/15/2023 05:36:48 - INFO - __main__ -   test: [epoch: 0 | batch: 4700/10010 ] | Loss: 0.859 | Acc: 78.606% (472994/601728)
01/15/2023 05:40:18 - INFO - __main__ -   test: [epoch: 0 | batch: 4800/10010 ] | Loss: 0.859 | Acc: 78.605% (483047/614528)
01/15/2023 05:43:49 - INFO - __main__ -   test: [epoch: 0 | batch: 4900/10010 ] | Loss: 0.859 | Acc: 78.607% (493123/627328)
01/15/2023 05:47:20 - INFO - __main__ -   test: [epoch: 0 | batch: 5000/10010 ] | Loss: 0.859 | Acc: 78.618% (503257/640128)
01/15/2023 05:50:52 - INFO - __main__ -   test: [epoch: 0 | batch: 5100/10010 ] | Loss: 0.858 | Acc: 78.621% (513337/652928)
01/15/2023 05:54:23 - INFO - __main__ -   test: [epoch: 0 | batch: 5200/10010 ] | Loss: 0.859 | Acc: 78.619% (523387/665728)
01/15/2023 05:57:54 - INFO - __main__ -   test: [epoch: 0 | batch: 5300/10010 ] | Loss: 0.859 | Acc: 78.627% (533509/678528)
01/15/2023 06:01:23 - INFO - __main__ -   test: [epoch: 0 | batch: 5400/10010 ] | Loss: 0.859 | Acc: 78.617% (543502/691328)
01/15/2023 06:04:53 - INFO - __main__ -   test: [epoch: 0 | batch: 5500/10010 ] | Loss: 0.860 | Acc: 78.606% (553488/704128)
01/15/2023 06:08:25 - INFO - __main__ -   test: [epoch: 0 | batch: 5600/10010 ] | Loss: 0.859 | Acc: 78.617% (563625/716928)
01/15/2023 06:11:56 - INFO - __main__ -   test: [epoch: 0 | batch: 5700/10010 ] | Loss: 0.859 | Acc: 78.620% (573715/729728)
01/15/2023 06:15:27 - INFO - __main__ -   test: [epoch: 0 | batch: 5800/10010 ] | Loss: 0.859 | Acc: 78.623% (583796/742528)
01/15/2023 06:18:57 - INFO - __main__ -   test: [epoch: 0 | batch: 5900/10010 ] | Loss: 0.859 | Acc: 78.627% (593894/755328)
01/15/2023 06:22:30 - INFO - __main__ -   test: [epoch: 0 | batch: 6000/10010 ] | Loss: 0.859 | Acc: 78.623% (603924/768128)
01/15/2023 06:26:03 - INFO - __main__ -   test: [epoch: 0 | batch: 6100/10010 ] | Loss: 0.859 | Acc: 78.622% (613978/780928)
01/15/2023 06:29:35 - INFO - __main__ -   test: [epoch: 0 | batch: 6200/10010 ] | Loss: 0.860 | Acc: 78.616% (623996/793728)
01/15/2023 06:33:05 - INFO - __main__ -   test: [epoch: 0 | batch: 6300/10010 ] | Loss: 0.860 | Acc: 78.614% (634047/806528)
01/15/2023 06:36:37 - INFO - __main__ -   test: [epoch: 0 | batch: 6400/10010 ] | Loss: 0.860 | Acc: 78.611% (644080/819328)
01/15/2023 06:40:07 - INFO - __main__ -   test: [epoch: 0 | batch: 6500/10010 ] | Loss: 0.860 | Acc: 78.610% (654136/832128)
01/15/2023 06:43:39 - INFO - __main__ -   test: [epoch: 0 | batch: 6600/10010 ] | Loss: 0.860 | Acc: 78.609% (664186/844928)
01/15/2023 06:47:12 - INFO - __main__ -   test: [epoch: 0 | batch: 6700/10010 ] | Loss: 0.860 | Acc: 78.613% (674288/857728)
01/15/2023 06:50:45 - INFO - __main__ -   test: [epoch: 0 | batch: 6800/10010 ] | Loss: 0.860 | Acc: 78.610% (684324/870528)
01/15/2023 06:54:18 - INFO - __main__ -   test: [epoch: 0 | batch: 6900/10010 ] | Loss: 0.860 | Acc: 78.614% (694422/883328)
01/15/2023 06:57:49 - INFO - __main__ -   test: [epoch: 0 | batch: 7000/10010 ] | Loss: 0.860 | Acc: 78.609% (704433/896128)
01/15/2023 07:01:21 - INFO - __main__ -   test: [epoch: 0 | batch: 7100/10010 ] | Loss: 0.859 | Acc: 78.616% (714567/908928)
01/15/2023 07:04:53 - INFO - __main__ -   test: [epoch: 0 | batch: 7200/10010 ] | Loss: 0.859 | Acc: 78.623% (724692/921728)
01/15/2023 07:08:24 - INFO - __main__ -   test: [epoch: 0 | batch: 7300/10010 ] | Loss: 0.859 | Acc: 78.614% (734667/934528)
01/15/2023 07:11:55 - INFO - __main__ -   test: [epoch: 0 | batch: 7400/10010 ] | Loss: 0.860 | Acc: 78.611% (744702/947328)
01/15/2023 07:15:27 - INFO - __main__ -   test: [epoch: 0 | batch: 7500/10010 ] | Loss: 0.860 | Acc: 78.611% (754764/960128)
01/15/2023 07:18:58 - INFO - __main__ -   test: [epoch: 0 | batch: 7600/10010 ] | Loss: 0.860 | Acc: 78.611% (764833/972928)
01/15/2023 07:22:31 - INFO - __main__ -   test: [epoch: 0 | batch: 7700/10010 ] | Loss: 0.860 | Acc: 78.601% (774795/985728)
01/15/2023 07:26:01 - INFO - __main__ -   test: [epoch: 0 | batch: 7800/10010 ] | Loss: 0.860 | Acc: 78.599% (784835/998528)
01/15/2023 07:29:32 - INFO - __main__ -   test: [epoch: 0 | batch: 7900/10010 ] | Loss: 0.860 | Acc: 78.603% (794939/1011328)
01/15/2023 07:33:03 - INFO - __main__ -   test: [epoch: 0 | batch: 8000/10010 ] | Loss: 0.860 | Acc: 78.601% (804975/1024128)
01/15/2023 07:36:34 - INFO - __main__ -   test: [epoch: 0 | batch: 8100/10010 ] | Loss: 0.860 | Acc: 78.607% (815096/1036928)
01/15/2023 07:40:06 - INFO - __main__ -   test: [epoch: 0 | batch: 8200/10010 ] | Loss: 0.860 | Acc: 78.602% (825103/1049728)
01/15/2023 07:43:36 - INFO - __main__ -   test: [epoch: 0 | batch: 8300/10010 ] | Loss: 0.860 | Acc: 78.603% (835181/1062528)
01/15/2023 07:47:08 - INFO - __main__ -   test: [epoch: 0 | batch: 8400/10010 ] | Loss: 0.860 | Acc: 78.606% (845271/1075328)
01/15/2023 07:50:41 - INFO - __main__ -   test: [epoch: 0 | batch: 8500/10010 ] | Loss: 0.860 | Acc: 78.610% (855377/1088128)
01/15/2023 07:54:13 - INFO - __main__ -   test: [epoch: 0 | batch: 8600/10010 ] | Loss: 0.860 | Acc: 78.605% (865389/1100928)
01/15/2023 07:57:45 - INFO - __main__ -   test: [epoch: 0 | batch: 8700/10010 ] | Loss: 0.860 | Acc: 78.605% (875446/1113728)
01/15/2023 08:01:18 - INFO - __main__ -   test: [epoch: 0 | batch: 8800/10010 ] | Loss: 0.860 | Acc: 78.607% (885534/1126528)
01/15/2023 08:04:49 - INFO - __main__ -   test: [epoch: 0 | batch: 8900/10010 ] | Loss: 0.860 | Acc: 78.613% (895656/1139328)
01/15/2023 08:08:20 - INFO - __main__ -   test: [epoch: 0 | batch: 9000/10010 ] | Loss: 0.860 | Acc: 78.616% (905758/1152128)
01/15/2023 08:11:53 - INFO - __main__ -   test: [epoch: 0 | batch: 9100/10010 ] | Loss: 0.860 | Acc: 78.614% (915799/1164928)
01/15/2023 08:15:24 - INFO - __main__ -   test: [epoch: 0 | batch: 9200/10010 ] | Loss: 0.860 | Acc: 78.614% (925858/1177728)
01/15/2023 08:18:55 - INFO - __main__ -   test: [epoch: 0 | batch: 9300/10010 ] | Loss: 0.860 | Acc: 78.616% (935949/1190528)
01/15/2023 08:22:27 - INFO - __main__ -   test: [epoch: 0 | batch: 9400/10010 ] | Loss: 0.860 | Acc: 78.614% (945990/1203328)
01/15/2023 08:25:56 - INFO - __main__ -   test: [epoch: 0 | batch: 9500/10010 ] | Loss: 0.860 | Acc: 78.618% (956090/1216128)
01/15/2023 08:29:28 - INFO - __main__ -   test: [epoch: 0 | batch: 9600/10010 ] | Loss: 0.860 | Acc: 78.617% (966147/1228928)
01/15/2023 08:33:00 - INFO - __main__ -   test: [epoch: 0 | batch: 9700/10010 ] | Loss: 0.860 | Acc: 78.613% (976162/1241728)
01/15/2023 08:36:32 - INFO - __main__ -   test: [epoch: 0 | batch: 9800/10010 ] | Loss: 0.860 | Acc: 78.608% (986163/1254528)
01/15/2023 08:40:05 - INFO - __main__ -   test: [epoch: 0 | batch: 9900/10010 ] | Loss: 0.860 | Acc: 78.612% (996277/1267328)
01/15/2023 08:43:38 - INFO - __main__ -   test: [epoch: 0 | batch: 10000/10010 ] | Loss: 0.860 | Acc: 78.608% (1006284/1280128)
01/15/2023 08:43:58 - INFO - __main__ -   Saving Checkpoint
01/15/2023 08:44:00 - INFO - __main__ -   test: [batch: 0/391 ] | Loss: 0.441 | Acc: 87.500% (112/128)/ 97.656% (125/128)
01/15/2023 08:44:02 - INFO - __main__ -   test: [batch: 1/391 ] | Loss: 0.463 | Acc: 86.719% (222/256)/ 98.047% (251/256)
01/15/2023 08:44:05 - INFO - __main__ -   test: [batch: 2/391 ] | Loss: 0.617 | Acc: 83.333% (320/384)/ 95.833% (368/384)
01/15/2023 08:44:07 - INFO - __main__ -   test: [batch: 3/391 ] | Loss: 0.570 | Acc: 84.961% (435/512)/ 96.484% (494/512)
01/15/2023 08:44:09 - INFO - __main__ -   test: [batch: 4/391 ] | Loss: 0.487 | Acc: 87.188% (558/640)/ 97.188% (622/640)
01/15/2023 08:44:11 - INFO - __main__ -   test: [batch: 5/391 ] | Loss: 0.431 | Acc: 88.281% (678/768)/ 97.656% (750/768)
01/15/2023 08:44:13 - INFO - __main__ -   test: [batch: 6/391 ] | Loss: 0.424 | Acc: 88.504% (793/896)/ 97.545% (874/896)
01/15/2023 08:44:15 - INFO - __main__ -   test: [batch: 7/391 ] | Loss: 0.407 | Acc: 89.453% (916/1024)/ 97.656% (1000/1024)
01/15/2023 08:44:17 - INFO - __main__ -   test: [batch: 8/391 ] | Loss: 0.425 | Acc: 89.410% (1030/1152)/ 97.483% (1123/1152)
01/15/2023 08:44:19 - INFO - __main__ -   test: [batch: 9/391 ] | Loss: 0.404 | Acc: 89.922% (1151/1280)/ 97.578% (1249/1280)
01/15/2023 08:44:22 - INFO - __main__ -   test: [batch: 10/391 ] | Loss: 0.453 | Acc: 88.707% (1249/1408)/ 97.514% (1373/1408)
01/15/2023 08:44:24 - INFO - __main__ -   test: [batch: 11/391 ] | Loss: 0.458 | Acc: 88.932% (1366/1536)/ 97.396% (1496/1536)
01/15/2023 08:44:26 - INFO - __main__ -   test: [batch: 12/391 ] | Loss: 0.508 | Acc: 87.740% (1460/1664)/ 97.055% (1615/1664)
01/15/2023 08:44:28 - INFO - __main__ -   test: [batch: 13/391 ] | Loss: 0.557 | Acc: 86.105% (1543/1792)/ 96.596% (1731/1792)
01/15/2023 08:44:30 - INFO - __main__ -   test: [batch: 14/391 ] | Loss: 0.576 | Acc: 85.260% (1637/1920)/ 96.667% (1856/1920)
01/15/2023 08:44:32 - INFO - __main__ -   test: [batch: 15/391 ] | Loss: 0.585 | Acc: 84.912% (1739/2048)/ 96.777% (1982/2048)
01/15/2023 08:44:34 - INFO - __main__ -   test: [batch: 16/391 ] | Loss: 0.593 | Acc: 84.743% (1844/2176)/ 96.599% (2102/2176)
01/15/2023 08:44:36 - INFO - __main__ -   test: [batch: 17/391 ] | Loss: 0.619 | Acc: 84.288% (1942/2304)/ 96.094% (2214/2304)
01/15/2023 08:44:38 - INFO - __main__ -   test: [batch: 18/391 ] | Loss: 0.641 | Acc: 83.758% (2037/2432)/ 95.929% (2333/2432)
01/15/2023 08:44:40 - INFO - __main__ -   test: [batch: 19/391 ] | Loss: 0.648 | Acc: 83.555% (2139/2560)/ 95.859% (2454/2560)
01/15/2023 08:44:43 - INFO - __main__ -   test: [batch: 20/391 ] | Loss: 0.645 | Acc: 83.668% (2249/2688)/ 95.759% (2574/2688)
01/15/2023 08:44:45 - INFO - __main__ -   test: [batch: 21/391 ] | Loss: 0.677 | Acc: 83.026% (2338/2816)/ 95.597% (2692/2816)
01/15/2023 08:44:47 - INFO - __main__ -   test: [batch: 22/391 ] | Loss: 0.677 | Acc: 82.846% (2439/2944)/ 95.618% (2815/2944)
01/15/2023 08:44:49 - INFO - __main__ -   test: [batch: 23/391 ] | Loss: 0.721 | Acc: 81.934% (2517/3072)/ 95.312% (2928/3072)
01/15/2023 08:44:51 - INFO - __main__ -   test: [batch: 24/391 ] | Loss: 0.740 | Acc: 81.438% (2606/3200)/ 95.125% (3044/3200)
01/15/2023 08:44:53 - INFO - __main__ -   test: [batch: 25/391 ] | Loss: 0.765 | Acc: 80.769% (2688/3328)/ 94.862% (3157/3328)
01/15/2023 08:44:55 - INFO - __main__ -   test: [batch: 26/391 ] | Loss: 0.778 | Acc: 80.064% (2767/3456)/ 94.821% (3277/3456)
01/15/2023 08:44:57 - INFO - __main__ -   test: [batch: 27/391 ] | Loss: 0.761 | Acc: 80.580% (2888/3584)/ 94.866% (3400/3584)
01/15/2023 08:45:00 - INFO - __main__ -   test: [batch: 28/391 ] | Loss: 0.770 | Acc: 80.038% (2971/3712)/ 94.935% (3524/3712)
01/15/2023 08:45:02 - INFO - __main__ -   test: [batch: 29/391 ] | Loss: 0.763 | Acc: 80.182% (3079/3840)/ 95.052% (3650/3840)
01/15/2023 08:45:04 - INFO - __main__ -   test: [batch: 30/391 ] | Loss: 0.777 | Acc: 80.066% (3177/3968)/ 94.934% (3767/3968)
01/15/2023 08:45:06 - INFO - __main__ -   test: [batch: 31/391 ] | Loss: 0.773 | Acc: 80.249% (3287/4096)/ 95.020% (3892/4096)
01/15/2023 08:45:08 - INFO - __main__ -   test: [batch: 32/391 ] | Loss: 0.758 | Acc: 80.563% (3403/4224)/ 95.123% (4018/4224)
01/15/2023 08:45:11 - INFO - __main__ -   test: [batch: 33/391 ] | Loss: 0.752 | Acc: 80.767% (3515/4352)/ 95.175% (4142/4352)
01/15/2023 08:45:13 - INFO - __main__ -   test: [batch: 34/391 ] | Loss: 0.738 | Acc: 81.138% (3635/4480)/ 95.246% (4267/4480)
01/15/2023 08:45:15 - INFO - __main__ -   test: [batch: 35/391 ] | Loss: 0.725 | Acc: 81.510% (3756/4608)/ 95.247% (4389/4608)
01/15/2023 08:45:17 - INFO - __main__ -   test: [batch: 36/391 ] | Loss: 0.710 | Acc: 81.947% (3881/4736)/ 95.355% (4516/4736)
01/15/2023 08:45:19 - INFO - __main__ -   test: [batch: 37/391 ] | Loss: 0.700 | Acc: 82.257% (4001/4864)/ 95.415% (4641/4864)
01/15/2023 08:45:21 - INFO - __main__ -   test: [batch: 38/391 ] | Loss: 0.692 | Acc: 82.432% (4115/4992)/ 95.493% (4767/4992)
01/15/2023 08:45:24 - INFO - __main__ -   test: [batch: 39/391 ] | Loss: 0.687 | Acc: 82.480% (4223/5120)/ 95.527% (4891/5120)
01/15/2023 08:45:26 - INFO - __main__ -   test: [batch: 40/391 ] | Loss: 0.686 | Acc: 82.565% (4333/5248)/ 95.427% (5008/5248)
01/15/2023 08:45:28 - INFO - __main__ -   test: [batch: 41/391 ] | Loss: 0.690 | Acc: 82.645% (4443/5376)/ 95.331% (5125/5376)
01/15/2023 08:45:30 - INFO - __main__ -   test: [batch: 42/391 ] | Loss: 0.689 | Acc: 82.613% (4547/5504)/ 95.385% (5250/5504)
01/15/2023 08:45:32 - INFO - __main__ -   test: [batch: 43/391 ] | Loss: 0.687 | Acc: 82.670% (4656/5632)/ 95.348% (5370/5632)
01/15/2023 08:45:35 - INFO - __main__ -   test: [batch: 44/391 ] | Loss: 0.690 | Acc: 82.656% (4761/5760)/ 95.243% (5486/5760)
01/15/2023 08:45:37 - INFO - __main__ -   test: [batch: 45/391 ] | Loss: 0.687 | Acc: 82.796% (4875/5888)/ 95.245% (5608/5888)
01/15/2023 08:45:39 - INFO - __main__ -   test: [batch: 46/391 ] | Loss: 0.691 | Acc: 82.729% (4977/6016)/ 95.263% (5731/6016)
01/15/2023 08:45:41 - INFO - __main__ -   test: [batch: 47/391 ] | Loss: 0.692 | Acc: 82.682% (5080/6144)/ 95.312% (5856/6144)
01/15/2023 08:45:43 - INFO - __main__ -   test: [batch: 48/391 ] | Loss: 0.697 | Acc: 82.557% (5178/6272)/ 95.297% (5977/6272)
01/15/2023 08:45:45 - INFO - __main__ -   test: [batch: 49/391 ] | Loss: 0.700 | Acc: 82.609% (5287/6400)/ 95.219% (6094/6400)
01/15/2023 08:45:48 - INFO - __main__ -   test: [batch: 50/391 ] | Loss: 0.691 | Acc: 82.812% (5406/6528)/ 95.267% (6219/6528)
01/15/2023 08:45:50 - INFO - __main__ -   test: [batch: 51/391 ] | Loss: 0.684 | Acc: 82.993% (5524/6656)/ 95.328% (6345/6656)
01/15/2023 08:45:52 - INFO - __main__ -   test: [batch: 52/391 ] | Loss: 0.681 | Acc: 83.078% (5636/6784)/ 95.357% (6469/6784)
01/15/2023 08:45:54 - INFO - __main__ -   test: [batch: 53/391 ] | Loss: 0.671 | Acc: 83.275% (5756/6912)/ 95.428% (6596/6912)
01/15/2023 08:45:56 - INFO - __main__ -   test: [batch: 54/391 ] | Loss: 0.665 | Acc: 83.409% (5872/7040)/ 95.440% (6719/7040)
01/15/2023 08:45:59 - INFO - __main__ -   test: [batch: 55/391 ] | Loss: 0.658 | Acc: 83.566% (5990/7168)/ 95.494% (6845/7168)
01/15/2023 08:46:01 - INFO - __main__ -   test: [batch: 56/391 ] | Loss: 0.650 | Acc: 83.772% (6112/7296)/ 95.559% (6972/7296)
01/15/2023 08:46:03 - INFO - __main__ -   test: [batch: 57/391 ] | Loss: 0.643 | Acc: 83.984% (6235/7424)/ 95.622% (7099/7424)
01/15/2023 08:46:05 - INFO - __main__ -   test: [batch: 58/391 ] | Loss: 0.641 | Acc: 84.017% (6345/7552)/ 95.630% (7222/7552)
01/15/2023 08:46:07 - INFO - __main__ -   test: [batch: 59/391 ] | Loss: 0.643 | Acc: 83.932% (6446/7680)/ 95.651% (7346/7680)
01/15/2023 08:46:09 - INFO - __main__ -   test: [batch: 60/391 ] | Loss: 0.648 | Acc: 83.811% (6544/7808)/ 95.620% (7466/7808)
01/15/2023 08:46:11 - INFO - __main__ -   test: [batch: 61/391 ] | Loss: 0.649 | Acc: 83.795% (6650/7936)/ 95.640% (7590/7936)
01/15/2023 08:46:14 - INFO - __main__ -   test: [batch: 62/391 ] | Loss: 0.648 | Acc: 83.780% (6756/8064)/ 95.660% (7714/8064)
01/15/2023 08:46:16 - INFO - __main__ -   test: [batch: 63/391 ] | Loss: 0.656 | Acc: 83.643% (6852/8192)/ 95.605% (7832/8192)
01/15/2023 08:46:18 - INFO - __main__ -   test: [batch: 64/391 ] | Loss: 0.664 | Acc: 83.462% (6944/8320)/ 95.565% (7951/8320)
01/15/2023 08:46:20 - INFO - __main__ -   test: [batch: 65/391 ] | Loss: 0.677 | Acc: 82.978% (7010/8448)/ 95.478% (8066/8448)
01/15/2023 08:46:22 - INFO - __main__ -   test: [batch: 66/391 ] | Loss: 0.683 | Acc: 82.952% (7114/8576)/ 95.441% (8185/8576)
01/15/2023 08:46:24 - INFO - __main__ -   test: [batch: 67/391 ] | Loss: 0.685 | Acc: 82.881% (7214/8704)/ 95.450% (8308/8704)
01/15/2023 08:46:26 - INFO - __main__ -   test: [batch: 68/391 ] | Loss: 0.686 | Acc: 82.846% (7317/8832)/ 95.471% (8432/8832)
01/15/2023 08:46:29 - INFO - __main__ -   test: [batch: 69/391 ] | Loss: 0.681 | Acc: 82.935% (7431/8960)/ 95.513% (8558/8960)
01/15/2023 08:46:31 - INFO - __main__ -   test: [batch: 70/391 ] | Loss: 0.683 | Acc: 82.846% (7529/9088)/ 95.511% (8680/9088)
01/15/2023 08:46:33 - INFO - __main__ -   test: [batch: 71/391 ] | Loss: 0.681 | Acc: 82.878% (7638/9216)/ 95.530% (8804/9216)
01/15/2023 08:46:35 - INFO - __main__ -   test: [batch: 72/391 ] | Loss: 0.684 | Acc: 82.748% (7732/9344)/ 95.537% (8927/9344)
01/15/2023 08:46:37 - INFO - __main__ -   test: [batch: 73/391 ] | Loss: 0.687 | Acc: 82.622% (7826/9472)/ 95.545% (9050/9472)
01/15/2023 08:46:39 - INFO - __main__ -   test: [batch: 74/391 ] | Loss: 0.688 | Acc: 82.615% (7931/9600)/ 95.531% (9171/9600)
01/15/2023 08:46:41 - INFO - __main__ -   test: [batch: 75/391 ] | Loss: 0.693 | Acc: 82.432% (8019/9728)/ 95.528% (9293/9728)
01/15/2023 08:46:43 - INFO - __main__ -   test: [batch: 76/391 ] | Loss: 0.694 | Acc: 82.417% (8123/9856)/ 95.526% (9415/9856)
01/15/2023 08:46:45 - INFO - __main__ -   test: [batch: 77/391 ] | Loss: 0.694 | Acc: 82.392% (8226/9984)/ 95.553% (9540/9984)
01/15/2023 08:46:48 - INFO - __main__ -   test: [batch: 78/391 ] | Loss: 0.694 | Acc: 82.298% (8322/10112)/ 95.599% (9667/10112)
01/15/2023 08:46:50 - INFO - __main__ -   test: [batch: 79/391 ] | Loss: 0.692 | Acc: 82.305% (8428/10240)/ 95.625% (9792/10240)
01/15/2023 08:46:52 - INFO - __main__ -   test: [batch: 80/391 ] | Loss: 0.692 | Acc: 82.301% (8533/10368)/ 95.631% (9915/10368)
01/15/2023 08:46:54 - INFO - __main__ -   test: [batch: 81/391 ] | Loss: 0.691 | Acc: 82.308% (8639/10496)/ 95.655% (10040/10496)
01/15/2023 08:46:56 - INFO - __main__ -   test: [batch: 82/391 ] | Loss: 0.691 | Acc: 82.314% (8745/10624)/ 95.651% (10162/10624)
01/15/2023 08:46:58 - INFO - __main__ -   test: [batch: 83/391 ] | Loss: 0.691 | Acc: 82.347% (8854/10752)/ 95.638% (10283/10752)
01/15/2023 08:47:00 - INFO - __main__ -   test: [batch: 84/391 ] | Loss: 0.687 | Acc: 82.454% (8971/10880)/ 95.671% (10409/10880)
01/15/2023 08:47:02 - INFO - __main__ -   test: [batch: 85/391 ] | Loss: 0.685 | Acc: 82.449% (9076/11008)/ 95.712% (10536/11008)
01/15/2023 08:47:05 - INFO - __main__ -   test: [batch: 86/391 ] | Loss: 0.690 | Acc: 82.372% (9173/11136)/ 95.690% (10656/11136)
01/15/2023 08:47:07 - INFO - __main__ -   test: [batch: 87/391 ] | Loss: 0.688 | Acc: 82.395% (9281/11264)/ 95.694% (10779/11264)
01/15/2023 08:47:09 - INFO - __main__ -   test: [batch: 88/391 ] | Loss: 0.695 | Acc: 82.330% (9379/11392)/ 95.629% (10894/11392)
01/15/2023 08:47:11 - INFO - __main__ -   test: [batch: 89/391 ] | Loss: 0.693 | Acc: 82.370% (9489/11520)/ 95.642% (11018/11520)
01/15/2023 08:47:13 - INFO - __main__ -   test: [batch: 90/391 ] | Loss: 0.694 | Acc: 82.272% (9583/11648)/ 95.647% (11141/11648)
01/15/2023 08:47:15 - INFO - __main__ -   test: [batch: 91/391 ] | Loss: 0.693 | Acc: 82.311% (9693/11776)/ 95.652% (11264/11776)
01/15/2023 08:47:17 - INFO - __main__ -   test: [batch: 92/391 ] | Loss: 0.694 | Acc: 82.283% (9795/11904)/ 95.640% (11385/11904)
01/15/2023 08:47:19 - INFO - __main__ -   test: [batch: 93/391 ] | Loss: 0.697 | Acc: 82.098% (9878/12032)/ 95.670% (11511/12032)
01/15/2023 08:47:21 - INFO - __main__ -   test: [batch: 94/391 ] | Loss: 0.699 | Acc: 81.957% (9966/12160)/ 95.683% (11635/12160)
01/15/2023 08:47:23 - INFO - __main__ -   test: [batch: 95/391 ] | Loss: 0.697 | Acc: 82.023% (10079/12288)/ 95.687% (11758/12288)
01/15/2023 08:47:25 - INFO - __main__ -   test: [batch: 96/391 ] | Loss: 0.700 | Acc: 81.935% (10173/12416)/ 95.707% (11883/12416)
01/15/2023 08:47:27 - INFO - __main__ -   test: [batch: 97/391 ] | Loss: 0.702 | Acc: 81.760% (10256/12544)/ 95.727% (12008/12544)
01/15/2023 08:47:30 - INFO - __main__ -   test: [batch: 98/391 ] | Loss: 0.699 | Acc: 81.866% (10374/12672)/ 95.762% (12135/12672)
01/15/2023 08:47:32 - INFO - __main__ -   test: [batch: 99/391 ] | Loss: 0.693 | Acc: 82.008% (10497/12800)/ 95.805% (12263/12800)
01/15/2023 08:47:34 - INFO - __main__ -   test: [batch: 100/391 ] | Loss: 0.692 | Acc: 82.062% (10609/12928)/ 95.823% (12388/12928)
01/15/2023 08:47:36 - INFO - __main__ -   test: [batch: 101/391 ] | Loss: 0.687 | Acc: 82.169% (10728/13056)/ 95.864% (12516/13056)
01/15/2023 08:47:38 - INFO - __main__ -   test: [batch: 102/391 ] | Loss: 0.684 | Acc: 82.282% (10848/13184)/ 95.889% (12642/13184)
01/15/2023 08:47:40 - INFO - __main__ -   test: [batch: 103/391 ] | Loss: 0.686 | Acc: 82.114% (10931/13312)/ 95.906% (12767/13312)
01/15/2023 08:47:43 - INFO - __main__ -   test: [batch: 104/391 ] | Loss: 0.685 | Acc: 82.054% (11028/13440)/ 95.915% (12891/13440)
01/15/2023 08:47:45 - INFO - __main__ -   test: [batch: 105/391 ] | Loss: 0.686 | Acc: 82.053% (11133/13568)/ 95.917% (13014/13568)
01/15/2023 08:47:47 - INFO - __main__ -   test: [batch: 106/391 ] | Loss: 0.694 | Acc: 81.951% (11224/13696)/ 95.853% (13128/13696)
01/15/2023 08:47:49 - INFO - __main__ -   test: [batch: 107/391 ] | Loss: 0.691 | Acc: 82.075% (11346/13824)/ 95.884% (13255/13824)
01/15/2023 08:47:51 - INFO - __main__ -   test: [batch: 108/391 ] | Loss: 0.694 | Acc: 81.938% (11432/13952)/ 95.893% (13379/13952)
01/15/2023 08:47:53 - INFO - __main__ -   test: [batch: 109/391 ] | Loss: 0.695 | Acc: 81.911% (11533/14080)/ 95.895% (13502/14080)
01/15/2023 08:47:55 - INFO - __main__ -   test: [batch: 110/391 ] | Loss: 0.698 | Acc: 81.729% (11612/14208)/ 95.911% (13627/14208)
01/15/2023 08:47:57 - INFO - __main__ -   test: [batch: 111/391 ] | Loss: 0.701 | Acc: 81.676% (11709/14336)/ 95.878% (13745/14336)
01/15/2023 08:47:59 - INFO - __main__ -   test: [batch: 112/391 ] | Loss: 0.701 | Acc: 81.686% (11815/14464)/ 95.893% (13870/14464)
01/15/2023 08:48:02 - INFO - __main__ -   test: [batch: 113/391 ] | Loss: 0.700 | Acc: 81.730% (11926/14592)/ 95.895% (13993/14592)
01/15/2023 08:48:04 - INFO - __main__ -   test: [batch: 114/391 ] | Loss: 0.697 | Acc: 81.821% (12044/14720)/ 95.917% (14119/14720)
01/15/2023 08:48:06 - INFO - __main__ -   test: [batch: 115/391 ] | Loss: 0.694 | Acc: 81.910% (12162/14848)/ 95.939% (14245/14848)
01/15/2023 08:48:08 - INFO - __main__ -   test: [batch: 116/391 ] | Loss: 0.697 | Acc: 81.878% (12262/14976)/ 95.907% (14363/14976)
01/15/2023 08:48:10 - INFO - __main__ -   test: [batch: 117/391 ] | Loss: 0.696 | Acc: 81.905% (12371/15104)/ 95.915% (14487/15104)
01/15/2023 08:48:12 - INFO - __main__ -   test: [batch: 118/391 ] | Loss: 0.699 | Acc: 81.788% (12458/15232)/ 95.930% (14612/15232)
01/15/2023 08:48:14 - INFO - __main__ -   test: [batch: 119/391 ] | Loss: 0.697 | Acc: 81.836% (12570/15360)/ 95.951% (14738/15360)
01/15/2023 08:48:17 - INFO - __main__ -   test: [batch: 120/391 ] | Loss: 0.696 | Acc: 81.889% (12683/15488)/ 95.965% (14863/15488)
01/15/2023 08:48:19 - INFO - __main__ -   test: [batch: 121/391 ] | Loss: 0.701 | Acc: 81.737% (12764/15616)/ 95.934% (14981/15616)
01/15/2023 08:48:21 - INFO - __main__ -   test: [batch: 122/391 ] | Loss: 0.703 | Acc: 81.682% (12860/15744)/ 95.903% (15099/15744)
01/15/2023 08:48:23 - INFO - __main__ -   test: [batch: 123/391 ] | Loss: 0.704 | Acc: 81.697% (12967/15872)/ 95.898% (15221/15872)
01/15/2023 08:48:26 - INFO - __main__ -   test: [batch: 124/391 ] | Loss: 0.703 | Acc: 81.719% (13075/16000)/ 95.912% (15346/16000)
01/15/2023 08:48:28 - INFO - __main__ -   test: [batch: 125/391 ] | Loss: 0.699 | Acc: 81.814% (13195/16128)/ 95.939% (15473/16128)
01/15/2023 08:48:30 - INFO - __main__ -   test: [batch: 126/391 ] | Loss: 0.695 | Acc: 81.902% (13314/16256)/ 95.952% (15598/16256)
01/15/2023 08:48:32 - INFO - __main__ -   test: [batch: 127/391 ] | Loss: 0.693 | Acc: 81.982% (13432/16384)/ 95.959% (15722/16384)
01/15/2023 08:48:34 - INFO - __main__ -   test: [batch: 128/391 ] | Loss: 0.694 | Acc: 81.965% (13534/16512)/ 95.936% (15841/16512)
01/15/2023 08:48:36 - INFO - __main__ -   test: [batch: 129/391 ] | Loss: 0.693 | Acc: 81.983% (13642/16640)/ 95.944% (15965/16640)
01/15/2023 08:48:38 - INFO - __main__ -   test: [batch: 130/391 ] | Loss: 0.690 | Acc: 82.085% (13764/16768)/ 95.963% (16091/16768)
01/15/2023 08:48:40 - INFO - __main__ -   test: [batch: 131/391 ] | Loss: 0.689 | Acc: 82.132% (13877/16896)/ 95.969% (16215/16896)
01/15/2023 08:48:43 - INFO - __main__ -   test: [batch: 132/391 ] | Loss: 0.687 | Acc: 82.207% (13995/17024)/ 95.982% (16340/17024)
01/15/2023 08:48:45 - INFO - __main__ -   test: [batch: 133/391 ] | Loss: 0.688 | Acc: 82.154% (14091/17152)/ 95.977% (16462/17152)
01/15/2023 08:48:47 - INFO - __main__ -   test: [batch: 134/391 ] | Loss: 0.686 | Acc: 82.188% (14202/17280)/ 96.001% (16589/17280)
01/15/2023 08:48:49 - INFO - __main__ -   test: [batch: 135/391 ] | Loss: 0.686 | Acc: 82.181% (14306/17408)/ 95.996% (16711/17408)
01/15/2023 08:48:51 - INFO - __main__ -   test: [batch: 136/391 ] | Loss: 0.685 | Acc: 82.134% (14403/17536)/ 96.014% (16837/17536)
01/15/2023 08:48:53 - INFO - __main__ -   test: [batch: 137/391 ] | Loss: 0.683 | Acc: 82.173% (14515/17664)/ 96.043% (16965/17664)
01/15/2023 08:48:55 - INFO - __main__ -   test: [batch: 138/391 ] | Loss: 0.682 | Acc: 82.194% (14624/17792)/ 96.049% (17089/17792)
01/15/2023 08:48:57 - INFO - __main__ -   test: [batch: 139/391 ] | Loss: 0.688 | Acc: 82.037% (14701/17920)/ 96.044% (17211/17920)
01/15/2023 08:49:00 - INFO - __main__ -   test: [batch: 140/391 ] | Loss: 0.690 | Acc: 81.976% (14795/18048)/ 96.038% (17333/18048)
01/15/2023 08:49:02 - INFO - __main__ -   test: [batch: 141/391 ] | Loss: 0.688 | Acc: 82.015% (14907/18176)/ 96.050% (17458/18176)
01/15/2023 08:49:04 - INFO - __main__ -   test: [batch: 142/391 ] | Loss: 0.686 | Acc: 82.097% (15027/18304)/ 96.066% (17584/18304)
01/15/2023 08:49:06 - INFO - __main__ -   test: [batch: 143/391 ] | Loss: 0.687 | Acc: 82.118% (15136/18432)/ 96.050% (17704/18432)
01/15/2023 08:49:08 - INFO - __main__ -   test: [batch: 144/391 ] | Loss: 0.689 | Acc: 82.091% (15236/18560)/ 96.024% (17822/18560)
01/15/2023 08:49:10 - INFO - __main__ -   test: [batch: 145/391 ] | Loss: 0.692 | Acc: 82.063% (15336/18688)/ 95.997% (17940/18688)
01/15/2023 08:49:12 - INFO - __main__ -   test: [batch: 146/391 ] | Loss: 0.693 | Acc: 82.042% (15437/18816)/ 95.982% (18060/18816)
01/15/2023 08:49:14 - INFO - __main__ -   test: [batch: 147/391 ] | Loss: 0.693 | Acc: 82.047% (15543/18944)/ 95.967% (18180/18944)
01/15/2023 08:49:16 - INFO - __main__ -   test: [batch: 148/391 ] | Loss: 0.695 | Acc: 81.989% (15637/19072)/ 95.973% (18304/19072)
01/15/2023 08:49:18 - INFO - __main__ -   test: [batch: 149/391 ] | Loss: 0.699 | Acc: 81.891% (15723/19200)/ 95.943% (18421/19200)
01/15/2023 08:49:21 - INFO - __main__ -   test: [batch: 150/391 ] | Loss: 0.699 | Acc: 81.819% (15814/19328)/ 95.949% (18545/19328)
01/15/2023 08:49:23 - INFO - __main__ -   test: [batch: 151/391 ] | Loss: 0.699 | Acc: 81.841% (15923/19456)/ 95.950% (18668/19456)
01/15/2023 08:49:25 - INFO - __main__ -   test: [batch: 152/391 ] | Loss: 0.700 | Acc: 81.822% (16024/19584)/ 95.935% (18788/19584)
01/15/2023 08:49:27 - INFO - __main__ -   test: [batch: 153/391 ] | Loss: 0.698 | Acc: 81.884% (16141/19712)/ 95.942% (18912/19712)
01/15/2023 08:49:29 - INFO - __main__ -   test: [batch: 154/391 ] | Loss: 0.698 | Acc: 81.870% (16243/19840)/ 95.917% (19030/19840)
01/15/2023 08:49:32 - INFO - __main__ -   test: [batch: 155/391 ] | Loss: 0.699 | Acc: 81.886% (16351/19968)/ 95.908% (19151/19968)
01/15/2023 08:49:34 - INFO - __main__ -   test: [batch: 156/391 ] | Loss: 0.701 | Acc: 81.797% (16438/20096)/ 95.900% (19272/20096)
01/15/2023 08:49:36 - INFO - __main__ -   test: [batch: 157/391 ] | Loss: 0.702 | Acc: 81.784% (16540/20224)/ 95.876% (19390/20224)
01/15/2023 08:49:38 - INFO - __main__ -   test: [batch: 158/391 ] | Loss: 0.703 | Acc: 81.746% (16637/20352)/ 95.848% (19507/20352)
01/15/2023 08:49:40 - INFO - __main__ -   test: [batch: 159/391 ] | Loss: 0.705 | Acc: 81.729% (16738/20480)/ 95.840% (19628/20480)
01/15/2023 08:49:42 - INFO - __main__ -   test: [batch: 160/391 ] | Loss: 0.705 | Acc: 81.701% (16837/20608)/ 95.817% (19746/20608)
01/15/2023 08:49:44 - INFO - __main__ -   test: [batch: 161/391 ] | Loss: 0.715 | Acc: 81.486% (16897/20736)/ 95.722% (19849/20736)
01/15/2023 08:49:47 - INFO - __main__ -   test: [batch: 162/391 ] | Loss: 0.720 | Acc: 81.389% (16981/20864)/ 95.643% (19955/20864)
01/15/2023 08:49:49 - INFO - __main__ -   test: [batch: 163/391 ] | Loss: 0.723 | Acc: 81.336% (17074/20992)/ 95.632% (20075/20992)
01/15/2023 08:49:51 - INFO - __main__ -   test: [batch: 164/391 ] | Loss: 0.723 | Acc: 81.326% (17176/21120)/ 95.649% (20201/21120)
01/15/2023 08:49:53 - INFO - __main__ -   test: [batch: 165/391 ] | Loss: 0.725 | Acc: 81.245% (17263/21248)/ 95.647% (20323/21248)
01/15/2023 08:49:55 - INFO - __main__ -   test: [batch: 166/391 ] | Loss: 0.724 | Acc: 81.264% (17371/21376)/ 95.631% (20442/21376)
01/15/2023 08:49:57 - INFO - __main__ -   test: [batch: 167/391 ] | Loss: 0.727 | Acc: 81.203% (17462/21504)/ 95.615% (20561/21504)
01/15/2023 08:49:59 - INFO - __main__ -   test: [batch: 168/391 ] | Loss: 0.726 | Acc: 81.204% (17566/21632)/ 95.613% (20683/21632)
01/15/2023 08:50:01 - INFO - __main__ -   test: [batch: 169/391 ] | Loss: 0.728 | Acc: 81.167% (17662/21760)/ 95.570% (20796/21760)
01/15/2023 08:50:04 - INFO - __main__ -   test: [batch: 170/391 ] | Loss: 0.733 | Acc: 81.058% (17742/21888)/ 95.536% (20911/21888)
01/15/2023 08:50:06 - INFO - __main__ -   test: [batch: 171/391 ] | Loss: 0.736 | Acc: 81.005% (17834/22016)/ 95.508% (21027/22016)
01/15/2023 08:50:08 - INFO - __main__ -   test: [batch: 172/391 ] | Loss: 0.737 | Acc: 80.952% (17926/22144)/ 95.502% (21148/22144)
01/15/2023 08:50:10 - INFO - __main__ -   test: [batch: 173/391 ] | Loss: 0.739 | Acc: 80.891% (18016/22272)/ 95.470% (21263/22272)
01/15/2023 08:50:12 - INFO - __main__ -   test: [batch: 174/391 ] | Loss: 0.743 | Acc: 80.808% (18101/22400)/ 95.424% (21375/22400)
01/15/2023 08:50:14 - INFO - __main__ -   test: [batch: 175/391 ] | Loss: 0.742 | Acc: 80.864% (18217/22528)/ 95.432% (21499/22528)
01/15/2023 08:50:16 - INFO - __main__ -   test: [batch: 176/391 ] | Loss: 0.742 | Acc: 80.866% (18321/22656)/ 95.410% (21616/22656)
01/15/2023 08:50:19 - INFO - __main__ -   test: [batch: 177/391 ] | Loss: 0.746 | Acc: 80.789% (18407/22784)/ 95.370% (21729/22784)
01/15/2023 08:50:21 - INFO - __main__ -   test: [batch: 178/391 ] | Loss: 0.747 | Acc: 80.774% (18507/22912)/ 95.347% (21846/22912)
01/15/2023 08:50:23 - INFO - __main__ -   test: [batch: 179/391 ] | Loss: 0.751 | Acc: 80.716% (18597/23040)/ 95.308% (21959/23040)
01/15/2023 08:50:25 - INFO - __main__ -   test: [batch: 180/391 ] | Loss: 0.757 | Acc: 80.590% (18671/23168)/ 95.274% (22073/23168)
01/15/2023 08:50:27 - INFO - __main__ -   test: [batch: 181/391 ] | Loss: 0.763 | Acc: 80.456% (18743/23296)/ 95.214% (22181/23296)
01/15/2023 08:50:29 - INFO - __main__ -   test: [batch: 182/391 ] | Loss: 0.762 | Acc: 80.473% (18850/23424)/ 95.214% (22303/23424)
01/15/2023 08:50:31 - INFO - __main__ -   test: [batch: 183/391 ] | Loss: 0.768 | Acc: 80.350% (18924/23552)/ 95.126% (22404/23552)
01/15/2023 08:50:33 - INFO - __main__ -   test: [batch: 184/391 ] | Loss: 0.767 | Acc: 80.372% (19032/23680)/ 95.114% (22523/23680)
01/15/2023 08:50:35 - INFO - __main__ -   test: [batch: 185/391 ] | Loss: 0.767 | Acc: 80.381% (19137/23808)/ 95.107% (22643/23808)
01/15/2023 08:50:38 - INFO - __main__ -   test: [batch: 186/391 ] | Loss: 0.770 | Acc: 80.339% (19230/23936)/ 95.070% (22756/23936)
01/15/2023 08:50:40 - INFO - __main__ -   test: [batch: 187/391 ] | Loss: 0.775 | Acc: 80.219% (19304/24064)/ 95.055% (22874/24064)
01/15/2023 08:50:42 - INFO - __main__ -   test: [batch: 188/391 ] | Loss: 0.779 | Acc: 80.097% (19377/24192)/ 95.036% (22991/24192)
01/15/2023 08:50:44 - INFO - __main__ -   test: [batch: 189/391 ] | Loss: 0.780 | Acc: 80.033% (19464/24320)/ 95.037% (23113/24320)
01/15/2023 08:50:46 - INFO - __main__ -   test: [batch: 190/391 ] | Loss: 0.785 | Acc: 79.945% (19545/24448)/ 95.014% (23229/24448)
01/15/2023 08:50:48 - INFO - __main__ -   test: [batch: 191/391 ] | Loss: 0.787 | Acc: 79.911% (19639/24576)/ 94.995% (23346/24576)
01/15/2023 08:50:50 - INFO - __main__ -   test: [batch: 192/391 ] | Loss: 0.793 | Acc: 79.797% (19713/24704)/ 94.916% (23448/24704)
01/15/2023 08:50:53 - INFO - __main__ -   test: [batch: 193/391 ] | Loss: 0.793 | Acc: 79.812% (19819/24832)/ 94.910% (23568/24832)
01/15/2023 08:50:55 - INFO - __main__ -   test: [batch: 194/391 ] | Loss: 0.796 | Acc: 79.748% (19905/24960)/ 94.888% (23684/24960)
01/15/2023 08:50:57 - INFO - __main__ -   test: [batch: 195/391 ] | Loss: 0.801 | Acc: 79.668% (19987/25088)/ 94.826% (23790/25088)
01/15/2023 08:50:59 - INFO - __main__ -   test: [batch: 196/391 ] | Loss: 0.805 | Acc: 79.565% (20063/25216)/ 94.785% (23901/25216)
01/15/2023 08:51:01 - INFO - __main__ -   test: [batch: 197/391 ] | Loss: 0.809 | Acc: 79.490% (20146/25344)/ 94.768% (24018/25344)
01/15/2023 08:51:03 - INFO - __main__ -   test: [batch: 198/391 ] | Loss: 0.811 | Acc: 79.452% (20238/25472)/ 94.739% (24132/25472)
01/15/2023 08:51:05 - INFO - __main__ -   test: [batch: 199/391 ] | Loss: 0.811 | Acc: 79.422% (20332/25600)/ 94.742% (24254/25600)
01/15/2023 08:51:07 - INFO - __main__ -   test: [batch: 200/391 ] | Loss: 0.810 | Acc: 79.388% (20425/25728)/ 94.741% (24375/25728)
01/15/2023 08:51:09 - INFO - __main__ -   test: [batch: 201/391 ] | Loss: 0.814 | Acc: 79.305% (20505/25856)/ 94.701% (24486/25856)
01/15/2023 08:51:12 - INFO - __main__ -   test: [batch: 202/391 ] | Loss: 0.815 | Acc: 79.299% (20605/25984)/ 94.689% (24604/25984)
01/15/2023 08:51:14 - INFO - __main__ -   test: [batch: 203/391 ] | Loss: 0.816 | Acc: 79.289% (20704/26112)/ 94.677% (24722/26112)
01/15/2023 08:51:16 - INFO - __main__ -   test: [batch: 204/391 ] | Loss: 0.819 | Acc: 79.200% (20782/26240)/ 94.661% (24839/26240)
01/15/2023 08:51:18 - INFO - __main__ -   test: [batch: 205/391 ] | Loss: 0.822 | Acc: 79.119% (20862/26368)/ 94.630% (24952/26368)
01/15/2023 08:51:20 - INFO - __main__ -   test: [batch: 206/391 ] | Loss: 0.823 | Acc: 79.095% (20957/26496)/ 94.626% (25072/26496)
01/15/2023 08:51:22 - INFO - __main__ -   test: [batch: 207/391 ] | Loss: 0.827 | Acc: 79.008% (21035/26624)/ 94.591% (25184/26624)
01/15/2023 08:51:24 - INFO - __main__ -   test: [batch: 208/391 ] | Loss: 0.829 | Acc: 78.966% (21125/26752)/ 94.561% (25297/26752)
01/15/2023 08:51:27 - INFO - __main__ -   test: [batch: 209/391 ] | Loss: 0.829 | Acc: 78.958% (21224/26880)/ 94.580% (25423/26880)
01/15/2023 08:51:29 - INFO - __main__ -   test: [batch: 210/391 ] | Loss: 0.830 | Acc: 78.943% (21321/27008)/ 94.565% (25540/27008)
01/15/2023 08:51:31 - INFO - __main__ -   test: [batch: 211/391 ] | Loss: 0.832 | Acc: 78.880% (21405/27136)/ 94.539% (25654/27136)
01/15/2023 08:51:33 - INFO - __main__ -   test: [batch: 212/391 ] | Loss: 0.834 | Acc: 78.822% (21490/27264)/ 94.517% (25769/27264)
01/15/2023 08:51:35 - INFO - __main__ -   test: [batch: 213/391 ] | Loss: 0.834 | Acc: 78.826% (21592/27392)/ 94.524% (25892/27392)
01/15/2023 08:51:37 - INFO - __main__ -   test: [batch: 214/391 ] | Loss: 0.834 | Acc: 78.805% (21687/27520)/ 94.528% (26014/27520)
01/15/2023 08:51:40 - INFO - __main__ -   test: [batch: 215/391 ] | Loss: 0.834 | Acc: 78.823% (21793/27648)/ 94.517% (26132/27648)
01/15/2023 08:51:42 - INFO - __main__ -   test: [batch: 216/391 ] | Loss: 0.833 | Acc: 78.863% (21905/27776)/ 94.535% (26258/27776)
01/15/2023 08:51:44 - INFO - __main__ -   test: [batch: 217/391 ] | Loss: 0.837 | Acc: 78.802% (21989/27904)/ 94.477% (26363/27904)
01/15/2023 08:51:46 - INFO - __main__ -   test: [batch: 218/391 ] | Loss: 0.840 | Acc: 78.749% (22075/28032)/ 94.438% (26473/28032)
01/15/2023 08:51:48 - INFO - __main__ -   test: [batch: 219/391 ] | Loss: 0.839 | Acc: 78.782% (22185/28160)/ 94.446% (26596/28160)
01/15/2023 08:51:50 - INFO - __main__ -   test: [batch: 220/391 ] | Loss: 0.837 | Acc: 78.818% (22296/28288)/ 94.461% (26721/28288)
01/15/2023 08:51:52 - INFO - __main__ -   test: [batch: 221/391 ] | Loss: 0.839 | Acc: 78.776% (22385/28416)/ 94.443% (26837/28416)
01/15/2023 08:51:55 - INFO - __main__ -   test: [batch: 222/391 ] | Loss: 0.837 | Acc: 78.833% (22502/28544)/ 94.458% (26962/28544)
01/15/2023 08:51:57 - INFO - __main__ -   test: [batch: 223/391 ] | Loss: 0.837 | Acc: 78.830% (22602/28672)/ 94.448% (27080/28672)
01/15/2023 08:51:59 - INFO - __main__ -   test: [batch: 224/391 ] | Loss: 0.837 | Acc: 78.854% (22710/28800)/ 94.444% (27200/28800)
01/15/2023 08:52:01 - INFO - __main__ -   test: [batch: 225/391 ] | Loss: 0.836 | Acc: 78.858% (22812/28928)/ 94.452% (27323/28928)
01/15/2023 08:52:03 - INFO - __main__ -   test: [batch: 226/391 ] | Loss: 0.836 | Acc: 78.834% (22906/29056)/ 94.459% (27446/29056)
01/15/2023 08:52:05 - INFO - __main__ -   test: [batch: 227/391 ] | Loss: 0.838 | Acc: 78.817% (23002/29184)/ 94.449% (27564/29184)
01/15/2023 08:52:07 - INFO - __main__ -   test: [batch: 228/391 ] | Loss: 0.842 | Acc: 78.746% (23082/29312)/ 94.391% (27668/29312)
01/15/2023 08:52:09 - INFO - __main__ -   test: [batch: 229/391 ] | Loss: 0.846 | Acc: 78.692% (23167/29440)/ 94.344% (27775/29440)
01/15/2023 08:52:11 - INFO - __main__ -   test: [batch: 230/391 ] | Loss: 0.849 | Acc: 78.636% (23251/29568)/ 94.311% (27886/29568)
01/15/2023 08:52:14 - INFO - __main__ -   test: [batch: 231/391 ] | Loss: 0.849 | Acc: 78.623% (23348/29696)/ 94.299% (28003/29696)
01/15/2023 08:52:16 - INFO - __main__ -   test: [batch: 232/391 ] | Loss: 0.848 | Acc: 78.655% (23458/29824)/ 94.313% (28128/29824)
01/15/2023 08:52:18 - INFO - __main__ -   test: [batch: 233/391 ] | Loss: 0.850 | Acc: 78.596% (23541/29952)/ 94.294% (28243/29952)
01/15/2023 08:52:20 - INFO - __main__ -   test: [batch: 234/391 ] | Loss: 0.856 | Acc: 78.477% (23606/30080)/ 94.222% (28342/30080)
01/15/2023 08:52:22 - INFO - __main__ -   test: [batch: 235/391 ] | Loss: 0.856 | Acc: 78.466% (23703/30208)/ 94.227% (28464/30208)
01/15/2023 08:52:24 - INFO - __main__ -   test: [batch: 236/391 ] | Loss: 0.855 | Acc: 78.507% (23816/30336)/ 94.235% (28587/30336)
01/15/2023 08:52:26 - INFO - __main__ -   test: [batch: 237/391 ] | Loss: 0.855 | Acc: 78.512% (23918/30464)/ 94.200% (28697/30464)
01/15/2023 08:52:28 - INFO - __main__ -   test: [batch: 238/391 ] | Loss: 0.854 | Acc: 78.550% (24030/30592)/ 94.208% (28820/30592)
01/15/2023 08:52:30 - INFO - __main__ -   test: [batch: 239/391 ] | Loss: 0.853 | Acc: 78.587% (24142/30720)/ 94.209% (28941/30720)
01/15/2023 08:52:32 - INFO - __main__ -   test: [batch: 240/391 ] | Loss: 0.853 | Acc: 78.605% (24248/30848)/ 94.201% (29059/30848)
01/15/2023 08:52:34 - INFO - __main__ -   test: [batch: 241/391 ] | Loss: 0.856 | Acc: 78.522% (24323/30976)/ 94.157% (29166/30976)
01/15/2023 08:52:37 - INFO - __main__ -   test: [batch: 242/391 ] | Loss: 0.858 | Acc: 78.414% (24390/31104)/ 94.142% (29282/31104)
01/15/2023 08:52:39 - INFO - __main__ -   test: [batch: 243/391 ] | Loss: 0.864 | Acc: 78.279% (24448/31232)/ 94.077% (29382/31232)
01/15/2023 08:52:41 - INFO - __main__ -   test: [batch: 244/391 ] | Loss: 0.864 | Acc: 78.281% (24549/31360)/ 94.069% (29500/31360)
01/15/2023 08:52:43 - INFO - __main__ -   test: [batch: 245/391 ] | Loss: 0.864 | Acc: 78.290% (24652/31488)/ 94.061% (29618/31488)
01/15/2023 08:52:45 - INFO - __main__ -   test: [batch: 246/391 ] | Loss: 0.864 | Acc: 78.267% (24745/31616)/ 94.054% (29736/31616)
01/15/2023 08:52:47 - INFO - __main__ -   test: [batch: 247/391 ] | Loss: 0.870 | Acc: 78.172% (24815/31744)/ 93.974% (29831/31744)
01/15/2023 08:52:49 - INFO - __main__ -   test: [batch: 248/391 ] | Loss: 0.871 | Acc: 78.138% (24904/31872)/ 93.966% (29949/31872)
01/15/2023 08:52:51 - INFO - __main__ -   test: [batch: 249/391 ] | Loss: 0.873 | Acc: 78.009% (24963/32000)/ 93.956% (30066/32000)
01/15/2023 08:52:54 - INFO - __main__ -   test: [batch: 250/391 ] | Loss: 0.871 | Acc: 78.038% (25072/32128)/ 93.974% (30192/32128)
01/15/2023 08:52:56 - INFO - __main__ -   test: [batch: 251/391 ] | Loss: 0.874 | Acc: 77.995% (25158/32256)/ 93.942% (30302/32256)
01/15/2023 08:52:58 - INFO - __main__ -   test: [batch: 252/391 ] | Loss: 0.873 | Acc: 78.023% (25267/32384)/ 93.935% (30420/32384)
01/15/2023 08:53:00 - INFO - __main__ -   test: [batch: 253/391 ] | Loss: 0.874 | Acc: 77.996% (25358/32512)/ 93.919% (30535/32512)
01/15/2023 08:53:02 - INFO - __main__ -   test: [batch: 254/391 ] | Loss: 0.878 | Acc: 77.929% (25436/32640)/ 93.876% (30641/32640)
01/15/2023 08:53:04 - INFO - __main__ -   test: [batch: 255/391 ] | Loss: 0.880 | Acc: 77.893% (25524/32768)/ 93.866% (30758/32768)
01/15/2023 08:53:06 - INFO - __main__ -   test: [batch: 256/391 ] | Loss: 0.886 | Acc: 77.775% (25585/32896)/ 93.832% (30867/32896)
01/15/2023 08:53:08 - INFO - __main__ -   test: [batch: 257/391 ] | Loss: 0.886 | Acc: 77.762% (25680/33024)/ 93.820% (30983/33024)
01/15/2023 08:53:10 - INFO - __main__ -   test: [batch: 258/391 ] | Loss: 0.887 | Acc: 77.754% (25777/33152)/ 93.804% (31098/33152)
01/15/2023 08:53:12 - INFO - __main__ -   test: [batch: 259/391 ] | Loss: 0.891 | Acc: 77.632% (25836/33280)/ 93.792% (31214/33280)
01/15/2023 08:53:14 - INFO - __main__ -   test: [batch: 260/391 ] | Loss: 0.892 | Acc: 77.607% (25927/33408)/ 93.786% (31332/33408)
01/15/2023 08:53:17 - INFO - __main__ -   test: [batch: 261/391 ] | Loss: 0.889 | Acc: 77.648% (26040/33536)/ 93.807% (31459/33536)
01/15/2023 08:53:19 - INFO - __main__ -   test: [batch: 262/391 ] | Loss: 0.889 | Acc: 77.676% (26149/33664)/ 93.815% (31582/33664)
01/15/2023 08:53:21 - INFO - __main__ -   test: [batch: 263/391 ] | Loss: 0.891 | Acc: 77.610% (26226/33792)/ 93.786% (31692/33792)
01/15/2023 08:53:23 - INFO - __main__ -   test: [batch: 264/391 ] | Loss: 0.897 | Acc: 77.547% (26304/33920)/ 93.723% (31791/33920)
01/15/2023 08:53:25 - INFO - __main__ -   test: [batch: 265/391 ] | Loss: 0.896 | Acc: 77.576% (26413/34048)/ 93.727% (31912/34048)
01/15/2023 08:53:27 - INFO - __main__ -   test: [batch: 266/391 ] | Loss: 0.898 | Acc: 77.508% (26489/34176)/ 93.709% (32026/34176)
01/15/2023 08:53:29 - INFO - __main__ -   test: [batch: 267/391 ] | Loss: 0.898 | Acc: 77.542% (26600/34304)/ 93.709% (32146/34304)
01/15/2023 08:53:32 - INFO - __main__ -   test: [batch: 268/391 ] | Loss: 0.897 | Acc: 77.567% (26708/34432)/ 93.706% (32265/34432)
01/15/2023 08:53:33 - INFO - __main__ -   test: [batch: 269/391 ] | Loss: 0.898 | Acc: 77.520% (26791/34560)/ 93.686% (32378/34560)
01/15/2023 08:53:35 - INFO - __main__ -   test: [batch: 270/391 ] | Loss: 0.902 | Acc: 77.456% (26868/34688)/ 93.658% (32488/34688)
01/15/2023 08:53:38 - INFO - __main__ -   test: [batch: 271/391 ] | Loss: 0.902 | Acc: 77.467% (26971/34816)/ 93.652% (32606/34816)
01/15/2023 08:53:40 - INFO - __main__ -   test: [batch: 272/391 ] | Loss: 0.903 | Acc: 77.432% (27058/34944)/ 93.650% (32725/34944)
01/15/2023 08:53:42 - INFO - __main__ -   test: [batch: 273/391 ] | Loss: 0.903 | Acc: 77.435% (27158/35072)/ 93.639% (32841/35072)
01/15/2023 08:53:44 - INFO - __main__ -   test: [batch: 274/391 ] | Loss: 0.903 | Acc: 77.429% (27255/35200)/ 93.642% (32962/35200)
01/15/2023 08:53:46 - INFO - __main__ -   test: [batch: 275/391 ] | Loss: 0.905 | Acc: 77.395% (27342/35328)/ 93.637% (33080/35328)
01/15/2023 08:53:48 - INFO - __main__ -   test: [batch: 276/391 ] | Loss: 0.906 | Acc: 77.366% (27431/35456)/ 93.637% (33200/35456)
01/15/2023 08:53:50 - INFO - __main__ -   test: [batch: 277/391 ] | Loss: 0.906 | Acc: 77.341% (27521/35584)/ 93.626% (33316/35584)
01/15/2023 08:53:52 - INFO - __main__ -   test: [batch: 278/391 ] | Loss: 0.906 | Acc: 77.352% (27624/35712)/ 93.624% (33435/35712)
01/15/2023 08:53:54 - INFO - __main__ -   test: [batch: 279/391 ] | Loss: 0.906 | Acc: 77.360% (27726/35840)/ 93.616% (33552/35840)
01/15/2023 08:53:56 - INFO - __main__ -   test: [batch: 280/391 ] | Loss: 0.907 | Acc: 77.341% (27818/35968)/ 93.605% (33668/35968)
01/15/2023 08:53:58 - INFO - __main__ -   test: [batch: 281/391 ] | Loss: 0.907 | Acc: 77.349% (27920/36096)/ 93.600% (33786/36096)
01/15/2023 08:54:00 - INFO - __main__ -   test: [batch: 282/391 ] | Loss: 0.906 | Acc: 77.385% (28032/36224)/ 93.606% (33908/36224)
01/15/2023 08:54:03 - INFO - __main__ -   test: [batch: 283/391 ] | Loss: 0.907 | Acc: 77.358% (28121/36352)/ 93.601% (34026/36352)
01/15/2023 08:54:05 - INFO - __main__ -   test: [batch: 284/391 ] | Loss: 0.910 | Acc: 77.308% (28202/36480)/ 93.564% (34132/36480)
01/15/2023 08:54:07 - INFO - __main__ -   test: [batch: 285/391 ] | Loss: 0.913 | Acc: 77.270% (28287/36608)/ 93.521% (34236/36608)
01/15/2023 08:54:09 - INFO - __main__ -   test: [batch: 286/391 ] | Loss: 0.914 | Acc: 77.238% (28374/36736)/ 93.508% (34351/36736)
01/15/2023 08:54:11 - INFO - __main__ -   test: [batch: 287/391 ] | Loss: 0.914 | Acc: 77.238% (28473/36864)/ 93.500% (34468/36864)
01/15/2023 08:54:14 - INFO - __main__ -   test: [batch: 288/391 ] | Loss: 0.913 | Acc: 77.257% (28579/36992)/ 93.504% (34589/36992)
01/15/2023 08:54:16 - INFO - __main__ -   test: [batch: 289/391 ] | Loss: 0.915 | Acc: 77.217% (28663/37120)/ 93.473% (34697/37120)
01/15/2023 08:54:18 - INFO - __main__ -   test: [batch: 290/391 ] | Loss: 0.917 | Acc: 77.132% (28730/37248)/ 93.476% (34818/37248)
01/15/2023 08:54:20 - INFO - __main__ -   test: [batch: 291/391 ] | Loss: 0.917 | Acc: 77.146% (28834/37376)/ 93.466% (34934/37376)
01/15/2023 08:54:22 - INFO - __main__ -   test: [batch: 292/391 ] | Loss: 0.918 | Acc: 77.093% (28913/37504)/ 93.449% (35047/37504)
01/15/2023 08:54:24 - INFO - __main__ -   test: [batch: 293/391 ] | Loss: 0.919 | Acc: 77.089% (29010/37632)/ 93.442% (35164/37632)
01/15/2023 08:54:26 - INFO - __main__ -   test: [batch: 294/391 ] | Loss: 0.920 | Acc: 77.063% (29099/37760)/ 93.435% (35281/37760)
01/15/2023 08:54:28 - INFO - __main__ -   test: [batch: 295/391 ] | Loss: 0.919 | Acc: 77.093% (29209/37888)/ 93.441% (35403/37888)
01/15/2023 08:54:30 - INFO - __main__ -   test: [batch: 296/391 ] | Loss: 0.919 | Acc: 77.096% (29309/38016)/ 93.432% (35519/38016)
01/15/2023 08:54:32 - INFO - __main__ -   test: [batch: 297/391 ] | Loss: 0.921 | Acc: 77.079% (29401/38144)/ 93.407% (35629/38144)
01/15/2023 08:54:34 - INFO - __main__ -   test: [batch: 298/391 ] | Loss: 0.923 | Acc: 77.048% (29488/38272)/ 93.384% (35740/38272)
01/15/2023 08:54:37 - INFO - __main__ -   test: [batch: 299/391 ] | Loss: 0.924 | Acc: 77.039% (29583/38400)/ 93.362% (35851/38400)
01/15/2023 08:54:39 - INFO - __main__ -   test: [batch: 300/391 ] | Loss: 0.924 | Acc: 77.043% (29683/38528)/ 93.355% (35968/38528)
01/15/2023 08:54:41 - INFO - __main__ -   test: [batch: 301/391 ] | Loss: 0.925 | Acc: 77.028% (29776/38656)/ 93.344% (36083/38656)
01/15/2023 08:54:43 - INFO - __main__ -   test: [batch: 302/391 ] | Loss: 0.927 | Acc: 76.991% (29860/38784)/ 93.327% (36196/38784)
01/15/2023 08:54:45 - INFO - __main__ -   test: [batch: 303/391 ] | Loss: 0.928 | Acc: 76.966% (29949/38912)/ 93.313% (36310/38912)
01/15/2023 08:54:47 - INFO - __main__ -   test: [batch: 304/391 ] | Loss: 0.927 | Acc: 76.975% (30051/39040)/ 93.317% (36431/39040)
01/15/2023 08:54:49 - INFO - __main__ -   test: [batch: 305/391 ] | Loss: 0.928 | Acc: 76.928% (30131/39168)/ 93.308% (36547/39168)
01/15/2023 08:54:51 - INFO - __main__ -   test: [batch: 306/391 ] | Loss: 0.929 | Acc: 76.909% (30222/39296)/ 93.284% (36657/39296)
01/15/2023 08:54:53 - INFO - __main__ -   test: [batch: 307/391 ] | Loss: 0.930 | Acc: 76.915% (30323/39424)/ 93.271% (36771/39424)
01/15/2023 08:54:55 - INFO - __main__ -   test: [batch: 308/391 ] | Loss: 0.930 | Acc: 76.896% (30414/39552)/ 93.272% (36891/39552)
01/15/2023 08:54:57 - INFO - __main__ -   test: [batch: 309/391 ] | Loss: 0.931 | Acc: 76.893% (30511/39680)/ 93.251% (37002/39680)
01/15/2023 08:54:59 - INFO - __main__ -   test: [batch: 310/391 ] | Loss: 0.932 | Acc: 76.884% (30606/39808)/ 93.238% (37116/39808)
01/15/2023 08:55:02 - INFO - __main__ -   test: [batch: 311/391 ] | Loss: 0.933 | Acc: 76.858% (30694/39936)/ 93.227% (37231/39936)
01/15/2023 08:55:04 - INFO - __main__ -   test: [batch: 312/391 ] | Loss: 0.934 | Acc: 76.845% (30787/40064)/ 93.203% (37341/40064)
01/15/2023 08:55:06 - INFO - __main__ -   test: [batch: 313/391 ] | Loss: 0.933 | Acc: 76.883% (30901/40192)/ 93.223% (37468/40192)
01/15/2023 08:55:08 - INFO - __main__ -   test: [batch: 314/391 ] | Loss: 0.933 | Acc: 76.875% (30996/40320)/ 93.217% (37585/40320)
01/15/2023 08:55:10 - INFO - __main__ -   test: [batch: 315/391 ] | Loss: 0.934 | Acc: 76.857% (31087/40448)/ 93.201% (37698/40448)
01/15/2023 08:55:12 - INFO - __main__ -   test: [batch: 316/391 ] | Loss: 0.937 | Acc: 76.765% (31148/40576)/ 93.173% (37806/40576)
01/15/2023 08:55:14 - INFO - __main__ -   test: [batch: 317/391 ] | Loss: 0.939 | Acc: 76.722% (31229/40704)/ 93.143% (37913/40704)
01/15/2023 08:55:16 - INFO - __main__ -   test: [batch: 318/391 ] | Loss: 0.937 | Acc: 76.754% (31340/40832)/ 93.160% (38039/40832)
01/15/2023 08:55:18 - INFO - __main__ -   test: [batch: 319/391 ] | Loss: 0.940 | Acc: 76.699% (31416/40960)/ 93.127% (38145/40960)
01/15/2023 08:55:20 - INFO - __main__ -   test: [batch: 320/391 ] | Loss: 0.939 | Acc: 76.733% (31528/41088)/ 93.134% (38267/41088)
01/15/2023 08:55:23 - INFO - __main__ -   test: [batch: 321/391 ] | Loss: 0.939 | Acc: 76.752% (31634/41216)/ 93.126% (38383/41216)
01/15/2023 08:55:25 - INFO - __main__ -   test: [batch: 322/391 ] | Loss: 0.940 | Acc: 76.727% (31722/41344)/ 93.119% (38499/41344)
01/15/2023 08:55:27 - INFO - __main__ -   test: [batch: 323/391 ] | Loss: 0.942 | Acc: 76.690% (31805/41472)/ 93.097% (38609/41472)
01/15/2023 08:55:29 - INFO - __main__ -   test: [batch: 324/391 ] | Loss: 0.943 | Acc: 76.683% (31900/41600)/ 93.091% (38726/41600)
01/15/2023 08:55:31 - INFO - __main__ -   test: [batch: 325/391 ] | Loss: 0.942 | Acc: 76.690% (32001/41728)/ 93.091% (38845/41728)
01/15/2023 08:55:34 - INFO - __main__ -   test: [batch: 326/391 ] | Loss: 0.945 | Acc: 76.601% (32062/41856)/ 93.060% (38951/41856)
01/15/2023 08:55:36 - INFO - __main__ -   test: [batch: 327/391 ] | Loss: 0.949 | Acc: 76.524% (32128/41984)/ 93.026% (39056/41984)
01/15/2023 08:55:38 - INFO - __main__ -   test: [batch: 328/391 ] | Loss: 0.950 | Acc: 76.470% (32203/42112)/ 93.004% (39166/42112)
01/15/2023 08:55:40 - INFO - __main__ -   test: [batch: 329/391 ] | Loss: 0.950 | Acc: 76.465% (32299/42240)/ 93.009% (39287/42240)
01/15/2023 08:55:42 - INFO - __main__ -   test: [batch: 330/391 ] | Loss: 0.952 | Acc: 76.423% (32379/42368)/ 92.983% (39395/42368)
01/15/2023 08:55:44 - INFO - __main__ -   test: [batch: 331/391 ] | Loss: 0.953 | Acc: 76.391% (32463/42496)/ 92.990% (39517/42496)
01/15/2023 08:55:46 - INFO - __main__ -   test: [batch: 332/391 ] | Loss: 0.953 | Acc: 76.380% (32556/42624)/ 92.990% (39636/42624)
01/15/2023 08:55:48 - INFO - __main__ -   test: [batch: 333/391 ] | Loss: 0.952 | Acc: 76.406% (32665/42752)/ 92.997% (39758/42752)
01/15/2023 08:55:50 - INFO - __main__ -   test: [batch: 334/391 ] | Loss: 0.953 | Acc: 76.381% (32752/42880)/ 92.978% (39869/42880)
01/15/2023 08:55:52 - INFO - __main__ -   test: [batch: 335/391 ] | Loss: 0.954 | Acc: 76.356% (32839/43008)/ 92.964% (39982/43008)
01/15/2023 08:55:55 - INFO - __main__ -   test: [batch: 336/391 ] | Loss: 0.956 | Acc: 76.312% (32918/43136)/ 92.948% (40094/43136)
01/15/2023 08:55:57 - INFO - __main__ -   test: [batch: 337/391 ] | Loss: 0.956 | Acc: 76.306% (33013/43264)/ 92.943% (40211/43264)
01/15/2023 08:55:59 - INFO - __main__ -   test: [batch: 338/391 ] | Loss: 0.956 | Acc: 76.297% (33107/43392)/ 92.953% (40334/43392)
01/15/2023 08:56:01 - INFO - __main__ -   test: [batch: 339/391 ] | Loss: 0.958 | Acc: 76.266% (33191/43520)/ 92.927% (40442/43520)
01/15/2023 08:56:03 - INFO - __main__ -   test: [batch: 340/391 ] | Loss: 0.958 | Acc: 76.260% (33286/43648)/ 92.939% (40566/43648)
01/15/2023 08:56:05 - INFO - __main__ -   test: [batch: 341/391 ] | Loss: 0.956 | Acc: 76.298% (33400/43776)/ 92.957% (40693/43776)
01/15/2023 08:56:07 - INFO - __main__ -   test: [batch: 342/391 ] | Loss: 0.957 | Acc: 76.239% (33472/43904)/ 92.951% (40809/43904)
01/15/2023 08:56:09 - INFO - __main__ -   test: [batch: 343/391 ] | Loss: 0.957 | Acc: 76.238% (33569/44032)/ 92.953% (40929/44032)
01/15/2023 08:56:12 - INFO - __main__ -   test: [batch: 344/391 ] | Loss: 0.957 | Acc: 76.234% (33665/44160)/ 92.942% (41043/44160)
01/15/2023 08:56:14 - INFO - __main__ -   test: [batch: 345/391 ] | Loss: 0.961 | Acc: 76.170% (33734/44288)/ 92.906% (41146/44288)
01/15/2023 08:56:16 - INFO - __main__ -   test: [batch: 346/391 ] | Loss: 0.962 | Acc: 76.153% (33824/44416)/ 92.897% (41261/44416)
01/15/2023 08:56:18 - INFO - __main__ -   test: [batch: 347/391 ] | Loss: 0.962 | Acc: 76.170% (33929/44544)/ 92.908% (41385/44544)
01/15/2023 08:56:20 - INFO - __main__ -   test: [batch: 348/391 ] | Loss: 0.963 | Acc: 76.148% (34017/44672)/ 92.886% (41494/44672)
01/15/2023 08:56:22 - INFO - __main__ -   test: [batch: 349/391 ] | Loss: 0.963 | Acc: 76.147% (34114/44800)/ 92.893% (41616/44800)
01/15/2023 08:56:24 - INFO - __main__ -   test: [batch: 350/391 ] | Loss: 0.963 | Acc: 76.162% (34218/44928)/ 92.893% (41735/44928)
01/15/2023 08:56:26 - INFO - __main__ -   test: [batch: 351/391 ] | Loss: 0.965 | Acc: 76.108% (34291/45056)/ 92.878% (41847/45056)
01/15/2023 08:56:29 - INFO - __main__ -   test: [batch: 352/391 ] | Loss: 0.965 | Acc: 76.104% (34387/45184)/ 92.876% (41965/45184)
01/15/2023 08:56:31 - INFO - __main__ -   test: [batch: 353/391 ] | Loss: 0.968 | Acc: 76.062% (34465/45312)/ 92.834% (42065/45312)
01/15/2023 08:56:33 - INFO - __main__ -   test: [batch: 354/391 ] | Loss: 0.970 | Acc: 76.001% (34535/45440)/ 92.812% (42174/45440)
01/15/2023 08:56:35 - INFO - __main__ -   test: [batch: 355/391 ] | Loss: 0.973 | Acc: 75.937% (34603/45568)/ 92.804% (42289/45568)
01/15/2023 08:56:37 - INFO - __main__ -   test: [batch: 356/391 ] | Loss: 0.973 | Acc: 75.930% (34697/45696)/ 92.807% (42409/45696)
01/15/2023 08:56:39 - INFO - __main__ -   test: [batch: 357/391 ] | Loss: 0.971 | Acc: 75.965% (34810/45824)/ 92.820% (42534/45824)
01/15/2023 08:56:41 - INFO - __main__ -   test: [batch: 358/391 ] | Loss: 0.971 | Acc: 75.995% (34921/45952)/ 92.823% (42654/45952)
01/15/2023 08:56:43 - INFO - __main__ -   test: [batch: 359/391 ] | Loss: 0.971 | Acc: 76.005% (35023/46080)/ 92.815% (42769/46080)
01/15/2023 08:56:45 - INFO - __main__ -   test: [batch: 360/391 ] | Loss: 0.973 | Acc: 75.978% (35108/46208)/ 92.804% (42883/46208)
01/15/2023 08:56:48 - INFO - __main__ -   test: [batch: 361/391 ] | Loss: 0.973 | Acc: 75.982% (35207/46336)/ 92.813% (43006/46336)
01/15/2023 08:56:50 - INFO - __main__ -   test: [batch: 362/391 ] | Loss: 0.972 | Acc: 75.979% (35303/46464)/ 92.827% (43131/46464)
01/15/2023 08:56:52 - INFO - __main__ -   test: [batch: 363/391 ] | Loss: 0.973 | Acc: 75.968% (35395/46592)/ 92.821% (43247/46592)
01/15/2023 08:56:54 - INFO - __main__ -   test: [batch: 364/391 ] | Loss: 0.971 | Acc: 75.997% (35506/46720)/ 92.830% (43370/46720)
01/15/2023 08:56:56 - INFO - __main__ -   test: [batch: 365/391 ] | Loss: 0.971 | Acc: 76.012% (35610/46848)/ 92.834% (43491/46848)
01/15/2023 08:56:58 - INFO - __main__ -   test: [batch: 366/391 ] | Loss: 0.969 | Acc: 76.045% (35723/46976)/ 92.852% (43618/46976)
01/15/2023 08:57:00 - INFO - __main__ -   test: [batch: 367/391 ] | Loss: 0.968 | Acc: 76.068% (35831/47104)/ 92.865% (43743/47104)
01/15/2023 08:57:02 - INFO - __main__ -   test: [batch: 368/391 ] | Loss: 0.968 | Acc: 76.076% (35932/47232)/ 92.873% (43866/47232)
01/15/2023 08:57:04 - INFO - __main__ -   test: [batch: 369/391 ] | Loss: 0.967 | Acc: 76.098% (36040/47360)/ 92.882% (43989/47360)
01/15/2023 08:57:06 - INFO - __main__ -   test: [batch: 370/391 ] | Loss: 0.967 | Acc: 76.084% (36131/47488)/ 92.885% (44109/47488)
01/15/2023 08:57:08 - INFO - __main__ -   test: [batch: 371/391 ] | Loss: 0.967 | Acc: 76.092% (36232/47616)/ 92.887% (44229/47616)
01/15/2023 08:57:10 - INFO - __main__ -   test: [batch: 372/391 ] | Loss: 0.965 | Acc: 76.135% (36350/47744)/ 92.902% (44355/47744)
01/15/2023 08:57:13 - INFO - __main__ -   test: [batch: 373/391 ] | Loss: 0.964 | Acc: 76.174% (36466/47872)/ 92.908% (44477/47872)
01/15/2023 08:57:15 - INFO - __main__ -   test: [batch: 374/391 ] | Loss: 0.963 | Acc: 76.204% (36578/48000)/ 92.910% (44597/48000)
01/15/2023 08:57:17 - INFO - __main__ -   test: [batch: 375/391 ] | Loss: 0.966 | Acc: 76.137% (36643/48128)/ 92.873% (44698/48128)
01/15/2023 08:57:19 - INFO - __main__ -   test: [batch: 376/391 ] | Loss: 0.966 | Acc: 76.131% (36738/48256)/ 92.859% (44810/48256)
01/15/2023 08:57:21 - INFO - __main__ -   test: [batch: 377/391 ] | Loss: 0.967 | Acc: 76.118% (36829/48384)/ 92.851% (44925/48384)
01/15/2023 08:57:23 - INFO - __main__ -   test: [batch: 378/391 ] | Loss: 0.970 | Acc: 76.039% (36888/48512)/ 92.810% (45024/48512)
01/15/2023 08:57:25 - INFO - __main__ -   test: [batch: 379/391 ] | Loss: 0.970 | Acc: 76.018% (36975/48640)/ 92.817% (45146/48640)
01/15/2023 08:57:27 - INFO - __main__ -   test: [batch: 380/391 ] | Loss: 0.970 | Acc: 76.015% (37071/48768)/ 92.825% (45269/48768)
01/15/2023 08:57:29 - INFO - __main__ -   test: [batch: 381/391 ] | Loss: 0.972 | Acc: 75.967% (37145/48896)/ 92.824% (45387/48896)
01/15/2023 08:57:31 - INFO - __main__ -   test: [batch: 382/391 ] | Loss: 0.973 | Acc: 75.932% (37225/49024)/ 92.810% (45499/49024)
01/15/2023 08:57:33 - INFO - __main__ -   test: [batch: 383/391 ] | Loss: 0.973 | Acc: 75.938% (37325/49152)/ 92.802% (45614/49152)
01/15/2023 08:57:35 - INFO - __main__ -   test: [batch: 384/391 ] | Loss: 0.972 | Acc: 75.976% (37441/49280)/ 92.815% (45739/49280)
01/15/2023 08:57:38 - INFO - __main__ -   test: [batch: 385/391 ] | Loss: 0.970 | Acc: 75.986% (37543/49408)/ 92.825% (45863/49408)
01/15/2023 08:57:40 - INFO - __main__ -   test: [batch: 386/391 ] | Loss: 0.968 | Acc: 76.034% (37664/49536)/ 92.842% (45990/49536)
01/15/2023 08:57:42 - INFO - __main__ -   test: [batch: 387/391 ] | Loss: 0.967 | Acc: 76.073% (37781/49664)/ 92.852% (46114/49664)
01/15/2023 08:57:44 - INFO - __main__ -   test: [batch: 388/391 ] | Loss: 0.965 | Acc: 76.119% (37901/49792)/ 92.864% (46239/49792)
01/15/2023 08:57:46 - INFO - __main__ -   test: [batch: 389/391 ] | Loss: 0.964 | Acc: 76.122% (38000/49920)/ 92.867% (46359/49920)
01/15/2023 08:57:49 - INFO - __main__ -   test: [batch: 390/391 ] | Loss: 0.966 | Acc: 76.074% (38037/50000)/ 92.858% (46429/50000)
01/15/2023 08:57:49 - INFO - __main__ -   Final accuracy: 76.074
01/15/2023 08:57:49 - INFO - __main__ -   {'milestones': Counter({2: 1, 3: 1, 4: 1}), 'gamma': 0.1, 'base_lrs': [0.00025], 'last_epoch': 1, '_step_count': 2, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.00025]}
01/15/2023 08:57:49 - INFO - __main__ -   
Epoch: 1
01/15/2023 08:57:51 - INFO - __main__ -   test: [epoch: 1 | batch: 0/10010 ] | Loss: 0.623 | Acc: 86.719% (111/128)
01/15/2023 09:01:22 - INFO - __main__ -   test: [epoch: 1 | batch: 100/10010 ] | Loss: 0.828 | Acc: 79.649% (10297/12928)
01/15/2023 09:04:53 - INFO - __main__ -   test: [epoch: 1 | batch: 200/10010 ] | Loss: 0.828 | Acc: 79.412% (20431/25728)
01/15/2023 09:08:25 - INFO - __main__ -   test: [epoch: 1 | batch: 300/10010 ] | Loss: 0.841 | Acc: 79.150% (30495/38528)
01/15/2023 09:11:56 - INFO - __main__ -   test: [epoch: 1 | batch: 400/10010 ] | Loss: 0.854 | Acc: 78.877% (40486/51328)
01/15/2023 09:15:28 - INFO - __main__ -   test: [epoch: 1 | batch: 500/10010 ] | Loss: 0.854 | Acc: 78.841% (50559/64128)
01/15/2023 09:18:58 - INFO - __main__ -   test: [epoch: 1 | batch: 600/10010 ] | Loss: 0.857 | Acc: 78.757% (60586/76928)
01/15/2023 09:22:28 - INFO - __main__ -   test: [epoch: 1 | batch: 700/10010 ] | Loss: 0.857 | Acc: 78.703% (70619/89728)
01/15/2023 09:25:58 - INFO - __main__ -   test: [epoch: 1 | batch: 800/10010 ] | Loss: 0.856 | Acc: 78.707% (80697/102528)
01/15/2023 09:29:29 - INFO - __main__ -   test: [epoch: 1 | batch: 900/10010 ] | Loss: 0.857 | Acc: 78.687% (90748/115328)
01/15/2023 09:33:02 - INFO - __main__ -   test: [epoch: 1 | batch: 1000/10010 ] | Loss: 0.857 | Acc: 78.639% (100759/128128)
01/15/2023 09:36:32 - INFO - __main__ -   test: [epoch: 1 | batch: 1100/10010 ] | Loss: 0.859 | Acc: 78.589% (110754/140928)
01/15/2023 09:40:02 - INFO - __main__ -   test: [epoch: 1 | batch: 1200/10010 ] | Loss: 0.860 | Acc: 78.599% (120829/153728)
01/15/2023 09:43:34 - INFO - __main__ -   test: [epoch: 1 | batch: 1300/10010 ] | Loss: 0.857 | Acc: 78.670% (131007/166528)
01/15/2023 09:47:06 - INFO - __main__ -   test: [epoch: 1 | batch: 1400/10010 ] | Loss: 0.857 | Acc: 78.680% (141095/179328)
01/15/2023 09:50:38 - INFO - __main__ -   test: [epoch: 1 | batch: 1500/10010 ] | Loss: 0.856 | Acc: 78.679% (151164/192128)
01/15/2023 09:54:10 - INFO - __main__ -   test: [epoch: 1 | batch: 1600/10010 ] | Loss: 0.859 | Acc: 78.643% (161162/204928)
01/15/2023 09:57:42 - INFO - __main__ -   test: [epoch: 1 | batch: 1700/10010 ] | Loss: 0.858 | Acc: 78.641% (171223/217728)
01/15/2023 10:01:14 - INFO - __main__ -   test: [epoch: 1 | batch: 1800/10010 ] | Loss: 0.858 | Acc: 78.653% (181318/230528)
01/15/2023 10:04:45 - INFO - __main__ -   test: [epoch: 1 | batch: 1900/10010 ] | Loss: 0.857 | Acc: 78.686% (191465/243328)
01/15/2023 10:08:17 - INFO - __main__ -   test: [epoch: 1 | batch: 2000/10010 ] | Loss: 0.857 | Acc: 78.700% (201574/256128)
01/15/2023 10:11:48 - INFO - __main__ -   test: [epoch: 1 | batch: 2100/10010 ] | Loss: 0.856 | Acc: 78.697% (211638/268928)
01/15/2023 10:15:20 - INFO - __main__ -   test: [epoch: 1 | batch: 2200/10010 ] | Loss: 0.858 | Acc: 78.671% (221639/281728)
01/15/2023 10:18:52 - INFO - __main__ -   test: [epoch: 1 | batch: 2300/10010 ] | Loss: 0.858 | Acc: 78.660% (231676/294528)
01/15/2023 10:22:22 - INFO - __main__ -   test: [epoch: 1 | batch: 2400/10010 ] | Loss: 0.857 | Acc: 78.690% (241837/307328)
01/15/2023 10:25:53 - INFO - __main__ -   test: [epoch: 1 | batch: 2500/10010 ] | Loss: 0.858 | Acc: 78.662% (251820/320128)
01/15/2023 10:29:25 - INFO - __main__ -   test: [epoch: 1 | batch: 2600/10010 ] | Loss: 0.859 | Acc: 78.663% (261892/332928)
01/15/2023 10:32:56 - INFO - __main__ -   test: [epoch: 1 | batch: 2700/10010 ] | Loss: 0.859 | Acc: 78.660% (271951/345728)
01/15/2023 10:36:29 - INFO - __main__ -   test: [epoch: 1 | batch: 2800/10010 ] | Loss: 0.859 | Acc: 78.652% (281990/358528)
01/15/2023 10:40:01 - INFO - __main__ -   test: [epoch: 1 | batch: 2900/10010 ] | Loss: 0.859 | Acc: 78.650% (292050/371328)
01/15/2023 10:43:33 - INFO - __main__ -   test: [epoch: 1 | batch: 3000/10010 ] | Loss: 0.859 | Acc: 78.664% (302169/384128)
01/15/2023 10:47:05 - INFO - __main__ -   test: [epoch: 1 | batch: 3100/10010 ] | Loss: 0.859 | Acc: 78.661% (312226/396928)
01/15/2023 10:50:36 - INFO - __main__ -   test: [epoch: 1 | batch: 3200/10010 ] | Loss: 0.859 | Acc: 78.640% (322210/409728)
01/15/2023 10:54:07 - INFO - __main__ -   test: [epoch: 1 | batch: 3300/10010 ] | Loss: 0.860 | Acc: 78.623% (332205/422528)
01/15/2023 10:57:38 - INFO - __main__ -   test: [epoch: 1 | batch: 3400/10010 ] | Loss: 0.860 | Acc: 78.631% (342302/435328)
01/15/2023 11:01:11 - INFO - __main__ -   test: [epoch: 1 | batch: 3500/10010 ] | Loss: 0.860 | Acc: 78.631% (352368/448128)
01/15/2023 11:04:42 - INFO - __main__ -   test: [epoch: 1 | batch: 3600/10010 ] | Loss: 0.860 | Acc: 78.631% (362432/460928)
01/15/2023 11:08:12 - INFO - __main__ -   test: [epoch: 1 | batch: 3700/10010 ] | Loss: 0.860 | Acc: 78.613% (372414/473728)
01/15/2023 11:11:43 - INFO - __main__ -   test: [epoch: 1 | batch: 3800/10010 ] | Loss: 0.860 | Acc: 78.612% (382470/486528)
01/15/2023 11:15:15 - INFO - __main__ -   test: [epoch: 1 | batch: 3900/10010 ] | Loss: 0.860 | Acc: 78.623% (392589/499328)
01/15/2023 11:18:46 - INFO - __main__ -   test: [epoch: 1 | batch: 4000/10010 ] | Loss: 0.860 | Acc: 78.623% (402652/512128)
01/15/2023 11:22:18 - INFO - __main__ -   test: [epoch: 1 | batch: 4100/10010 ] | Loss: 0.860 | Acc: 78.637% (412787/524928)
01/15/2023 11:25:51 - INFO - __main__ -   test: [epoch: 1 | batch: 4200/10010 ] | Loss: 0.860 | Acc: 78.632% (422824/537728)
01/15/2023 11:29:22 - INFO - __main__ -   test: [epoch: 1 | batch: 4300/10010 ] | Loss: 0.859 | Acc: 78.639% (432930/550528)
01/15/2023 11:32:54 - INFO - __main__ -   test: [epoch: 1 | batch: 4400/10010 ] | Loss: 0.859 | Acc: 78.645% (443029/563328)
01/15/2023 11:36:27 - INFO - __main__ -   test: [epoch: 1 | batch: 4500/10010 ] | Loss: 0.859 | Acc: 78.640% (453066/576128)
01/15/2023 11:39:59 - INFO - __main__ -   test: [epoch: 1 | batch: 4600/10010 ] | Loss: 0.859 | Acc: 78.638% (463124/588928)
01/15/2023 11:43:30 - INFO - __main__ -   test: [epoch: 1 | batch: 4700/10010 ] | Loss: 0.859 | Acc: 78.651% (473263/601728)
01/15/2023 11:47:02 - INFO - __main__ -   test: [epoch: 1 | batch: 4800/10010 ] | Loss: 0.858 | Acc: 78.659% (483384/614528)
01/15/2023 11:50:33 - INFO - __main__ -   test: [epoch: 1 | batch: 4900/10010 ] | Loss: 0.859 | Acc: 78.652% (493407/627328)
01/15/2023 11:54:04 - INFO - __main__ -   test: [epoch: 1 | batch: 5000/10010 ] | Loss: 0.858 | Acc: 78.665% (503555/640128)
01/15/2023 11:57:36 - INFO - __main__ -   test: [epoch: 1 | batch: 5100/10010 ] | Loss: 0.858 | Acc: 78.663% (513615/652928)
01/15/2023 12:01:08 - INFO - __main__ -   test: [epoch: 1 | batch: 5200/10010 ] | Loss: 0.859 | Acc: 78.655% (523631/665728)
01/15/2023 12:04:40 - INFO - __main__ -   test: [epoch: 1 | batch: 5300/10010 ] | Loss: 0.859 | Acc: 78.652% (533675/678528)
01/15/2023 12:08:10 - INFO - __main__ -   test: [epoch: 1 | batch: 5400/10010 ] | Loss: 0.859 | Acc: 78.654% (543754/691328)
01/15/2023 12:11:41 - INFO - __main__ -   test: [epoch: 1 | batch: 5500/10010 ] | Loss: 0.859 | Acc: 78.647% (553775/704128)
01/15/2023 12:15:14 - INFO - __main__ -   test: [epoch: 1 | batch: 5600/10010 ] | Loss: 0.859 | Acc: 78.656% (563910/716928)
01/15/2023 12:18:46 - INFO - __main__ -   test: [epoch: 1 | batch: 5700/10010 ] | Loss: 0.858 | Acc: 78.657% (573985/729728)
01/15/2023 12:22:16 - INFO - __main__ -   test: [epoch: 1 | batch: 5800/10010 ] | Loss: 0.858 | Acc: 78.665% (584107/742528)
01/15/2023 12:25:47 - INFO - __main__ -   test: [epoch: 1 | batch: 5900/10010 ] | Loss: 0.858 | Acc: 78.661% (594149/755328)
01/15/2023 12:29:19 - INFO - __main__ -   test: [epoch: 1 | batch: 6000/10010 ] | Loss: 0.858 | Acc: 78.665% (604251/768128)
01/15/2023 12:32:50 - INFO - __main__ -   test: [epoch: 1 | batch: 6100/10010 ] | Loss: 0.858 | Acc: 78.651% (614210/780928)
01/15/2023 12:36:22 - INFO - __main__ -   test: [epoch: 1 | batch: 6200/10010 ] | Loss: 0.858 | Acc: 78.653% (624291/793728)
01/15/2023 12:39:52 - INFO - __main__ -   test: [epoch: 1 | batch: 6300/10010 ] | Loss: 0.858 | Acc: 78.650% (634334/806528)
01/15/2023 12:43:22 - INFO - __main__ -   test: [epoch: 1 | batch: 6400/10010 ] | Loss: 0.859 | Acc: 78.636% (644287/819328)
01/15/2023 12:46:55 - INFO - __main__ -   test: [epoch: 1 | batch: 6500/10010 ] | Loss: 0.858 | Acc: 78.636% (654353/832128)
01/15/2023 12:50:27 - INFO - __main__ -   test: [epoch: 1 | batch: 6600/10010 ] | Loss: 0.858 | Acc: 78.643% (664479/844928)
01/15/2023 12:53:58 - INFO - __main__ -   test: [epoch: 1 | batch: 6700/10010 ] | Loss: 0.859 | Acc: 78.637% (674488/857728)
01/15/2023 12:57:28 - INFO - __main__ -   test: [epoch: 1 | batch: 6800/10010 ] | Loss: 0.859 | Acc: 78.629% (684490/870528)
01/15/2023 13:01:00 - INFO - __main__ -   test: [epoch: 1 | batch: 6900/10010 ] | Loss: 0.859 | Acc: 78.631% (694566/883328)
01/15/2023 13:04:31 - INFO - __main__ -   test: [epoch: 1 | batch: 7000/10010 ] | Loss: 0.859 | Acc: 78.624% (704570/896128)
01/15/2023 13:08:03 - INFO - __main__ -   test: [epoch: 1 | batch: 7100/10010 ] | Loss: 0.859 | Acc: 78.628% (714674/908928)
01/15/2023 13:11:35 - INFO - __main__ -   test: [epoch: 1 | batch: 7200/10010 ] | Loss: 0.858 | Acc: 78.628% (724737/921728)
01/15/2023 13:15:07 - INFO - __main__ -   test: [epoch: 1 | batch: 7300/10010 ] | Loss: 0.859 | Acc: 78.627% (734792/934528)
01/15/2023 13:18:38 - INFO - __main__ -   test: [epoch: 1 | batch: 7400/10010 ] | Loss: 0.859 | Acc: 78.620% (744789/947328)
01/15/2023 13:22:08 - INFO - __main__ -   test: [epoch: 1 | batch: 7500/10010 ] | Loss: 0.859 | Acc: 78.621% (754858/960128)
01/15/2023 13:25:39 - INFO - __main__ -   test: [epoch: 1 | batch: 7600/10010 ] | Loss: 0.859 | Acc: 78.616% (764877/972928)
01/15/2023 13:29:11 - INFO - __main__ -   test: [epoch: 1 | batch: 7700/10010 ] | Loss: 0.859 | Acc: 78.613% (774907/985728)
01/15/2023 13:32:43 - INFO - __main__ -   test: [epoch: 1 | batch: 7800/10010 ] | Loss: 0.859 | Acc: 78.601% (784854/998528)
01/15/2023 13:36:15 - INFO - __main__ -   test: [epoch: 1 | batch: 7900/10010 ] | Loss: 0.859 | Acc: 78.604% (794947/1011328)
01/15/2023 13:39:47 - INFO - __main__ -   test: [epoch: 1 | batch: 8000/10010 ] | Loss: 0.859 | Acc: 78.607% (805036/1024128)
01/15/2023 13:43:17 - INFO - __main__ -   test: [epoch: 1 | batch: 8100/10010 ] | Loss: 0.859 | Acc: 78.617% (815202/1036928)
01/15/2023 13:46:49 - INFO - __main__ -   test: [epoch: 1 | batch: 8200/10010 ] | Loss: 0.859 | Acc: 78.612% (825211/1049728)
01/15/2023 13:50:20 - INFO - __main__ -   test: [epoch: 1 | batch: 8300/10010 ] | Loss: 0.859 | Acc: 78.607% (835225/1062528)
01/15/2023 13:53:50 - INFO - __main__ -   test: [epoch: 1 | batch: 8400/10010 ] | Loss: 0.859 | Acc: 78.609% (845306/1075328)
01/15/2023 13:57:23 - INFO - __main__ -   test: [epoch: 1 | batch: 8500/10010 ] | Loss: 0.859 | Acc: 78.612% (855399/1088128)
01/15/2023 14:00:54 - INFO - __main__ -   test: [epoch: 1 | batch: 8600/10010 ] | Loss: 0.859 | Acc: 78.611% (865446/1100928)
01/15/2023 14:04:26 - INFO - __main__ -   test: [epoch: 1 | batch: 8700/10010 ] | Loss: 0.859 | Acc: 78.617% (875584/1113728)
01/15/2023 14:07:57 - INFO - __main__ -   test: [epoch: 1 | batch: 8800/10010 ] | Loss: 0.859 | Acc: 78.624% (885716/1126528)
01/15/2023 14:11:28 - INFO - __main__ -   test: [epoch: 1 | batch: 8900/10010 ] | Loss: 0.859 | Acc: 78.627% (895816/1139328)
01/15/2023 14:14:58 - INFO - __main__ -   test: [epoch: 1 | batch: 9000/10010 ] | Loss: 0.859 | Acc: 78.628% (905900/1152128)
01/15/2023 14:18:28 - INFO - __main__ -   test: [epoch: 1 | batch: 9100/10010 ] | Loss: 0.859 | Acc: 78.630% (915981/1164928)
01/15/2023 14:22:01 - INFO - __main__ -   test: [epoch: 1 | batch: 9200/10010 ] | Loss: 0.859 | Acc: 78.630% (926048/1177728)
01/15/2023 14:25:32 - INFO - __main__ -   test: [epoch: 1 | batch: 9300/10010 ] | Loss: 0.858 | Acc: 78.636% (936189/1190528)
01/15/2023 14:29:03 - INFO - __main__ -   test: [epoch: 1 | batch: 9400/10010 ] | Loss: 0.859 | Acc: 78.634% (946225/1203328)
01/15/2023 14:32:36 - INFO - __main__ -   test: [epoch: 1 | batch: 9500/10010 ] | Loss: 0.859 | Acc: 78.635% (956303/1216128)
01/15/2023 14:36:08 - INFO - __main__ -   test: [epoch: 1 | batch: 9600/10010 ] | Loss: 0.859 | Acc: 78.634% (966359/1228928)
01/15/2023 14:39:40 - INFO - __main__ -   test: [epoch: 1 | batch: 9700/10010 ] | Loss: 0.859 | Acc: 78.636% (976440/1241728)
01/15/2023 14:43:12 - INFO - __main__ -   test: [epoch: 1 | batch: 9800/10010 ] | Loss: 0.859 | Acc: 78.634% (986480/1254528)
01/15/2023 14:46:44 - INFO - __main__ -   test: [epoch: 1 | batch: 9900/10010 ] | Loss: 0.859 | Acc: 78.634% (996546/1267328)
01/15/2023 14:50:16 - INFO - __main__ -   test: [epoch: 1 | batch: 10000/10010 ] | Loss: 0.859 | Acc: 78.631% (1006580/1280128)
01/15/2023 14:50:36 - INFO - __main__ -   Saving Checkpoint
01/15/2023 14:50:38 - INFO - __main__ -   test: [batch: 0/391 ] | Loss: 0.431 | Acc: 86.719% (111/128)/ 97.656% (125/128)
01/15/2023 14:50:40 - INFO - __main__ -   test: [batch: 1/391 ] | Loss: 0.447 | Acc: 87.109% (223/256)/ 98.047% (251/256)
01/15/2023 14:50:42 - INFO - __main__ -   test: [batch: 2/391 ] | Loss: 0.604 | Acc: 83.594% (321/384)/ 95.833% (368/384)
01/15/2023 14:50:44 - INFO - __main__ -   test: [batch: 3/391 ] | Loss: 0.559 | Acc: 85.156% (436/512)/ 96.484% (494/512)
01/15/2023 14:50:47 - INFO - __main__ -   test: [batch: 4/391 ] | Loss: 0.479 | Acc: 87.344% (559/640)/ 97.188% (622/640)
01/15/2023 14:50:49 - INFO - __main__ -   test: [batch: 5/391 ] | Loss: 0.423 | Acc: 88.672% (681/768)/ 97.656% (750/768)
01/15/2023 14:50:51 - INFO - __main__ -   test: [batch: 6/391 ] | Loss: 0.419 | Acc: 88.839% (796/896)/ 97.545% (874/896)
01/15/2023 14:50:53 - INFO - __main__ -   test: [batch: 7/391 ] | Loss: 0.402 | Acc: 89.746% (919/1024)/ 97.559% (999/1024)
01/15/2023 14:50:55 - INFO - __main__ -   test: [batch: 8/391 ] | Loss: 0.420 | Acc: 89.670% (1033/1152)/ 97.483% (1123/1152)
01/15/2023 14:50:57 - INFO - __main__ -   test: [batch: 9/391 ] | Loss: 0.399 | Acc: 90.156% (1154/1280)/ 97.578% (1249/1280)
01/15/2023 14:51:00 - INFO - __main__ -   test: [batch: 10/391 ] | Loss: 0.449 | Acc: 88.849% (1251/1408)/ 97.585% (1374/1408)
01/15/2023 14:51:02 - INFO - __main__ -   test: [batch: 11/391 ] | Loss: 0.454 | Acc: 88.997% (1367/1536)/ 97.526% (1498/1536)
01/15/2023 14:51:04 - INFO - __main__ -   test: [batch: 12/391 ] | Loss: 0.503 | Acc: 87.861% (1462/1664)/ 97.175% (1617/1664)
01/15/2023 14:51:06 - INFO - __main__ -   test: [batch: 13/391 ] | Loss: 0.554 | Acc: 86.217% (1545/1792)/ 96.652% (1732/1792)
01/15/2023 14:51:08 - INFO - __main__ -   test: [batch: 14/391 ] | Loss: 0.575 | Acc: 85.365% (1639/1920)/ 96.719% (1857/1920)
01/15/2023 14:51:10 - INFO - __main__ -   test: [batch: 15/391 ] | Loss: 0.583 | Acc: 84.912% (1739/2048)/ 96.826% (1983/2048)
01/15/2023 14:51:12 - INFO - __main__ -   test: [batch: 16/391 ] | Loss: 0.592 | Acc: 84.697% (1843/2176)/ 96.691% (2104/2176)
01/15/2023 14:51:14 - INFO - __main__ -   test: [batch: 17/391 ] | Loss: 0.619 | Acc: 84.245% (1941/2304)/ 96.181% (2216/2304)
01/15/2023 14:51:17 - INFO - __main__ -   test: [batch: 18/391 ] | Loss: 0.642 | Acc: 83.758% (2037/2432)/ 96.053% (2336/2432)
01/15/2023 14:51:19 - INFO - __main__ -   test: [batch: 19/391 ] | Loss: 0.651 | Acc: 83.477% (2137/2560)/ 95.938% (2456/2560)
01/15/2023 14:51:21 - INFO - __main__ -   test: [batch: 20/391 ] | Loss: 0.648 | Acc: 83.557% (2246/2688)/ 95.871% (2577/2688)
01/15/2023 14:51:23 - INFO - __main__ -   test: [batch: 21/391 ] | Loss: 0.680 | Acc: 82.884% (2334/2816)/ 95.739% (2696/2816)
01/15/2023 14:51:25 - INFO - __main__ -   test: [batch: 22/391 ] | Loss: 0.679 | Acc: 82.745% (2436/2944)/ 95.754% (2819/2944)
01/15/2023 14:51:27 - INFO - __main__ -   test: [batch: 23/391 ] | Loss: 0.723 | Acc: 81.868% (2515/3072)/ 95.443% (2932/3072)
01/15/2023 14:51:29 - INFO - __main__ -   test: [batch: 24/391 ] | Loss: 0.743 | Acc: 81.375% (2604/3200)/ 95.250% (3048/3200)
01/15/2023 14:51:31 - INFO - __main__ -   test: [batch: 25/391 ] | Loss: 0.768 | Acc: 80.709% (2686/3328)/ 94.982% (3161/3328)
01/15/2023 14:51:33 - INFO - __main__ -   test: [batch: 26/391 ] | Loss: 0.784 | Acc: 79.948% (2763/3456)/ 94.907% (3280/3456)
01/15/2023 14:51:36 - INFO - __main__ -   test: [batch: 27/391 ] | Loss: 0.766 | Acc: 80.469% (2884/3584)/ 94.950% (3403/3584)
01/15/2023 14:51:38 - INFO - __main__ -   test: [batch: 28/391 ] | Loss: 0.775 | Acc: 79.903% (2966/3712)/ 95.016% (3527/3712)
01/15/2023 14:51:40 - INFO - __main__ -   test: [batch: 29/391 ] | Loss: 0.768 | Acc: 80.052% (3074/3840)/ 95.130% (3653/3840)
01/15/2023 14:51:42 - INFO - __main__ -   test: [batch: 30/391 ] | Loss: 0.782 | Acc: 79.990% (3174/3968)/ 95.010% (3770/3968)
01/15/2023 14:51:44 - INFO - __main__ -   test: [batch: 31/391 ] | Loss: 0.777 | Acc: 80.151% (3283/4096)/ 95.068% (3894/4096)
01/15/2023 14:51:46 - INFO - __main__ -   test: [batch: 32/391 ] | Loss: 0.763 | Acc: 80.445% (3398/4224)/ 95.170% (4020/4224)
01/15/2023 14:51:49 - INFO - __main__ -   test: [batch: 33/391 ] | Loss: 0.757 | Acc: 80.584% (3507/4352)/ 95.221% (4144/4352)
01/15/2023 14:51:51 - INFO - __main__ -   test: [batch: 34/391 ] | Loss: 0.743 | Acc: 80.982% (3628/4480)/ 95.290% (4269/4480)
01/15/2023 14:51:53 - INFO - __main__ -   test: [batch: 35/391 ] | Loss: 0.730 | Acc: 81.380% (3750/4608)/ 95.312% (4392/4608)
01/15/2023 14:51:55 - INFO - __main__ -   test: [batch: 36/391 ] | Loss: 0.714 | Acc: 81.820% (3875/4736)/ 95.418% (4519/4736)
01/15/2023 14:51:57 - INFO - __main__ -   test: [batch: 37/391 ] | Loss: 0.705 | Acc: 82.134% (3995/4864)/ 95.477% (4644/4864)
01/15/2023 14:52:00 - INFO - __main__ -   test: [batch: 38/391 ] | Loss: 0.697 | Acc: 82.292% (4108/4992)/ 95.553% (4770/4992)
01/15/2023 14:52:02 - INFO - __main__ -   test: [batch: 39/391 ] | Loss: 0.692 | Acc: 82.344% (4216/5120)/ 95.605% (4895/5120)
01/15/2023 14:52:04 - INFO - __main__ -   test: [batch: 40/391 ] | Loss: 0.690 | Acc: 82.431% (4326/5248)/ 95.503% (5012/5248)
01/15/2023 14:52:06 - INFO - __main__ -   test: [batch: 41/391 ] | Loss: 0.694 | Acc: 82.515% (4436/5376)/ 95.443% (5131/5376)
01/15/2023 14:52:08 - INFO - __main__ -   test: [batch: 42/391 ] | Loss: 0.693 | Acc: 82.485% (4540/5504)/ 95.512% (5257/5504)
01/15/2023 14:52:10 - INFO - __main__ -   test: [batch: 43/391 ] | Loss: 0.691 | Acc: 82.546% (4649/5632)/ 95.455% (5376/5632)
01/15/2023 14:52:12 - INFO - __main__ -   test: [batch: 44/391 ] | Loss: 0.693 | Acc: 82.569% (4756/5760)/ 95.365% (5493/5760)
01/15/2023 14:52:15 - INFO - __main__ -   test: [batch: 45/391 ] | Loss: 0.690 | Acc: 82.762% (4873/5888)/ 95.363% (5615/5888)
01/15/2023 14:52:17 - INFO - __main__ -   test: [batch: 46/391 ] | Loss: 0.693 | Acc: 82.729% (4977/6016)/ 95.396% (5739/6016)
01/15/2023 14:52:19 - INFO - __main__ -   test: [batch: 47/391 ] | Loss: 0.695 | Acc: 82.682% (5080/6144)/ 95.443% (5864/6144)
01/15/2023 14:52:21 - INFO - __main__ -   test: [batch: 48/391 ] | Loss: 0.700 | Acc: 82.541% (5177/6272)/ 95.408% (5984/6272)
01/15/2023 14:52:23 - INFO - __main__ -   test: [batch: 49/391 ] | Loss: 0.703 | Acc: 82.609% (5287/6400)/ 95.312% (6100/6400)
01/15/2023 14:52:25 - INFO - __main__ -   test: [batch: 50/391 ] | Loss: 0.694 | Acc: 82.812% (5406/6528)/ 95.358% (6225/6528)
01/15/2023 14:52:27 - INFO - __main__ -   test: [batch: 51/391 ] | Loss: 0.687 | Acc: 82.993% (5524/6656)/ 95.418% (6351/6656)
01/15/2023 14:52:29 - INFO - __main__ -   test: [batch: 52/391 ] | Loss: 0.683 | Acc: 83.078% (5636/6784)/ 95.445% (6475/6784)
01/15/2023 14:52:31 - INFO - __main__ -   test: [batch: 53/391 ] | Loss: 0.674 | Acc: 83.275% (5756/6912)/ 95.530% (6603/6912)
01/15/2023 14:52:33 - INFO - __main__ -   test: [batch: 54/391 ] | Loss: 0.667 | Acc: 83.409% (5872/7040)/ 95.540% (6726/7040)
01/15/2023 14:52:36 - INFO - __main__ -   test: [batch: 55/391 ] | Loss: 0.660 | Acc: 83.566% (5990/7168)/ 95.578% (6851/7168)
01/15/2023 14:52:38 - INFO - __main__ -   test: [batch: 56/391 ] | Loss: 0.652 | Acc: 83.772% (6112/7296)/ 95.641% (6978/7296)
01/15/2023 14:52:40 - INFO - __main__ -   test: [batch: 57/391 ] | Loss: 0.645 | Acc: 83.957% (6233/7424)/ 95.703% (7105/7424)
01/15/2023 14:52:42 - INFO - __main__ -   test: [batch: 58/391 ] | Loss: 0.643 | Acc: 84.017% (6345/7552)/ 95.697% (7227/7552)
01/15/2023 14:52:44 - INFO - __main__ -   test: [batch: 59/391 ] | Loss: 0.645 | Acc: 83.932% (6446/7680)/ 95.703% (7350/7680)
01/15/2023 14:52:46 - INFO - __main__ -   test: [batch: 60/391 ] | Loss: 0.650 | Acc: 83.824% (6545/7808)/ 95.671% (7470/7808)
01/15/2023 14:52:48 - INFO - __main__ -   test: [batch: 61/391 ] | Loss: 0.651 | Acc: 83.808% (6651/7936)/ 95.691% (7594/7936)
01/15/2023 14:52:50 - INFO - __main__ -   test: [batch: 62/391 ] | Loss: 0.650 | Acc: 83.780% (6756/8064)/ 95.709% (7718/8064)
01/15/2023 14:52:52 - INFO - __main__ -   test: [batch: 63/391 ] | Loss: 0.658 | Acc: 83.655% (6853/8192)/ 95.654% (7836/8192)
01/15/2023 14:52:54 - INFO - __main__ -   test: [batch: 64/391 ] | Loss: 0.666 | Acc: 83.462% (6944/8320)/ 95.601% (7954/8320)
01/15/2023 14:52:57 - INFO - __main__ -   test: [batch: 65/391 ] | Loss: 0.680 | Acc: 82.943% (7007/8448)/ 95.514% (8069/8448)
01/15/2023 14:52:59 - INFO - __main__ -   test: [batch: 66/391 ] | Loss: 0.685 | Acc: 82.964% (7115/8576)/ 95.476% (8188/8576)
01/15/2023 14:53:01 - INFO - __main__ -   test: [batch: 67/391 ] | Loss: 0.688 | Acc: 82.916% (7217/8704)/ 95.485% (8311/8704)
01/15/2023 14:53:03 - INFO - __main__ -   test: [batch: 68/391 ] | Loss: 0.689 | Acc: 82.869% (7319/8832)/ 95.505% (8435/8832)
01/15/2023 14:53:05 - INFO - __main__ -   test: [batch: 69/391 ] | Loss: 0.684 | Acc: 82.946% (7432/8960)/ 95.547% (8561/8960)
01/15/2023 14:53:07 - INFO - __main__ -   test: [batch: 70/391 ] | Loss: 0.685 | Acc: 82.868% (7531/9088)/ 95.544% (8683/9088)
01/15/2023 14:53:09 - INFO - __main__ -   test: [batch: 71/391 ] | Loss: 0.684 | Acc: 82.921% (7642/9216)/ 95.551% (8806/9216)
01/15/2023 14:53:11 - INFO - __main__ -   test: [batch: 72/391 ] | Loss: 0.686 | Acc: 82.812% (7738/9344)/ 95.569% (8930/9344)
01/15/2023 14:53:14 - INFO - __main__ -   test: [batch: 73/391 ] | Loss: 0.690 | Acc: 82.696% (7833/9472)/ 95.576% (9053/9472)
01/15/2023 14:53:16 - INFO - __main__ -   test: [batch: 74/391 ] | Loss: 0.691 | Acc: 82.688% (7938/9600)/ 95.562% (9174/9600)
01/15/2023 14:53:18 - INFO - __main__ -   test: [batch: 75/391 ] | Loss: 0.697 | Acc: 82.504% (8026/9728)/ 95.559% (9296/9728)
01/15/2023 14:53:20 - INFO - __main__ -   test: [batch: 76/391 ] | Loss: 0.696 | Acc: 82.498% (8131/9856)/ 95.556% (9418/9856)
01/15/2023 14:53:22 - INFO - __main__ -   test: [batch: 77/391 ] | Loss: 0.697 | Acc: 82.452% (8232/9984)/ 95.583% (9543/9984)
01/15/2023 14:53:24 - INFO - __main__ -   test: [batch: 78/391 ] | Loss: 0.697 | Acc: 82.358% (8328/10112)/ 95.629% (9670/10112)
01/15/2023 14:53:26 - INFO - __main__ -   test: [batch: 79/391 ] | Loss: 0.695 | Acc: 82.363% (8434/10240)/ 95.654% (9795/10240)
01/15/2023 14:53:28 - INFO - __main__ -   test: [batch: 80/391 ] | Loss: 0.695 | Acc: 82.340% (8537/10368)/ 95.660% (9918/10368)
01/15/2023 14:53:31 - INFO - __main__ -   test: [batch: 81/391 ] | Loss: 0.694 | Acc: 82.365% (8645/10496)/ 95.684% (10043/10496)
01/15/2023 14:53:33 - INFO - __main__ -   test: [batch: 82/391 ] | Loss: 0.693 | Acc: 82.370% (8751/10624)/ 95.680% (10165/10624)
01/15/2023 14:53:35 - INFO - __main__ -   test: [batch: 83/391 ] | Loss: 0.693 | Acc: 82.413% (8861/10752)/ 95.666% (10286/10752)
01/15/2023 14:53:37 - INFO - __main__ -   test: [batch: 84/391 ] | Loss: 0.689 | Acc: 82.537% (8980/10880)/ 95.699% (10412/10880)
01/15/2023 14:53:39 - INFO - __main__ -   test: [batch: 85/391 ] | Loss: 0.688 | Acc: 82.531% (9085/11008)/ 95.739% (10539/11008)
01/15/2023 14:53:41 - INFO - __main__ -   test: [batch: 86/391 ] | Loss: 0.692 | Acc: 82.453% (9182/11136)/ 95.717% (10659/11136)
01/15/2023 14:53:43 - INFO - __main__ -   test: [batch: 87/391 ] | Loss: 0.690 | Acc: 82.466% (9289/11264)/ 95.721% (10782/11264)
01/15/2023 14:53:45 - INFO - __main__ -   test: [batch: 88/391 ] | Loss: 0.697 | Acc: 82.400% (9387/11392)/ 95.655% (10897/11392)
01/15/2023 14:53:48 - INFO - __main__ -   test: [batch: 89/391 ] | Loss: 0.695 | Acc: 82.448% (9498/11520)/ 95.668% (11021/11520)
01/15/2023 14:53:50 - INFO - __main__ -   test: [batch: 90/391 ] | Loss: 0.696 | Acc: 82.349% (9592/11648)/ 95.673% (11144/11648)
01/15/2023 14:53:52 - INFO - __main__ -   test: [batch: 91/391 ] | Loss: 0.695 | Acc: 82.388% (9702/11776)/ 95.678% (11267/11776)
01/15/2023 14:53:54 - INFO - __main__ -   test: [batch: 92/391 ] | Loss: 0.696 | Acc: 82.367% (9805/11904)/ 95.657% (11387/11904)
01/15/2023 14:53:56 - INFO - __main__ -   test: [batch: 93/391 ] | Loss: 0.699 | Acc: 82.181% (9888/12032)/ 95.687% (11513/12032)
01/15/2023 14:53:58 - INFO - __main__ -   test: [batch: 94/391 ] | Loss: 0.702 | Acc: 82.048% (9977/12160)/ 95.699% (11637/12160)
01/15/2023 14:54:00 - INFO - __main__ -   test: [batch: 95/391 ] | Loss: 0.699 | Acc: 82.113% (10090/12288)/ 95.703% (11760/12288)
01/15/2023 14:54:02 - INFO - __main__ -   test: [batch: 96/391 ] | Loss: 0.703 | Acc: 81.999% (10181/12416)/ 95.723% (11885/12416)
01/15/2023 14:54:04 - INFO - __main__ -   test: [batch: 97/391 ] | Loss: 0.705 | Acc: 81.808% (10262/12544)/ 95.743% (12010/12544)
01/15/2023 14:54:07 - INFO - __main__ -   test: [batch: 98/391 ] | Loss: 0.701 | Acc: 81.921% (10381/12672)/ 95.778% (12137/12672)
01/15/2023 14:54:09 - INFO - __main__ -   test: [batch: 99/391 ] | Loss: 0.695 | Acc: 82.070% (10505/12800)/ 95.820% (12265/12800)
01/15/2023 14:54:11 - INFO - __main__ -   test: [batch: 100/391 ] | Loss: 0.694 | Acc: 82.124% (10617/12928)/ 95.838% (12390/12928)
01/15/2023 14:54:13 - INFO - __main__ -   test: [batch: 101/391 ] | Loss: 0.690 | Acc: 82.223% (10735/13056)/ 95.872% (12517/13056)
01/15/2023 14:54:15 - INFO - __main__ -   test: [batch: 102/391 ] | Loss: 0.687 | Acc: 82.342% (10856/13184)/ 95.897% (12643/13184)
01/15/2023 14:54:17 - INFO - __main__ -   test: [batch: 103/391 ] | Loss: 0.688 | Acc: 82.166% (10938/13312)/ 95.913% (12768/13312)
01/15/2023 14:54:19 - INFO - __main__ -   test: [batch: 104/391 ] | Loss: 0.688 | Acc: 82.106% (11035/13440)/ 95.923% (12892/13440)
01/15/2023 14:54:21 - INFO - __main__ -   test: [batch: 105/391 ] | Loss: 0.689 | Acc: 82.098% (11139/13568)/ 95.917% (13014/13568)
01/15/2023 14:54:23 - INFO - __main__ -   test: [batch: 106/391 ] | Loss: 0.697 | Acc: 81.980% (11228/13696)/ 95.846% (13127/13696)
01/15/2023 14:54:25 - INFO - __main__ -   test: [batch: 107/391 ] | Loss: 0.693 | Acc: 82.096% (11349/13824)/ 95.877% (13254/13824)
01/15/2023 14:54:28 - INFO - __main__ -   test: [batch: 108/391 ] | Loss: 0.696 | Acc: 81.960% (11435/13952)/ 95.886% (13378/13952)
01/15/2023 14:54:30 - INFO - __main__ -   test: [batch: 109/391 ] | Loss: 0.697 | Acc: 81.932% (11536/14080)/ 95.895% (13502/14080)
01/15/2023 14:54:32 - INFO - __main__ -   test: [batch: 110/391 ] | Loss: 0.700 | Acc: 81.743% (11614/14208)/ 95.904% (13626/14208)
01/15/2023 14:54:34 - INFO - __main__ -   test: [batch: 111/391 ] | Loss: 0.703 | Acc: 81.717% (11715/14336)/ 95.857% (13742/14336)
01/15/2023 14:54:36 - INFO - __main__ -   test: [batch: 112/391 ] | Loss: 0.704 | Acc: 81.741% (11823/14464)/ 95.873% (13867/14464)
01/15/2023 14:54:38 - INFO - __main__ -   test: [batch: 113/391 ] | Loss: 0.703 | Acc: 81.778% (11933/14592)/ 95.881% (13991/14592)
01/15/2023 14:54:41 - INFO - __main__ -   test: [batch: 114/391 ] | Loss: 0.700 | Acc: 81.868% (12051/14720)/ 95.904% (14117/14720)
01/15/2023 14:54:43 - INFO - __main__ -   test: [batch: 115/391 ] | Loss: 0.696 | Acc: 81.957% (12169/14848)/ 95.925% (14243/14848)
01/15/2023 14:54:45 - INFO - __main__ -   test: [batch: 116/391 ] | Loss: 0.700 | Acc: 81.924% (12269/14976)/ 95.893% (14361/14976)
01/15/2023 14:54:47 - INFO - __main__ -   test: [batch: 117/391 ] | Loss: 0.698 | Acc: 81.952% (12378/15104)/ 95.902% (14485/15104)
01/15/2023 14:54:49 - INFO - __main__ -   test: [batch: 118/391 ] | Loss: 0.702 | Acc: 81.834% (12465/15232)/ 95.916% (14610/15232)
01/15/2023 14:54:51 - INFO - __main__ -   test: [batch: 119/391 ] | Loss: 0.700 | Acc: 81.882% (12577/15360)/ 95.938% (14736/15360)
01/15/2023 14:54:53 - INFO - __main__ -   test: [batch: 120/391 ] | Loss: 0.698 | Acc: 81.928% (12689/15488)/ 95.952% (14861/15488)
01/15/2023 14:54:56 - INFO - __main__ -   test: [batch: 121/391 ] | Loss: 0.703 | Acc: 81.769% (12769/15616)/ 95.927% (14980/15616)
01/15/2023 14:54:58 - INFO - __main__ -   test: [batch: 122/391 ] | Loss: 0.705 | Acc: 81.707% (12864/15744)/ 95.903% (15099/15744)
01/15/2023 14:55:00 - INFO - __main__ -   test: [batch: 123/391 ] | Loss: 0.706 | Acc: 81.723% (12971/15872)/ 95.905% (15222/15872)
01/15/2023 14:55:02 - INFO - __main__ -   test: [batch: 124/391 ] | Loss: 0.704 | Acc: 81.731% (13077/16000)/ 95.919% (15347/16000)
01/15/2023 14:55:04 - INFO - __main__ -   test: [batch: 125/391 ] | Loss: 0.700 | Acc: 81.833% (13198/16128)/ 95.945% (15474/16128)
01/15/2023 14:55:06 - INFO - __main__ -   test: [batch: 126/391 ] | Loss: 0.697 | Acc: 81.921% (13317/16256)/ 95.958% (15599/16256)
01/15/2023 14:55:08 - INFO - __main__ -   test: [batch: 127/391 ] | Loss: 0.695 | Acc: 82.001% (13435/16384)/ 95.966% (15723/16384)
01/15/2023 14:55:10 - INFO - __main__ -   test: [batch: 128/391 ] | Loss: 0.695 | Acc: 81.989% (13538/16512)/ 95.942% (15842/16512)
01/15/2023 14:55:12 - INFO - __main__ -   test: [batch: 129/391 ] | Loss: 0.694 | Acc: 82.001% (13645/16640)/ 95.950% (15966/16640)
01/15/2023 14:55:15 - INFO - __main__ -   test: [batch: 130/391 ] | Loss: 0.691 | Acc: 82.103% (13767/16768)/ 95.969% (16092/16768)
01/15/2023 14:55:17 - INFO - __main__ -   test: [batch: 131/391 ] | Loss: 0.691 | Acc: 82.150% (13880/16896)/ 95.975% (16216/16896)
01/15/2023 14:55:19 - INFO - __main__ -   test: [batch: 132/391 ] | Loss: 0.688 | Acc: 82.219% (13997/17024)/ 95.988% (16341/17024)
01/15/2023 14:55:21 - INFO - __main__ -   test: [batch: 133/391 ] | Loss: 0.689 | Acc: 82.165% (14093/17152)/ 95.977% (16462/17152)
01/15/2023 14:55:23 - INFO - __main__ -   test: [batch: 134/391 ] | Loss: 0.687 | Acc: 82.199% (14204/17280)/ 96.001% (16589/17280)
01/15/2023 14:55:25 - INFO - __main__ -   test: [batch: 135/391 ] | Loss: 0.687 | Acc: 82.186% (14307/17408)/ 95.996% (16711/17408)
01/15/2023 14:55:27 - INFO - __main__ -   test: [batch: 136/391 ] | Loss: 0.686 | Acc: 82.140% (14404/17536)/ 96.020% (16838/17536)
01/15/2023 14:55:29 - INFO - __main__ -   test: [batch: 137/391 ] | Loss: 0.684 | Acc: 82.178% (14516/17664)/ 96.048% (16966/17664)
01/15/2023 14:55:32 - INFO - __main__ -   test: [batch: 138/391 ] | Loss: 0.683 | Acc: 82.205% (14626/17792)/ 96.049% (17089/17792)
01/15/2023 14:55:34 - INFO - __main__ -   test: [batch: 139/391 ] | Loss: 0.689 | Acc: 82.054% (14704/17920)/ 96.038% (17210/17920)
01/15/2023 14:55:36 - INFO - __main__ -   test: [batch: 140/391 ] | Loss: 0.691 | Acc: 81.987% (14797/18048)/ 96.033% (17332/18048)
01/15/2023 14:55:38 - INFO - __main__ -   test: [batch: 141/391 ] | Loss: 0.689 | Acc: 82.020% (14908/18176)/ 96.044% (17457/18176)
01/15/2023 14:55:40 - INFO - __main__ -   test: [batch: 142/391 ] | Loss: 0.687 | Acc: 82.102% (15028/18304)/ 96.061% (17583/18304)
01/15/2023 14:55:42 - INFO - __main__ -   test: [batch: 143/391 ] | Loss: 0.688 | Acc: 82.118% (15136/18432)/ 96.045% (17703/18432)
01/15/2023 14:55:44 - INFO - __main__ -   test: [batch: 144/391 ] | Loss: 0.690 | Acc: 82.091% (15236/18560)/ 96.008% (17819/18560)
01/15/2023 14:55:47 - INFO - __main__ -   test: [batch: 145/391 ] | Loss: 0.693 | Acc: 82.063% (15336/18688)/ 95.981% (17937/18688)
01/15/2023 14:55:49 - INFO - __main__ -   test: [batch: 146/391 ] | Loss: 0.694 | Acc: 82.047% (15438/18816)/ 95.972% (18058/18816)
01/15/2023 14:55:51 - INFO - __main__ -   test: [batch: 147/391 ] | Loss: 0.694 | Acc: 82.047% (15543/18944)/ 95.951% (18177/18944)
01/15/2023 14:55:53 - INFO - __main__ -   test: [batch: 148/391 ] | Loss: 0.696 | Acc: 81.995% (15638/19072)/ 95.952% (18300/19072)
01/15/2023 14:55:55 - INFO - __main__ -   test: [batch: 149/391 ] | Loss: 0.699 | Acc: 81.906% (15726/19200)/ 95.922% (18417/19200)
01/15/2023 14:55:58 - INFO - __main__ -   test: [batch: 150/391 ] | Loss: 0.700 | Acc: 81.845% (15819/19328)/ 95.928% (18541/19328)
01/15/2023 14:56:00 - INFO - __main__ -   test: [batch: 151/391 ] | Loss: 0.700 | Acc: 81.872% (15929/19456)/ 95.929% (18664/19456)
01/15/2023 14:56:02 - INFO - __main__ -   test: [batch: 152/391 ] | Loss: 0.701 | Acc: 81.858% (16031/19584)/ 95.920% (18785/19584)
01/15/2023 14:56:04 - INFO - __main__ -   test: [batch: 153/391 ] | Loss: 0.699 | Acc: 81.909% (16146/19712)/ 95.931% (18910/19712)
01/15/2023 14:56:06 - INFO - __main__ -   test: [batch: 154/391 ] | Loss: 0.699 | Acc: 81.905% (16250/19840)/ 95.912% (19029/19840)
01/15/2023 14:56:08 - INFO - __main__ -   test: [batch: 155/391 ] | Loss: 0.700 | Acc: 81.916% (16357/19968)/ 95.908% (19151/19968)
01/15/2023 14:56:11 - INFO - __main__ -   test: [batch: 156/391 ] | Loss: 0.701 | Acc: 81.827% (16444/20096)/ 95.890% (19270/20096)
01/15/2023 14:56:13 - INFO - __main__ -   test: [batch: 157/391 ] | Loss: 0.703 | Acc: 81.809% (16545/20224)/ 95.866% (19388/20224)
01/15/2023 14:56:15 - INFO - __main__ -   test: [batch: 158/391 ] | Loss: 0.704 | Acc: 81.781% (16644/20352)/ 95.843% (19506/20352)
01/15/2023 14:56:17 - INFO - __main__ -   test: [batch: 159/391 ] | Loss: 0.706 | Acc: 81.753% (16743/20480)/ 95.835% (19627/20480)
01/15/2023 14:56:19 - INFO - __main__ -   test: [batch: 160/391 ] | Loss: 0.706 | Acc: 81.740% (16845/20608)/ 95.822% (19747/20608)
01/15/2023 14:56:21 - INFO - __main__ -   test: [batch: 161/391 ] | Loss: 0.715 | Acc: 81.506% (16901/20736)/ 95.727% (19850/20736)
01/15/2023 14:56:23 - INFO - __main__ -   test: [batch: 162/391 ] | Loss: 0.721 | Acc: 81.413% (16986/20864)/ 95.653% (19957/20864)
01/15/2023 14:56:26 - INFO - __main__ -   test: [batch: 163/391 ] | Loss: 0.723 | Acc: 81.360% (17079/20992)/ 95.646% (20078/20992)
01/15/2023 14:56:28 - INFO - __main__ -   test: [batch: 164/391 ] | Loss: 0.723 | Acc: 81.349% (17181/21120)/ 95.653% (20202/21120)
01/15/2023 14:56:30 - INFO - __main__ -   test: [batch: 165/391 ] | Loss: 0.725 | Acc: 81.269% (17268/21248)/ 95.647% (20323/21248)
01/15/2023 14:56:32 - INFO - __main__ -   test: [batch: 166/391 ] | Loss: 0.725 | Acc: 81.287% (17376/21376)/ 95.635% (20443/21376)
01/15/2023 14:56:34 - INFO - __main__ -   test: [batch: 167/391 ] | Loss: 0.727 | Acc: 81.227% (17467/21504)/ 95.619% (20562/21504)
01/15/2023 14:56:36 - INFO - __main__ -   test: [batch: 168/391 ] | Loss: 0.727 | Acc: 81.232% (17572/21632)/ 95.618% (20684/21632)
01/15/2023 14:56:39 - INFO - __main__ -   test: [batch: 169/391 ] | Loss: 0.728 | Acc: 81.204% (17670/21760)/ 95.570% (20796/21760)
01/15/2023 14:56:41 - INFO - __main__ -   test: [batch: 170/391 ] | Loss: 0.733 | Acc: 81.099% (17751/21888)/ 95.532% (20910/21888)
01/15/2023 14:56:43 - INFO - __main__ -   test: [batch: 171/391 ] | Loss: 0.737 | Acc: 81.046% (17843/22016)/ 95.503% (21026/22016)
01/15/2023 14:56:45 - INFO - __main__ -   test: [batch: 172/391 ] | Loss: 0.737 | Acc: 80.997% (17936/22144)/ 95.502% (21148/22144)
01/15/2023 14:56:47 - INFO - __main__ -   test: [batch: 173/391 ] | Loss: 0.740 | Acc: 80.931% (18025/22272)/ 95.474% (21264/22272)
01/15/2023 14:56:49 - INFO - __main__ -   test: [batch: 174/391 ] | Loss: 0.744 | Acc: 80.844% (18109/22400)/ 95.429% (21376/22400)
01/15/2023 14:56:52 - INFO - __main__ -   test: [batch: 175/391 ] | Loss: 0.742 | Acc: 80.895% (18224/22528)/ 95.437% (21500/22528)
01/15/2023 14:56:54 - INFO - __main__ -   test: [batch: 176/391 ] | Loss: 0.743 | Acc: 80.901% (18329/22656)/ 95.423% (21619/22656)
01/15/2023 14:56:56 - INFO - __main__ -   test: [batch: 177/391 ] | Loss: 0.747 | Acc: 80.829% (18416/22784)/ 95.383% (21732/22784)
01/15/2023 14:56:58 - INFO - __main__ -   test: [batch: 178/391 ] | Loss: 0.748 | Acc: 80.809% (18515/22912)/ 95.365% (21850/22912)
01/15/2023 14:57:00 - INFO - __main__ -   test: [batch: 179/391 ] | Loss: 0.751 | Acc: 80.742% (18603/23040)/ 95.321% (21962/23040)
01/15/2023 14:57:02 - INFO - __main__ -   test: [batch: 180/391 ] | Loss: 0.757 | Acc: 80.616% (18677/23168)/ 95.287% (22076/23168)
01/15/2023 14:57:04 - INFO - __main__ -   test: [batch: 181/391 ] | Loss: 0.764 | Acc: 80.486% (18750/23296)/ 95.231% (22185/23296)
01/15/2023 14:57:07 - INFO - __main__ -   test: [batch: 182/391 ] | Loss: 0.763 | Acc: 80.499% (18856/23424)/ 95.236% (22308/23424)
01/15/2023 14:57:09 - INFO - __main__ -   test: [batch: 183/391 ] | Loss: 0.769 | Acc: 80.375% (18930/23552)/ 95.143% (22408/23552)
01/15/2023 14:57:11 - INFO - __main__ -   test: [batch: 184/391 ] | Loss: 0.768 | Acc: 80.405% (19040/23680)/ 95.131% (22527/23680)
01/15/2023 14:57:13 - INFO - __main__ -   test: [batch: 185/391 ] | Loss: 0.768 | Acc: 80.414% (19145/23808)/ 95.123% (22647/23808)
01/15/2023 14:57:15 - INFO - __main__ -   test: [batch: 186/391 ] | Loss: 0.771 | Acc: 80.373% (19238/23936)/ 95.087% (22760/23936)
01/15/2023 14:57:17 - INFO - __main__ -   test: [batch: 187/391 ] | Loss: 0.776 | Acc: 80.249% (19311/24064)/ 95.059% (22875/24064)
01/15/2023 14:57:19 - INFO - __main__ -   test: [batch: 188/391 ] | Loss: 0.780 | Acc: 80.117% (19382/24192)/ 95.031% (22990/24192)
01/15/2023 14:57:22 - INFO - __main__ -   test: [batch: 189/391 ] | Loss: 0.781 | Acc: 80.053% (19469/24320)/ 95.033% (23112/24320)
01/15/2023 14:57:24 - INFO - __main__ -   test: [batch: 190/391 ] | Loss: 0.786 | Acc: 79.966% (19550/24448)/ 95.006% (23227/24448)
01/15/2023 14:57:26 - INFO - __main__ -   test: [batch: 191/391 ] | Loss: 0.788 | Acc: 79.932% (19644/24576)/ 94.983% (23343/24576)
01/15/2023 14:57:28 - INFO - __main__ -   test: [batch: 192/391 ] | Loss: 0.794 | Acc: 79.825% (19720/24704)/ 94.912% (23447/24704)
01/15/2023 14:57:30 - INFO - __main__ -   test: [batch: 193/391 ] | Loss: 0.794 | Acc: 79.845% (19827/24832)/ 94.906% (23567/24832)
01/15/2023 14:57:32 - INFO - __main__ -   test: [batch: 194/391 ] | Loss: 0.797 | Acc: 79.776% (19912/24960)/ 94.888% (23684/24960)
01/15/2023 14:57:34 - INFO - __main__ -   test: [batch: 195/391 ] | Loss: 0.802 | Acc: 79.699% (19995/25088)/ 94.826% (23790/25088)
01/15/2023 14:57:36 - INFO - __main__ -   test: [batch: 196/391 ] | Loss: 0.806 | Acc: 79.600% (20072/25216)/ 94.781% (23900/25216)
01/15/2023 14:57:38 - INFO - __main__ -   test: [batch: 197/391 ] | Loss: 0.810 | Acc: 79.526% (20155/25344)/ 94.760% (24016/25344)
01/15/2023 14:57:41 - INFO - __main__ -   test: [batch: 198/391 ] | Loss: 0.812 | Acc: 79.476% (20244/25472)/ 94.731% (24130/25472)
01/15/2023 14:57:43 - INFO - __main__ -   test: [batch: 199/391 ] | Loss: 0.812 | Acc: 79.461% (20342/25600)/ 94.730% (24251/25600)
01/15/2023 14:57:45 - INFO - __main__ -   test: [batch: 200/391 ] | Loss: 0.812 | Acc: 79.427% (20435/25728)/ 94.729% (24372/25728)
01/15/2023 14:57:47 - INFO - __main__ -   test: [batch: 201/391 ] | Loss: 0.815 | Acc: 79.339% (20514/25856)/ 94.694% (24484/25856)
01/15/2023 14:57:50 - INFO - __main__ -   test: [batch: 202/391 ] | Loss: 0.816 | Acc: 79.330% (20613/25984)/ 94.685% (24603/25984)
01/15/2023 14:57:52 - INFO - __main__ -   test: [batch: 203/391 ] | Loss: 0.817 | Acc: 79.316% (20711/26112)/ 94.673% (24721/26112)
01/15/2023 14:57:54 - INFO - __main__ -   test: [batch: 204/391 ] | Loss: 0.820 | Acc: 79.215% (20786/26240)/ 94.657% (24838/26240)
01/15/2023 14:57:56 - INFO - __main__ -   test: [batch: 205/391 ] | Loss: 0.823 | Acc: 79.130% (20865/26368)/ 94.626% (24951/26368)
01/15/2023 14:57:58 - INFO - __main__ -   test: [batch: 206/391 ] | Loss: 0.825 | Acc: 79.103% (20959/26496)/ 94.622% (25071/26496)
01/15/2023 14:58:00 - INFO - __main__ -   test: [batch: 207/391 ] | Loss: 0.828 | Acc: 79.023% (21039/26624)/ 94.584% (25182/26624)
01/15/2023 14:58:03 - INFO - __main__ -   test: [batch: 208/391 ] | Loss: 0.830 | Acc: 78.985% (21130/26752)/ 94.550% (25294/26752)
01/15/2023 14:58:05 - INFO - __main__ -   test: [batch: 209/391 ] | Loss: 0.830 | Acc: 78.981% (21230/26880)/ 94.565% (25419/26880)
01/15/2023 14:58:07 - INFO - __main__ -   test: [batch: 210/391 ] | Loss: 0.831 | Acc: 78.969% (21328/27008)/ 94.550% (25536/27008)
01/15/2023 14:58:09 - INFO - __main__ -   test: [batch: 211/391 ] | Loss: 0.833 | Acc: 78.906% (21412/27136)/ 94.528% (25651/27136)
01/15/2023 14:58:11 - INFO - __main__ -   test: [batch: 212/391 ] | Loss: 0.835 | Acc: 78.848% (21497/27264)/ 94.506% (25766/27264)
01/15/2023 14:58:13 - INFO - __main__ -   test: [batch: 213/391 ] | Loss: 0.835 | Acc: 78.855% (21600/27392)/ 94.513% (25889/27392)
01/15/2023 14:58:15 - INFO - __main__ -   test: [batch: 214/391 ] | Loss: 0.835 | Acc: 78.830% (21694/27520)/ 94.517% (26011/27520)
01/15/2023 14:58:17 - INFO - __main__ -   test: [batch: 215/391 ] | Loss: 0.835 | Acc: 78.845% (21799/27648)/ 94.513% (26131/27648)
01/15/2023 14:58:19 - INFO - __main__ -   test: [batch: 216/391 ] | Loss: 0.833 | Acc: 78.885% (21911/27776)/ 94.531% (26257/27776)
01/15/2023 14:58:22 - INFO - __main__ -   test: [batch: 217/391 ] | Loss: 0.838 | Acc: 78.820% (21994/27904)/ 94.474% (26362/27904)
01/15/2023 14:58:24 - INFO - __main__ -   test: [batch: 218/391 ] | Loss: 0.841 | Acc: 78.764% (22079/28032)/ 94.438% (26473/28032)
01/15/2023 14:58:26 - INFO - __main__ -   test: [batch: 219/391 ] | Loss: 0.840 | Acc: 78.800% (22190/28160)/ 94.446% (26596/28160)
01/15/2023 14:58:28 - INFO - __main__ -   test: [batch: 220/391 ] | Loss: 0.838 | Acc: 78.839% (22302/28288)/ 94.461% (26721/28288)
01/15/2023 14:58:30 - INFO - __main__ -   test: [batch: 221/391 ] | Loss: 0.840 | Acc: 78.794% (22390/28416)/ 94.443% (26837/28416)
01/15/2023 14:58:32 - INFO - __main__ -   test: [batch: 222/391 ] | Loss: 0.838 | Acc: 78.843% (22505/28544)/ 94.458% (26962/28544)
01/15/2023 14:58:34 - INFO - __main__ -   test: [batch: 223/391 ] | Loss: 0.838 | Acc: 78.843% (22606/28672)/ 94.444% (27079/28672)
01/15/2023 14:58:37 - INFO - __main__ -   test: [batch: 224/391 ] | Loss: 0.838 | Acc: 78.865% (22713/28800)/ 94.438% (27198/28800)
01/15/2023 14:58:39 - INFO - __main__ -   test: [batch: 225/391 ] | Loss: 0.837 | Acc: 78.868% (22815/28928)/ 94.448% (27322/28928)
01/15/2023 14:58:41 - INFO - __main__ -   test: [batch: 226/391 ] | Loss: 0.837 | Acc: 78.844% (22909/29056)/ 94.452% (27444/29056)
01/15/2023 14:58:43 - INFO - __main__ -   test: [batch: 227/391 ] | Loss: 0.840 | Acc: 78.838% (23008/29184)/ 94.439% (27561/29184)
01/15/2023 14:58:45 - INFO - __main__ -   test: [batch: 228/391 ] | Loss: 0.844 | Acc: 78.763% (23087/29312)/ 94.381% (27665/29312)
01/15/2023 14:58:47 - INFO - __main__ -   test: [batch: 229/391 ] | Loss: 0.847 | Acc: 78.706% (23171/29440)/ 94.338% (27773/29440)
01/15/2023 14:58:49 - INFO - __main__ -   test: [batch: 230/391 ] | Loss: 0.850 | Acc: 78.649% (23255/29568)/ 94.305% (27884/29568)
01/15/2023 14:58:51 - INFO - __main__ -   test: [batch: 231/391 ] | Loss: 0.850 | Acc: 78.640% (23353/29696)/ 94.289% (28000/29696)
01/15/2023 14:58:53 - INFO - __main__ -   test: [batch: 232/391 ] | Loss: 0.849 | Acc: 78.672% (23463/29824)/ 94.303% (28125/29824)
01/15/2023 14:58:55 - INFO - __main__ -   test: [batch: 233/391 ] | Loss: 0.851 | Acc: 78.622% (23549/29952)/ 94.284% (28240/29952)
01/15/2023 14:58:58 - INFO - __main__ -   test: [batch: 234/391 ] | Loss: 0.857 | Acc: 78.501% (23613/30080)/ 94.215% (28340/30080)
01/15/2023 14:59:00 - INFO - __main__ -   test: [batch: 235/391 ] | Loss: 0.857 | Acc: 78.489% (23710/30208)/ 94.217% (28461/30208)
01/15/2023 14:59:02 - INFO - __main__ -   test: [batch: 236/391 ] | Loss: 0.856 | Acc: 78.527% (23822/30336)/ 94.221% (28583/30336)
01/15/2023 14:59:04 - INFO - __main__ -   test: [batch: 237/391 ] | Loss: 0.857 | Acc: 78.522% (23921/30464)/ 94.187% (28693/30464)
01/15/2023 14:59:06 - INFO - __main__ -   test: [batch: 238/391 ] | Loss: 0.856 | Acc: 78.560% (24033/30592)/ 94.195% (28816/30592)
01/15/2023 14:59:08 - INFO - __main__ -   test: [batch: 239/391 ] | Loss: 0.854 | Acc: 78.597% (24145/30720)/ 94.196% (28937/30720)
01/15/2023 14:59:10 - INFO - __main__ -   test: [batch: 240/391 ] | Loss: 0.854 | Acc: 78.614% (24251/30848)/ 94.188% (29055/30848)
01/15/2023 14:59:12 - INFO - __main__ -   test: [batch: 241/391 ] | Loss: 0.857 | Acc: 78.529% (24325/30976)/ 94.144% (29162/30976)
01/15/2023 14:59:15 - INFO - __main__ -   test: [batch: 242/391 ] | Loss: 0.860 | Acc: 78.430% (24395/31104)/ 94.129% (29278/31104)
01/15/2023 14:59:17 - INFO - __main__ -   test: [batch: 243/391 ] | Loss: 0.866 | Acc: 78.291% (24452/31232)/ 94.057% (29376/31232)
01/15/2023 14:59:19 - INFO - __main__ -   test: [batch: 244/391 ] | Loss: 0.866 | Acc: 78.297% (24554/31360)/ 94.053% (29495/31360)
01/15/2023 14:59:21 - INFO - __main__ -   test: [batch: 245/391 ] | Loss: 0.865 | Acc: 78.306% (24657/31488)/ 94.042% (29612/31488)
01/15/2023 14:59:23 - INFO - __main__ -   test: [batch: 246/391 ] | Loss: 0.866 | Acc: 78.283% (24750/31616)/ 94.035% (29730/31616)
01/15/2023 14:59:26 - INFO - __main__ -   test: [batch: 247/391 ] | Loss: 0.871 | Acc: 78.185% (24819/31744)/ 93.964% (29828/31744)
01/15/2023 14:59:28 - INFO - __main__ -   test: [batch: 248/391 ] | Loss: 0.872 | Acc: 78.144% (24906/31872)/ 93.957% (29946/31872)
01/15/2023 14:59:30 - INFO - __main__ -   test: [batch: 249/391 ] | Loss: 0.874 | Acc: 78.013% (24964/32000)/ 93.947% (30063/32000)
01/15/2023 14:59:32 - INFO - __main__ -   test: [batch: 250/391 ] | Loss: 0.873 | Acc: 78.047% (25075/32128)/ 93.965% (30189/32128)
01/15/2023 14:59:34 - INFO - __main__ -   test: [batch: 251/391 ] | Loss: 0.875 | Acc: 78.001% (25160/32256)/ 93.936% (30300/32256)
01/15/2023 14:59:36 - INFO - __main__ -   test: [batch: 252/391 ] | Loss: 0.874 | Acc: 78.026% (25268/32384)/ 93.932% (30419/32384)
01/15/2023 14:59:38 - INFO - __main__ -   test: [batch: 253/391 ] | Loss: 0.876 | Acc: 77.996% (25358/32512)/ 93.919% (30535/32512)
01/15/2023 14:59:41 - INFO - __main__ -   test: [batch: 254/391 ] | Loss: 0.880 | Acc: 77.935% (25438/32640)/ 93.869% (30639/32640)
01/15/2023 14:59:43 - INFO - __main__ -   test: [batch: 255/391 ] | Loss: 0.882 | Acc: 77.902% (25527/32768)/ 93.857% (30755/32768)
01/15/2023 14:59:45 - INFO - __main__ -   test: [batch: 256/391 ] | Loss: 0.887 | Acc: 77.778% (25586/32896)/ 93.820% (30863/32896)
01/15/2023 14:59:47 - INFO - __main__ -   test: [batch: 257/391 ] | Loss: 0.888 | Acc: 77.762% (25680/33024)/ 93.808% (30979/33024)
01/15/2023 14:59:49 - INFO - __main__ -   test: [batch: 258/391 ] | Loss: 0.889 | Acc: 77.748% (25775/33152)/ 93.795% (31095/33152)
01/15/2023 14:59:51 - INFO - __main__ -   test: [batch: 259/391 ] | Loss: 0.893 | Acc: 77.629% (25835/33280)/ 93.780% (31210/33280)
01/15/2023 14:59:53 - INFO - __main__ -   test: [batch: 260/391 ] | Loss: 0.893 | Acc: 77.601% (25925/33408)/ 93.774% (31328/33408)
01/15/2023 14:59:55 - INFO - __main__ -   test: [batch: 261/391 ] | Loss: 0.891 | Acc: 77.651% (26041/33536)/ 93.795% (31455/33536)
01/15/2023 14:59:58 - INFO - __main__ -   test: [batch: 262/391 ] | Loss: 0.890 | Acc: 77.673% (26148/33664)/ 93.800% (31577/33664)
01/15/2023 15:00:00 - INFO - __main__ -   test: [batch: 263/391 ] | Loss: 0.893 | Acc: 77.604% (26224/33792)/ 93.771% (31687/33792)
01/15/2023 15:00:02 - INFO - __main__ -   test: [batch: 264/391 ] | Loss: 0.898 | Acc: 77.544% (26303/33920)/ 93.709% (31786/33920)
01/15/2023 15:00:04 - INFO - __main__ -   test: [batch: 265/391 ] | Loss: 0.898 | Acc: 77.573% (26412/34048)/ 93.712% (31907/34048)
01/15/2023 15:00:06 - INFO - __main__ -   test: [batch: 266/391 ] | Loss: 0.900 | Acc: 77.505% (26488/34176)/ 93.694% (32021/34176)
01/15/2023 15:00:08 - INFO - __main__ -   test: [batch: 267/391 ] | Loss: 0.899 | Acc: 77.539% (26599/34304)/ 93.695% (32141/34304)
01/15/2023 15:00:11 - INFO - __main__ -   test: [batch: 268/391 ] | Loss: 0.898 | Acc: 77.562% (26706/34432)/ 93.692% (32260/34432)
01/15/2023 15:00:13 - INFO - __main__ -   test: [batch: 269/391 ] | Loss: 0.900 | Acc: 77.509% (26787/34560)/ 93.672% (32373/34560)
01/15/2023 15:00:15 - INFO - __main__ -   test: [batch: 270/391 ] | Loss: 0.904 | Acc: 77.442% (26863/34688)/ 93.643% (32483/34688)
01/15/2023 15:00:17 - INFO - __main__ -   test: [batch: 271/391 ] | Loss: 0.904 | Acc: 77.456% (26967/34816)/ 93.638% (32601/34816)
01/15/2023 15:00:19 - INFO - __main__ -   test: [batch: 272/391 ] | Loss: 0.905 | Acc: 77.418% (27053/34944)/ 93.636% (32720/34944)
01/15/2023 15:00:21 - INFO - __main__ -   test: [batch: 273/391 ] | Loss: 0.905 | Acc: 77.424% (27154/35072)/ 93.622% (32835/35072)
01/15/2023 15:00:23 - INFO - __main__ -   test: [batch: 274/391 ] | Loss: 0.905 | Acc: 77.415% (27250/35200)/ 93.628% (32957/35200)
01/15/2023 15:00:25 - INFO - __main__ -   test: [batch: 275/391 ] | Loss: 0.907 | Acc: 77.381% (27337/35328)/ 93.623% (33075/35328)
01/15/2023 15:00:28 - INFO - __main__ -   test: [batch: 276/391 ] | Loss: 0.908 | Acc: 77.347% (27424/35456)/ 93.620% (33194/35456)
01/15/2023 15:00:30 - INFO - __main__ -   test: [batch: 277/391 ] | Loss: 0.908 | Acc: 77.321% (27514/35584)/ 93.604% (33308/35584)
01/15/2023 15:00:32 - INFO - __main__ -   test: [batch: 278/391 ] | Loss: 0.908 | Acc: 77.327% (27615/35712)/ 93.602% (33427/35712)
01/15/2023 15:00:34 - INFO - __main__ -   test: [batch: 279/391 ] | Loss: 0.908 | Acc: 77.335% (27717/35840)/ 93.594% (33544/35840)
01/15/2023 15:00:36 - INFO - __main__ -   test: [batch: 280/391 ] | Loss: 0.909 | Acc: 77.313% (27808/35968)/ 93.589% (33662/35968)
01/15/2023 15:00:38 - INFO - __main__ -   test: [batch: 281/391 ] | Loss: 0.909 | Acc: 77.324% (27911/36096)/ 93.584% (33780/36096)
01/15/2023 15:00:41 - INFO - __main__ -   test: [batch: 282/391 ] | Loss: 0.908 | Acc: 77.360% (28023/36224)/ 93.590% (33902/36224)
01/15/2023 15:00:43 - INFO - __main__ -   test: [batch: 283/391 ] | Loss: 0.909 | Acc: 77.344% (28116/36352)/ 93.585% (34020/36352)
01/15/2023 15:00:45 - INFO - __main__ -   test: [batch: 284/391 ] | Loss: 0.912 | Acc: 77.300% (28199/36480)/ 93.561% (34131/36480)
01/15/2023 15:00:47 - INFO - __main__ -   test: [batch: 285/391 ] | Loss: 0.915 | Acc: 77.256% (28282/36608)/ 93.521% (34236/36608)
01/15/2023 15:00:49 - INFO - __main__ -   test: [batch: 286/391 ] | Loss: 0.916 | Acc: 77.221% (28368/36736)/ 93.510% (34352/36736)
01/15/2023 15:00:51 - INFO - __main__ -   test: [batch: 287/391 ] | Loss: 0.916 | Acc: 77.222% (28467/36864)/ 93.503% (34469/36864)
01/15/2023 15:00:53 - INFO - __main__ -   test: [batch: 288/391 ] | Loss: 0.915 | Acc: 77.246% (28575/36992)/ 93.507% (34590/36992)
01/15/2023 15:00:55 - INFO - __main__ -   test: [batch: 289/391 ] | Loss: 0.917 | Acc: 77.206% (28659/37120)/ 93.475% (34698/37120)
01/15/2023 15:00:57 - INFO - __main__ -   test: [batch: 290/391 ] | Loss: 0.918 | Acc: 77.124% (28727/37248)/ 93.476% (34818/37248)
01/15/2023 15:01:00 - INFO - __main__ -   test: [batch: 291/391 ] | Loss: 0.918 | Acc: 77.135% (28830/37376)/ 93.469% (34935/37376)
01/15/2023 15:01:02 - INFO - __main__ -   test: [batch: 292/391 ] | Loss: 0.920 | Acc: 77.077% (28907/37504)/ 93.449% (35047/37504)
01/15/2023 15:01:04 - INFO - __main__ -   test: [batch: 293/391 ] | Loss: 0.920 | Acc: 77.073% (29004/37632)/ 93.442% (35164/37632)
01/15/2023 15:01:06 - INFO - __main__ -   test: [batch: 294/391 ] | Loss: 0.921 | Acc: 77.060% (29098/37760)/ 93.435% (35281/37760)
01/15/2023 15:01:08 - INFO - __main__ -   test: [batch: 295/391 ] | Loss: 0.920 | Acc: 77.093% (29209/37888)/ 93.441% (35403/37888)
01/15/2023 15:01:10 - INFO - __main__ -   test: [batch: 296/391 ] | Loss: 0.920 | Acc: 77.089% (29306/38016)/ 93.434% (35520/38016)
01/15/2023 15:01:12 - INFO - __main__ -   test: [batch: 297/391 ] | Loss: 0.922 | Acc: 77.063% (29395/38144)/ 93.409% (35630/38144)
01/15/2023 15:01:14 - INFO - __main__ -   test: [batch: 298/391 ] | Loss: 0.924 | Acc: 77.033% (29482/38272)/ 93.382% (35739/38272)
01/15/2023 15:01:16 - INFO - __main__ -   test: [batch: 299/391 ] | Loss: 0.925 | Acc: 77.023% (29577/38400)/ 93.362% (35851/38400)
01/15/2023 15:01:19 - INFO - __main__ -   test: [batch: 300/391 ] | Loss: 0.926 | Acc: 77.030% (29678/38528)/ 93.358% (35969/38528)
01/15/2023 15:01:21 - INFO - __main__ -   test: [batch: 301/391 ] | Loss: 0.926 | Acc: 77.015% (29771/38656)/ 93.346% (36084/38656)
01/15/2023 15:01:23 - INFO - __main__ -   test: [batch: 302/391 ] | Loss: 0.928 | Acc: 76.980% (29856/38784)/ 93.330% (36197/38784)
01/15/2023 15:01:25 - INFO - __main__ -   test: [batch: 303/391 ] | Loss: 0.929 | Acc: 76.958% (29946/38912)/ 93.316% (36311/38912)
01/15/2023 15:01:27 - INFO - __main__ -   test: [batch: 304/391 ] | Loss: 0.928 | Acc: 76.972% (30050/39040)/ 93.320% (36432/39040)
01/15/2023 15:01:29 - INFO - __main__ -   test: [batch: 305/391 ] | Loss: 0.929 | Acc: 76.928% (30131/39168)/ 93.313% (36549/39168)
01/15/2023 15:01:31 - INFO - __main__ -   test: [batch: 306/391 ] | Loss: 0.930 | Acc: 76.909% (30222/39296)/ 93.292% (36660/39296)
01/15/2023 15:01:34 - INFO - __main__ -   test: [batch: 307/391 ] | Loss: 0.931 | Acc: 76.907% (30320/39424)/ 93.278% (36774/39424)
01/15/2023 15:01:36 - INFO - __main__ -   test: [batch: 308/391 ] | Loss: 0.931 | Acc: 76.889% (30411/39552)/ 93.277% (36893/39552)
01/15/2023 15:01:38 - INFO - __main__ -   test: [batch: 309/391 ] | Loss: 0.932 | Acc: 76.883% (30507/39680)/ 93.256% (37004/39680)
01/15/2023 15:01:40 - INFO - __main__ -   test: [batch: 310/391 ] | Loss: 0.933 | Acc: 76.871% (30601/39808)/ 93.245% (37119/39808)
01/15/2023 15:01:42 - INFO - __main__ -   test: [batch: 311/391 ] | Loss: 0.934 | Acc: 76.850% (30691/39936)/ 93.234% (37234/39936)
01/15/2023 15:01:44 - INFO - __main__ -   test: [batch: 312/391 ] | Loss: 0.936 | Acc: 76.835% (30783/40064)/ 93.208% (37343/40064)
01/15/2023 15:01:46 - INFO - __main__ -   test: [batch: 313/391 ] | Loss: 0.934 | Acc: 76.874% (30897/40192)/ 93.228% (37470/40192)
01/15/2023 15:01:48 - INFO - __main__ -   test: [batch: 314/391 ] | Loss: 0.934 | Acc: 76.860% (30990/40320)/ 93.224% (37588/40320)
01/15/2023 15:01:50 - INFO - __main__ -   test: [batch: 315/391 ] | Loss: 0.935 | Acc: 76.844% (31082/40448)/ 93.211% (37702/40448)
01/15/2023 15:01:52 - INFO - __main__ -   test: [batch: 316/391 ] | Loss: 0.938 | Acc: 76.752% (31143/40576)/ 93.186% (37811/40576)
01/15/2023 15:01:55 - INFO - __main__ -   test: [batch: 317/391 ] | Loss: 0.939 | Acc: 76.707% (31223/40704)/ 93.163% (37921/40704)
01/15/2023 15:01:57 - INFO - __main__ -   test: [batch: 318/391 ] | Loss: 0.938 | Acc: 76.739% (31334/40832)/ 93.179% (38047/40832)
01/15/2023 15:01:59 - INFO - __main__ -   test: [batch: 319/391 ] | Loss: 0.941 | Acc: 76.685% (31410/40960)/ 93.152% (38155/40960)
01/15/2023 15:02:01 - INFO - __main__ -   test: [batch: 320/391 ] | Loss: 0.940 | Acc: 76.721% (31523/41088)/ 93.161% (38278/41088)
01/15/2023 15:02:03 - INFO - __main__ -   test: [batch: 321/391 ] | Loss: 0.939 | Acc: 76.740% (31629/41216)/ 93.156% (38395/41216)
01/15/2023 15:02:05 - INFO - __main__ -   test: [batch: 322/391 ] | Loss: 0.941 | Acc: 76.715% (31717/41344)/ 93.150% (38512/41344)
01/15/2023 15:02:07 - INFO - __main__ -   test: [batch: 323/391 ] | Loss: 0.943 | Acc: 76.681% (31801/41472)/ 93.128% (38622/41472)
01/15/2023 15:02:09 - INFO - __main__ -   test: [batch: 324/391 ] | Loss: 0.943 | Acc: 76.678% (31898/41600)/ 93.127% (38741/41600)
01/15/2023 15:02:12 - INFO - __main__ -   test: [batch: 325/391 ] | Loss: 0.943 | Acc: 76.685% (31999/41728)/ 93.127% (38860/41728)
01/15/2023 15:02:14 - INFO - __main__ -   test: [batch: 326/391 ] | Loss: 0.946 | Acc: 76.598% (32061/41856)/ 93.098% (38967/41856)
01/15/2023 15:02:16 - INFO - __main__ -   test: [batch: 327/391 ] | Loss: 0.949 | Acc: 76.515% (32124/41984)/ 93.066% (39073/41984)
01/15/2023 15:02:18 - INFO - __main__ -   test: [batch: 328/391 ] | Loss: 0.951 | Acc: 76.463% (32200/42112)/ 93.042% (39182/42112)
01/15/2023 15:02:20 - INFO - __main__ -   test: [batch: 329/391 ] | Loss: 0.951 | Acc: 76.458% (32296/42240)/ 93.042% (39301/42240)
01/15/2023 15:02:22 - INFO - __main__ -   test: [batch: 330/391 ] | Loss: 0.953 | Acc: 76.416% (32376/42368)/ 93.018% (39410/42368)
01/15/2023 15:02:24 - INFO - __main__ -   test: [batch: 331/391 ] | Loss: 0.953 | Acc: 76.381% (32459/42496)/ 93.030% (39534/42496)
01/15/2023 15:02:26 - INFO - __main__ -   test: [batch: 332/391 ] | Loss: 0.954 | Acc: 76.370% (32552/42624)/ 93.030% (39653/42624)
01/15/2023 15:02:28 - INFO - __main__ -   test: [batch: 333/391 ] | Loss: 0.953 | Acc: 76.401% (32663/42752)/ 93.039% (39776/42752)
01/15/2023 15:02:30 - INFO - __main__ -   test: [batch: 334/391 ] | Loss: 0.954 | Acc: 76.371% (32748/42880)/ 93.022% (39888/42880)
01/15/2023 15:02:32 - INFO - __main__ -   test: [batch: 335/391 ] | Loss: 0.955 | Acc: 76.346% (32835/43008)/ 93.008% (40001/43008)
01/15/2023 15:02:35 - INFO - __main__ -   test: [batch: 336/391 ] | Loss: 0.957 | Acc: 76.303% (32914/43136)/ 92.992% (40113/43136)
01/15/2023 15:02:37 - INFO - __main__ -   test: [batch: 337/391 ] | Loss: 0.957 | Acc: 76.301% (33011/43264)/ 92.985% (40229/43264)
01/15/2023 15:02:39 - INFO - __main__ -   test: [batch: 338/391 ] | Loss: 0.957 | Acc: 76.286% (33102/43392)/ 92.996% (40353/43392)
01/15/2023 15:02:41 - INFO - __main__ -   test: [batch: 339/391 ] | Loss: 0.959 | Acc: 76.250% (33184/43520)/ 92.973% (40462/43520)
01/15/2023 15:02:43 - INFO - __main__ -   test: [batch: 340/391 ] | Loss: 0.959 | Acc: 76.246% (33280/43648)/ 92.982% (40585/43648)
01/15/2023 15:02:45 - INFO - __main__ -   test: [batch: 341/391 ] | Loss: 0.957 | Acc: 76.286% (33395/43776)/ 93.001% (40712/43776)
01/15/2023 15:02:47 - INFO - __main__ -   test: [batch: 342/391 ] | Loss: 0.958 | Acc: 76.237% (33471/43904)/ 92.994% (40828/43904)
01/15/2023 15:02:49 - INFO - __main__ -   test: [batch: 343/391 ] | Loss: 0.958 | Acc: 76.238% (33569/44032)/ 92.996% (40948/44032)
01/15/2023 15:02:51 - INFO - __main__ -   test: [batch: 344/391 ] | Loss: 0.958 | Acc: 76.236% (33666/44160)/ 92.987% (41063/44160)
01/15/2023 15:02:53 - INFO - __main__ -   test: [batch: 345/391 ] | Loss: 0.962 | Acc: 76.170% (33734/44288)/ 92.946% (41164/44288)
01/15/2023 15:02:55 - INFO - __main__ -   test: [batch: 346/391 ] | Loss: 0.964 | Acc: 76.153% (33824/44416)/ 92.935% (41278/44416)
01/15/2023 15:02:57 - INFO - __main__ -   test: [batch: 347/391 ] | Loss: 0.963 | Acc: 76.167% (33928/44544)/ 92.944% (41401/44544)
01/15/2023 15:03:00 - INFO - __main__ -   test: [batch: 348/391 ] | Loss: 0.964 | Acc: 76.146% (34016/44672)/ 92.924% (41511/44672)
01/15/2023 15:03:02 - INFO - __main__ -   test: [batch: 349/391 ] | Loss: 0.964 | Acc: 76.141% (34111/44800)/ 92.935% (41635/44800)
01/15/2023 15:03:04 - INFO - __main__ -   test: [batch: 350/391 ] | Loss: 0.964 | Acc: 76.157% (34216/44928)/ 92.935% (41754/44928)
01/15/2023 15:03:06 - INFO - __main__ -   test: [batch: 351/391 ] | Loss: 0.966 | Acc: 76.103% (34289/45056)/ 92.918% (41865/45056)
01/15/2023 15:03:08 - INFO - __main__ -   test: [batch: 352/391 ] | Loss: 0.966 | Acc: 76.100% (34385/45184)/ 92.913% (41982/45184)
01/15/2023 15:03:10 - INFO - __main__ -   test: [batch: 353/391 ] | Loss: 0.969 | Acc: 76.057% (34463/45312)/ 92.878% (42085/45312)
01/15/2023 15:03:12 - INFO - __main__ -   test: [batch: 354/391 ] | Loss: 0.971 | Acc: 75.997% (34533/45440)/ 92.859% (42195/45440)
01/15/2023 15:03:14 - INFO - __main__ -   test: [batch: 355/391 ] | Loss: 0.973 | Acc: 75.935% (34602/45568)/ 92.850% (42310/45568)
01/15/2023 15:03:16 - INFO - __main__ -   test: [batch: 356/391 ] | Loss: 0.974 | Acc: 75.930% (34697/45696)/ 92.851% (42429/45696)
01/15/2023 15:03:18 - INFO - __main__ -   test: [batch: 357/391 ] | Loss: 0.972 | Acc: 75.965% (34810/45824)/ 92.864% (42554/45824)
01/15/2023 15:03:20 - INFO - __main__ -   test: [batch: 358/391 ] | Loss: 0.972 | Acc: 75.995% (34921/45952)/ 92.866% (42674/45952)
01/15/2023 15:03:22 - INFO - __main__ -   test: [batch: 359/391 ] | Loss: 0.972 | Acc: 76.005% (35023/46080)/ 92.860% (42790/46080)
01/15/2023 15:03:25 - INFO - __main__ -   test: [batch: 360/391 ] | Loss: 0.974 | Acc: 75.978% (35108/46208)/ 92.852% (42905/46208)
01/15/2023 15:03:27 - INFO - __main__ -   test: [batch: 361/391 ] | Loss: 0.974 | Acc: 75.982% (35207/46336)/ 92.859% (43027/46336)
01/15/2023 15:03:29 - INFO - __main__ -   test: [batch: 362/391 ] | Loss: 0.973 | Acc: 75.979% (35303/46464)/ 92.872% (43152/46464)
01/15/2023 15:03:31 - INFO - __main__ -   test: [batch: 363/391 ] | Loss: 0.973 | Acc: 75.970% (35396/46592)/ 92.866% (43268/46592)
01/15/2023 15:03:33 - INFO - __main__ -   test: [batch: 364/391 ] | Loss: 0.972 | Acc: 75.997% (35506/46720)/ 92.875% (43391/46720)
01/15/2023 15:03:35 - INFO - __main__ -   test: [batch: 365/391 ] | Loss: 0.972 | Acc: 76.012% (35610/46848)/ 92.879% (43512/46848)
01/15/2023 15:03:37 - INFO - __main__ -   test: [batch: 366/391 ] | Loss: 0.970 | Acc: 76.047% (35724/46976)/ 92.896% (43639/46976)
01/15/2023 15:03:39 - INFO - __main__ -   test: [batch: 367/391 ] | Loss: 0.969 | Acc: 76.070% (35832/47104)/ 92.909% (43764/47104)
01/15/2023 15:03:41 - INFO - __main__ -   test: [batch: 368/391 ] | Loss: 0.969 | Acc: 76.073% (35931/47232)/ 92.920% (43888/47232)
01/15/2023 15:03:43 - INFO - __main__ -   test: [batch: 369/391 ] | Loss: 0.968 | Acc: 76.094% (36038/47360)/ 92.929% (44011/47360)
01/15/2023 15:03:45 - INFO - __main__ -   test: [batch: 370/391 ] | Loss: 0.968 | Acc: 76.080% (36129/47488)/ 92.929% (44130/47488)
01/15/2023 15:03:47 - INFO - __main__ -   test: [batch: 371/391 ] | Loss: 0.968 | Acc: 76.088% (36230/47616)/ 92.933% (44251/47616)
01/15/2023 15:03:49 - INFO - __main__ -   test: [batch: 372/391 ] | Loss: 0.966 | Acc: 76.135% (36350/47744)/ 92.948% (44377/47744)
01/15/2023 15:03:51 - INFO - __main__ -   test: [batch: 373/391 ] | Loss: 0.965 | Acc: 76.174% (36466/47872)/ 92.956% (44500/47872)
01/15/2023 15:03:54 - INFO - __main__ -   test: [batch: 374/391 ] | Loss: 0.963 | Acc: 76.204% (36578/48000)/ 92.963% (44622/48000)
01/15/2023 15:03:56 - INFO - __main__ -   test: [batch: 375/391 ] | Loss: 0.966 | Acc: 76.137% (36643/48128)/ 92.923% (44722/48128)
01/15/2023 15:03:58 - INFO - __main__ -   test: [batch: 376/391 ] | Loss: 0.967 | Acc: 76.129% (36737/48256)/ 92.911% (44835/48256)
01/15/2023 15:04:00 - INFO - __main__ -   test: [batch: 377/391 ] | Loss: 0.967 | Acc: 76.116% (36828/48384)/ 92.903% (44950/48384)
01/15/2023 15:04:02 - INFO - __main__ -   test: [batch: 378/391 ] | Loss: 0.971 | Acc: 76.039% (36888/48512)/ 92.857% (45047/48512)
01/15/2023 15:04:04 - INFO - __main__ -   test: [batch: 379/391 ] | Loss: 0.971 | Acc: 76.022% (36977/48640)/ 92.862% (45168/48640)
01/15/2023 15:04:06 - INFO - __main__ -   test: [batch: 380/391 ] | Loss: 0.971 | Acc: 76.019% (37073/48768)/ 92.868% (45290/48768)
01/15/2023 15:04:09 - INFO - __main__ -   test: [batch: 381/391 ] | Loss: 0.972 | Acc: 75.967% (37145/48896)/ 92.866% (45408/48896)
01/15/2023 15:04:11 - INFO - __main__ -   test: [batch: 382/391 ] | Loss: 0.974 | Acc: 75.932% (37225/49024)/ 92.850% (45519/49024)
01/15/2023 15:04:13 - INFO - __main__ -   test: [batch: 383/391 ] | Loss: 0.974 | Acc: 75.940% (37326/49152)/ 92.843% (45634/49152)
01/15/2023 15:04:15 - INFO - __main__ -   test: [batch: 384/391 ] | Loss: 0.972 | Acc: 75.978% (37442/49280)/ 92.855% (45759/49280)
01/15/2023 15:04:17 - INFO - __main__ -   test: [batch: 385/391 ] | Loss: 0.971 | Acc: 75.988% (37544/49408)/ 92.866% (45883/49408)
01/15/2023 15:04:19 - INFO - __main__ -   test: [batch: 386/391 ] | Loss: 0.969 | Acc: 76.034% (37664/49536)/ 92.880% (46009/49536)
01/15/2023 15:04:21 - INFO - __main__ -   test: [batch: 387/391 ] | Loss: 0.968 | Acc: 76.073% (37781/49664)/ 92.890% (46133/49664)
01/15/2023 15:04:23 - INFO - __main__ -   test: [batch: 388/391 ] | Loss: 0.966 | Acc: 76.119% (37901/49792)/ 92.902% (46258/49792)
01/15/2023 15:04:25 - INFO - __main__ -   test: [batch: 389/391 ] | Loss: 0.965 | Acc: 76.118% (37998/49920)/ 92.905% (46378/49920)
01/15/2023 15:04:28 - INFO - __main__ -   test: [batch: 390/391 ] | Loss: 0.967 | Acc: 76.070% (38035/50000)/ 92.896% (46448/50000)
01/15/2023 15:04:28 - INFO - __main__ -   Final accuracy: 76.070
01/15/2023 15:04:28 - INFO - __main__ -   {'milestones': Counter({2: 1, 3: 1, 4: 1}), 'gamma': 0.1, 'base_lrs': [0.00025], 'last_epoch': 2, '_step_count': 3, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [2.5e-05]}
01/15/2023 15:04:28 - INFO - __main__ -   
Epoch: 2
01/15/2023 15:04:30 - INFO - __main__ -   test: [epoch: 2 | batch: 0/10010 ] | Loss: 0.976 | Acc: 77.344% (99/128)
01/15/2023 15:08:00 - INFO - __main__ -   test: [epoch: 2 | batch: 100/10010 ] | Loss: 0.830 | Acc: 79.688% (10302/12928)
01/15/2023 15:11:33 - INFO - __main__ -   test: [epoch: 2 | batch: 200/10010 ] | Loss: 0.839 | Acc: 79.077% (20345/25728)
01/15/2023 15:15:04 - INFO - __main__ -   test: [epoch: 2 | batch: 300/10010 ] | Loss: 0.843 | Acc: 79.046% (30455/38528)
01/15/2023 15:18:37 - INFO - __main__ -   test: [epoch: 2 | batch: 400/10010 ] | Loss: 0.850 | Acc: 78.854% (40474/51328)
01/15/2023 15:22:08 - INFO - __main__ -   test: [epoch: 2 | batch: 500/10010 ] | Loss: 0.853 | Acc: 78.796% (50530/64128)
01/15/2023 15:25:39 - INFO - __main__ -   test: [epoch: 2 | batch: 600/10010 ] | Loss: 0.857 | Acc: 78.693% (60537/76928)
01/15/2023 15:29:10 - INFO - __main__ -   test: [epoch: 2 | batch: 700/10010 ] | Loss: 0.858 | Acc: 78.653% (70574/89728)
01/15/2023 15:32:41 - INFO - __main__ -   test: [epoch: 2 | batch: 800/10010 ] | Loss: 0.859 | Acc: 78.615% (80602/102528)
01/15/2023 15:36:11 - INFO - __main__ -   test: [epoch: 2 | batch: 900/10010 ] | Loss: 0.857 | Acc: 78.682% (90742/115328)
01/15/2023 15:39:44 - INFO - __main__ -   test: [epoch: 2 | batch: 1000/10010 ] | Loss: 0.859 | Acc: 78.625% (100741/128128)
01/15/2023 15:43:16 - INFO - __main__ -   test: [epoch: 2 | batch: 1100/10010 ] | Loss: 0.860 | Acc: 78.625% (110804/140928)
01/15/2023 15:46:49 - INFO - __main__ -   test: [epoch: 2 | batch: 1200/10010 ] | Loss: 0.862 | Acc: 78.589% (120814/153728)
01/15/2023 15:50:18 - INFO - __main__ -   test: [epoch: 2 | batch: 1300/10010 ] | Loss: 0.860 | Acc: 78.604% (130898/166528)
01/15/2023 15:53:50 - INFO - __main__ -   test: [epoch: 2 | batch: 1400/10010 ] | Loss: 0.860 | Acc: 78.630% (141005/179328)
01/15/2023 15:57:20 - INFO - __main__ -   test: [epoch: 2 | batch: 1500/10010 ] | Loss: 0.861 | Acc: 78.598% (151008/192128)
01/15/2023 16:00:53 - INFO - __main__ -   test: [epoch: 2 | batch: 1600/10010 ] | Loss: 0.861 | Acc: 78.565% (161001/204928)
01/15/2023 16:04:26 - INFO - __main__ -   test: [epoch: 2 | batch: 1700/10010 ] | Loss: 0.861 | Acc: 78.560% (171047/217728)
01/15/2023 16:07:57 - INFO - __main__ -   test: [epoch: 2 | batch: 1800/10010 ] | Loss: 0.860 | Acc: 78.566% (181117/230528)
01/15/2023 16:11:29 - INFO - __main__ -   test: [epoch: 2 | batch: 1900/10010 ] | Loss: 0.859 | Acc: 78.582% (191213/243328)
01/15/2023 16:15:00 - INFO - __main__ -   test: [epoch: 2 | batch: 2000/10010 ] | Loss: 0.858 | Acc: 78.599% (201313/256128)
01/15/2023 16:18:32 - INFO - __main__ -   test: [epoch: 2 | batch: 2100/10010 ] | Loss: 0.858 | Acc: 78.594% (211361/268928)
01/15/2023 16:22:02 - INFO - __main__ -   test: [epoch: 2 | batch: 2200/10010 ] | Loss: 0.859 | Acc: 78.584% (221392/281728)
01/15/2023 16:25:33 - INFO - __main__ -   test: [epoch: 2 | batch: 2300/10010 ] | Loss: 0.860 | Acc: 78.564% (231393/294528)
01/15/2023 16:29:04 - INFO - __main__ -   test: [epoch: 2 | batch: 2400/10010 ] | Loss: 0.860 | Acc: 78.568% (241461/307328)
01/15/2023 16:32:36 - INFO - __main__ -   test: [epoch: 2 | batch: 2500/10010 ] | Loss: 0.860 | Acc: 78.561% (251495/320128)
01/15/2023 16:36:08 - INFO - __main__ -   test: [epoch: 2 | batch: 2600/10010 ] | Loss: 0.860 | Acc: 78.574% (261596/332928)
01/15/2023 16:39:39 - INFO - __main__ -   test: [epoch: 2 | batch: 2700/10010 ] | Loss: 0.859 | Acc: 78.611% (271779/345728)
01/15/2023 16:43:11 - INFO - __main__ -   test: [epoch: 2 | batch: 2800/10010 ] | Loss: 0.860 | Acc: 78.606% (281823/358528)
01/15/2023 16:46:43 - INFO - __main__ -   test: [epoch: 2 | batch: 2900/10010 ] | Loss: 0.859 | Acc: 78.609% (291896/371328)
01/15/2023 16:50:15 - INFO - __main__ -   test: [epoch: 2 | batch: 3000/10010 ] | Loss: 0.859 | Acc: 78.607% (301950/384128)
01/15/2023 16:53:46 - INFO - __main__ -   test: [epoch: 2 | batch: 3100/10010 ] | Loss: 0.859 | Acc: 78.618% (312058/396928)
01/15/2023 16:57:17 - INFO - __main__ -   test: [epoch: 2 | batch: 3200/10010 ] | Loss: 0.859 | Acc: 78.622% (322136/409728)
01/15/2023 17:00:47 - INFO - __main__ -   test: [epoch: 2 | batch: 3300/10010 ] | Loss: 0.860 | Acc: 78.603% (332121/422528)
01/15/2023 17:04:19 - INFO - __main__ -   test: [epoch: 2 | batch: 3400/10010 ] | Loss: 0.859 | Acc: 78.615% (342235/435328)
01/15/2023 17:07:50 - INFO - __main__ -   test: [epoch: 2 | batch: 3500/10010 ] | Loss: 0.859 | Acc: 78.618% (352309/448128)
01/15/2023 17:11:21 - INFO - __main__ -   test: [epoch: 2 | batch: 3600/10010 ] | Loss: 0.858 | Acc: 78.637% (362458/460928)
01/15/2023 17:14:53 - INFO - __main__ -   test: [epoch: 2 | batch: 3700/10010 ] | Loss: 0.859 | Acc: 78.626% (372475/473728)
01/15/2023 17:18:25 - INFO - __main__ -   test: [epoch: 2 | batch: 3800/10010 ] | Loss: 0.858 | Acc: 78.649% (382651/486528)
01/15/2023 17:21:57 - INFO - __main__ -   test: [epoch: 2 | batch: 3900/10010 ] | Loss: 0.858 | Acc: 78.639% (392665/499328)
01/15/2023 17:25:28 - INFO - __main__ -   test: [epoch: 2 | batch: 4000/10010 ] | Loss: 0.859 | Acc: 78.625% (402662/512128)
01/15/2023 17:29:00 - INFO - __main__ -   test: [epoch: 2 | batch: 4100/10010 ] | Loss: 0.859 | Acc: 78.626% (412730/524928)
01/15/2023 17:32:31 - INFO - __main__ -   test: [epoch: 2 | batch: 4200/10010 ] | Loss: 0.859 | Acc: 78.621% (422769/537728)
01/15/2023 17:36:02 - INFO - __main__ -   test: [epoch: 2 | batch: 4300/10010 ] | Loss: 0.859 | Acc: 78.622% (432838/550528)
01/15/2023 17:39:34 - INFO - __main__ -   test: [epoch: 2 | batch: 4400/10010 ] | Loss: 0.859 | Acc: 78.635% (442973/563328)
01/15/2023 17:43:04 - INFO - __main__ -   test: [epoch: 2 | batch: 4500/10010 ] | Loss: 0.859 | Acc: 78.625% (452981/576128)
01/15/2023 17:46:34 - INFO - __main__ -   test: [epoch: 2 | batch: 4600/10010 ] | Loss: 0.859 | Acc: 78.615% (462985/588928)
01/15/2023 17:50:05 - INFO - __main__ -   test: [epoch: 2 | batch: 4700/10010 ] | Loss: 0.859 | Acc: 78.614% (473041/601728)
01/15/2023 17:53:37 - INFO - __main__ -   test: [epoch: 2 | batch: 4800/10010 ] | Loss: 0.859 | Acc: 78.621% (483148/614528)
01/15/2023 17:57:09 - INFO - __main__ -   test: [epoch: 2 | batch: 4900/10010 ] | Loss: 0.859 | Acc: 78.619% (493199/627328)
01/15/2023 18:00:40 - INFO - __main__ -   test: [epoch: 2 | batch: 5000/10010 ] | Loss: 0.859 | Acc: 78.634% (503361/640128)
01/15/2023 18:04:11 - INFO - __main__ -   test: [epoch: 2 | batch: 5100/10010 ] | Loss: 0.859 | Acc: 78.623% (513350/652928)
01/15/2023 18:07:40 - INFO - __main__ -   test: [epoch: 2 | batch: 5200/10010 ] | Loss: 0.859 | Acc: 78.616% (523370/665728)
01/15/2023 18:11:10 - INFO - __main__ -   test: [epoch: 2 | batch: 5300/10010 ] | Loss: 0.859 | Acc: 78.612% (533403/678528)
01/15/2023 18:14:41 - INFO - __main__ -   test: [epoch: 2 | batch: 5400/10010 ] | Loss: 0.860 | Acc: 78.607% (543435/691328)
01/15/2023 18:18:13 - INFO - __main__ -   test: [epoch: 2 | batch: 5500/10010 ] | Loss: 0.860 | Acc: 78.604% (553473/704128)
01/15/2023 18:21:44 - INFO - __main__ -   test: [epoch: 2 | batch: 5600/10010 ] | Loss: 0.860 | Acc: 78.610% (563575/716928)
01/15/2023 18:25:16 - INFO - __main__ -   test: [epoch: 2 | batch: 5700/10010 ] | Loss: 0.860 | Acc: 78.612% (573653/729728)
01/15/2023 18:28:47 - INFO - __main__ -   test: [epoch: 2 | batch: 5800/10010 ] | Loss: 0.859 | Acc: 78.618% (583760/742528)
01/15/2023 18:32:18 - INFO - __main__ -   test: [epoch: 2 | batch: 5900/10010 ] | Loss: 0.859 | Acc: 78.627% (593889/755328)
01/15/2023 18:35:48 - INFO - __main__ -   test: [epoch: 2 | batch: 6000/10010 ] | Loss: 0.859 | Acc: 78.625% (603944/768128)
01/15/2023 18:39:19 - INFO - __main__ -   test: [epoch: 2 | batch: 6100/10010 ] | Loss: 0.860 | Acc: 78.614% (613918/780928)
01/15/2023 18:42:53 - INFO - __main__ -   test: [epoch: 2 | batch: 6200/10010 ] | Loss: 0.860 | Acc: 78.612% (623969/793728)
01/15/2023 18:46:23 - INFO - __main__ -   test: [epoch: 2 | batch: 6300/10010 ] | Loss: 0.860 | Acc: 78.615% (634052/806528)
01/15/2023 18:49:54 - INFO - __main__ -   test: [epoch: 2 | batch: 6400/10010 ] | Loss: 0.860 | Acc: 78.607% (644051/819328)
01/15/2023 18:53:24 - INFO - __main__ -   test: [epoch: 2 | batch: 6500/10010 ] | Loss: 0.860 | Acc: 78.611% (654145/832128)
01/15/2023 18:56:54 - INFO - __main__ -   test: [epoch: 2 | batch: 6600/10010 ] | Loss: 0.860 | Acc: 78.604% (664151/844928)
01/15/2023 19:00:25 - INFO - __main__ -   test: [epoch: 2 | batch: 6700/10010 ] | Loss: 0.860 | Acc: 78.601% (674182/857728)
01/15/2023 19:03:58 - INFO - __main__ -   test: [epoch: 2 | batch: 6800/10010 ] | Loss: 0.860 | Acc: 78.598% (684220/870528)
01/15/2023 19:07:28 - INFO - __main__ -   test: [epoch: 2 | batch: 6900/10010 ] | Loss: 0.860 | Acc: 78.601% (694304/883328)
01/15/2023 19:11:00 - INFO - __main__ -   test: [epoch: 2 | batch: 7000/10010 ] | Loss: 0.860 | Acc: 78.593% (704297/896128)
01/15/2023 19:14:33 - INFO - __main__ -   test: [epoch: 2 | batch: 7100/10010 ] | Loss: 0.859 | Acc: 78.598% (714403/908928)
01/15/2023 19:18:05 - INFO - __main__ -   test: [epoch: 2 | batch: 7200/10010 ] | Loss: 0.859 | Acc: 78.602% (724495/921728)
01/15/2023 19:21:37 - INFO - __main__ -   test: [epoch: 2 | batch: 7300/10010 ] | Loss: 0.859 | Acc: 78.599% (734525/934528)
01/15/2023 19:25:08 - INFO - __main__ -   test: [epoch: 2 | batch: 7400/10010 ] | Loss: 0.859 | Acc: 78.603% (744625/947328)
01/15/2023 19:28:41 - INFO - __main__ -   test: [epoch: 2 | batch: 7500/10010 ] | Loss: 0.859 | Acc: 78.610% (754752/960128)
01/15/2023 19:32:12 - INFO - __main__ -   test: [epoch: 2 | batch: 7600/10010 ] | Loss: 0.859 | Acc: 78.618% (764893/972928)
01/15/2023 19:35:42 - INFO - __main__ -   test: [epoch: 2 | batch: 7700/10010 ] | Loss: 0.859 | Acc: 78.611% (774892/985728)
01/15/2023 19:39:14 - INFO - __main__ -   test: [epoch: 2 | batch: 7800/10010 ] | Loss: 0.859 | Acc: 78.607% (784914/998528)
01/15/2023 19:42:45 - INFO - __main__ -   test: [epoch: 2 | batch: 7900/10010 ] | Loss: 0.859 | Acc: 78.613% (795031/1011328)
01/15/2023 19:46:17 - INFO - __main__ -   test: [epoch: 2 | batch: 8000/10010 ] | Loss: 0.859 | Acc: 78.612% (805084/1024128)
01/15/2023 19:49:49 - INFO - __main__ -   test: [epoch: 2 | batch: 8100/10010 ] | Loss: 0.859 | Acc: 78.615% (815181/1036928)
01/15/2023 19:53:21 - INFO - __main__ -   test: [epoch: 2 | batch: 8200/10010 ] | Loss: 0.859 | Acc: 78.616% (825255/1049728)
01/15/2023 19:56:53 - INFO - __main__ -   test: [epoch: 2 | batch: 8300/10010 ] | Loss: 0.859 | Acc: 78.613% (835290/1062528)
01/15/2023 20:00:25 - INFO - __main__ -   test: [epoch: 2 | batch: 8400/10010 ] | Loss: 0.859 | Acc: 78.615% (845370/1075328)
01/15/2023 20:03:57 - INFO - __main__ -   test: [epoch: 2 | batch: 8500/10010 ] | Loss: 0.859 | Acc: 78.620% (855484/1088128)
01/15/2023 20:07:30 - INFO - __main__ -   test: [epoch: 2 | batch: 8600/10010 ] | Loss: 0.859 | Acc: 78.623% (865587/1100928)
01/15/2023 20:11:02 - INFO - __main__ -   test: [epoch: 2 | batch: 8700/10010 ] | Loss: 0.859 | Acc: 78.625% (875672/1113728)
01/15/2023 20:14:34 - INFO - __main__ -   test: [epoch: 2 | batch: 8800/10010 ] | Loss: 0.859 | Acc: 78.630% (885789/1126528)
01/15/2023 20:18:07 - INFO - __main__ -   test: [epoch: 2 | batch: 8900/10010 ] | Loss: 0.859 | Acc: 78.629% (895840/1139328)
01/15/2023 20:21:39 - INFO - __main__ -   test: [epoch: 2 | batch: 9000/10010 ] | Loss: 0.859 | Acc: 78.621% (905817/1152128)
01/15/2023 20:25:10 - INFO - __main__ -   test: [epoch: 2 | batch: 9100/10010 ] | Loss: 0.859 | Acc: 78.616% (915823/1164928)
01/15/2023 20:28:42 - INFO - __main__ -   test: [epoch: 2 | batch: 9200/10010 ] | Loss: 0.859 | Acc: 78.621% (925947/1177728)
01/15/2023 20:32:15 - INFO - __main__ -   test: [epoch: 2 | batch: 9300/10010 ] | Loss: 0.859 | Acc: 78.617% (935957/1190528)
01/15/2023 20:35:46 - INFO - __main__ -   test: [epoch: 2 | batch: 9400/10010 ] | Loss: 0.859 | Acc: 78.620% (946059/1203328)
01/15/2023 20:39:16 - INFO - __main__ -   test: [epoch: 2 | batch: 9500/10010 ] | Loss: 0.859 | Acc: 78.615% (956058/1216128)
01/15/2023 20:42:49 - INFO - __main__ -   test: [epoch: 2 | batch: 9600/10010 ] | Loss: 0.859 | Acc: 78.615% (966125/1228928)
01/15/2023 20:46:20 - INFO - __main__ -   test: [epoch: 2 | batch: 9700/10010 ] | Loss: 0.860 | Acc: 78.610% (976128/1241728)
01/15/2023 20:49:53 - INFO - __main__ -   test: [epoch: 2 | batch: 9800/10010 ] | Loss: 0.860 | Acc: 78.608% (986160/1254528)
01/15/2023 20:53:10 - INFO - __main__ -   test: [epoch: 2 | batch: 9900/10010 ] | Loss: 0.860 | Acc: 78.610% (996243/1267328)
01/15/2023 20:55:43 - INFO - __main__ -   test: [epoch: 2 | batch: 10000/10010 ] | Loss: 0.860 | Acc: 78.607% (1006267/1280128)
01/15/2023 20:55:57 - INFO - __main__ -   Saving Checkpoint
01/15/2023 20:55:59 - INFO - __main__ -   test: [batch: 0/391 ] | Loss: 0.478 | Acc: 86.719% (111/128)/ 97.656% (125/128)
01/15/2023 20:56:01 - INFO - __main__ -   test: [batch: 1/391 ] | Loss: 0.475 | Acc: 86.719% (222/256)/ 98.047% (251/256)
01/15/2023 20:56:02 - INFO - __main__ -   test: [batch: 2/391 ] | Loss: 0.625 | Acc: 83.073% (319/384)/ 95.833% (368/384)
01/15/2023 20:56:04 - INFO - __main__ -   test: [batch: 3/391 ] | Loss: 0.579 | Acc: 84.766% (434/512)/ 96.289% (493/512)
01/15/2023 20:56:05 - INFO - __main__ -   test: [batch: 4/391 ] | Loss: 0.496 | Acc: 87.031% (557/640)/ 97.031% (621/640)
01/15/2023 20:56:07 - INFO - __main__ -   test: [batch: 5/391 ] | Loss: 0.438 | Acc: 88.281% (678/768)/ 97.526% (749/768)
01/15/2023 20:56:09 - INFO - __main__ -   test: [batch: 6/391 ] | Loss: 0.431 | Acc: 88.393% (792/896)/ 97.433% (873/896)
01/15/2023 20:56:10 - INFO - __main__ -   test: [batch: 7/391 ] | Loss: 0.410 | Acc: 89.355% (915/1024)/ 97.559% (999/1024)
01/15/2023 20:56:12 - INFO - __main__ -   test: [batch: 8/391 ] | Loss: 0.428 | Acc: 89.149% (1027/1152)/ 97.483% (1123/1152)
01/15/2023 20:56:14 - INFO - __main__ -   test: [batch: 9/391 ] | Loss: 0.408 | Acc: 89.688% (1148/1280)/ 97.578% (1249/1280)
01/15/2023 20:56:15 - INFO - __main__ -   test: [batch: 10/391 ] | Loss: 0.457 | Acc: 88.494% (1246/1408)/ 97.656% (1375/1408)
01/15/2023 20:56:17 - INFO - __main__ -   test: [batch: 11/391 ] | Loss: 0.461 | Acc: 88.672% (1362/1536)/ 97.591% (1499/1536)
01/15/2023 20:56:18 - INFO - __main__ -   test: [batch: 12/391 ] | Loss: 0.507 | Acc: 87.620% (1458/1664)/ 97.296% (1619/1664)
01/15/2023 20:56:20 - INFO - __main__ -   test: [batch: 13/391 ] | Loss: 0.554 | Acc: 86.105% (1543/1792)/ 96.819% (1735/1792)
01/15/2023 20:56:21 - INFO - __main__ -   test: [batch: 14/391 ] | Loss: 0.575 | Acc: 85.260% (1637/1920)/ 96.875% (1860/1920)
01/15/2023 20:56:23 - INFO - __main__ -   test: [batch: 15/391 ] | Loss: 0.584 | Acc: 84.766% (1736/2048)/ 96.875% (1984/2048)
01/15/2023 20:56:24 - INFO - __main__ -   test: [batch: 16/391 ] | Loss: 0.592 | Acc: 84.559% (1840/2176)/ 96.737% (2105/2176)
01/15/2023 20:56:25 - INFO - __main__ -   test: [batch: 17/391 ] | Loss: 0.617 | Acc: 84.158% (1939/2304)/ 96.267% (2218/2304)
01/15/2023 20:56:27 - INFO - __main__ -   test: [batch: 18/391 ] | Loss: 0.640 | Acc: 83.635% (2034/2432)/ 96.135% (2338/2432)
01/15/2023 20:56:29 - INFO - __main__ -   test: [batch: 19/391 ] | Loss: 0.647 | Acc: 83.359% (2134/2560)/ 96.055% (2459/2560)
01/15/2023 20:56:30 - INFO - __main__ -   test: [batch: 20/391 ] | Loss: 0.645 | Acc: 83.482% (2244/2688)/ 95.945% (2579/2688)
01/15/2023 20:56:32 - INFO - __main__ -   test: [batch: 21/391 ] | Loss: 0.677 | Acc: 82.741% (2330/2816)/ 95.774% (2697/2816)
01/15/2023 20:56:33 - INFO - __main__ -   test: [batch: 22/391 ] | Loss: 0.676 | Acc: 82.609% (2432/2944)/ 95.788% (2820/2944)
01/15/2023 20:56:34 - INFO - __main__ -   test: [batch: 23/391 ] | Loss: 0.720 | Acc: 81.738% (2511/3072)/ 95.443% (2932/3072)
01/15/2023 20:56:36 - INFO - __main__ -   test: [batch: 24/391 ] | Loss: 0.740 | Acc: 81.188% (2598/3200)/ 95.250% (3048/3200)
01/15/2023 20:56:38 - INFO - __main__ -   test: [batch: 25/391 ] | Loss: 0.764 | Acc: 80.529% (2680/3328)/ 95.042% (3163/3328)
01/15/2023 20:56:39 - INFO - __main__ -   test: [batch: 26/391 ] | Loss: 0.777 | Acc: 79.832% (2759/3456)/ 95.023% (3284/3456)
01/15/2023 20:56:41 - INFO - __main__ -   test: [batch: 27/391 ] | Loss: 0.759 | Acc: 80.357% (2880/3584)/ 95.061% (3407/3584)
01/15/2023 20:56:43 - INFO - __main__ -   test: [batch: 28/391 ] | Loss: 0.768 | Acc: 79.822% (2963/3712)/ 95.124% (3531/3712)
01/15/2023 20:56:44 - INFO - __main__ -   test: [batch: 29/391 ] | Loss: 0.761 | Acc: 79.974% (3071/3840)/ 95.234% (3657/3840)
01/15/2023 20:56:46 - INFO - __main__ -   test: [batch: 30/391 ] | Loss: 0.774 | Acc: 79.940% (3172/3968)/ 95.111% (3774/3968)
01/15/2023 20:56:47 - INFO - __main__ -   test: [batch: 31/391 ] | Loss: 0.769 | Acc: 80.127% (3282/4096)/ 95.166% (3898/4096)
01/15/2023 20:56:49 - INFO - __main__ -   test: [batch: 32/391 ] | Loss: 0.754 | Acc: 80.445% (3398/4224)/ 95.265% (4024/4224)
01/15/2023 20:56:51 - INFO - __main__ -   test: [batch: 33/391 ] | Loss: 0.749 | Acc: 80.630% (3509/4352)/ 95.312% (4148/4352)
01/15/2023 20:56:52 - INFO - __main__ -   test: [batch: 34/391 ] | Loss: 0.735 | Acc: 81.004% (3629/4480)/ 95.357% (4272/4480)
01/15/2023 20:56:54 - INFO - __main__ -   test: [batch: 35/391 ] | Loss: 0.723 | Acc: 81.402% (3751/4608)/ 95.378% (4395/4608)
01/15/2023 20:56:55 - INFO - __main__ -   test: [batch: 36/391 ] | Loss: 0.708 | Acc: 81.841% (3876/4736)/ 95.481% (4522/4736)
01/15/2023 20:56:57 - INFO - __main__ -   test: [batch: 37/391 ] | Loss: 0.699 | Acc: 82.134% (3995/4864)/ 95.539% (4647/4864)
01/15/2023 20:56:58 - INFO - __main__ -   test: [batch: 38/391 ] | Loss: 0.691 | Acc: 82.252% (4106/4992)/ 95.593% (4772/4992)
01/15/2023 20:56:59 - INFO - __main__ -   test: [batch: 39/391 ] | Loss: 0.686 | Acc: 82.305% (4214/5120)/ 95.625% (4896/5120)
01/15/2023 20:57:01 - INFO - __main__ -   test: [batch: 40/391 ] | Loss: 0.684 | Acc: 82.412% (4325/5248)/ 95.541% (5014/5248)
01/15/2023 20:57:02 - INFO - __main__ -   test: [batch: 41/391 ] | Loss: 0.688 | Acc: 82.515% (4436/5376)/ 95.480% (5133/5376)
01/15/2023 20:57:04 - INFO - __main__ -   test: [batch: 42/391 ] | Loss: 0.688 | Acc: 82.485% (4540/5504)/ 95.531% (5258/5504)
01/15/2023 20:57:05 - INFO - __main__ -   test: [batch: 43/391 ] | Loss: 0.685 | Acc: 82.546% (4649/5632)/ 95.490% (5378/5632)
01/15/2023 20:57:07 - INFO - __main__ -   test: [batch: 44/391 ] | Loss: 0.689 | Acc: 82.552% (4755/5760)/ 95.417% (5496/5760)
01/15/2023 20:57:09 - INFO - __main__ -   test: [batch: 45/391 ] | Loss: 0.686 | Acc: 82.728% (4871/5888)/ 95.414% (5618/5888)
01/15/2023 20:57:10 - INFO - __main__ -   test: [batch: 46/391 ] | Loss: 0.689 | Acc: 82.680% (4974/6016)/ 95.445% (5742/6016)
01/15/2023 20:57:12 - INFO - __main__ -   test: [batch: 47/391 ] | Loss: 0.691 | Acc: 82.617% (5076/6144)/ 95.492% (5867/6144)
01/15/2023 20:57:13 - INFO - __main__ -   test: [batch: 48/391 ] | Loss: 0.695 | Acc: 82.462% (5172/6272)/ 95.456% (5987/6272)
01/15/2023 20:57:14 - INFO - __main__ -   test: [batch: 49/391 ] | Loss: 0.699 | Acc: 82.531% (5282/6400)/ 95.359% (6103/6400)
01/15/2023 20:57:16 - INFO - __main__ -   test: [batch: 50/391 ] | Loss: 0.691 | Acc: 82.721% (5400/6528)/ 95.404% (6228/6528)
01/15/2023 20:57:17 - INFO - __main__ -   test: [batch: 51/391 ] | Loss: 0.683 | Acc: 82.903% (5518/6656)/ 95.463% (6354/6656)
01/15/2023 20:57:19 - INFO - __main__ -   test: [batch: 52/391 ] | Loss: 0.681 | Acc: 82.975% (5629/6784)/ 95.489% (6478/6784)
01/15/2023 20:57:21 - INFO - __main__ -   test: [batch: 53/391 ] | Loss: 0.671 | Acc: 83.189% (5750/6912)/ 95.558% (6605/6912)
01/15/2023 20:57:22 - INFO - __main__ -   test: [batch: 54/391 ] | Loss: 0.665 | Acc: 83.338% (5867/7040)/ 95.597% (6730/7040)
01/15/2023 20:57:24 - INFO - __main__ -   test: [batch: 55/391 ] | Loss: 0.658 | Acc: 83.496% (5985/7168)/ 95.633% (6855/7168)
01/15/2023 20:57:25 - INFO - __main__ -   test: [batch: 56/391 ] | Loss: 0.650 | Acc: 83.703% (6107/7296)/ 95.683% (6981/7296)
01/15/2023 20:57:26 - INFO - __main__ -   test: [batch: 57/391 ] | Loss: 0.643 | Acc: 83.904% (6229/7424)/ 95.744% (7108/7424)
01/15/2023 20:57:28 - INFO - __main__ -   test: [batch: 58/391 ] | Loss: 0.642 | Acc: 83.951% (6340/7552)/ 95.736% (7230/7552)
01/15/2023 20:57:29 - INFO - __main__ -   test: [batch: 59/391 ] | Loss: 0.644 | Acc: 83.867% (6441/7680)/ 95.755% (7354/7680)
01/15/2023 20:57:31 - INFO - __main__ -   test: [batch: 60/391 ] | Loss: 0.648 | Acc: 83.760% (6540/7808)/ 95.722% (7474/7808)
01/15/2023 20:57:32 - INFO - __main__ -   test: [batch: 61/391 ] | Loss: 0.649 | Acc: 83.770% (6648/7936)/ 95.754% (7599/7936)
01/15/2023 20:57:34 - INFO - __main__ -   test: [batch: 62/391 ] | Loss: 0.648 | Acc: 83.743% (6753/8064)/ 95.759% (7722/8064)
01/15/2023 20:57:35 - INFO - __main__ -   test: [batch: 63/391 ] | Loss: 0.656 | Acc: 83.606% (6849/8192)/ 95.703% (7840/8192)
01/15/2023 20:57:37 - INFO - __main__ -   test: [batch: 64/391 ] | Loss: 0.664 | Acc: 83.413% (6940/8320)/ 95.661% (7959/8320)
01/15/2023 20:57:38 - INFO - __main__ -   test: [batch: 65/391 ] | Loss: 0.677 | Acc: 82.931% (7006/8448)/ 95.573% (8074/8448)
01/15/2023 20:57:40 - INFO - __main__ -   test: [batch: 66/391 ] | Loss: 0.683 | Acc: 82.917% (7111/8576)/ 95.534% (8193/8576)
01/15/2023 20:57:41 - INFO - __main__ -   test: [batch: 67/391 ] | Loss: 0.686 | Acc: 82.870% (7213/8704)/ 95.542% (8316/8704)
01/15/2023 20:57:42 - INFO - __main__ -   test: [batch: 68/391 ] | Loss: 0.686 | Acc: 82.835% (7316/8832)/ 95.562% (8440/8832)
01/15/2023 20:57:44 - INFO - __main__ -   test: [batch: 69/391 ] | Loss: 0.682 | Acc: 82.935% (7431/8960)/ 95.603% (8566/8960)
01/15/2023 20:57:46 - INFO - __main__ -   test: [batch: 70/391 ] | Loss: 0.683 | Acc: 82.857% (7530/9088)/ 95.599% (8688/9088)
01/15/2023 20:57:47 - INFO - __main__ -   test: [batch: 71/391 ] | Loss: 0.681 | Acc: 82.899% (7640/9216)/ 95.605% (8811/9216)
01/15/2023 20:57:49 - INFO - __main__ -   test: [batch: 72/391 ] | Loss: 0.684 | Acc: 82.791% (7736/9344)/ 95.623% (8935/9344)
01/15/2023 20:57:50 - INFO - __main__ -   test: [batch: 73/391 ] | Loss: 0.687 | Acc: 82.675% (7831/9472)/ 95.629% (9058/9472)
01/15/2023 20:57:52 - INFO - __main__ -   test: [batch: 74/391 ] | Loss: 0.688 | Acc: 82.646% (7934/9600)/ 95.615% (9179/9600)
01/15/2023 20:57:53 - INFO - __main__ -   test: [batch: 75/391 ] | Loss: 0.694 | Acc: 82.442% (8020/9728)/ 95.611% (9301/9728)
01/15/2023 20:57:54 - INFO - __main__ -   test: [batch: 76/391 ] | Loss: 0.694 | Acc: 82.437% (8125/9856)/ 95.607% (9423/9856)
01/15/2023 20:57:56 - INFO - __main__ -   test: [batch: 77/391 ] | Loss: 0.695 | Acc: 82.392% (8226/9984)/ 95.633% (9548/9984)
01/15/2023 20:57:58 - INFO - __main__ -   test: [batch: 78/391 ] | Loss: 0.695 | Acc: 82.288% (8321/10112)/ 95.669% (9674/10112)
01/15/2023 20:57:59 - INFO - __main__ -   test: [batch: 79/391 ] | Loss: 0.693 | Acc: 82.285% (8426/10240)/ 95.693% (9799/10240)
01/15/2023 20:58:01 - INFO - __main__ -   test: [batch: 80/391 ] | Loss: 0.693 | Acc: 82.243% (8527/10368)/ 95.689% (9921/10368)
01/15/2023 20:58:02 - INFO - __main__ -   test: [batch: 81/391 ] | Loss: 0.692 | Acc: 82.250% (8633/10496)/ 95.713% (10046/10496)
01/15/2023 20:58:04 - INFO - __main__ -   test: [batch: 82/391 ] | Loss: 0.692 | Acc: 82.267% (8740/10624)/ 95.698% (10167/10624)
01/15/2023 20:58:06 - INFO - __main__ -   test: [batch: 83/391 ] | Loss: 0.692 | Acc: 82.301% (8849/10752)/ 95.675% (10287/10752)
01/15/2023 20:58:07 - INFO - __main__ -   test: [batch: 84/391 ] | Loss: 0.688 | Acc: 82.408% (8966/10880)/ 95.708% (10413/10880)
01/15/2023 20:58:09 - INFO - __main__ -   test: [batch: 85/391 ] | Loss: 0.686 | Acc: 82.404% (9071/11008)/ 95.749% (10540/11008)
01/15/2023 20:58:10 - INFO - __main__ -   test: [batch: 86/391 ] | Loss: 0.691 | Acc: 82.319% (9167/11136)/ 95.726% (10660/11136)
01/15/2023 20:58:12 - INFO - __main__ -   test: [batch: 87/391 ] | Loss: 0.689 | Acc: 82.360% (9277/11264)/ 95.730% (10783/11264)
01/15/2023 20:58:13 - INFO - __main__ -   test: [batch: 88/391 ] | Loss: 0.696 | Acc: 82.286% (9374/11392)/ 95.672% (10899/11392)
01/15/2023 20:58:15 - INFO - __main__ -   test: [batch: 89/391 ] | Loss: 0.694 | Acc: 82.335% (9485/11520)/ 95.686% (11023/11520)
01/15/2023 20:58:16 - INFO - __main__ -   test: [batch: 90/391 ] | Loss: 0.695 | Acc: 82.246% (9580/11648)/ 95.690% (11146/11648)
01/15/2023 20:58:18 - INFO - __main__ -   test: [batch: 91/391 ] | Loss: 0.694 | Acc: 82.269% (9688/11776)/ 95.695% (11269/11776)
01/15/2023 20:58:20 - INFO - __main__ -   test: [batch: 92/391 ] | Loss: 0.695 | Acc: 82.250% (9791/11904)/ 95.682% (11390/11904)
01/15/2023 20:58:21 - INFO - __main__ -   test: [batch: 93/391 ] | Loss: 0.698 | Acc: 82.064% (9874/12032)/ 95.711% (11516/12032)
01/15/2023 20:58:23 - INFO - __main__ -   test: [batch: 94/391 ] | Loss: 0.701 | Acc: 81.924% (9962/12160)/ 95.724% (11640/12160)
01/15/2023 20:58:24 - INFO - __main__ -   test: [batch: 95/391 ] | Loss: 0.698 | Acc: 81.991% (10075/12288)/ 95.728% (11763/12288)
01/15/2023 20:58:26 - INFO - __main__ -   test: [batch: 96/391 ] | Loss: 0.702 | Acc: 81.910% (10170/12416)/ 95.747% (11888/12416)
01/15/2023 20:58:27 - INFO - __main__ -   test: [batch: 97/391 ] | Loss: 0.704 | Acc: 81.736% (10253/12544)/ 95.767% (12013/12544)
01/15/2023 20:58:29 - INFO - __main__ -   test: [batch: 98/391 ] | Loss: 0.700 | Acc: 81.842% (10371/12672)/ 95.802% (12140/12672)
01/15/2023 20:58:30 - INFO - __main__ -   test: [batch: 99/391 ] | Loss: 0.694 | Acc: 81.992% (10495/12800)/ 95.844% (12268/12800)
01/15/2023 20:58:32 - INFO - __main__ -   test: [batch: 100/391 ] | Loss: 0.693 | Acc: 82.047% (10607/12928)/ 95.846% (12391/12928)
01/15/2023 20:58:33 - INFO - __main__ -   test: [batch: 101/391 ] | Loss: 0.689 | Acc: 82.154% (10726/13056)/ 95.879% (12518/13056)
01/15/2023 20:58:35 - INFO - __main__ -   test: [batch: 102/391 ] | Loss: 0.685 | Acc: 82.274% (10847/13184)/ 95.904% (12644/13184)
01/15/2023 20:58:37 - INFO - __main__ -   test: [batch: 103/391 ] | Loss: 0.687 | Acc: 82.106% (10930/13312)/ 95.928% (12770/13312)
01/15/2023 20:58:38 - INFO - __main__ -   test: [batch: 104/391 ] | Loss: 0.686 | Acc: 82.054% (11028/13440)/ 95.938% (12894/13440)
01/15/2023 20:58:40 - INFO - __main__ -   test: [batch: 105/391 ] | Loss: 0.687 | Acc: 82.053% (11133/13568)/ 95.939% (13017/13568)
01/15/2023 20:58:41 - INFO - __main__ -   test: [batch: 106/391 ] | Loss: 0.695 | Acc: 81.936% (11222/13696)/ 95.875% (13131/13696)
01/15/2023 20:58:43 - INFO - __main__ -   test: [batch: 107/391 ] | Loss: 0.692 | Acc: 82.053% (11343/13824)/ 95.906% (13258/13824)
01/15/2023 20:58:44 - INFO - __main__ -   test: [batch: 108/391 ] | Loss: 0.695 | Acc: 81.902% (11427/13952)/ 95.915% (13382/13952)
01/15/2023 20:58:45 - INFO - __main__ -   test: [batch: 109/391 ] | Loss: 0.696 | Acc: 81.882% (11529/14080)/ 95.916% (13505/14080)
01/15/2023 20:58:47 - INFO - __main__ -   test: [batch: 110/391 ] | Loss: 0.699 | Acc: 81.700% (11608/14208)/ 95.932% (13630/14208)
01/15/2023 20:58:48 - INFO - __main__ -   test: [batch: 111/391 ] | Loss: 0.702 | Acc: 81.676% (11709/14336)/ 95.898% (13748/14336)
01/15/2023 20:58:50 - INFO - __main__ -   test: [batch: 112/391 ] | Loss: 0.702 | Acc: 81.692% (11816/14464)/ 95.914% (13873/14464)
01/15/2023 20:58:52 - INFO - __main__ -   test: [batch: 113/391 ] | Loss: 0.701 | Acc: 81.737% (11927/14592)/ 95.922% (13997/14592)
01/15/2023 20:58:53 - INFO - __main__ -   test: [batch: 114/391 ] | Loss: 0.698 | Acc: 81.827% (12045/14720)/ 95.951% (14124/14720)
01/15/2023 20:58:55 - INFO - __main__ -   test: [batch: 115/391 ] | Loss: 0.695 | Acc: 81.903% (12161/14848)/ 95.973% (14250/14848)
01/15/2023 20:58:56 - INFO - __main__ -   test: [batch: 116/391 ] | Loss: 0.698 | Acc: 81.871% (12261/14976)/ 95.940% (14368/14976)
01/15/2023 20:58:58 - INFO - __main__ -   test: [batch: 117/391 ] | Loss: 0.696 | Acc: 81.892% (12369/15104)/ 95.955% (14493/15104)
01/15/2023 20:59:00 - INFO - __main__ -   test: [batch: 118/391 ] | Loss: 0.700 | Acc: 81.788% (12458/15232)/ 95.969% (14618/15232)
01/15/2023 20:59:01 - INFO - __main__ -   test: [batch: 119/391 ] | Loss: 0.698 | Acc: 81.836% (12570/15360)/ 95.990% (14744/15360)
01/15/2023 20:59:03 - INFO - __main__ -   test: [batch: 120/391 ] | Loss: 0.696 | Acc: 81.883% (12682/15488)/ 96.003% (14869/15488)
01/15/2023 20:59:04 - INFO - __main__ -   test: [batch: 121/391 ] | Loss: 0.701 | Acc: 81.743% (12765/15616)/ 95.985% (14989/15616)
01/15/2023 20:59:06 - INFO - __main__ -   test: [batch: 122/391 ] | Loss: 0.703 | Acc: 81.695% (12862/15744)/ 95.954% (15107/15744)
01/15/2023 20:59:07 - INFO - __main__ -   test: [batch: 123/391 ] | Loss: 0.704 | Acc: 81.710% (12969/15872)/ 95.955% (15230/15872)
01/15/2023 20:59:09 - INFO - __main__ -   test: [batch: 124/391 ] | Loss: 0.703 | Acc: 81.719% (13075/16000)/ 95.969% (15355/16000)
01/15/2023 20:59:10 - INFO - __main__ -   test: [batch: 125/391 ] | Loss: 0.698 | Acc: 81.827% (13197/16128)/ 95.995% (15482/16128)
01/15/2023 20:59:12 - INFO - __main__ -   test: [batch: 126/391 ] | Loss: 0.695 | Acc: 81.914% (13316/16256)/ 96.008% (15607/16256)
01/15/2023 20:59:14 - INFO - __main__ -   test: [batch: 127/391 ] | Loss: 0.693 | Acc: 81.995% (13434/16384)/ 96.014% (15731/16384)
01/15/2023 20:59:15 - INFO - __main__ -   test: [batch: 128/391 ] | Loss: 0.694 | Acc: 81.971% (13535/16512)/ 95.991% (15850/16512)
01/15/2023 20:59:17 - INFO - __main__ -   test: [batch: 129/391 ] | Loss: 0.692 | Acc: 81.989% (13643/16640)/ 95.998% (15974/16640)
01/15/2023 20:59:18 - INFO - __main__ -   test: [batch: 130/391 ] | Loss: 0.689 | Acc: 82.091% (13765/16768)/ 96.016% (16100/16768)
01/15/2023 20:59:19 - INFO - __main__ -   test: [batch: 131/391 ] | Loss: 0.689 | Acc: 82.132% (13877/16896)/ 96.023% (16224/16896)
01/15/2023 20:59:21 - INFO - __main__ -   test: [batch: 132/391 ] | Loss: 0.686 | Acc: 82.196% (13993/17024)/ 96.035% (16349/17024)
01/15/2023 20:59:23 - INFO - __main__ -   test: [batch: 133/391 ] | Loss: 0.688 | Acc: 82.148% (14090/17152)/ 96.030% (16471/17152)
01/15/2023 20:59:24 - INFO - __main__ -   test: [batch: 134/391 ] | Loss: 0.686 | Acc: 82.182% (14201/17280)/ 96.053% (16598/17280)
01/15/2023 20:59:26 - INFO - __main__ -   test: [batch: 135/391 ] | Loss: 0.686 | Acc: 82.175% (14305/17408)/ 96.054% (16721/17408)
01/15/2023 20:59:27 - INFO - __main__ -   test: [batch: 136/391 ] | Loss: 0.684 | Acc: 82.134% (14403/17536)/ 96.077% (16848/17536)
01/15/2023 20:59:29 - INFO - __main__ -   test: [batch: 137/391 ] | Loss: 0.682 | Acc: 82.167% (14514/17664)/ 96.105% (16976/17664)
01/15/2023 20:59:31 - INFO - __main__ -   test: [batch: 138/391 ] | Loss: 0.681 | Acc: 82.200% (14625/17792)/ 96.105% (17099/17792)
01/15/2023 20:59:32 - INFO - __main__ -   test: [batch: 139/391 ] | Loss: 0.687 | Acc: 82.054% (14704/17920)/ 96.094% (17220/17920)
01/15/2023 20:59:34 - INFO - __main__ -   test: [batch: 140/391 ] | Loss: 0.689 | Acc: 81.981% (14796/18048)/ 96.088% (17342/18048)
01/15/2023 20:59:35 - INFO - __main__ -   test: [batch: 141/391 ] | Loss: 0.687 | Acc: 82.009% (14906/18176)/ 96.099% (17467/18176)
01/15/2023 20:59:37 - INFO - __main__ -   test: [batch: 142/391 ] | Loss: 0.685 | Acc: 82.086% (15025/18304)/ 96.116% (17593/18304)
01/15/2023 20:59:38 - INFO - __main__ -   test: [batch: 143/391 ] | Loss: 0.686 | Acc: 82.107% (15134/18432)/ 96.105% (17714/18432)
01/15/2023 20:59:39 - INFO - __main__ -   test: [batch: 144/391 ] | Loss: 0.688 | Acc: 82.069% (15232/18560)/ 96.072% (17831/18560)
01/15/2023 20:59:41 - INFO - __main__ -   test: [batch: 145/391 ] | Loss: 0.691 | Acc: 82.042% (15332/18688)/ 96.046% (17949/18688)
01/15/2023 20:59:43 - INFO - __main__ -   test: [batch: 146/391 ] | Loss: 0.692 | Acc: 82.015% (15432/18816)/ 96.019% (18067/18816)
01/15/2023 20:59:44 - INFO - __main__ -   test: [batch: 147/391 ] | Loss: 0.692 | Acc: 82.015% (15537/18944)/ 95.999% (18186/18944)
01/15/2023 20:59:46 - INFO - __main__ -   test: [batch: 148/391 ] | Loss: 0.694 | Acc: 81.953% (15630/19072)/ 95.994% (18308/19072)
01/15/2023 20:59:47 - INFO - __main__ -   test: [batch: 149/391 ] | Loss: 0.698 | Acc: 81.854% (15716/19200)/ 95.958% (18424/19200)
01/15/2023 20:59:49 - INFO - __main__ -   test: [batch: 150/391 ] | Loss: 0.698 | Acc: 81.793% (15809/19328)/ 95.964% (18548/19328)
01/15/2023 20:59:51 - INFO - __main__ -   test: [batch: 151/391 ] | Loss: 0.698 | Acc: 81.821% (15919/19456)/ 95.965% (18671/19456)
01/15/2023 20:59:52 - INFO - __main__ -   test: [batch: 152/391 ] | Loss: 0.700 | Acc: 81.807% (16021/19584)/ 95.956% (18792/19584)
01/15/2023 20:59:54 - INFO - __main__ -   test: [batch: 153/391 ] | Loss: 0.698 | Acc: 81.869% (16138/19712)/ 95.962% (18916/19712)
01/15/2023 20:59:55 - INFO - __main__ -   test: [batch: 154/391 ] | Loss: 0.698 | Acc: 81.850% (16239/19840)/ 95.938% (19034/19840)
01/15/2023 20:59:57 - INFO - __main__ -   test: [batch: 155/391 ] | Loss: 0.698 | Acc: 81.871% (16348/19968)/ 95.933% (19156/19968)
01/15/2023 20:59:59 - INFO - __main__ -   test: [batch: 156/391 ] | Loss: 0.701 | Acc: 81.782% (16435/20096)/ 95.920% (19276/20096)
01/15/2023 21:00:00 - INFO - __main__ -   test: [batch: 157/391 ] | Loss: 0.702 | Acc: 81.774% (16538/20224)/ 95.896% (19394/20224)
01/15/2023 21:00:02 - INFO - __main__ -   test: [batch: 158/391 ] | Loss: 0.703 | Acc: 81.741% (16636/20352)/ 95.873% (19512/20352)
01/15/2023 21:00:03 - INFO - __main__ -   test: [batch: 159/391 ] | Loss: 0.705 | Acc: 81.719% (16736/20480)/ 95.869% (19634/20480)
01/15/2023 21:00:05 - INFO - __main__ -   test: [batch: 160/391 ] | Loss: 0.705 | Acc: 81.706% (16838/20608)/ 95.856% (19754/20608)
01/15/2023 21:00:06 - INFO - __main__ -   test: [batch: 161/391 ] | Loss: 0.715 | Acc: 81.477% (16895/20736)/ 95.766% (19858/20736)
01/15/2023 21:00:08 - INFO - __main__ -   test: [batch: 162/391 ] | Loss: 0.720 | Acc: 81.370% (16977/20864)/ 95.686% (19964/20864)
01/15/2023 21:00:10 - INFO - __main__ -   test: [batch: 163/391 ] | Loss: 0.723 | Acc: 81.321% (17071/20992)/ 95.675% (20084/20992)
01/15/2023 21:00:11 - INFO - __main__ -   test: [batch: 164/391 ] | Loss: 0.722 | Acc: 81.307% (17172/21120)/ 95.691% (20210/21120)
01/15/2023 21:00:13 - INFO - __main__ -   test: [batch: 165/391 ] | Loss: 0.724 | Acc: 81.226% (17259/21248)/ 95.689% (20332/21248)
01/15/2023 21:00:14 - INFO - __main__ -   test: [batch: 166/391 ] | Loss: 0.724 | Acc: 81.236% (17365/21376)/ 95.673% (20451/21376)
01/15/2023 21:00:16 - INFO - __main__ -   test: [batch: 167/391 ] | Loss: 0.726 | Acc: 81.176% (17456/21504)/ 95.657% (20570/21504)
01/15/2023 21:00:18 - INFO - __main__ -   test: [batch: 168/391 ] | Loss: 0.726 | Acc: 81.176% (17560/21632)/ 95.650% (20691/21632)
01/15/2023 21:00:19 - INFO - __main__ -   test: [batch: 169/391 ] | Loss: 0.728 | Acc: 81.144% (17657/21760)/ 95.616% (20806/21760)
01/15/2023 21:00:21 - INFO - __main__ -   test: [batch: 170/391 ] | Loss: 0.732 | Acc: 81.031% (17736/21888)/ 95.577% (20920/21888)
01/15/2023 21:00:22 - INFO - __main__ -   test: [batch: 171/391 ] | Loss: 0.736 | Acc: 80.964% (17825/22016)/ 95.549% (21036/22016)
01/15/2023 21:00:24 - INFO - __main__ -   test: [batch: 172/391 ] | Loss: 0.737 | Acc: 80.916% (17918/22144)/ 95.547% (21158/22144)
01/15/2023 21:00:25 - INFO - __main__ -   test: [batch: 173/391 ] | Loss: 0.739 | Acc: 80.864% (18010/22272)/ 95.519% (21274/22272)
01/15/2023 21:00:27 - INFO - __main__ -   test: [batch: 174/391 ] | Loss: 0.743 | Acc: 80.772% (18093/22400)/ 95.469% (21385/22400)
01/15/2023 21:00:28 - INFO - __main__ -   test: [batch: 175/391 ] | Loss: 0.742 | Acc: 80.819% (18207/22528)/ 95.477% (21509/22528)
01/15/2023 21:00:30 - INFO - __main__ -   test: [batch: 176/391 ] | Loss: 0.742 | Acc: 80.817% (18310/22656)/ 95.463% (21628/22656)
01/15/2023 21:00:31 - INFO - __main__ -   test: [batch: 177/391 ] | Loss: 0.746 | Acc: 80.745% (18397/22784)/ 95.422% (21741/22784)
01/15/2023 21:00:32 - INFO - __main__ -   test: [batch: 178/391 ] | Loss: 0.747 | Acc: 80.735% (18498/22912)/ 95.400% (21858/22912)
01/15/2023 21:00:34 - INFO - __main__ -   test: [batch: 179/391 ] | Loss: 0.751 | Acc: 80.677% (18588/23040)/ 95.356% (21970/23040)
01/15/2023 21:00:35 - INFO - __main__ -   test: [batch: 180/391 ] | Loss: 0.757 | Acc: 80.559% (18664/23168)/ 95.317% (22083/23168)
01/15/2023 21:00:37 - INFO - __main__ -   test: [batch: 181/391 ] | Loss: 0.763 | Acc: 80.430% (18737/23296)/ 95.252% (22190/23296)
01/15/2023 21:00:38 - INFO - __main__ -   test: [batch: 182/391 ] | Loss: 0.762 | Acc: 80.443% (18843/23424)/ 95.253% (22312/23424)
01/15/2023 21:00:40 - INFO - __main__ -   test: [batch: 183/391 ] | Loss: 0.768 | Acc: 80.320% (18917/23552)/ 95.155% (22411/23552)
01/15/2023 21:00:41 - INFO - __main__ -   test: [batch: 184/391 ] | Loss: 0.768 | Acc: 80.342% (19025/23680)/ 95.144% (22530/23680)
01/15/2023 21:00:43 - INFO - __main__ -   test: [batch: 185/391 ] | Loss: 0.768 | Acc: 80.347% (19129/23808)/ 95.136% (22650/23808)
01/15/2023 21:00:44 - INFO - __main__ -   test: [batch: 186/391 ] | Loss: 0.771 | Acc: 80.306% (19222/23936)/ 95.095% (22762/23936)
01/15/2023 21:00:46 - INFO - __main__ -   test: [batch: 187/391 ] | Loss: 0.776 | Acc: 80.190% (19297/24064)/ 95.071% (22878/24064)
01/15/2023 21:00:47 - INFO - __main__ -   test: [batch: 188/391 ] | Loss: 0.779 | Acc: 80.064% (19369/24192)/ 95.052% (22995/24192)
01/15/2023 21:00:49 - INFO - __main__ -   test: [batch: 189/391 ] | Loss: 0.781 | Acc: 79.988% (19453/24320)/ 95.049% (23116/24320)
01/15/2023 21:00:50 - INFO - __main__ -   test: [batch: 190/391 ] | Loss: 0.785 | Acc: 79.900% (19534/24448)/ 95.022% (23231/24448)
01/15/2023 21:00:52 - INFO - __main__ -   test: [batch: 191/391 ] | Loss: 0.788 | Acc: 79.862% (19627/24576)/ 95.007% (23349/24576)
01/15/2023 21:00:53 - INFO - __main__ -   test: [batch: 192/391 ] | Loss: 0.794 | Acc: 79.752% (19702/24704)/ 94.924% (23450/24704)
01/15/2023 21:00:55 - INFO - __main__ -   test: [batch: 193/391 ] | Loss: 0.793 | Acc: 79.772% (19809/24832)/ 94.918% (23570/24832)
01/15/2023 21:00:57 - INFO - __main__ -   test: [batch: 194/391 ] | Loss: 0.796 | Acc: 79.708% (19895/24960)/ 94.900% (23687/24960)
01/15/2023 21:00:58 - INFO - __main__ -   test: [batch: 195/391 ] | Loss: 0.801 | Acc: 79.628% (19977/25088)/ 94.838% (23793/25088)
01/15/2023 21:01:00 - INFO - __main__ -   test: [batch: 196/391 ] | Loss: 0.805 | Acc: 79.525% (20053/25216)/ 94.797% (23904/25216)
01/15/2023 21:01:01 - INFO - __main__ -   test: [batch: 197/391 ] | Loss: 0.809 | Acc: 79.447% (20135/25344)/ 94.776% (24020/25344)
01/15/2023 21:01:03 - INFO - __main__ -   test: [batch: 198/391 ] | Loss: 0.811 | Acc: 79.409% (20227/25472)/ 94.747% (24134/25472)
01/15/2023 21:01:04 - INFO - __main__ -   test: [batch: 199/391 ] | Loss: 0.811 | Acc: 79.391% (20324/25600)/ 94.754% (24257/25600)
01/15/2023 21:01:06 - INFO - __main__ -   test: [batch: 200/391 ] | Loss: 0.811 | Acc: 79.353% (20416/25728)/ 94.753% (24378/25728)
01/15/2023 21:01:07 - INFO - __main__ -   test: [batch: 201/391 ] | Loss: 0.814 | Acc: 79.270% (20496/25856)/ 94.713% (24489/25856)
01/15/2023 21:01:09 - INFO - __main__ -   test: [batch: 202/391 ] | Loss: 0.815 | Acc: 79.253% (20593/25984)/ 94.701% (24607/25984)
01/15/2023 21:01:10 - INFO - __main__ -   test: [batch: 203/391 ] | Loss: 0.816 | Acc: 79.239% (20691/26112)/ 94.688% (24725/26112)
01/15/2023 21:01:12 - INFO - __main__ -   test: [batch: 204/391 ] | Loss: 0.819 | Acc: 79.150% (20769/26240)/ 94.668% (24841/26240)
01/15/2023 21:01:13 - INFO - __main__ -   test: [batch: 205/391 ] | Loss: 0.822 | Acc: 79.069% (20849/26368)/ 94.634% (24953/26368)
01/15/2023 21:01:15 - INFO - __main__ -   test: [batch: 206/391 ] | Loss: 0.823 | Acc: 79.053% (20946/26496)/ 94.629% (25073/26496)
01/15/2023 21:01:17 - INFO - __main__ -   test: [batch: 207/391 ] | Loss: 0.827 | Acc: 78.963% (21023/26624)/ 94.591% (25184/26624)
01/15/2023 21:01:18 - INFO - __main__ -   test: [batch: 208/391 ] | Loss: 0.829 | Acc: 78.925% (21114/26752)/ 94.565% (25298/26752)
01/15/2023 21:01:20 - INFO - __main__ -   test: [batch: 209/391 ] | Loss: 0.829 | Acc: 78.921% (21214/26880)/ 94.580% (25423/26880)
01/15/2023 21:01:22 - INFO - __main__ -   test: [batch: 210/391 ] | Loss: 0.830 | Acc: 78.910% (21312/27008)/ 94.561% (25539/27008)
01/15/2023 21:01:23 - INFO - __main__ -   test: [batch: 211/391 ] | Loss: 0.832 | Acc: 78.847% (21396/27136)/ 94.535% (25653/27136)
01/15/2023 21:01:25 - INFO - __main__ -   test: [batch: 212/391 ] | Loss: 0.834 | Acc: 78.789% (21481/27264)/ 94.513% (25768/27264)
01/15/2023 21:01:26 - INFO - __main__ -   test: [batch: 213/391 ] | Loss: 0.834 | Acc: 78.786% (21581/27392)/ 94.520% (25891/27392)
01/15/2023 21:01:28 - INFO - __main__ -   test: [batch: 214/391 ] | Loss: 0.834 | Acc: 78.761% (21675/27520)/ 94.517% (26011/27520)
01/15/2023 21:01:29 - INFO - __main__ -   test: [batch: 215/391 ] | Loss: 0.834 | Acc: 78.776% (21780/27648)/ 94.510% (26130/27648)
01/15/2023 21:01:31 - INFO - __main__ -   test: [batch: 216/391 ] | Loss: 0.832 | Acc: 78.816% (21892/27776)/ 94.528% (26256/27776)
01/15/2023 21:01:33 - INFO - __main__ -   test: [batch: 217/391 ] | Loss: 0.837 | Acc: 78.749% (21974/27904)/ 94.467% (26360/27904)
01/15/2023 21:01:34 - INFO - __main__ -   test: [batch: 218/391 ] | Loss: 0.840 | Acc: 78.699% (22061/28032)/ 94.428% (26470/28032)
01/15/2023 21:01:36 - INFO - __main__ -   test: [batch: 219/391 ] | Loss: 0.839 | Acc: 78.732% (22171/28160)/ 94.432% (26592/28160)
01/15/2023 21:01:38 - INFO - __main__ -   test: [batch: 220/391 ] | Loss: 0.837 | Acc: 78.768% (22282/28288)/ 94.446% (26717/28288)
01/15/2023 21:01:39 - INFO - __main__ -   test: [batch: 221/391 ] | Loss: 0.839 | Acc: 78.720% (22369/28416)/ 94.429% (26833/28416)
01/15/2023 21:01:41 - INFO - __main__ -   test: [batch: 222/391 ] | Loss: 0.837 | Acc: 78.770% (22484/28544)/ 94.444% (26958/28544)
01/15/2023 21:01:42 - INFO - __main__ -   test: [batch: 223/391 ] | Loss: 0.837 | Acc: 78.767% (22584/28672)/ 94.434% (27076/28672)
01/15/2023 21:01:44 - INFO - __main__ -   test: [batch: 224/391 ] | Loss: 0.837 | Acc: 78.792% (22692/28800)/ 94.427% (27195/28800)
01/15/2023 21:01:45 - INFO - __main__ -   test: [batch: 225/391 ] | Loss: 0.836 | Acc: 78.796% (22794/28928)/ 94.431% (27317/28928)
01/15/2023 21:01:47 - INFO - __main__ -   test: [batch: 226/391 ] | Loss: 0.836 | Acc: 78.772% (22888/29056)/ 94.442% (27441/29056)
01/15/2023 21:01:48 - INFO - __main__ -   test: [batch: 227/391 ] | Loss: 0.838 | Acc: 78.749% (22982/29184)/ 94.432% (27559/29184)
01/15/2023 21:01:50 - INFO - __main__ -   test: [batch: 228/391 ] | Loss: 0.843 | Acc: 78.667% (23059/29312)/ 94.371% (27662/29312)
01/15/2023 21:01:52 - INFO - __main__ -   test: [batch: 229/391 ] | Loss: 0.846 | Acc: 78.604% (23141/29440)/ 94.324% (27769/29440)
01/15/2023 21:01:53 - INFO - __main__ -   test: [batch: 230/391 ] | Loss: 0.849 | Acc: 78.544% (23224/29568)/ 94.288% (27879/29568)
01/15/2023 21:01:55 - INFO - __main__ -   test: [batch: 231/391 ] | Loss: 0.849 | Acc: 78.529% (23320/29696)/ 94.272% (27995/29696)
01/15/2023 21:01:56 - INFO - __main__ -   test: [batch: 232/391 ] | Loss: 0.848 | Acc: 78.554% (23428/29824)/ 94.286% (28120/29824)
01/15/2023 21:01:58 - INFO - __main__ -   test: [batch: 233/391 ] | Loss: 0.850 | Acc: 78.509% (23515/29952)/ 94.271% (28236/29952)
01/15/2023 21:01:59 - INFO - __main__ -   test: [batch: 234/391 ] | Loss: 0.856 | Acc: 78.394% (23581/30080)/ 94.199% (28335/30080)
01/15/2023 21:02:01 - INFO - __main__ -   test: [batch: 235/391 ] | Loss: 0.856 | Acc: 78.383% (23678/30208)/ 94.190% (28453/30208)
01/15/2023 21:02:03 - INFO - __main__ -   test: [batch: 236/391 ] | Loss: 0.855 | Acc: 78.422% (23790/30336)/ 94.198% (28576/30336)
01/15/2023 21:02:04 - INFO - __main__ -   test: [batch: 237/391 ] | Loss: 0.855 | Acc: 78.427% (23892/30464)/ 94.164% (28686/30464)
01/15/2023 21:02:05 - INFO - __main__ -   test: [batch: 238/391 ] | Loss: 0.854 | Acc: 78.468% (24005/30592)/ 94.172% (28809/30592)
01/15/2023 21:02:07 - INFO - __main__ -   test: [batch: 239/391 ] | Loss: 0.853 | Acc: 78.503% (24116/30720)/ 94.173% (28930/30720)
01/15/2023 21:02:08 - INFO - __main__ -   test: [batch: 240/391 ] | Loss: 0.853 | Acc: 78.520% (24222/30848)/ 94.162% (29047/30848)
01/15/2023 21:02:10 - INFO - __main__ -   test: [batch: 241/391 ] | Loss: 0.856 | Acc: 78.428% (24294/30976)/ 94.121% (29155/30976)
01/15/2023 21:02:11 - INFO - __main__ -   test: [batch: 242/391 ] | Loss: 0.858 | Acc: 78.321% (24361/31104)/ 94.104% (29270/31104)
01/15/2023 21:02:13 - INFO - __main__ -   test: [batch: 243/391 ] | Loss: 0.864 | Acc: 78.195% (24422/31232)/ 94.038% (29370/31232)
01/15/2023 21:02:14 - INFO - __main__ -   test: [batch: 244/391 ] | Loss: 0.864 | Acc: 78.205% (24525/31360)/ 94.040% (29491/31360)
01/15/2023 21:02:16 - INFO - __main__ -   test: [batch: 245/391 ] | Loss: 0.864 | Acc: 78.220% (24630/31488)/ 94.033% (29609/31488)
01/15/2023 21:02:17 - INFO - __main__ -   test: [batch: 246/391 ] | Loss: 0.864 | Acc: 78.198% (24723/31616)/ 94.025% (29727/31616)
01/15/2023 21:02:19 - INFO - __main__ -   test: [batch: 247/391 ] | Loss: 0.870 | Acc: 78.106% (24794/31744)/ 93.958% (29826/31744)
01/15/2023 21:02:20 - INFO - __main__ -   test: [batch: 248/391 ] | Loss: 0.871 | Acc: 78.062% (24880/31872)/ 93.954% (29945/31872)
01/15/2023 21:02:22 - INFO - __main__ -   test: [batch: 249/391 ] | Loss: 0.873 | Acc: 77.928% (24937/32000)/ 93.947% (30063/32000)
01/15/2023 21:02:24 - INFO - __main__ -   test: [batch: 250/391 ] | Loss: 0.871 | Acc: 77.963% (25048/32128)/ 93.965% (30189/32128)
01/15/2023 21:02:25 - INFO - __main__ -   test: [batch: 251/391 ] | Loss: 0.874 | Acc: 77.914% (25132/32256)/ 93.942% (30302/32256)
01/15/2023 21:02:27 - INFO - __main__ -   test: [batch: 252/391 ] | Loss: 0.873 | Acc: 77.943% (25241/32384)/ 93.935% (30420/32384)
01/15/2023 21:02:28 - INFO - __main__ -   test: [batch: 253/391 ] | Loss: 0.874 | Acc: 77.913% (25331/32512)/ 93.922% (30536/32512)
01/15/2023 21:02:30 - INFO - __main__ -   test: [batch: 254/391 ] | Loss: 0.878 | Acc: 77.849% (25410/32640)/ 93.873% (30640/32640)
01/15/2023 21:02:31 - INFO - __main__ -   test: [batch: 255/391 ] | Loss: 0.880 | Acc: 77.817% (25499/32768)/ 93.860% (30756/32768)
01/15/2023 21:02:32 - INFO - __main__ -   test: [batch: 256/391 ] | Loss: 0.886 | Acc: 77.699% (25560/32896)/ 93.823% (30864/32896)
01/15/2023 21:02:34 - INFO - __main__ -   test: [batch: 257/391 ] | Loss: 0.886 | Acc: 77.686% (25655/33024)/ 93.811% (30980/33024)
01/15/2023 21:02:36 - INFO - __main__ -   test: [batch: 258/391 ] | Loss: 0.887 | Acc: 77.679% (25752/33152)/ 93.795% (31095/33152)
01/15/2023 21:02:37 - INFO - __main__ -   test: [batch: 259/391 ] | Loss: 0.891 | Acc: 77.554% (25810/33280)/ 93.786% (31212/33280)
01/15/2023 21:02:39 - INFO - __main__ -   test: [batch: 260/391 ] | Loss: 0.891 | Acc: 77.526% (25900/33408)/ 93.780% (31330/33408)
01/15/2023 21:02:40 - INFO - __main__ -   test: [batch: 261/391 ] | Loss: 0.889 | Acc: 77.573% (26015/33536)/ 93.801% (31457/33536)
01/15/2023 21:02:41 - INFO - __main__ -   test: [batch: 262/391 ] | Loss: 0.888 | Acc: 77.593% (26121/33664)/ 93.806% (31579/33664)
01/15/2023 21:02:43 - INFO - __main__ -   test: [batch: 263/391 ] | Loss: 0.891 | Acc: 77.518% (26195/33792)/ 93.780% (31690/33792)
01/15/2023 21:02:45 - INFO - __main__ -   test: [batch: 264/391 ] | Loss: 0.897 | Acc: 77.450% (26271/33920)/ 93.723% (31791/33920)
01/15/2023 21:02:46 - INFO - __main__ -   test: [batch: 265/391 ] | Loss: 0.896 | Acc: 77.479% (26380/34048)/ 93.724% (31911/34048)
01/15/2023 21:02:48 - INFO - __main__ -   test: [batch: 266/391 ] | Loss: 0.898 | Acc: 77.411% (26456/34176)/ 93.709% (32026/34176)
01/15/2023 21:02:49 - INFO - __main__ -   test: [batch: 267/391 ] | Loss: 0.897 | Acc: 77.443% (26566/34304)/ 93.709% (32146/34304)
01/15/2023 21:02:51 - INFO - __main__ -   test: [batch: 268/391 ] | Loss: 0.897 | Acc: 77.463% (26672/34432)/ 93.704% (32264/34432)
01/15/2023 21:02:52 - INFO - __main__ -   test: [batch: 269/391 ] | Loss: 0.899 | Acc: 77.413% (26754/34560)/ 93.683% (32377/34560)
01/15/2023 21:02:54 - INFO - __main__ -   test: [batch: 270/391 ] | Loss: 0.902 | Acc: 77.350% (26831/34688)/ 93.649% (32485/34688)
01/15/2023 21:02:55 - INFO - __main__ -   test: [batch: 271/391 ] | Loss: 0.902 | Acc: 77.358% (26933/34816)/ 93.644% (32603/34816)
01/15/2023 21:02:56 - INFO - __main__ -   test: [batch: 272/391 ] | Loss: 0.903 | Acc: 77.321% (27019/34944)/ 93.638% (32721/34944)
01/15/2023 21:02:58 - INFO - __main__ -   test: [batch: 273/391 ] | Loss: 0.904 | Acc: 77.321% (27118/35072)/ 93.625% (32836/35072)
01/15/2023 21:02:59 - INFO - __main__ -   test: [batch: 274/391 ] | Loss: 0.903 | Acc: 77.321% (27217/35200)/ 93.631% (32958/35200)
01/15/2023 21:03:01 - INFO - __main__ -   test: [batch: 275/391 ] | Loss: 0.905 | Acc: 77.284% (27303/35328)/ 93.625% (33076/35328)
01/15/2023 21:03:02 - INFO - __main__ -   test: [batch: 276/391 ] | Loss: 0.906 | Acc: 77.237% (27385/35456)/ 93.626% (33196/35456)
01/15/2023 21:03:04 - INFO - __main__ -   test: [batch: 277/391 ] | Loss: 0.907 | Acc: 77.212% (27475/35584)/ 93.618% (33313/35584)
01/15/2023 21:03:06 - INFO - __main__ -   test: [batch: 278/391 ] | Loss: 0.907 | Acc: 77.218% (27576/35712)/ 93.616% (33432/35712)
01/15/2023 21:03:07 - INFO - __main__ -   test: [batch: 279/391 ] | Loss: 0.907 | Acc: 77.227% (27678/35840)/ 93.605% (33548/35840)
01/15/2023 21:03:09 - INFO - __main__ -   test: [batch: 280/391 ] | Loss: 0.908 | Acc: 77.208% (27770/35968)/ 93.600% (33666/35968)
01/15/2023 21:03:10 - INFO - __main__ -   test: [batch: 281/391 ] | Loss: 0.908 | Acc: 77.222% (27874/36096)/ 93.595% (33784/36096)
01/15/2023 21:03:12 - INFO - __main__ -   test: [batch: 282/391 ] | Loss: 0.907 | Acc: 77.258% (27986/36224)/ 93.598% (33905/36224)
01/15/2023 21:03:14 - INFO - __main__ -   test: [batch: 283/391 ] | Loss: 0.908 | Acc: 77.239% (28078/36352)/ 93.593% (34023/36352)
01/15/2023 21:03:15 - INFO - __main__ -   test: [batch: 284/391 ] | Loss: 0.911 | Acc: 77.185% (28157/36480)/ 93.561% (34131/36480)
01/15/2023 21:03:17 - INFO - __main__ -   test: [batch: 285/391 ] | Loss: 0.914 | Acc: 77.144% (28241/36608)/ 93.512% (34233/36608)
01/15/2023 21:03:18 - INFO - __main__ -   test: [batch: 286/391 ] | Loss: 0.915 | Acc: 77.112% (28328/36736)/ 93.502% (34349/36736)
01/15/2023 21:03:20 - INFO - __main__ -   test: [batch: 287/391 ] | Loss: 0.915 | Acc: 77.110% (28426/36864)/ 93.495% (34466/36864)
01/15/2023 21:03:21 - INFO - __main__ -   test: [batch: 288/391 ] | Loss: 0.914 | Acc: 77.130% (28532/36992)/ 93.499% (34587/36992)
01/15/2023 21:03:23 - INFO - __main__ -   test: [batch: 289/391 ] | Loss: 0.916 | Acc: 77.093% (28617/37120)/ 93.462% (34693/37120)
01/15/2023 21:03:24 - INFO - __main__ -   test: [batch: 290/391 ] | Loss: 0.917 | Acc: 77.019% (28688/37248)/ 93.463% (34813/37248)
01/15/2023 21:03:26 - INFO - __main__ -   test: [batch: 291/391 ] | Loss: 0.917 | Acc: 77.031% (28791/37376)/ 93.456% (34930/37376)
01/15/2023 21:03:27 - INFO - __main__ -   test: [batch: 292/391 ] | Loss: 0.919 | Acc: 76.978% (28870/37504)/ 93.438% (35043/37504)
01/15/2023 21:03:27 - INFO - __main__ -   test: [batch: 293/391 ] | Loss: 0.919 | Acc: 76.972% (28966/37632)/ 93.436% (35162/37632)
01/15/2023 21:03:28 - INFO - __main__ -   test: [batch: 294/391 ] | Loss: 0.920 | Acc: 76.949% (29056/37760)/ 93.432% (35280/37760)
01/15/2023 21:03:29 - INFO - __main__ -   test: [batch: 295/391 ] | Loss: 0.919 | Acc: 76.980% (29166/37888)/ 93.436% (35401/37888)
01/15/2023 21:03:30 - INFO - __main__ -   test: [batch: 296/391 ] | Loss: 0.920 | Acc: 76.975% (29263/38016)/ 93.426% (35517/38016)
01/15/2023 21:03:31 - INFO - __main__ -   test: [batch: 297/391 ] | Loss: 0.922 | Acc: 76.958% (29355/38144)/ 93.404% (35628/38144)
01/15/2023 21:03:32 - INFO - __main__ -   test: [batch: 298/391 ] | Loss: 0.924 | Acc: 76.928% (29442/38272)/ 93.382% (35739/38272)
01/15/2023 21:03:33 - INFO - __main__ -   test: [batch: 299/391 ] | Loss: 0.924 | Acc: 76.922% (29538/38400)/ 93.359% (35850/38400)
01/15/2023 21:03:34 - INFO - __main__ -   test: [batch: 300/391 ] | Loss: 0.925 | Acc: 76.926% (29638/38528)/ 93.353% (35967/38528)
01/15/2023 21:03:35 - INFO - __main__ -   test: [batch: 301/391 ] | Loss: 0.925 | Acc: 76.917% (29733/38656)/ 93.339% (36081/38656)
01/15/2023 21:03:36 - INFO - __main__ -   test: [batch: 302/391 ] | Loss: 0.927 | Acc: 76.877% (29816/38784)/ 93.325% (36195/38784)
01/15/2023 21:03:37 - INFO - __main__ -   test: [batch: 303/391 ] | Loss: 0.928 | Acc: 76.855% (29906/38912)/ 93.313% (36310/38912)
01/15/2023 21:03:38 - INFO - __main__ -   test: [batch: 304/391 ] | Loss: 0.927 | Acc: 76.872% (30011/39040)/ 93.317% (36431/39040)
01/15/2023 21:03:39 - INFO - __main__ -   test: [batch: 305/391 ] | Loss: 0.928 | Acc: 76.828% (30092/39168)/ 93.308% (36547/39168)
01/15/2023 21:03:40 - INFO - __main__ -   test: [batch: 306/391 ] | Loss: 0.929 | Acc: 76.812% (30184/39296)/ 93.284% (36657/39296)
01/15/2023 21:03:41 - INFO - __main__ -   test: [batch: 307/391 ] | Loss: 0.930 | Acc: 76.816% (30284/39424)/ 93.268% (36770/39424)
01/15/2023 21:03:42 - INFO - __main__ -   test: [batch: 308/391 ] | Loss: 0.930 | Acc: 76.803% (30377/39552)/ 93.262% (36887/39552)
01/15/2023 21:03:43 - INFO - __main__ -   test: [batch: 309/391 ] | Loss: 0.932 | Acc: 76.799% (30474/39680)/ 93.238% (36997/39680)
01/15/2023 21:03:44 - INFO - __main__ -   test: [batch: 310/391 ] | Loss: 0.932 | Acc: 76.789% (30568/39808)/ 93.227% (37112/39808)
01/15/2023 21:03:44 - INFO - __main__ -   test: [batch: 311/391 ] | Loss: 0.934 | Acc: 76.758% (30654/39936)/ 93.214% (37226/39936)
01/15/2023 21:03:45 - INFO - __main__ -   test: [batch: 312/391 ] | Loss: 0.935 | Acc: 76.745% (30747/40064)/ 93.191% (37336/40064)
01/15/2023 21:03:46 - INFO - __main__ -   test: [batch: 313/391 ] | Loss: 0.933 | Acc: 76.784% (30861/40192)/ 93.210% (37463/40192)
01/15/2023 21:03:47 - INFO - __main__ -   test: [batch: 314/391 ] | Loss: 0.934 | Acc: 76.783% (30959/40320)/ 93.207% (37581/40320)
01/15/2023 21:03:48 - INFO - __main__ -   test: [batch: 315/391 ] | Loss: 0.935 | Acc: 76.765% (31050/40448)/ 93.194% (37695/40448)
01/15/2023 21:03:49 - INFO - __main__ -   test: [batch: 316/391 ] | Loss: 0.937 | Acc: 76.673% (31111/40576)/ 93.168% (37804/40576)
01/15/2023 21:03:50 - INFO - __main__ -   test: [batch: 317/391 ] | Loss: 0.939 | Acc: 76.629% (31191/40704)/ 93.146% (37914/40704)
01/15/2023 21:03:51 - INFO - __main__ -   test: [batch: 318/391 ] | Loss: 0.938 | Acc: 76.658% (31301/40832)/ 93.162% (38040/40832)
01/15/2023 21:03:52 - INFO - __main__ -   test: [batch: 319/391 ] | Loss: 0.940 | Acc: 76.604% (31377/40960)/ 93.140% (38150/40960)
01/15/2023 21:03:53 - INFO - __main__ -   test: [batch: 320/391 ] | Loss: 0.939 | Acc: 76.640% (31490/41088)/ 93.149% (38273/41088)
01/15/2023 21:03:54 - INFO - __main__ -   test: [batch: 321/391 ] | Loss: 0.939 | Acc: 76.657% (31595/41216)/ 93.141% (38389/41216)
01/15/2023 21:03:55 - INFO - __main__ -   test: [batch: 322/391 ] | Loss: 0.940 | Acc: 76.630% (31682/41344)/ 93.136% (38506/41344)
01/15/2023 21:03:56 - INFO - __main__ -   test: [batch: 323/391 ] | Loss: 0.943 | Acc: 76.591% (31764/41472)/ 93.111% (38615/41472)
01/15/2023 21:03:57 - INFO - __main__ -   test: [batch: 324/391 ] | Loss: 0.943 | Acc: 76.594% (31863/41600)/ 93.103% (38731/41600)
01/15/2023 21:03:58 - INFO - __main__ -   test: [batch: 325/391 ] | Loss: 0.942 | Acc: 76.601% (31964/41728)/ 93.103% (38850/41728)
01/15/2023 21:03:59 - INFO - __main__ -   test: [batch: 326/391 ] | Loss: 0.946 | Acc: 76.517% (32027/41856)/ 93.074% (38957/41856)
01/15/2023 21:04:00 - INFO - __main__ -   test: [batch: 327/391 ] | Loss: 0.949 | Acc: 76.441% (32093/41984)/ 93.043% (39063/41984)
01/15/2023 21:04:01 - INFO - __main__ -   test: [batch: 328/391 ] | Loss: 0.951 | Acc: 76.392% (32170/42112)/ 93.023% (39174/42112)
01/15/2023 21:04:02 - INFO - __main__ -   test: [batch: 329/391 ] | Loss: 0.951 | Acc: 76.387% (32266/42240)/ 93.028% (39295/42240)
01/15/2023 21:04:03 - INFO - __main__ -   test: [batch: 330/391 ] | Loss: 0.952 | Acc: 76.348% (32347/42368)/ 93.007% (39405/42368)
01/15/2023 21:04:04 - INFO - __main__ -   test: [batch: 331/391 ] | Loss: 0.953 | Acc: 76.315% (32431/42496)/ 93.018% (39529/42496)
01/15/2023 21:04:05 - INFO - __main__ -   test: [batch: 332/391 ] | Loss: 0.953 | Acc: 76.309% (32526/42624)/ 93.018% (39648/42624)
01/15/2023 21:04:06 - INFO - __main__ -   test: [batch: 333/391 ] | Loss: 0.952 | Acc: 76.338% (32636/42752)/ 93.027% (39771/42752)
01/15/2023 21:04:07 - INFO - __main__ -   test: [batch: 334/391 ] | Loss: 0.953 | Acc: 76.308% (32721/42880)/ 93.008% (39882/42880)
01/15/2023 21:04:08 - INFO - __main__ -   test: [batch: 335/391 ] | Loss: 0.954 | Acc: 76.283% (32808/43008)/ 92.997% (39996/43008)
01/15/2023 21:04:08 - INFO - __main__ -   test: [batch: 336/391 ] | Loss: 0.956 | Acc: 76.233% (32884/43136)/ 92.983% (40109/43136)
01/15/2023 21:04:09 - INFO - __main__ -   test: [batch: 337/391 ] | Loss: 0.956 | Acc: 76.225% (32978/43264)/ 92.978% (40226/43264)
01/15/2023 21:04:10 - INFO - __main__ -   test: [batch: 338/391 ] | Loss: 0.956 | Acc: 76.208% (33068/43392)/ 92.985% (40348/43392)
01/15/2023 21:04:11 - INFO - __main__ -   test: [batch: 339/391 ] | Loss: 0.958 | Acc: 76.170% (33149/43520)/ 92.962% (40457/43520)
01/15/2023 21:04:12 - INFO - __main__ -   test: [batch: 340/391 ] | Loss: 0.958 | Acc: 76.164% (33244/43648)/ 92.973% (40581/43648)
01/15/2023 21:04:12 - INFO - __main__ -   test: [batch: 341/391 ] | Loss: 0.957 | Acc: 76.204% (33359/43776)/ 92.992% (40708/43776)
01/15/2023 21:04:13 - INFO - __main__ -   test: [batch: 342/391 ] | Loss: 0.957 | Acc: 76.153% (33434/43904)/ 92.982% (40823/43904)
01/15/2023 21:04:14 - INFO - __main__ -   test: [batch: 343/391 ] | Loss: 0.957 | Acc: 76.151% (33531/44032)/ 92.985% (40943/44032)
01/15/2023 21:04:15 - INFO - __main__ -   test: [batch: 344/391 ] | Loss: 0.957 | Acc: 76.150% (33628/44160)/ 92.976% (41058/44160)
01/15/2023 21:04:16 - INFO - __main__ -   test: [batch: 345/391 ] | Loss: 0.961 | Acc: 76.088% (33698/44288)/ 92.935% (41159/44288)
01/15/2023 21:04:17 - INFO - __main__ -   test: [batch: 346/391 ] | Loss: 0.963 | Acc: 76.069% (33787/44416)/ 92.924% (41273/44416)
01/15/2023 21:04:18 - INFO - __main__ -   test: [batch: 347/391 ] | Loss: 0.962 | Acc: 76.084% (33891/44544)/ 92.935% (41397/44544)
01/15/2023 21:04:19 - INFO - __main__ -   test: [batch: 348/391 ] | Loss: 0.964 | Acc: 76.063% (33979/44672)/ 92.913% (41506/44672)
01/15/2023 21:04:20 - INFO - __main__ -   test: [batch: 349/391 ] | Loss: 0.963 | Acc: 76.062% (34076/44800)/ 92.917% (41627/44800)
01/15/2023 21:04:20 - INFO - __main__ -   test: [batch: 350/391 ] | Loss: 0.963 | Acc: 76.080% (34181/44928)/ 92.918% (41746/44928)
01/15/2023 21:04:21 - INFO - __main__ -   test: [batch: 351/391 ] | Loss: 0.965 | Acc: 76.028% (34255/45056)/ 92.904% (41859/45056)
01/15/2023 21:04:22 - INFO - __main__ -   test: [batch: 352/391 ] | Loss: 0.965 | Acc: 76.022% (34350/45184)/ 92.900% (41976/45184)
01/15/2023 21:04:23 - INFO - __main__ -   test: [batch: 353/391 ] | Loss: 0.968 | Acc: 75.980% (34428/45312)/ 92.861% (42077/45312)
01/15/2023 21:04:24 - INFO - __main__ -   test: [batch: 354/391 ] | Loss: 0.970 | Acc: 75.922% (34499/45440)/ 92.841% (42187/45440)
01/15/2023 21:04:25 - INFO - __main__ -   test: [batch: 355/391 ] | Loss: 0.973 | Acc: 75.858% (34567/45568)/ 92.833% (42302/45568)
01/15/2023 21:04:25 - INFO - __main__ -   test: [batch: 356/391 ] | Loss: 0.973 | Acc: 75.860% (34665/45696)/ 92.833% (42421/45696)
01/15/2023 21:04:26 - INFO - __main__ -   test: [batch: 357/391 ] | Loss: 0.972 | Acc: 75.897% (34779/45824)/ 92.847% (42546/45824)
01/15/2023 21:04:27 - INFO - __main__ -   test: [batch: 358/391 ] | Loss: 0.971 | Acc: 75.925% (34889/45952)/ 92.849% (42666/45952)
01/15/2023 21:04:27 - INFO - __main__ -   test: [batch: 359/391 ] | Loss: 0.971 | Acc: 75.935% (34991/46080)/ 92.845% (42783/46080)
01/15/2023 21:04:28 - INFO - __main__ -   test: [batch: 360/391 ] | Loss: 0.973 | Acc: 75.905% (35074/46208)/ 92.835% (42897/46208)
01/15/2023 21:04:29 - INFO - __main__ -   test: [batch: 361/391 ] | Loss: 0.973 | Acc: 75.900% (35169/46336)/ 92.841% (43019/46336)
01/15/2023 21:04:29 - INFO - __main__ -   test: [batch: 362/391 ] | Loss: 0.973 | Acc: 75.897% (35265/46464)/ 92.855% (43144/46464)
01/15/2023 21:04:30 - INFO - __main__ -   test: [batch: 363/391 ] | Loss: 0.973 | Acc: 75.891% (35359/46592)/ 92.846% (43259/46592)
01/15/2023 21:04:31 - INFO - __main__ -   test: [batch: 364/391 ] | Loss: 0.972 | Acc: 75.916% (35468/46720)/ 92.855% (43382/46720)
01/15/2023 21:04:31 - INFO - __main__ -   test: [batch: 365/391 ] | Loss: 0.971 | Acc: 75.929% (35571/46848)/ 92.860% (43503/46848)
01/15/2023 21:04:32 - INFO - __main__ -   test: [batch: 366/391 ] | Loss: 0.970 | Acc: 75.964% (35685/46976)/ 92.877% (43630/46976)
01/15/2023 21:04:33 - INFO - __main__ -   test: [batch: 367/391 ] | Loss: 0.969 | Acc: 75.989% (35794/47104)/ 92.888% (43754/47104)
01/15/2023 21:04:33 - INFO - __main__ -   test: [batch: 368/391 ] | Loss: 0.968 | Acc: 75.993% (35893/47232)/ 92.897% (43877/47232)
01/15/2023 21:04:34 - INFO - __main__ -   test: [batch: 369/391 ] | Loss: 0.967 | Acc: 76.011% (35999/47360)/ 92.905% (44000/47360)
01/15/2023 21:04:34 - INFO - __main__ -   test: [batch: 370/391 ] | Loss: 0.968 | Acc: 75.996% (36089/47488)/ 92.906% (44119/47488)
01/15/2023 21:04:35 - INFO - __main__ -   test: [batch: 371/391 ] | Loss: 0.967 | Acc: 76.002% (36189/47616)/ 92.908% (44239/47616)
01/15/2023 21:04:36 - INFO - __main__ -   test: [batch: 372/391 ] | Loss: 0.966 | Acc: 76.049% (36309/47744)/ 92.923% (44365/47744)
01/15/2023 21:04:36 - INFO - __main__ -   test: [batch: 373/391 ] | Loss: 0.964 | Acc: 76.086% (36424/47872)/ 92.929% (44487/47872)
01/15/2023 21:04:37 - INFO - __main__ -   test: [batch: 374/391 ] | Loss: 0.963 | Acc: 76.117% (36536/48000)/ 92.933% (44608/48000)
01/15/2023 21:04:38 - INFO - __main__ -   test: [batch: 375/391 ] | Loss: 0.966 | Acc: 76.053% (36603/48128)/ 92.900% (44711/48128)
01/15/2023 21:04:38 - INFO - __main__ -   test: [batch: 376/391 ] | Loss: 0.967 | Acc: 76.044% (36696/48256)/ 92.886% (44823/48256)
01/15/2023 21:04:39 - INFO - __main__ -   test: [batch: 377/391 ] | Loss: 0.967 | Acc: 76.031% (36787/48384)/ 92.878% (44938/48384)
01/15/2023 21:04:40 - INFO - __main__ -   test: [batch: 378/391 ] | Loss: 0.970 | Acc: 75.959% (36849/48512)/ 92.839% (45038/48512)
01/15/2023 21:04:40 - INFO - __main__ -   test: [batch: 379/391 ] | Loss: 0.971 | Acc: 75.940% (36937/48640)/ 92.845% (45160/48640)
01/15/2023 21:04:41 - INFO - __main__ -   test: [batch: 380/391 ] | Loss: 0.970 | Acc: 75.931% (37030/48768)/ 92.854% (45283/48768)
01/15/2023 21:04:41 - INFO - __main__ -   test: [batch: 381/391 ] | Loss: 0.972 | Acc: 75.884% (37104/48896)/ 92.848% (45399/48896)
01/15/2023 21:04:42 - INFO - __main__ -   test: [batch: 382/391 ] | Loss: 0.974 | Acc: 75.853% (37186/49024)/ 92.836% (45512/49024)
01/15/2023 21:04:42 - INFO - __main__ -   test: [batch: 383/391 ] | Loss: 0.974 | Acc: 75.863% (37288/49152)/ 92.830% (45628/49152)
01/15/2023 21:04:42 - INFO - __main__ -   test: [batch: 384/391 ] | Loss: 0.972 | Acc: 75.899% (37403/49280)/ 92.843% (45753/49280)
01/15/2023 21:04:43 - INFO - __main__ -   test: [batch: 385/391 ] | Loss: 0.971 | Acc: 75.909% (37505/49408)/ 92.853% (45877/49408)
01/15/2023 21:04:43 - INFO - __main__ -   test: [batch: 386/391 ] | Loss: 0.969 | Acc: 75.953% (37624/49536)/ 92.870% (46004/49536)
01/15/2023 21:04:43 - INFO - __main__ -   test: [batch: 387/391 ] | Loss: 0.967 | Acc: 75.993% (37741/49664)/ 92.880% (46128/49664)
01/15/2023 21:04:44 - INFO - __main__ -   test: [batch: 388/391 ] | Loss: 0.965 | Acc: 76.038% (37861/49792)/ 92.892% (46253/49792)
01/15/2023 21:04:44 - INFO - __main__ -   test: [batch: 389/391 ] | Loss: 0.965 | Acc: 76.042% (37960/49920)/ 92.893% (46372/49920)
01/15/2023 21:04:44 - INFO - __main__ -   test: [batch: 390/391 ] | Loss: 0.967 | Acc: 75.992% (37996/50000)/ 92.884% (46442/50000)
01/15/2023 21:04:44 - INFO - __main__ -   Final accuracy: 75.992
