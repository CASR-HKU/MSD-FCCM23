/home/jjc/miniconda3/envs/ant_quant/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
Namespace(a_low=75, a_up=150, abit=8, batch_size=128, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='csd_eb3', epoch=3, layer_4bit_l=None, layer_8bit_l='25,47,46,35,29,38,46,5,8,15,29,1,10,31,37,40,8,10,1,17,14,27,42,33,0,14,40,31,15,37,13,42,27,33,43', layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='resnet50', percent=100, ptq=False, resume=False, search=False, tag='', train=True, w_low=75, w_up=150, wbit=8)
01/13/2023 15:45:34 - INFO - __main__ -   output/resnet50_imagenet/int_W8A8_40416/gpu_0
01/13/2023 15:45:34 - INFO - __main__ -   Namespace(a_low=75, a_up=150, abit=8, batch_size=128, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='csd_eb3', epoch=3, layer_4bit_l=None, layer_8bit_l='25,47,46,35,29,38,46,5,8,15,29,1,10,31,37,40,8,10,1,17,14,27,42,33,0,14,40,31,15,37,13,42,27,33,43', layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='resnet50', percent=100, ptq=False, resume=False, search=False, tag='', train=True, w_low=75, w_up=150, wbit=8)
01/13/2023 15:45:34 - INFO - __main__ -   ==> Preparing data..
01/13/2023 15:45:37 - INFO - __main__ -   ==> Setting quantizer..
Namespace(a_low=75, a_up=150, abit=8, batch_size=128, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='csd_eb3', epoch=3, layer_4bit_l=None, layer_8bit_l='25,47,46,35,29,38,46,5,8,15,29,1,10,31,37,40,8,10,1,17,14,27,42,33,0,14,40,31,15,37,13,42,27,33,43', layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='resnet50', percent=100, ptq=False, resume=False, search=False, tag='', train=True, w_low=75, w_up=150, wbit=8)
01/13/2023 15:45:37 - INFO - __main__ -   Namespace(a_low=75, a_up=150, abit=8, batch_size=128, ckpt_path=None, dataset='imagenet', dataset_path='/home/mnt/data/imagenet', disable_input_quantization=False, disable_quant=False, eb='csd_eb3', epoch=3, layer_4bit_l=None, layer_8bit_l='25,47,46,35,29,38,46,5,8,15,29,1,10,31,37,40,8,10,1,17,14,27,42,33,0,14,40,31,15,37,13,42,27,33,43', layer_8bit_n=0, local_rank=0, lr=0.0005, mode='int', model='resnet50', percent=100, ptq=False, resume=False, search=False, tag='', train=True, w_low=75, w_up=150, wbit=8)
01/13/2023 15:45:37 - INFO - __main__ -   ==> Building model..
ResNet(
  (conv1): Conv2dQuantizer(
    (quant_weight): TensorQuantizer()
    (quant_input): TensorQuantizer()
  )
  (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (1): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2dQuantizer(
          (quant_weight): TensorQuantizer()
          (quant_input): TensorQuantizer()
        )
        (1): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2dQuantizer(
        (quant_weight): TensorQuantizer()
        (quant_input): TensorQuantizer()
      )
      (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): LinearQuantizer(
    (quant_weight): TensorQuantizer()
    (quant_input): TensorQuantizer()
  )
)
01/13/2023 15:45:43 - INFO - __main__ -   {'milestones': Counter({2: 1, 3: 1, 4: 1}), 'gamma': 0.1, 'base_lrs': [0.00025], 'last_epoch': 0, '_step_count': 1, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.00025]}
01/13/2023 15:45:43 - INFO - __main__ -   
Epoch: 0
Layer quant EB csd_eb3
int	8-bit 	 conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.3.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.3.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.3.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.3.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.3.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.3.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.4.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.4.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.4.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.5.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.5.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.5.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 fc.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 fc.quant_input,
set init to 1
------------- 8-bit EB2 Re-SET -------------
35
conv1.quant_weight 0
conv1.quant_input 0
layer1.0.conv1.quant_weight 1
layer1.0.conv1.quant_input 1
layer1.1.conv1.quant_weight 5
layer1.1.conv1.quant_input 5
layer1.2.conv1.quant_weight 8
layer1.2.conv1.quant_input 8
layer1.2.conv3.quant_weight 10
layer1.2.conv3.quant_input 10
layer2.0.conv3.quant_weight 13
layer2.0.conv3.quant_input 13
layer2.0.downsample.0.quant_weight 14
layer2.0.downsample.0.quant_input 14
layer2.1.conv1.quant_weight 15
layer2.1.conv1.quant_input 15
layer2.1.conv3.quant_weight 17
layer2.1.conv3.quant_input 17
layer3.0.conv2.quant_weight 25
layer3.0.conv2.quant_input 25
layer3.0.downsample.0.quant_weight 27
layer3.0.downsample.0.quant_input 27
layer3.1.conv2.quant_weight 29
layer3.1.conv2.quant_input 29
layer3.2.conv1.quant_weight 31
layer3.2.conv1.quant_input 31
layer3.2.conv3.quant_weight 33
layer3.2.conv3.quant_input 33
layer3.3.conv2.quant_weight 35
layer3.3.conv2.quant_input 35
layer3.4.conv1.quant_weight 37
layer3.4.conv1.quant_input 37
layer3.4.conv2.quant_weight 38
layer3.4.conv2.quant_input 38
layer3.5.conv1.quant_weight 40
layer3.5.conv1.quant_input 40
layer3.5.conv3.quant_weight 42
layer3.5.conv3.quant_input 42
layer4.0.conv1.quant_weight 43
layer4.0.conv1.quant_input 43
layer4.0.downsample.0.quant_weight 46
layer4.0.downsample.0.quant_input 46
layer4.1.conv1.quant_weight 47
layer4.1.conv1.quant_input 47
------------- 8-bit EB2 Re-SET -------------
Layer quant EB csd_eb2
int	8-bit 	 conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer1.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer1.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer1.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer2.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.3.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.3.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer2.3.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer2.3.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.3.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.3.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.3.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.3.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.4.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv1.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.4.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.4.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.4.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.5.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer3.5.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv2.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer3.5.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer3.5.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.0.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.0.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.0.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.conv3.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.0.downsample.0.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.0.downsample.0.quant_input,
set init to 1
Layer quant EB csd_eb2
int	8-bit 	 layer4.1.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.1.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.1.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.1.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.2.conv1.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv1.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.2.conv2.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv2.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 layer4.2.conv3.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 layer4.2.conv3.quant_input,
set init to 1
Layer quant EB csd_eb3
int	8-bit 	 fc.quant_weight,
set init to 1
Layer quant EB abit-1
int	8-bit 	 fc.quant_input,
set init to 1
01/13/2023 15:46:26 - INFO - __main__ -   test: [epoch: 0 | batch: 0/10010 ] | Loss: 0.676 | Acc: 81.250% (104/128)
01/13/2023 15:50:43 - INFO - __main__ -   test: [epoch: 0 | batch: 100/10010 ] | Loss: 0.841 | Acc: 79.200% (10239/12928)
01/13/2023 15:55:03 - INFO - __main__ -   test: [epoch: 0 | batch: 200/10010 ] | Loss: 0.827 | Acc: 79.314% (20406/25728)
01/13/2023 15:59:23 - INFO - __main__ -   test: [epoch: 0 | batch: 300/10010 ] | Loss: 0.840 | Acc: 79.044% (30454/38528)
01/13/2023 16:03:44 - INFO - __main__ -   test: [epoch: 0 | batch: 400/10010 ] | Loss: 0.851 | Acc: 78.782% (40437/51328)
01/13/2023 16:08:04 - INFO - __main__ -   test: [epoch: 0 | batch: 500/10010 ] | Loss: 0.852 | Acc: 78.641% (50431/64128)
01/13/2023 16:12:25 - INFO - __main__ -   test: [epoch: 0 | batch: 600/10010 ] | Loss: 0.853 | Acc: 78.648% (60502/76928)
01/13/2023 16:16:45 - INFO - __main__ -   test: [epoch: 0 | batch: 700/10010 ] | Loss: 0.854 | Acc: 78.691% (70608/89728)
01/13/2023 16:21:07 - INFO - __main__ -   test: [epoch: 0 | batch: 800/10010 ] | Loss: 0.855 | Acc: 78.642% (80630/102528)
01/13/2023 16:25:28 - INFO - __main__ -   test: [epoch: 0 | batch: 900/10010 ] | Loss: 0.857 | Acc: 78.624% (90675/115328)
01/13/2023 16:29:50 - INFO - __main__ -   test: [epoch: 0 | batch: 1000/10010 ] | Loss: 0.857 | Acc: 78.612% (100724/128128)
01/13/2023 16:34:12 - INFO - __main__ -   test: [epoch: 0 | batch: 1100/10010 ] | Loss: 0.859 | Acc: 78.603% (110774/140928)
01/13/2023 16:38:35 - INFO - __main__ -   test: [epoch: 0 | batch: 1200/10010 ] | Loss: 0.861 | Acc: 78.560% (120768/153728)
01/13/2023 16:42:57 - INFO - __main__ -   test: [epoch: 0 | batch: 1300/10010 ] | Loss: 0.859 | Acc: 78.616% (130917/166528)
01/13/2023 16:47:19 - INFO - __main__ -   test: [epoch: 0 | batch: 1400/10010 ] | Loss: 0.860 | Acc: 78.622% (140992/179328)
01/13/2023 16:51:41 - INFO - __main__ -   test: [epoch: 0 | batch: 1500/10010 ] | Loss: 0.858 | Acc: 78.671% (151149/192128)
01/13/2023 16:56:02 - INFO - __main__ -   test: [epoch: 0 | batch: 1600/10010 ] | Loss: 0.859 | Acc: 78.655% (161187/204928)
01/13/2023 17:00:21 - INFO - __main__ -   test: [epoch: 0 | batch: 1700/10010 ] | Loss: 0.859 | Acc: 78.650% (171244/217728)
01/13/2023 17:04:43 - INFO - __main__ -   test: [epoch: 0 | batch: 1800/10010 ] | Loss: 0.857 | Acc: 78.692% (181407/230528)
01/13/2023 17:09:02 - INFO - __main__ -   test: [epoch: 0 | batch: 1900/10010 ] | Loss: 0.856 | Acc: 78.689% (191473/243328)
01/13/2023 17:13:24 - INFO - __main__ -   test: [epoch: 0 | batch: 2000/10010 ] | Loss: 0.856 | Acc: 78.702% (201579/256128)
01/13/2023 17:17:44 - INFO - __main__ -   test: [epoch: 0 | batch: 2100/10010 ] | Loss: 0.856 | Acc: 78.679% (211591/268928)
01/13/2023 17:22:05 - INFO - __main__ -   test: [epoch: 0 | batch: 2200/10010 ] | Loss: 0.856 | Acc: 78.663% (221616/281728)
01/13/2023 17:26:26 - INFO - __main__ -   test: [epoch: 0 | batch: 2300/10010 ] | Loss: 0.857 | Acc: 78.654% (231658/294528)
01/13/2023 17:30:47 - INFO - __main__ -   test: [epoch: 0 | batch: 2400/10010 ] | Loss: 0.857 | Acc: 78.655% (241728/307328)
01/13/2023 17:35:08 - INFO - __main__ -   test: [epoch: 0 | batch: 2500/10010 ] | Loss: 0.858 | Acc: 78.657% (251802/320128)
01/13/2023 17:39:30 - INFO - __main__ -   test: [epoch: 0 | batch: 2600/10010 ] | Loss: 0.858 | Acc: 78.660% (261882/332928)
01/13/2023 17:43:51 - INFO - __main__ -   test: [epoch: 0 | batch: 2700/10010 ] | Loss: 0.857 | Acc: 78.665% (271968/345728)
01/13/2023 17:48:12 - INFO - __main__ -   test: [epoch: 0 | batch: 2800/10010 ] | Loss: 0.858 | Acc: 78.652% (281989/358528)
01/13/2023 17:52:33 - INFO - __main__ -   test: [epoch: 0 | batch: 2900/10010 ] | Loss: 0.858 | Acc: 78.644% (292029/371328)
01/13/2023 17:56:55 - INFO - __main__ -   test: [epoch: 0 | batch: 3000/10010 ] | Loss: 0.857 | Acc: 78.641% (302084/384128)
01/13/2023 18:01:17 - INFO - __main__ -   test: [epoch: 0 | batch: 3100/10010 ] | Loss: 0.857 | Acc: 78.647% (312173/396928)
01/13/2023 18:05:38 - INFO - __main__ -   test: [epoch: 0 | batch: 3200/10010 ] | Loss: 0.858 | Acc: 78.645% (322231/409728)
01/13/2023 18:09:59 - INFO - __main__ -   test: [epoch: 0 | batch: 3300/10010 ] | Loss: 0.858 | Acc: 78.631% (332237/422528)
01/13/2023 18:14:20 - INFO - __main__ -   test: [epoch: 0 | batch: 3400/10010 ] | Loss: 0.858 | Acc: 78.639% (342337/435328)
01/13/2023 18:18:41 - INFO - __main__ -   test: [epoch: 0 | batch: 3500/10010 ] | Loss: 0.858 | Acc: 78.644% (352426/448128)
01/13/2023 18:23:04 - INFO - __main__ -   test: [epoch: 0 | batch: 3600/10010 ] | Loss: 0.858 | Acc: 78.633% (362441/460928)
01/13/2023 18:27:24 - INFO - __main__ -   test: [epoch: 0 | batch: 3700/10010 ] | Loss: 0.859 | Acc: 78.619% (372439/473728)
01/13/2023 18:31:46 - INFO - __main__ -   test: [epoch: 0 | batch: 3800/10010 ] | Loss: 0.858 | Acc: 78.616% (382487/486528)
01/13/2023 18:36:09 - INFO - __main__ -   test: [epoch: 0 | batch: 3900/10010 ] | Loss: 0.859 | Acc: 78.605% (392498/499328)
01/13/2023 18:40:30 - INFO - __main__ -   test: [epoch: 0 | batch: 4000/10010 ] | Loss: 0.859 | Acc: 78.601% (402539/512128)
01/13/2023 18:44:52 - INFO - __main__ -   test: [epoch: 0 | batch: 4100/10010 ] | Loss: 0.859 | Acc: 78.602% (412606/524928)
01/13/2023 18:49:13 - INFO - __main__ -   test: [epoch: 0 | batch: 4200/10010 ] | Loss: 0.859 | Acc: 78.599% (422647/537728)
01/13/2023 18:53:35 - INFO - __main__ -   test: [epoch: 0 | batch: 4300/10010 ] | Loss: 0.859 | Acc: 78.596% (432694/550528)
01/13/2023 18:57:57 - INFO - __main__ -   test: [epoch: 0 | batch: 4400/10010 ] | Loss: 0.859 | Acc: 78.605% (442803/563328)
01/13/2023 19:02:18 - INFO - __main__ -   test: [epoch: 0 | batch: 4500/10010 ] | Loss: 0.859 | Acc: 78.598% (452825/576128)
01/13/2023 19:06:39 - INFO - __main__ -   test: [epoch: 0 | batch: 4600/10010 ] | Loss: 0.859 | Acc: 78.607% (462937/588928)
01/13/2023 19:11:02 - INFO - __main__ -   test: [epoch: 0 | batch: 4700/10010 ] | Loss: 0.859 | Acc: 78.606% (472994/601728)
01/13/2023 19:15:23 - INFO - __main__ -   test: [epoch: 0 | batch: 4800/10010 ] | Loss: 0.859 | Acc: 78.605% (483050/614528)
01/13/2023 19:19:44 - INFO - __main__ -   test: [epoch: 0 | batch: 4900/10010 ] | Loss: 0.859 | Acc: 78.606% (493118/627328)
01/13/2023 19:24:05 - INFO - __main__ -   test: [epoch: 0 | batch: 5000/10010 ] | Loss: 0.859 | Acc: 78.619% (503262/640128)
01/13/2023 19:28:27 - INFO - __main__ -   test: [epoch: 0 | batch: 5100/10010 ] | Loss: 0.858 | Acc: 78.620% (513334/652928)
01/13/2023 19:32:47 - INFO - __main__ -   test: [epoch: 0 | batch: 5200/10010 ] | Loss: 0.859 | Acc: 78.619% (523386/665728)
01/13/2023 19:37:09 - INFO - __main__ -   test: [epoch: 0 | batch: 5300/10010 ] | Loss: 0.859 | Acc: 78.629% (533517/678528)
01/13/2023 19:41:31 - INFO - __main__ -   test: [epoch: 0 | batch: 5400/10010 ] | Loss: 0.859 | Acc: 78.618% (543508/691328)
01/13/2023 19:45:51 - INFO - __main__ -   test: [epoch: 0 | batch: 5500/10010 ] | Loss: 0.859 | Acc: 78.608% (553500/704128)
01/13/2023 19:50:12 - INFO - __main__ -   test: [epoch: 0 | batch: 5600/10010 ] | Loss: 0.859 | Acc: 78.616% (563617/716928)
01/13/2023 19:54:36 - INFO - __main__ -   test: [epoch: 0 | batch: 5700/10010 ] | Loss: 0.859 | Acc: 78.617% (573691/729728)
01/13/2023 19:58:58 - INFO - __main__ -   test: [epoch: 0 | batch: 5800/10010 ] | Loss: 0.859 | Acc: 78.621% (583782/742528)
01/13/2023 20:03:18 - INFO - __main__ -   test: [epoch: 0 | batch: 5900/10010 ] | Loss: 0.859 | Acc: 78.626% (593884/755328)
01/13/2023 20:07:40 - INFO - __main__ -   test: [epoch: 0 | batch: 6000/10010 ] | Loss: 0.859 | Acc: 78.622% (603917/768128)
01/13/2023 20:12:01 - INFO - __main__ -   test: [epoch: 0 | batch: 6100/10010 ] | Loss: 0.859 | Acc: 78.620% (613967/780928)
01/13/2023 20:16:22 - INFO - __main__ -   test: [epoch: 0 | batch: 6200/10010 ] | Loss: 0.859 | Acc: 78.614% (623984/793728)
01/13/2023 20:20:45 - INFO - __main__ -   test: [epoch: 0 | batch: 6300/10010 ] | Loss: 0.860 | Acc: 78.613% (634035/806528)
01/13/2023 20:25:05 - INFO - __main__ -   test: [epoch: 0 | batch: 6400/10010 ] | Loss: 0.860 | Acc: 78.610% (644075/819328)
01/13/2023 20:29:28 - INFO - __main__ -   test: [epoch: 0 | batch: 6500/10010 ] | Loss: 0.860 | Acc: 78.609% (654125/832128)
01/13/2023 20:33:49 - INFO - __main__ -   test: [epoch: 0 | batch: 6600/10010 ] | Loss: 0.860 | Acc: 78.607% (664174/844928)
01/13/2023 20:38:12 - INFO - __main__ -   test: [epoch: 0 | batch: 6700/10010 ] | Loss: 0.860 | Acc: 78.609% (674251/857728)
01/13/2023 20:42:34 - INFO - __main__ -   test: [epoch: 0 | batch: 6800/10010 ] | Loss: 0.860 | Acc: 78.605% (684281/870528)
01/13/2023 20:46:57 - INFO - __main__ -   test: [epoch: 0 | batch: 6900/10010 ] | Loss: 0.859 | Acc: 78.610% (694383/883328)
01/13/2023 20:51:18 - INFO - __main__ -   test: [epoch: 0 | batch: 7000/10010 ] | Loss: 0.860 | Acc: 78.605% (704398/896128)
01/13/2023 20:55:40 - INFO - __main__ -   test: [epoch: 0 | batch: 7100/10010 ] | Loss: 0.859 | Acc: 78.613% (714534/908928)
01/13/2023 21:00:01 - INFO - __main__ -   test: [epoch: 0 | batch: 7200/10010 ] | Loss: 0.859 | Acc: 78.620% (724664/921728)
01/13/2023 21:04:23 - INFO - __main__ -   test: [epoch: 0 | batch: 7300/10010 ] | Loss: 0.859 | Acc: 78.612% (734651/934528)
01/13/2023 21:08:45 - INFO - __main__ -   test: [epoch: 0 | batch: 7400/10010 ] | Loss: 0.859 | Acc: 78.608% (744679/947328)
01/13/2023 21:13:07 - INFO - __main__ -   test: [epoch: 0 | batch: 7500/10010 ] | Loss: 0.859 | Acc: 78.609% (754750/960128)
01/13/2023 21:17:29 - INFO - __main__ -   test: [epoch: 0 | batch: 7600/10010 ] | Loss: 0.859 | Acc: 78.611% (764831/972928)
01/13/2023 21:21:49 - INFO - __main__ -   test: [epoch: 0 | batch: 7700/10010 ] | Loss: 0.860 | Acc: 78.601% (774794/985728)
01/13/2023 21:26:11 - INFO - __main__ -   test: [epoch: 0 | batch: 7800/10010 ] | Loss: 0.860 | Acc: 78.598% (784828/998528)
01/13/2023 21:30:32 - INFO - __main__ -   test: [epoch: 0 | batch: 7900/10010 ] | Loss: 0.860 | Acc: 78.601% (794916/1011328)
01/13/2023 21:34:53 - INFO - __main__ -   test: [epoch: 0 | batch: 8000/10010 ] | Loss: 0.860 | Acc: 78.600% (804969/1024128)
01/13/2023 21:39:14 - INFO - __main__ -   test: [epoch: 0 | batch: 8100/10010 ] | Loss: 0.860 | Acc: 78.605% (815078/1036928)
01/13/2023 21:43:34 - INFO - __main__ -   test: [epoch: 0 | batch: 8200/10010 ] | Loss: 0.860 | Acc: 78.600% (825087/1049728)
01/13/2023 21:47:56 - INFO - __main__ -   test: [epoch: 0 | batch: 8300/10010 ] | Loss: 0.860 | Acc: 78.601% (835156/1062528)
01/13/2023 21:52:18 - INFO - __main__ -   test: [epoch: 0 | batch: 8400/10010 ] | Loss: 0.860 | Acc: 78.604% (845252/1075328)
01/13/2023 21:56:41 - INFO - __main__ -   test: [epoch: 0 | batch: 8500/10010 ] | Loss: 0.860 | Acc: 78.607% (855341/1088128)
01/13/2023 22:01:00 - INFO - __main__ -   test: [epoch: 0 | batch: 8600/10010 ] | Loss: 0.860 | Acc: 78.601% (865342/1100928)
01/13/2023 22:05:22 - INFO - __main__ -   test: [epoch: 0 | batch: 8700/10010 ] | Loss: 0.860 | Acc: 78.601% (875402/1113728)
01/13/2023 22:09:45 - INFO - __main__ -   test: [epoch: 0 | batch: 8800/10010 ] | Loss: 0.860 | Acc: 78.605% (885505/1126528)
01/13/2023 22:14:05 - INFO - __main__ -   test: [epoch: 0 | batch: 8900/10010 ] | Loss: 0.860 | Acc: 78.610% (895629/1139328)
01/13/2023 22:18:27 - INFO - __main__ -   test: [epoch: 0 | batch: 9000/10010 ] | Loss: 0.860 | Acc: 78.613% (905721/1152128)
01/13/2023 22:22:50 - INFO - __main__ -   test: [epoch: 0 | batch: 9100/10010 ] | Loss: 0.860 | Acc: 78.612% (915769/1164928)
01/13/2023 22:27:12 - INFO - __main__ -   test: [epoch: 0 | batch: 9200/10010 ] | Loss: 0.860 | Acc: 78.611% (925828/1177728)
01/13/2023 22:31:32 - INFO - __main__ -   test: [epoch: 0 | batch: 9300/10010 ] | Loss: 0.860 | Acc: 78.613% (935906/1190528)
01/13/2023 22:35:51 - INFO - __main__ -   test: [epoch: 0 | batch: 9400/10010 ] | Loss: 0.860 | Acc: 78.611% (945952/1203328)
01/13/2023 22:40:14 - INFO - __main__ -   test: [epoch: 0 | batch: 9500/10010 ] | Loss: 0.860 | Acc: 78.615% (956061/1216128)
01/13/2023 22:44:35 - INFO - __main__ -   test: [epoch: 0 | batch: 9600/10010 ] | Loss: 0.860 | Acc: 78.615% (966121/1228928)
01/13/2023 22:48:57 - INFO - __main__ -   test: [epoch: 0 | batch: 9700/10010 ] | Loss: 0.860 | Acc: 78.612% (976148/1241728)
01/13/2023 22:53:17 - INFO - __main__ -   test: [epoch: 0 | batch: 9800/10010 ] | Loss: 0.860 | Acc: 78.607% (986152/1254528)
01/13/2023 22:57:40 - INFO - __main__ -   test: [epoch: 0 | batch: 9900/10010 ] | Loss: 0.860 | Acc: 78.612% (996273/1267328)
01/13/2023 23:02:03 - INFO - __main__ -   test: [epoch: 0 | batch: 10000/10010 ] | Loss: 0.860 | Acc: 78.608% (1006282/1280128)
01/13/2023 23:02:27 - INFO - __main__ -   Saving Checkpoint
01/13/2023 23:02:30 - INFO - __main__ -   test: [batch: 0/391 ] | Loss: 0.441 | Acc: 87.500% (112/128)/ 98.438% (126/128)
01/13/2023 23:02:33 - INFO - __main__ -   test: [batch: 1/391 ] | Loss: 0.465 | Acc: 86.719% (222/256)/ 98.438% (252/256)
01/13/2023 23:02:35 - INFO - __main__ -   test: [batch: 2/391 ] | Loss: 0.618 | Acc: 83.333% (320/384)/ 96.094% (369/384)
01/13/2023 23:02:38 - INFO - __main__ -   test: [batch: 3/391 ] | Loss: 0.570 | Acc: 84.961% (435/512)/ 96.680% (495/512)
01/13/2023 23:02:41 - INFO - __main__ -   test: [batch: 4/391 ] | Loss: 0.487 | Acc: 87.188% (558/640)/ 97.344% (623/640)
01/13/2023 23:02:43 - INFO - __main__ -   test: [batch: 5/391 ] | Loss: 0.431 | Acc: 88.281% (678/768)/ 97.786% (751/768)
01/13/2023 23:02:46 - INFO - __main__ -   test: [batch: 6/391 ] | Loss: 0.425 | Acc: 88.504% (793/896)/ 97.656% (875/896)
01/13/2023 23:02:48 - INFO - __main__ -   test: [batch: 7/391 ] | Loss: 0.407 | Acc: 89.453% (916/1024)/ 97.754% (1001/1024)
01/13/2023 23:02:51 - INFO - __main__ -   test: [batch: 8/391 ] | Loss: 0.425 | Acc: 89.236% (1028/1152)/ 97.656% (1125/1152)
01/13/2023 23:02:54 - INFO - __main__ -   test: [batch: 9/391 ] | Loss: 0.404 | Acc: 89.766% (1149/1280)/ 97.734% (1251/1280)
01/13/2023 23:02:56 - INFO - __main__ -   test: [batch: 10/391 ] | Loss: 0.454 | Acc: 88.494% (1246/1408)/ 97.656% (1375/1408)
01/13/2023 23:02:59 - INFO - __main__ -   test: [batch: 11/391 ] | Loss: 0.458 | Acc: 88.672% (1362/1536)/ 97.526% (1498/1536)
01/13/2023 23:03:01 - INFO - __main__ -   test: [batch: 12/391 ] | Loss: 0.508 | Acc: 87.500% (1456/1664)/ 97.175% (1617/1664)
01/13/2023 23:03:04 - INFO - __main__ -   test: [batch: 13/391 ] | Loss: 0.556 | Acc: 85.938% (1540/1792)/ 96.652% (1732/1792)
01/13/2023 23:03:07 - INFO - __main__ -   test: [batch: 14/391 ] | Loss: 0.576 | Acc: 85.208% (1636/1920)/ 96.719% (1857/1920)
01/13/2023 23:03:09 - INFO - __main__ -   test: [batch: 15/391 ] | Loss: 0.584 | Acc: 84.814% (1737/2048)/ 96.826% (1983/2048)
01/13/2023 23:03:12 - INFO - __main__ -   test: [batch: 16/391 ] | Loss: 0.593 | Acc: 84.605% (1841/2176)/ 96.645% (2103/2176)
01/13/2023 23:03:14 - INFO - __main__ -   test: [batch: 17/391 ] | Loss: 0.619 | Acc: 84.158% (1939/2304)/ 96.137% (2215/2304)
01/13/2023 23:03:17 - INFO - __main__ -   test: [batch: 18/391 ] | Loss: 0.641 | Acc: 83.635% (2034/2432)/ 95.970% (2334/2432)
01/13/2023 23:03:20 - INFO - __main__ -   test: [batch: 19/391 ] | Loss: 0.648 | Acc: 83.438% (2136/2560)/ 95.898% (2455/2560)
01/13/2023 23:03:23 - INFO - __main__ -   test: [batch: 20/391 ] | Loss: 0.645 | Acc: 83.557% (2246/2688)/ 95.796% (2575/2688)
01/13/2023 23:03:25 - INFO - __main__ -   test: [batch: 21/391 ] | Loss: 0.677 | Acc: 82.848% (2333/2816)/ 95.632% (2693/2816)
01/13/2023 23:03:28 - INFO - __main__ -   test: [batch: 22/391 ] | Loss: 0.677 | Acc: 82.643% (2433/2944)/ 95.652% (2816/2944)
01/13/2023 23:03:31 - INFO - __main__ -   test: [batch: 23/391 ] | Loss: 0.721 | Acc: 81.738% (2511/3072)/ 95.345% (2929/3072)
01/13/2023 23:03:33 - INFO - __main__ -   test: [batch: 24/391 ] | Loss: 0.740 | Acc: 81.312% (2602/3200)/ 95.188% (3046/3200)
01/13/2023 23:03:36 - INFO - __main__ -   test: [batch: 25/391 ] | Loss: 0.765 | Acc: 80.619% (2683/3328)/ 94.922% (3159/3328)
01/13/2023 23:03:39 - INFO - __main__ -   test: [batch: 26/391 ] | Loss: 0.778 | Acc: 79.919% (2762/3456)/ 94.907% (3280/3456)
01/13/2023 23:03:41 - INFO - __main__ -   test: [batch: 27/391 ] | Loss: 0.761 | Acc: 80.441% (2883/3584)/ 94.950% (3403/3584)
01/13/2023 23:03:44 - INFO - __main__ -   test: [batch: 28/391 ] | Loss: 0.770 | Acc: 79.930% (2967/3712)/ 95.016% (3527/3712)
01/13/2023 23:03:46 - INFO - __main__ -   test: [batch: 29/391 ] | Loss: 0.763 | Acc: 80.078% (3075/3840)/ 95.130% (3653/3840)
01/13/2023 23:03:49 - INFO - __main__ -   test: [batch: 30/391 ] | Loss: 0.777 | Acc: 79.965% (3173/3968)/ 95.010% (3770/3968)
01/13/2023 23:03:52 - INFO - __main__ -   test: [batch: 31/391 ] | Loss: 0.772 | Acc: 80.176% (3284/4096)/ 95.093% (3895/4096)
01/13/2023 23:03:54 - INFO - __main__ -   test: [batch: 32/391 ] | Loss: 0.758 | Acc: 80.492% (3400/4224)/ 95.194% (4021/4224)
01/13/2023 23:03:57 - INFO - __main__ -   test: [batch: 33/391 ] | Loss: 0.752 | Acc: 80.676% (3511/4352)/ 95.244% (4145/4352)
01/13/2023 23:03:59 - INFO - __main__ -   test: [batch: 34/391 ] | Loss: 0.737 | Acc: 81.049% (3631/4480)/ 95.312% (4270/4480)
01/13/2023 23:04:02 - INFO - __main__ -   test: [batch: 35/391 ] | Loss: 0.725 | Acc: 81.424% (3752/4608)/ 95.312% (4392/4608)
01/13/2023 23:04:05 - INFO - __main__ -   test: [batch: 36/391 ] | Loss: 0.710 | Acc: 81.862% (3877/4736)/ 95.418% (4519/4736)
01/13/2023 23:04:07 - INFO - __main__ -   test: [batch: 37/391 ] | Loss: 0.700 | Acc: 82.175% (3997/4864)/ 95.477% (4644/4864)
01/13/2023 23:04:10 - INFO - __main__ -   test: [batch: 38/391 ] | Loss: 0.692 | Acc: 82.332% (4110/4992)/ 95.513% (4768/4992)
01/13/2023 23:04:12 - INFO - __main__ -   test: [batch: 39/391 ] | Loss: 0.687 | Acc: 82.363% (4217/5120)/ 95.547% (4892/5120)
01/13/2023 23:04:15 - INFO - __main__ -   test: [batch: 40/391 ] | Loss: 0.685 | Acc: 82.470% (4328/5248)/ 95.446% (5009/5248)
01/13/2023 23:04:18 - INFO - __main__ -   test: [batch: 41/391 ] | Loss: 0.690 | Acc: 82.533% (4437/5376)/ 95.350% (5126/5376)
01/13/2023 23:04:20 - INFO - __main__ -   test: [batch: 42/391 ] | Loss: 0.689 | Acc: 82.504% (4541/5504)/ 95.403% (5251/5504)
01/13/2023 23:04:23 - INFO - __main__ -   test: [batch: 43/391 ] | Loss: 0.686 | Acc: 82.564% (4650/5632)/ 95.366% (5371/5632)
01/13/2023 23:04:26 - INFO - __main__ -   test: [batch: 44/391 ] | Loss: 0.690 | Acc: 82.569% (4756/5760)/ 95.278% (5488/5760)
01/13/2023 23:04:28 - INFO - __main__ -   test: [batch: 45/391 ] | Loss: 0.687 | Acc: 82.711% (4870/5888)/ 95.279% (5610/5888)
01/13/2023 23:04:31 - INFO - __main__ -   test: [batch: 46/391 ] | Loss: 0.690 | Acc: 82.663% (4973/6016)/ 95.296% (5733/6016)
01/13/2023 23:04:33 - INFO - __main__ -   test: [batch: 47/391 ] | Loss: 0.691 | Acc: 82.617% (5076/6144)/ 95.361% (5859/6144)
01/13/2023 23:04:36 - INFO - __main__ -   test: [batch: 48/391 ] | Loss: 0.696 | Acc: 82.462% (5172/6272)/ 95.344% (5980/6272)
01/13/2023 23:04:39 - INFO - __main__ -   test: [batch: 49/391 ] | Loss: 0.699 | Acc: 82.516% (5281/6400)/ 95.250% (6096/6400)
01/13/2023 23:04:41 - INFO - __main__ -   test: [batch: 50/391 ] | Loss: 0.691 | Acc: 82.736% (5401/6528)/ 95.312% (6222/6528)
01/13/2023 23:04:44 - INFO - __main__ -   test: [batch: 51/391 ] | Loss: 0.683 | Acc: 82.933% (5520/6656)/ 95.403% (6350/6656)
01/13/2023 23:04:47 - INFO - __main__ -   test: [batch: 52/391 ] | Loss: 0.680 | Acc: 83.019% (5632/6784)/ 95.430% (6474/6784)
01/13/2023 23:04:49 - INFO - __main__ -   test: [batch: 53/391 ] | Loss: 0.671 | Acc: 83.218% (5752/6912)/ 95.501% (6601/6912)
01/13/2023 23:04:52 - INFO - __main__ -   test: [batch: 54/391 ] | Loss: 0.664 | Acc: 83.352% (5868/7040)/ 95.526% (6725/7040)
01/13/2023 23:04:55 - INFO - __main__ -   test: [batch: 55/391 ] | Loss: 0.658 | Acc: 83.510% (5986/7168)/ 95.578% (6851/7168)
01/13/2023 23:04:57 - INFO - __main__ -   test: [batch: 56/391 ] | Loss: 0.649 | Acc: 83.717% (6108/7296)/ 95.628% (6977/7296)
01/13/2023 23:05:00 - INFO - __main__ -   test: [batch: 57/391 ] | Loss: 0.642 | Acc: 83.930% (6231/7424)/ 95.690% (7104/7424)
01/13/2023 23:05:03 - INFO - __main__ -   test: [batch: 58/391 ] | Loss: 0.640 | Acc: 83.965% (6341/7552)/ 95.697% (7227/7552)
01/13/2023 23:05:06 - INFO - __main__ -   test: [batch: 59/391 ] | Loss: 0.643 | Acc: 83.880% (6442/7680)/ 95.716% (7351/7680)
01/13/2023 23:05:08 - INFO - __main__ -   test: [batch: 60/391 ] | Loss: 0.648 | Acc: 83.760% (6540/7808)/ 95.684% (7471/7808)
01/13/2023 23:05:11 - INFO - __main__ -   test: [batch: 61/391 ] | Loss: 0.648 | Acc: 83.758% (6647/7936)/ 95.703% (7595/7936)
01/13/2023 23:05:13 - INFO - __main__ -   test: [batch: 62/391 ] | Loss: 0.648 | Acc: 83.743% (6753/8064)/ 95.722% (7719/8064)
01/13/2023 23:05:16 - INFO - __main__ -   test: [batch: 63/391 ] | Loss: 0.656 | Acc: 83.618% (6850/8192)/ 95.667% (7837/8192)
01/13/2023 23:05:18 - INFO - __main__ -   test: [batch: 64/391 ] | Loss: 0.664 | Acc: 83.438% (6942/8320)/ 95.613% (7955/8320)
01/13/2023 23:05:21 - INFO - __main__ -   test: [batch: 65/391 ] | Loss: 0.677 | Acc: 82.931% (7006/8448)/ 95.526% (8070/8448)
01/13/2023 23:05:24 - INFO - __main__ -   test: [batch: 66/391 ] | Loss: 0.682 | Acc: 82.929% (7112/8576)/ 95.487% (8189/8576)
01/13/2023 23:05:26 - INFO - __main__ -   test: [batch: 67/391 ] | Loss: 0.685 | Acc: 82.870% (7213/8704)/ 95.496% (8312/8704)
01/13/2023 23:05:29 - INFO - __main__ -   test: [batch: 68/391 ] | Loss: 0.685 | Acc: 82.835% (7316/8832)/ 95.516% (8436/8832)
01/13/2023 23:05:31 - INFO - __main__ -   test: [batch: 69/391 ] | Loss: 0.681 | Acc: 82.924% (7430/8960)/ 95.558% (8562/8960)
01/13/2023 23:05:34 - INFO - __main__ -   test: [batch: 70/391 ] | Loss: 0.682 | Acc: 82.835% (7528/9088)/ 95.555% (8684/9088)
01/13/2023 23:05:36 - INFO - __main__ -   test: [batch: 71/391 ] | Loss: 0.680 | Acc: 82.878% (7638/9216)/ 95.562% (8807/9216)
01/13/2023 23:05:39 - INFO - __main__ -   test: [batch: 72/391 ] | Loss: 0.683 | Acc: 82.759% (7733/9344)/ 95.559% (8929/9344)
01/13/2023 23:05:42 - INFO - __main__ -   test: [batch: 73/391 ] | Loss: 0.686 | Acc: 82.633% (7827/9472)/ 95.576% (9053/9472)
01/13/2023 23:05:44 - INFO - __main__ -   test: [batch: 74/391 ] | Loss: 0.687 | Acc: 82.615% (7931/9600)/ 95.562% (9174/9600)
01/13/2023 23:05:47 - INFO - __main__ -   test: [batch: 75/391 ] | Loss: 0.693 | Acc: 82.432% (8019/9728)/ 95.559% (9296/9728)
01/13/2023 23:05:49 - INFO - __main__ -   test: [batch: 76/391 ] | Loss: 0.693 | Acc: 82.417% (8123/9856)/ 95.556% (9418/9856)
01/13/2023 23:05:52 - INFO - __main__ -   test: [batch: 77/391 ] | Loss: 0.693 | Acc: 82.392% (8226/9984)/ 95.583% (9543/9984)
01/13/2023 23:05:55 - INFO - __main__ -   test: [batch: 78/391 ] | Loss: 0.693 | Acc: 82.308% (8323/10112)/ 95.629% (9670/10112)
01/13/2023 23:05:57 - INFO - __main__ -   test: [batch: 79/391 ] | Loss: 0.692 | Acc: 82.314% (8429/10240)/ 95.654% (9795/10240)
01/13/2023 23:06:00 - INFO - __main__ -   test: [batch: 80/391 ] | Loss: 0.691 | Acc: 82.282% (8531/10368)/ 95.660% (9918/10368)
01/13/2023 23:06:02 - INFO - __main__ -   test: [batch: 81/391 ] | Loss: 0.691 | Acc: 82.298% (8638/10496)/ 95.684% (10043/10496)
01/13/2023 23:06:05 - INFO - __main__ -   test: [batch: 82/391 ] | Loss: 0.690 | Acc: 82.304% (8744/10624)/ 95.680% (10165/10624)
01/13/2023 23:06:08 - INFO - __main__ -   test: [batch: 83/391 ] | Loss: 0.690 | Acc: 82.329% (8852/10752)/ 95.666% (10286/10752)
01/13/2023 23:06:10 - INFO - __main__ -   test: [batch: 84/391 ] | Loss: 0.686 | Acc: 82.445% (8970/10880)/ 95.699% (10412/10880)
01/13/2023 23:06:13 - INFO - __main__ -   test: [batch: 85/391 ] | Loss: 0.685 | Acc: 82.431% (9074/11008)/ 95.739% (10539/11008)
01/13/2023 23:06:16 - INFO - __main__ -   test: [batch: 86/391 ] | Loss: 0.689 | Acc: 82.355% (9171/11136)/ 95.717% (10659/11136)
01/13/2023 23:06:18 - INFO - __main__ -   test: [batch: 87/391 ] | Loss: 0.687 | Acc: 82.404% (9282/11264)/ 95.721% (10782/11264)
01/13/2023 23:06:21 - INFO - __main__ -   test: [batch: 88/391 ] | Loss: 0.694 | Acc: 82.338% (9380/11392)/ 95.655% (10897/11392)
01/13/2023 23:06:23 - INFO - __main__ -   test: [batch: 89/391 ] | Loss: 0.692 | Acc: 82.370% (9489/11520)/ 95.668% (11021/11520)
01/13/2023 23:06:26 - INFO - __main__ -   test: [batch: 90/391 ] | Loss: 0.693 | Acc: 82.272% (9583/11648)/ 95.673% (11144/11648)
01/13/2023 23:06:28 - INFO - __main__ -   test: [batch: 91/391 ] | Loss: 0.692 | Acc: 82.303% (9692/11776)/ 95.678% (11267/11776)
01/13/2023 23:06:31 - INFO - __main__ -   test: [batch: 92/391 ] | Loss: 0.693 | Acc: 82.266% (9793/11904)/ 95.665% (11388/11904)
01/13/2023 23:06:34 - INFO - __main__ -   test: [batch: 93/391 ] | Loss: 0.696 | Acc: 82.064% (9874/12032)/ 95.695% (11514/12032)
01/13/2023 23:06:36 - INFO - __main__ -   test: [batch: 94/391 ] | Loss: 0.699 | Acc: 81.908% (9960/12160)/ 95.707% (11638/12160)
01/13/2023 23:06:39 - INFO - __main__ -   test: [batch: 95/391 ] | Loss: 0.696 | Acc: 81.974% (10073/12288)/ 95.711% (11761/12288)
01/13/2023 23:06:41 - INFO - __main__ -   test: [batch: 96/391 ] | Loss: 0.700 | Acc: 81.878% (10166/12416)/ 95.731% (11886/12416)
01/13/2023 23:06:44 - INFO - __main__ -   test: [batch: 97/391 ] | Loss: 0.701 | Acc: 81.712% (10250/12544)/ 95.751% (12011/12544)
01/13/2023 23:06:47 - INFO - __main__ -   test: [batch: 98/391 ] | Loss: 0.698 | Acc: 81.826% (10369/12672)/ 95.778% (12137/12672)
01/13/2023 23:06:49 - INFO - __main__ -   test: [batch: 99/391 ] | Loss: 0.692 | Acc: 81.977% (10493/12800)/ 95.820% (12265/12800)
01/13/2023 23:06:52 - INFO - __main__ -   test: [batch: 100/391 ] | Loss: 0.691 | Acc: 82.031% (10605/12928)/ 95.831% (12389/12928)
01/13/2023 23:06:54 - INFO - __main__ -   test: [batch: 101/391 ] | Loss: 0.687 | Acc: 82.138% (10724/13056)/ 95.864% (12516/13056)
01/13/2023 23:06:57 - INFO - __main__ -   test: [batch: 102/391 ] | Loss: 0.683 | Acc: 82.259% (10845/13184)/ 95.889% (12642/13184)
01/13/2023 23:07:00 - INFO - __main__ -   test: [batch: 103/391 ] | Loss: 0.685 | Acc: 82.076% (10926/13312)/ 95.906% (12767/13312)
01/13/2023 23:07:02 - INFO - __main__ -   test: [batch: 104/391 ] | Loss: 0.685 | Acc: 82.016% (11023/13440)/ 95.915% (12891/13440)
01/13/2023 23:07:05 - INFO - __main__ -   test: [batch: 105/391 ] | Loss: 0.685 | Acc: 82.009% (11127/13568)/ 95.909% (13013/13568)
01/13/2023 23:07:08 - INFO - __main__ -   test: [batch: 106/391 ] | Loss: 0.694 | Acc: 81.893% (11216/13696)/ 95.838% (13126/13696)
01/13/2023 23:07:10 - INFO - __main__ -   test: [batch: 107/391 ] | Loss: 0.690 | Acc: 82.017% (11338/13824)/ 95.870% (13253/13824)
01/13/2023 23:07:13 - INFO - __main__ -   test: [batch: 108/391 ] | Loss: 0.693 | Acc: 81.859% (11421/13952)/ 95.879% (13377/13952)
01/13/2023 23:07:16 - INFO - __main__ -   test: [batch: 109/391 ] | Loss: 0.694 | Acc: 81.839% (11523/14080)/ 95.881% (13500/14080)
01/13/2023 23:07:18 - INFO - __main__ -   test: [batch: 110/391 ] | Loss: 0.697 | Acc: 81.658% (11602/14208)/ 95.897% (13625/14208)
01/13/2023 23:07:21 - INFO - __main__ -   test: [batch: 111/391 ] | Loss: 0.700 | Acc: 81.613% (11700/14336)/ 95.871% (13744/14336)
01/13/2023 23:07:23 - INFO - __main__ -   test: [batch: 112/391 ] | Loss: 0.701 | Acc: 81.630% (11807/14464)/ 95.886% (13869/14464)
01/13/2023 23:07:26 - INFO - __main__ -   test: [batch: 113/391 ] | Loss: 0.700 | Acc: 81.675% (11918/14592)/ 95.888% (13992/14592)
01/13/2023 23:07:29 - INFO - __main__ -   test: [batch: 114/391 ] | Loss: 0.696 | Acc: 81.766% (12036/14720)/ 95.910% (14118/14720)
01/13/2023 23:07:31 - INFO - __main__ -   test: [batch: 115/391 ] | Loss: 0.693 | Acc: 81.856% (12154/14848)/ 95.932% (14244/14848)
01/13/2023 23:07:34 - INFO - __main__ -   test: [batch: 116/391 ] | Loss: 0.697 | Acc: 81.831% (12255/14976)/ 95.900% (14362/14976)
01/13/2023 23:07:37 - INFO - __main__ -   test: [batch: 117/391 ] | Loss: 0.695 | Acc: 81.852% (12363/15104)/ 95.908% (14486/15104)
01/13/2023 23:07:39 - INFO - __main__ -   test: [batch: 118/391 ] | Loss: 0.698 | Acc: 81.736% (12450/15232)/ 95.923% (14611/15232)
01/13/2023 23:07:42 - INFO - __main__ -   test: [batch: 119/391 ] | Loss: 0.696 | Acc: 81.777% (12561/15360)/ 95.944% (14737/15360)
01/13/2023 23:07:44 - INFO - __main__ -   test: [batch: 120/391 ] | Loss: 0.695 | Acc: 81.818% (12672/15488)/ 95.958% (14862/15488)
01/13/2023 23:07:47 - INFO - __main__ -   test: [batch: 121/391 ] | Loss: 0.700 | Acc: 81.666% (12753/15616)/ 95.927% (14980/15616)
01/13/2023 23:07:50 - INFO - __main__ -   test: [batch: 122/391 ] | Loss: 0.702 | Acc: 81.612% (12849/15744)/ 95.897% (15098/15744)
01/13/2023 23:07:52 - INFO - __main__ -   test: [batch: 123/391 ] | Loss: 0.703 | Acc: 81.628% (12956/15872)/ 95.886% (15219/15872)
01/13/2023 23:07:55 - INFO - __main__ -   test: [batch: 124/391 ] | Loss: 0.702 | Acc: 81.638% (13062/16000)/ 95.900% (15344/16000)
01/13/2023 23:07:57 - INFO - __main__ -   test: [batch: 125/391 ] | Loss: 0.698 | Acc: 81.740% (13183/16128)/ 95.926% (15471/16128)
01/13/2023 23:08:00 - INFO - __main__ -   test: [batch: 126/391 ] | Loss: 0.694 | Acc: 81.828% (13302/16256)/ 95.940% (15596/16256)
01/13/2023 23:08:03 - INFO - __main__ -   test: [batch: 127/391 ] | Loss: 0.692 | Acc: 81.909% (13420/16384)/ 95.947% (15720/16384)
01/13/2023 23:08:05 - INFO - __main__ -   test: [batch: 128/391 ] | Loss: 0.693 | Acc: 81.898% (13523/16512)/ 95.918% (15838/16512)
01/13/2023 23:08:08 - INFO - __main__ -   test: [batch: 129/391 ] | Loss: 0.692 | Acc: 81.917% (13631/16640)/ 95.925% (15962/16640)
01/13/2023 23:08:11 - INFO - __main__ -   test: [batch: 130/391 ] | Loss: 0.689 | Acc: 82.019% (13753/16768)/ 95.945% (16088/16768)
01/13/2023 23:08:13 - INFO - __main__ -   test: [batch: 131/391 ] | Loss: 0.688 | Acc: 82.067% (13866/16896)/ 95.952% (16212/16896)
01/13/2023 23:08:16 - INFO - __main__ -   test: [batch: 132/391 ] | Loss: 0.686 | Acc: 82.137% (13983/17024)/ 95.965% (16337/17024)
01/13/2023 23:08:18 - INFO - __main__ -   test: [batch: 133/391 ] | Loss: 0.687 | Acc: 82.084% (14079/17152)/ 95.954% (16458/17152)
01/13/2023 23:08:21 - INFO - __main__ -   test: [batch: 134/391 ] | Loss: 0.685 | Acc: 82.112% (14189/17280)/ 95.978% (16585/17280)
01/13/2023 23:08:24 - INFO - __main__ -   test: [batch: 135/391 ] | Loss: 0.685 | Acc: 82.106% (14293/17408)/ 95.979% (16708/17408)
01/13/2023 23:08:26 - INFO - __main__ -   test: [batch: 136/391 ] | Loss: 0.684 | Acc: 82.065% (14391/17536)/ 95.997% (16834/17536)
01/13/2023 23:08:29 - INFO - __main__ -   test: [batch: 137/391 ] | Loss: 0.682 | Acc: 82.116% (14505/17664)/ 96.026% (16962/17664)
01/13/2023 23:08:31 - INFO - __main__ -   test: [batch: 138/391 ] | Loss: 0.681 | Acc: 82.138% (14614/17792)/ 96.032% (17086/17792)
01/13/2023 23:08:34 - INFO - __main__ -   test: [batch: 139/391 ] | Loss: 0.687 | Acc: 81.981% (14691/17920)/ 96.027% (17208/17920)
01/13/2023 23:08:37 - INFO - __main__ -   test: [batch: 140/391 ] | Loss: 0.689 | Acc: 81.909% (14783/18048)/ 96.022% (17330/18048)
01/13/2023 23:08:39 - INFO - __main__ -   test: [batch: 141/391 ] | Loss: 0.687 | Acc: 81.954% (14896/18176)/ 96.033% (17455/18176)
01/13/2023 23:08:42 - INFO - __main__ -   test: [batch: 142/391 ] | Loss: 0.685 | Acc: 82.037% (15016/18304)/ 96.050% (17581/18304)
01/13/2023 23:08:44 - INFO - __main__ -   test: [batch: 143/391 ] | Loss: 0.686 | Acc: 82.058% (15125/18432)/ 96.034% (17701/18432)
01/13/2023 23:08:47 - INFO - __main__ -   test: [batch: 144/391 ] | Loss: 0.688 | Acc: 82.037% (15226/18560)/ 96.008% (17819/18560)
01/13/2023 23:08:50 - INFO - __main__ -   test: [batch: 145/391 ] | Loss: 0.691 | Acc: 82.010% (15326/18688)/ 95.981% (17937/18688)
01/13/2023 23:08:52 - INFO - __main__ -   test: [batch: 146/391 ] | Loss: 0.692 | Acc: 81.983% (15426/18816)/ 95.966% (18057/18816)
01/13/2023 23:08:55 - INFO - __main__ -   test: [batch: 147/391 ] | Loss: 0.692 | Acc: 81.989% (15532/18944)/ 95.951% (18177/18944)
01/13/2023 23:08:57 - INFO - __main__ -   test: [batch: 148/391 ] | Loss: 0.694 | Acc: 81.926% (15625/19072)/ 95.952% (18300/19072)
01/13/2023 23:09:00 - INFO - __main__ -   test: [batch: 149/391 ] | Loss: 0.698 | Acc: 81.823% (15710/19200)/ 95.917% (18416/19200)
01/13/2023 23:09:03 - INFO - __main__ -   test: [batch: 150/391 ] | Loss: 0.698 | Acc: 81.752% (15801/19328)/ 95.923% (18540/19328)
01/13/2023 23:09:06 - INFO - __main__ -   test: [batch: 151/391 ] | Loss: 0.698 | Acc: 81.774% (15910/19456)/ 95.924% (18663/19456)
01/13/2023 23:09:08 - INFO - __main__ -   test: [batch: 152/391 ] | Loss: 0.699 | Acc: 81.756% (16011/19584)/ 95.910% (18783/19584)
01/13/2023 23:09:11 - INFO - __main__ -   test: [batch: 153/391 ] | Loss: 0.697 | Acc: 81.818% (16128/19712)/ 95.916% (18907/19712)
01/13/2023 23:09:14 - INFO - __main__ -   test: [batch: 154/391 ] | Loss: 0.697 | Acc: 81.804% (16230/19840)/ 95.892% (19025/19840)
01/13/2023 23:09:16 - INFO - __main__ -   test: [batch: 155/391 ] | Loss: 0.698 | Acc: 81.821% (16338/19968)/ 95.883% (19146/19968)
01/13/2023 23:09:19 - INFO - __main__ -   test: [batch: 156/391 ] | Loss: 0.700 | Acc: 81.743% (16427/20096)/ 95.875% (19267/20096)
01/13/2023 23:09:21 - INFO - __main__ -   test: [batch: 157/391 ] | Loss: 0.701 | Acc: 81.730% (16529/20224)/ 95.856% (19386/20224)
01/13/2023 23:09:24 - INFO - __main__ -   test: [batch: 158/391 ] | Loss: 0.702 | Acc: 81.692% (16626/20352)/ 95.828% (19503/20352)
01/13/2023 23:09:26 - INFO - __main__ -   test: [batch: 159/391 ] | Loss: 0.704 | Acc: 81.675% (16727/20480)/ 95.820% (19624/20480)
01/13/2023 23:09:29 - INFO - __main__ -   test: [batch: 160/391 ] | Loss: 0.704 | Acc: 81.653% (16827/20608)/ 95.798% (19742/20608)
01/13/2023 23:09:32 - INFO - __main__ -   test: [batch: 161/391 ] | Loss: 0.714 | Acc: 81.428% (16885/20736)/ 95.703% (19845/20736)
01/13/2023 23:09:34 - INFO - __main__ -   test: [batch: 162/391 ] | Loss: 0.719 | Acc: 81.327% (16968/20864)/ 95.629% (19952/20864)
01/13/2023 23:09:37 - INFO - __main__ -   test: [batch: 163/391 ] | Loss: 0.722 | Acc: 81.269% (17060/20992)/ 95.617% (20072/20992)
01/13/2023 23:09:39 - INFO - __main__ -   test: [batch: 164/391 ] | Loss: 0.722 | Acc: 81.259% (17162/21120)/ 95.634% (20198/21120)
01/13/2023 23:09:42 - INFO - __main__ -   test: [batch: 165/391 ] | Loss: 0.724 | Acc: 81.194% (17252/21248)/ 95.633% (20320/21248)
01/13/2023 23:09:45 - INFO - __main__ -   test: [batch: 166/391 ] | Loss: 0.723 | Acc: 81.208% (17359/21376)/ 95.617% (20439/21376)
01/13/2023 23:09:47 - INFO - __main__ -   test: [batch: 167/391 ] | Loss: 0.726 | Acc: 81.148% (17450/21504)/ 95.601% (20558/21504)
01/13/2023 23:09:50 - INFO - __main__ -   test: [batch: 168/391 ] | Loss: 0.725 | Acc: 81.148% (17554/21632)/ 95.604% (20681/21632)
01/13/2023 23:09:53 - INFO - __main__ -   test: [batch: 169/391 ] | Loss: 0.727 | Acc: 81.117% (17651/21760)/ 95.565% (20795/21760)
01/13/2023 23:09:55 - INFO - __main__ -   test: [batch: 170/391 ] | Loss: 0.732 | Acc: 81.008% (17731/21888)/ 95.532% (20910/21888)
01/13/2023 23:09:58 - INFO - __main__ -   test: [batch: 171/391 ] | Loss: 0.735 | Acc: 80.955% (17823/22016)/ 95.503% (21026/22016)
01/13/2023 23:10:00 - INFO - __main__ -   test: [batch: 172/391 ] | Loss: 0.736 | Acc: 80.902% (17915/22144)/ 95.498% (21147/22144)
01/13/2023 23:10:03 - INFO - __main__ -   test: [batch: 173/391 ] | Loss: 0.739 | Acc: 80.841% (18005/22272)/ 95.470% (21263/22272)
01/13/2023 23:10:06 - INFO - __main__ -   test: [batch: 174/391 ] | Loss: 0.743 | Acc: 80.746% (18087/22400)/ 95.424% (21375/22400)
01/13/2023 23:10:08 - INFO - __main__ -   test: [batch: 175/391 ] | Loss: 0.741 | Acc: 80.797% (18202/22528)/ 95.432% (21499/22528)
01/13/2023 23:10:11 - INFO - __main__ -   test: [batch: 176/391 ] | Loss: 0.741 | Acc: 80.800% (18306/22656)/ 95.410% (21616/22656)
01/13/2023 23:10:13 - INFO - __main__ -   test: [batch: 177/391 ] | Loss: 0.745 | Acc: 80.723% (18392/22784)/ 95.370% (21729/22784)
01/13/2023 23:10:16 - INFO - __main__ -   test: [batch: 178/391 ] | Loss: 0.747 | Acc: 80.709% (18492/22912)/ 95.347% (21846/22912)
01/13/2023 23:10:18 - INFO - __main__ -   test: [batch: 179/391 ] | Loss: 0.750 | Acc: 80.651% (18582/23040)/ 95.312% (21960/23040)
01/13/2023 23:10:21 - INFO - __main__ -   test: [batch: 180/391 ] | Loss: 0.756 | Acc: 80.525% (18656/23168)/ 95.269% (22072/23168)
01/13/2023 23:10:24 - INFO - __main__ -   test: [batch: 181/391 ] | Loss: 0.762 | Acc: 80.391% (18728/23296)/ 95.209% (22180/23296)
01/13/2023 23:10:26 - INFO - __main__ -   test: [batch: 182/391 ] | Loss: 0.761 | Acc: 80.405% (18834/23424)/ 95.210% (22302/23424)
01/13/2023 23:10:29 - INFO - __main__ -   test: [batch: 183/391 ] | Loss: 0.767 | Acc: 80.286% (18909/23552)/ 95.117% (22402/23552)
01/13/2023 23:10:32 - INFO - __main__ -   test: [batch: 184/391 ] | Loss: 0.767 | Acc: 80.308% (19017/23680)/ 95.106% (22521/23680)
01/13/2023 23:10:34 - INFO - __main__ -   test: [batch: 185/391 ] | Loss: 0.767 | Acc: 80.318% (19122/23808)/ 95.098% (22641/23808)
01/13/2023 23:10:37 - INFO - __main__ -   test: [batch: 186/391 ] | Loss: 0.770 | Acc: 80.281% (19216/23936)/ 95.058% (22753/23936)
01/13/2023 23:10:39 - INFO - __main__ -   test: [batch: 187/391 ] | Loss: 0.774 | Acc: 80.161% (19290/24064)/ 95.038% (22870/24064)
01/13/2023 23:10:42 - INFO - __main__ -   test: [batch: 188/391 ] | Loss: 0.778 | Acc: 80.039% (19363/24192)/ 95.015% (22986/24192)
01/13/2023 23:10:45 - INFO - __main__ -   test: [batch: 189/391 ] | Loss: 0.779 | Acc: 79.971% (19449/24320)/ 95.016% (23108/24320)
01/13/2023 23:10:47 - INFO - __main__ -   test: [batch: 190/391 ] | Loss: 0.784 | Acc: 79.884% (19530/24448)/ 94.989% (23223/24448)
01/13/2023 23:10:50 - INFO - __main__ -   test: [batch: 191/391 ] | Loss: 0.786 | Acc: 79.846% (19623/24576)/ 94.971% (23340/24576)
01/13/2023 23:10:52 - INFO - __main__ -   test: [batch: 192/391 ] | Loss: 0.793 | Acc: 79.736% (19698/24704)/ 94.892% (23442/24704)
01/13/2023 23:10:55 - INFO - __main__ -   test: [batch: 193/391 ] | Loss: 0.792 | Acc: 79.756% (19805/24832)/ 94.890% (23563/24832)
01/13/2023 23:10:57 - INFO - __main__ -   test: [batch: 194/391 ] | Loss: 0.795 | Acc: 79.692% (19891/24960)/ 94.864% (23678/24960)
01/13/2023 23:11:00 - INFO - __main__ -   test: [batch: 195/391 ] | Loss: 0.801 | Acc: 79.616% (19974/25088)/ 94.798% (23783/25088)
01/13/2023 23:11:02 - INFO - __main__ -   test: [batch: 196/391 ] | Loss: 0.804 | Acc: 79.513% (20050/25216)/ 94.757% (23894/25216)
01/13/2023 23:11:05 - INFO - __main__ -   test: [batch: 197/391 ] | Loss: 0.808 | Acc: 79.439% (20133/25344)/ 94.740% (24011/25344)
01/13/2023 23:11:08 - INFO - __main__ -   test: [batch: 198/391 ] | Loss: 0.810 | Acc: 79.405% (20226/25472)/ 94.708% (24124/25472)
01/13/2023 23:11:10 - INFO - __main__ -   test: [batch: 199/391 ] | Loss: 0.810 | Acc: 79.379% (20321/25600)/ 94.711% (24246/25600)
01/13/2023 23:11:13 - INFO - __main__ -   test: [batch: 200/391 ] | Loss: 0.810 | Acc: 79.345% (20414/25728)/ 94.710% (24367/25728)
01/13/2023 23:11:15 - INFO - __main__ -   test: [batch: 201/391 ] | Loss: 0.813 | Acc: 79.262% (20494/25856)/ 94.670% (24478/25856)
01/13/2023 23:11:18 - INFO - __main__ -   test: [batch: 202/391 ] | Loss: 0.814 | Acc: 79.264% (20596/25984)/ 94.658% (24596/25984)
01/13/2023 23:11:21 - INFO - __main__ -   test: [batch: 203/391 ] | Loss: 0.815 | Acc: 79.255% (20695/26112)/ 94.646% (24714/26112)
01/13/2023 23:11:24 - INFO - __main__ -   test: [batch: 204/391 ] | Loss: 0.818 | Acc: 79.165% (20773/26240)/ 94.627% (24830/26240)
01/13/2023 23:11:26 - INFO - __main__ -   test: [batch: 205/391 ] | Loss: 0.821 | Acc: 79.088% (20854/26368)/ 94.596% (24943/26368)
01/13/2023 23:11:29 - INFO - __main__ -   test: [batch: 206/391 ] | Loss: 0.823 | Acc: 79.065% (20949/26496)/ 94.592% (25063/26496)
01/13/2023 23:11:31 - INFO - __main__ -   test: [batch: 207/391 ] | Loss: 0.826 | Acc: 78.981% (21028/26624)/ 94.554% (25174/26624)
01/13/2023 23:11:34 - INFO - __main__ -   test: [batch: 208/391 ] | Loss: 0.828 | Acc: 78.944% (21119/26752)/ 94.524% (25287/26752)
01/13/2023 23:11:37 - INFO - __main__ -   test: [batch: 209/391 ] | Loss: 0.828 | Acc: 78.940% (21219/26880)/ 94.542% (25413/26880)
01/13/2023 23:11:39 - INFO - __main__ -   test: [batch: 210/391 ] | Loss: 0.829 | Acc: 78.921% (21315/27008)/ 94.528% (25530/27008)
01/13/2023 23:11:42 - INFO - __main__ -   test: [batch: 211/391 ] | Loss: 0.831 | Acc: 78.855% (21398/27136)/ 94.502% (25644/27136)
01/13/2023 23:11:45 - INFO - __main__ -   test: [batch: 212/391 ] | Loss: 0.834 | Acc: 78.793% (21482/27264)/ 94.476% (25758/27264)
01/13/2023 23:11:47 - INFO - __main__ -   test: [batch: 213/391 ] | Loss: 0.833 | Acc: 78.789% (21582/27392)/ 94.484% (25881/27392)
01/13/2023 23:11:50 - INFO - __main__ -   test: [batch: 214/391 ] | Loss: 0.834 | Acc: 78.768% (21677/27520)/ 94.491% (26004/27520)
01/13/2023 23:11:52 - INFO - __main__ -   test: [batch: 215/391 ] | Loss: 0.834 | Acc: 78.787% (21783/27648)/ 94.481% (26122/27648)
01/13/2023 23:11:55 - INFO - __main__ -   test: [batch: 216/391 ] | Loss: 0.832 | Acc: 78.827% (21895/27776)/ 94.499% (26248/27776)
01/13/2023 23:11:58 - INFO - __main__ -   test: [batch: 217/391 ] | Loss: 0.837 | Acc: 78.763% (21978/27904)/ 94.431% (26350/27904)
01/13/2023 23:12:00 - INFO - __main__ -   test: [batch: 218/391 ] | Loss: 0.840 | Acc: 78.706% (22063/28032)/ 94.392% (26460/28032)
01/13/2023 23:12:03 - INFO - __main__ -   test: [batch: 219/391 ] | Loss: 0.838 | Acc: 78.739% (22173/28160)/ 94.403% (26584/28160)
01/13/2023 23:12:06 - INFO - __main__ -   test: [batch: 220/391 ] | Loss: 0.836 | Acc: 78.775% (22284/28288)/ 94.418% (26709/28288)
01/13/2023 23:12:08 - INFO - __main__ -   test: [batch: 221/391 ] | Loss: 0.838 | Acc: 78.734% (22373/28416)/ 94.398% (26824/28416)
01/13/2023 23:12:11 - INFO - __main__ -   test: [batch: 222/391 ] | Loss: 0.836 | Acc: 78.791% (22490/28544)/ 94.412% (26949/28544)
01/13/2023 23:12:13 - INFO - __main__ -   test: [batch: 223/391 ] | Loss: 0.837 | Acc: 78.795% (22592/28672)/ 94.402% (27067/28672)
01/13/2023 23:12:16 - INFO - __main__ -   test: [batch: 224/391 ] | Loss: 0.836 | Acc: 78.819% (22700/28800)/ 94.396% (27186/28800)
01/13/2023 23:12:18 - INFO - __main__ -   test: [batch: 225/391 ] | Loss: 0.835 | Acc: 78.820% (22801/28928)/ 94.403% (27309/28928)
01/13/2023 23:12:21 - INFO - __main__ -   test: [batch: 226/391 ] | Loss: 0.835 | Acc: 78.793% (22894/29056)/ 94.414% (27433/29056)
01/13/2023 23:12:23 - INFO - __main__ -   test: [batch: 227/391 ] | Loss: 0.838 | Acc: 78.779% (22991/29184)/ 94.401% (27550/29184)
01/13/2023 23:12:26 - INFO - __main__ -   test: [batch: 228/391 ] | Loss: 0.842 | Acc: 78.708% (23071/29312)/ 94.344% (27654/29312)
01/13/2023 23:12:29 - INFO - __main__ -   test: [batch: 229/391 ] | Loss: 0.845 | Acc: 78.655% (23156/29440)/ 94.297% (27761/29440)
01/13/2023 23:12:31 - INFO - __main__ -   test: [batch: 230/391 ] | Loss: 0.848 | Acc: 78.595% (23239/29568)/ 94.264% (27872/29568)
01/13/2023 23:12:34 - INFO - __main__ -   test: [batch: 231/391 ] | Loss: 0.848 | Acc: 78.583% (23336/29696)/ 94.248% (27988/29696)
01/13/2023 23:12:37 - INFO - __main__ -   test: [batch: 232/391 ] | Loss: 0.847 | Acc: 78.615% (23446/29824)/ 94.263% (28113/29824)
01/13/2023 23:12:39 - INFO - __main__ -   test: [batch: 233/391 ] | Loss: 0.849 | Acc: 78.556% (23529/29952)/ 94.241% (28227/29952)
01/13/2023 23:12:42 - INFO - __main__ -   test: [batch: 234/391 ] | Loss: 0.856 | Acc: 78.441% (23595/30080)/ 94.172% (28327/30080)
01/13/2023 23:12:44 - INFO - __main__ -   test: [batch: 235/391 ] | Loss: 0.856 | Acc: 78.430% (23692/30208)/ 94.177% (28449/30208)
01/13/2023 23:12:47 - INFO - __main__ -   test: [batch: 236/391 ] | Loss: 0.854 | Acc: 78.478% (23807/30336)/ 94.182% (28571/30336)
01/13/2023 23:12:50 - INFO - __main__ -   test: [batch: 237/391 ] | Loss: 0.855 | Acc: 78.483% (23909/30464)/ 94.147% (28681/30464)
01/13/2023 23:12:52 - INFO - __main__ -   test: [batch: 238/391 ] | Loss: 0.853 | Acc: 78.524% (24022/30592)/ 94.155% (28804/30592)
01/13/2023 23:12:55 - INFO - __main__ -   test: [batch: 239/391 ] | Loss: 0.852 | Acc: 78.561% (24134/30720)/ 94.157% (28925/30720)
01/13/2023 23:12:57 - INFO - __main__ -   test: [batch: 240/391 ] | Loss: 0.852 | Acc: 78.579% (24240/30848)/ 94.149% (29043/30848)
01/13/2023 23:13:00 - INFO - __main__ -   test: [batch: 241/391 ] | Loss: 0.855 | Acc: 78.496% (24315/30976)/ 94.108% (29151/30976)
01/13/2023 23:13:03 - INFO - __main__ -   test: [batch: 242/391 ] | Loss: 0.858 | Acc: 78.389% (24382/31104)/ 94.091% (29266/31104)
01/13/2023 23:13:05 - INFO - __main__ -   test: [batch: 243/391 ] | Loss: 0.864 | Acc: 78.256% (24441/31232)/ 94.022% (29365/31232)
01/13/2023 23:13:07 - INFO - __main__ -   test: [batch: 244/391 ] | Loss: 0.864 | Acc: 78.262% (24543/31360)/ 94.018% (29484/31360)
01/13/2023 23:13:10 - INFO - __main__ -   test: [batch: 245/391 ] | Loss: 0.863 | Acc: 78.274% (24647/31488)/ 94.007% (29601/31488)
01/13/2023 23:13:13 - INFO - __main__ -   test: [batch: 246/391 ] | Loss: 0.864 | Acc: 78.252% (24740/31616)/ 94.000% (29719/31616)
01/13/2023 23:13:15 - INFO - __main__ -   test: [batch: 247/391 ] | Loss: 0.869 | Acc: 78.157% (24810/31744)/ 93.923% (29815/31744)
01/13/2023 23:13:18 - INFO - __main__ -   test: [batch: 248/391 ] | Loss: 0.870 | Acc: 78.122% (24899/31872)/ 93.916% (29933/31872)
01/13/2023 23:13:20 - INFO - __main__ -   test: [batch: 249/391 ] | Loss: 0.872 | Acc: 77.991% (24957/32000)/ 93.906% (30050/32000)
01/13/2023 23:13:23 - INFO - __main__ -   test: [batch: 250/391 ] | Loss: 0.871 | Acc: 78.025% (25068/32128)/ 93.924% (30176/32128)
01/13/2023 23:13:25 - INFO - __main__ -   test: [batch: 251/391 ] | Loss: 0.873 | Acc: 77.985% (25155/32256)/ 93.896% (30287/32256)
01/13/2023 23:13:28 - INFO - __main__ -   test: [batch: 252/391 ] | Loss: 0.872 | Acc: 78.014% (25264/32384)/ 93.889% (30405/32384)
01/13/2023 23:13:31 - INFO - __main__ -   test: [batch: 253/391 ] | Loss: 0.873 | Acc: 77.984% (25354/32512)/ 93.876% (30521/32512)
01/13/2023 23:13:33 - INFO - __main__ -   test: [batch: 254/391 ] | Loss: 0.878 | Acc: 77.920% (25433/32640)/ 93.836% (30628/32640)
01/13/2023 23:13:36 - INFO - __main__ -   test: [batch: 255/391 ] | Loss: 0.880 | Acc: 77.884% (25521/32768)/ 93.826% (30745/32768)
01/13/2023 23:13:39 - INFO - __main__ -   test: [batch: 256/391 ] | Loss: 0.885 | Acc: 77.766% (25582/32896)/ 93.796% (30855/32896)
01/13/2023 23:13:41 - INFO - __main__ -   test: [batch: 257/391 ] | Loss: 0.886 | Acc: 77.746% (25675/33024)/ 93.783% (30971/33024)
01/13/2023 23:13:44 - INFO - __main__ -   test: [batch: 258/391 ] | Loss: 0.887 | Acc: 77.736% (25771/33152)/ 93.768% (31086/33152)
01/13/2023 23:13:46 - INFO - __main__ -   test: [batch: 259/391 ] | Loss: 0.890 | Acc: 77.620% (25832/33280)/ 93.756% (31202/33280)
01/13/2023 23:13:49 - INFO - __main__ -   test: [batch: 260/391 ] | Loss: 0.891 | Acc: 77.592% (25922/33408)/ 93.747% (31319/33408)
01/13/2023 23:13:52 - INFO - __main__ -   test: [batch: 261/391 ] | Loss: 0.889 | Acc: 77.639% (26037/33536)/ 93.768% (31446/33536)
01/13/2023 23:13:54 - INFO - __main__ -   test: [batch: 262/391 ] | Loss: 0.888 | Acc: 77.668% (26146/33664)/ 93.777% (31569/33664)
01/13/2023 23:13:57 - INFO - __main__ -   test: [batch: 263/391 ] | Loss: 0.891 | Acc: 77.595% (26221/33792)/ 93.753% (31681/33792)
01/13/2023 23:14:00 - INFO - __main__ -   test: [batch: 264/391 ] | Loss: 0.896 | Acc: 77.532% (26299/33920)/ 93.688% (31779/33920)
01/13/2023 23:14:02 - INFO - __main__ -   test: [batch: 265/391 ] | Loss: 0.895 | Acc: 77.555% (26406/34048)/ 93.691% (31900/34048)
01/13/2023 23:14:05 - INFO - __main__ -   test: [batch: 266/391 ] | Loss: 0.898 | Acc: 77.487% (26482/34176)/ 93.674% (32014/34176)
01/13/2023 23:14:07 - INFO - __main__ -   test: [batch: 267/391 ] | Loss: 0.897 | Acc: 77.522% (26593/34304)/ 93.674% (32134/34304)
01/13/2023 23:14:10 - INFO - __main__ -   test: [batch: 268/391 ] | Loss: 0.896 | Acc: 77.547% (26701/34432)/ 93.672% (32253/34432)
01/13/2023 23:14:13 - INFO - __main__ -   test: [batch: 269/391 ] | Loss: 0.898 | Acc: 77.500% (26784/34560)/ 93.652% (32366/34560)
01/13/2023 23:14:15 - INFO - __main__ -   test: [batch: 270/391 ] | Loss: 0.901 | Acc: 77.439% (26862/34688)/ 93.620% (32475/34688)
01/13/2023 23:14:18 - INFO - __main__ -   test: [batch: 271/391 ] | Loss: 0.901 | Acc: 77.450% (26965/34816)/ 93.615% (32593/34816)
01/13/2023 23:14:21 - INFO - __main__ -   test: [batch: 272/391 ] | Loss: 0.902 | Acc: 77.415% (27052/34944)/ 93.613% (32712/34944)
01/13/2023 23:14:23 - INFO - __main__ -   test: [batch: 273/391 ] | Loss: 0.902 | Acc: 77.421% (27153/35072)/ 93.599% (32827/35072)
01/13/2023 23:14:26 - INFO - __main__ -   test: [batch: 274/391 ] | Loss: 0.902 | Acc: 77.418% (27251/35200)/ 93.602% (32948/35200)
01/13/2023 23:14:28 - INFO - __main__ -   test: [batch: 275/391 ] | Loss: 0.904 | Acc: 77.383% (27338/35328)/ 93.597% (33066/35328)
01/13/2023 23:14:31 - INFO - __main__ -   test: [batch: 276/391 ] | Loss: 0.905 | Acc: 77.355% (27427/35456)/ 93.595% (33185/35456)
01/13/2023 23:14:34 - INFO - __main__ -   test: [batch: 277/391 ] | Loss: 0.906 | Acc: 77.327% (27516/35584)/ 93.584% (33301/35584)
01/13/2023 23:14:36 - INFO - __main__ -   test: [batch: 278/391 ] | Loss: 0.905 | Acc: 77.335% (27618/35712)/ 93.582% (33420/35712)
01/13/2023 23:14:39 - INFO - __main__ -   test: [batch: 279/391 ] | Loss: 0.906 | Acc: 77.347% (27721/35840)/ 93.577% (33538/35840)
01/13/2023 23:14:42 - INFO - __main__ -   test: [batch: 280/391 ] | Loss: 0.906 | Acc: 77.327% (27813/35968)/ 93.569% (33655/35968)
01/13/2023 23:14:44 - INFO - __main__ -   test: [batch: 281/391 ] | Loss: 0.907 | Acc: 77.335% (27915/36096)/ 93.564% (33773/36096)
01/13/2023 23:14:47 - INFO - __main__ -   test: [batch: 282/391 ] | Loss: 0.906 | Acc: 77.369% (28026/36224)/ 93.571% (33895/36224)
01/13/2023 23:14:49 - INFO - __main__ -   test: [batch: 283/391 ] | Loss: 0.906 | Acc: 77.341% (28115/36352)/ 93.566% (34013/36352)
01/13/2023 23:14:52 - INFO - __main__ -   test: [batch: 284/391 ] | Loss: 0.910 | Acc: 77.294% (28197/36480)/ 93.531% (34120/36480)
01/13/2023 23:14:55 - INFO - __main__ -   test: [batch: 285/391 ] | Loss: 0.912 | Acc: 77.251% (28280/36608)/ 93.485% (34223/36608)
01/13/2023 23:14:57 - INFO - __main__ -   test: [batch: 286/391 ] | Loss: 0.913 | Acc: 77.213% (28365/36736)/ 93.472% (34338/36736)
01/13/2023 23:15:00 - INFO - __main__ -   test: [batch: 287/391 ] | Loss: 0.913 | Acc: 77.214% (28464/36864)/ 93.462% (34454/36864)
01/13/2023 23:15:03 - INFO - __main__ -   test: [batch: 288/391 ] | Loss: 0.913 | Acc: 77.230% (28569/36992)/ 93.463% (34574/36992)
01/13/2023 23:15:05 - INFO - __main__ -   test: [batch: 289/391 ] | Loss: 0.915 | Acc: 77.190% (28653/37120)/ 93.432% (34682/37120)
01/13/2023 23:15:08 - INFO - __main__ -   test: [batch: 290/391 ] | Loss: 0.916 | Acc: 77.107% (28721/37248)/ 93.436% (34803/37248)
01/13/2023 23:15:10 - INFO - __main__ -   test: [batch: 291/391 ] | Loss: 0.916 | Acc: 77.122% (28825/37376)/ 93.426% (34919/37376)
01/13/2023 23:15:13 - INFO - __main__ -   test: [batch: 292/391 ] | Loss: 0.918 | Acc: 77.072% (28905/37504)/ 93.409% (35032/37504)
01/13/2023 23:15:16 - INFO - __main__ -   test: [batch: 293/391 ] | Loss: 0.918 | Acc: 77.067% (29002/37632)/ 93.402% (35149/37632)
01/13/2023 23:15:18 - INFO - __main__ -   test: [batch: 294/391 ] | Loss: 0.919 | Acc: 77.042% (29091/37760)/ 93.395% (35266/37760)
01/13/2023 23:15:21 - INFO - __main__ -   test: [batch: 295/391 ] | Loss: 0.918 | Acc: 77.072% (29201/37888)/ 93.404% (35389/37888)
01/13/2023 23:15:23 - INFO - __main__ -   test: [batch: 296/391 ] | Loss: 0.918 | Acc: 77.073% (29300/38016)/ 93.398% (35506/38016)
01/13/2023 23:15:26 - INFO - __main__ -   test: [batch: 297/391 ] | Loss: 0.920 | Acc: 77.055% (29392/38144)/ 93.372% (35616/38144)
01/13/2023 23:15:29 - INFO - __main__ -   test: [batch: 298/391 ] | Loss: 0.922 | Acc: 77.028% (29480/38272)/ 93.350% (35727/38272)
01/13/2023 23:15:31 - INFO - __main__ -   test: [batch: 299/391 ] | Loss: 0.923 | Acc: 77.018% (29575/38400)/ 93.328% (35838/38400)
01/13/2023 23:15:34 - INFO - __main__ -   test: [batch: 300/391 ] | Loss: 0.924 | Acc: 77.022% (29675/38528)/ 93.324% (35956/38528)
01/13/2023 23:15:37 - INFO - __main__ -   test: [batch: 301/391 ] | Loss: 0.924 | Acc: 77.007% (29768/38656)/ 93.310% (36070/38656)
01/13/2023 23:15:39 - INFO - __main__ -   test: [batch: 302/391 ] | Loss: 0.926 | Acc: 76.967% (29851/38784)/ 93.294% (36183/38784)
01/13/2023 23:15:42 - INFO - __main__ -   test: [batch: 303/391 ] | Loss: 0.927 | Acc: 76.943% (29940/38912)/ 93.280% (36297/38912)
01/13/2023 23:15:44 - INFO - __main__ -   test: [batch: 304/391 ] | Loss: 0.926 | Acc: 76.952% (30042/39040)/ 93.284% (36418/39040)
01/13/2023 23:15:47 - INFO - __main__ -   test: [batch: 305/391 ] | Loss: 0.927 | Acc: 76.907% (30123/39168)/ 93.278% (36535/39168)
01/13/2023 23:15:50 - INFO - __main__ -   test: [batch: 306/391 ] | Loss: 0.929 | Acc: 76.888% (30214/39296)/ 93.251% (36644/39296)
01/13/2023 23:15:52 - INFO - __main__ -   test: [batch: 307/391 ] | Loss: 0.929 | Acc: 76.892% (30314/39424)/ 93.238% (36758/39424)
01/13/2023 23:15:55 - INFO - __main__ -   test: [batch: 308/391 ] | Loss: 0.930 | Acc: 76.873% (30405/39552)/ 93.239% (36878/39552)
01/13/2023 23:15:57 - INFO - __main__ -   test: [batch: 309/391 ] | Loss: 0.931 | Acc: 76.870% (30502/39680)/ 93.218% (36989/39680)
01/13/2023 23:16:00 - INFO - __main__ -   test: [batch: 310/391 ] | Loss: 0.931 | Acc: 76.861% (30597/39808)/ 93.205% (37103/39808)
01/13/2023 23:16:02 - INFO - __main__ -   test: [batch: 311/391 ] | Loss: 0.933 | Acc: 76.835% (30685/39936)/ 93.194% (37218/39936)
01/13/2023 23:16:05 - INFO - __main__ -   test: [batch: 312/391 ] | Loss: 0.934 | Acc: 76.820% (30777/40064)/ 93.171% (37328/40064)
01/13/2023 23:16:08 - INFO - __main__ -   test: [batch: 313/391 ] | Loss: 0.932 | Acc: 76.859% (30891/40192)/ 93.190% (37455/40192)
01/13/2023 23:16:10 - INFO - __main__ -   test: [batch: 314/391 ] | Loss: 0.933 | Acc: 76.850% (30986/40320)/ 93.185% (37572/40320)
01/13/2023 23:16:13 - INFO - __main__ -   test: [batch: 315/391 ] | Loss: 0.934 | Acc: 76.832% (31077/40448)/ 93.167% (37684/40448)
01/13/2023 23:16:15 - INFO - __main__ -   test: [batch: 316/391 ] | Loss: 0.936 | Acc: 76.740% (31138/40576)/ 93.139% (37792/40576)
01/13/2023 23:16:18 - INFO - __main__ -   test: [batch: 317/391 ] | Loss: 0.938 | Acc: 76.698% (31219/40704)/ 93.111% (37900/40704)
01/13/2023 23:16:20 - INFO - __main__ -   test: [batch: 318/391 ] | Loss: 0.937 | Acc: 76.731% (31331/40832)/ 93.128% (38026/40832)
01/13/2023 23:16:23 - INFO - __main__ -   test: [batch: 319/391 ] | Loss: 0.939 | Acc: 76.675% (31406/40960)/ 93.101% (38134/40960)
01/13/2023 23:16:26 - INFO - __main__ -   test: [batch: 320/391 ] | Loss: 0.938 | Acc: 76.709% (31518/41088)/ 93.107% (38256/41088)
01/13/2023 23:16:28 - INFO - __main__ -   test: [batch: 321/391 ] | Loss: 0.938 | Acc: 76.730% (31625/41216)/ 93.100% (38372/41216)
01/13/2023 23:16:31 - INFO - __main__ -   test: [batch: 322/391 ] | Loss: 0.939 | Acc: 76.705% (31713/41344)/ 93.092% (38488/41344)
01/13/2023 23:16:33 - INFO - __main__ -   test: [batch: 323/391 ] | Loss: 0.942 | Acc: 76.666% (31795/41472)/ 93.072% (38599/41472)
01/13/2023 23:16:36 - INFO - __main__ -   test: [batch: 324/391 ] | Loss: 0.942 | Acc: 76.661% (31891/41600)/ 93.065% (38715/41600)
01/13/2023 23:16:38 - INFO - __main__ -   test: [batch: 325/391 ] | Loss: 0.942 | Acc: 76.670% (31993/41728)/ 93.067% (38835/41728)
01/13/2023 23:16:41 - INFO - __main__ -   test: [batch: 326/391 ] | Loss: 0.945 | Acc: 76.586% (32056/41856)/ 93.038% (38942/41856)
01/13/2023 23:16:44 - INFO - __main__ -   test: [batch: 327/391 ] | Loss: 0.948 | Acc: 76.512% (32123/41984)/ 93.002% (39046/41984)
01/13/2023 23:16:46 - INFO - __main__ -   test: [batch: 328/391 ] | Loss: 0.950 | Acc: 76.460% (32199/42112)/ 92.976% (39154/42112)
01/13/2023 23:16:49 - INFO - __main__ -   test: [batch: 329/391 ] | Loss: 0.950 | Acc: 76.449% (32292/42240)/ 92.978% (39274/42240)
01/13/2023 23:16:51 - INFO - __main__ -   test: [batch: 330/391 ] | Loss: 0.952 | Acc: 76.407% (32372/42368)/ 92.955% (39383/42368)
01/13/2023 23:16:54 - INFO - __main__ -   test: [batch: 331/391 ] | Loss: 0.952 | Acc: 76.374% (32456/42496)/ 92.964% (39506/42496)
01/13/2023 23:16:57 - INFO - __main__ -   test: [batch: 332/391 ] | Loss: 0.953 | Acc: 76.363% (32549/42624)/ 92.964% (39625/42624)
01/13/2023 23:16:59 - INFO - __main__ -   test: [batch: 333/391 ] | Loss: 0.951 | Acc: 76.387% (32657/42752)/ 92.973% (39748/42752)
01/13/2023 23:17:02 - INFO - __main__ -   test: [batch: 334/391 ] | Loss: 0.953 | Acc: 76.362% (32744/42880)/ 92.957% (39860/42880)
01/13/2023 23:17:05 - INFO - __main__ -   test: [batch: 335/391 ] | Loss: 0.954 | Acc: 76.337% (32831/43008)/ 92.943% (39973/43008)
01/13/2023 23:17:07 - INFO - __main__ -   test: [batch: 336/391 ] | Loss: 0.955 | Acc: 76.294% (32910/43136)/ 92.927% (40085/43136)
01/13/2023 23:17:10 - INFO - __main__ -   test: [batch: 337/391 ] | Loss: 0.955 | Acc: 76.290% (33006/43264)/ 92.923% (40202/43264)
01/13/2023 23:17:13 - INFO - __main__ -   test: [batch: 338/391 ] | Loss: 0.955 | Acc: 76.281% (33100/43392)/ 92.932% (40325/43392)
01/13/2023 23:17:15 - INFO - __main__ -   test: [batch: 339/391 ] | Loss: 0.958 | Acc: 76.245% (33182/43520)/ 92.907% (40433/43520)
01/13/2023 23:17:18 - INFO - __main__ -   test: [batch: 340/391 ] | Loss: 0.958 | Acc: 76.239% (33277/43648)/ 92.918% (40557/43648)
01/13/2023 23:17:20 - INFO - __main__ -   test: [batch: 341/391 ] | Loss: 0.956 | Acc: 76.275% (33390/43776)/ 92.937% (40684/43776)
01/13/2023 23:17:23 - INFO - __main__ -   test: [batch: 342/391 ] | Loss: 0.957 | Acc: 76.221% (33464/43904)/ 92.932% (40801/43904)
01/13/2023 23:17:26 - INFO - __main__ -   test: [batch: 343/391 ] | Loss: 0.956 | Acc: 76.222% (33562/44032)/ 92.935% (40921/44032)
01/13/2023 23:17:28 - INFO - __main__ -   test: [batch: 344/391 ] | Loss: 0.957 | Acc: 76.218% (33658/44160)/ 92.923% (41035/44160)
01/13/2023 23:17:31 - INFO - __main__ -   test: [batch: 345/391 ] | Loss: 0.960 | Acc: 76.154% (33727/44288)/ 92.885% (41137/44288)
01/13/2023 23:17:33 - INFO - __main__ -   test: [batch: 346/391 ] | Loss: 0.962 | Acc: 76.137% (33817/44416)/ 92.876% (41252/44416)
01/13/2023 23:17:36 - INFO - __main__ -   test: [batch: 347/391 ] | Loss: 0.961 | Acc: 76.154% (33922/44544)/ 92.886% (41375/44544)
01/13/2023 23:17:38 - INFO - __main__ -   test: [batch: 348/391 ] | Loss: 0.963 | Acc: 76.133% (34010/44672)/ 92.861% (41483/44672)
01/13/2023 23:17:41 - INFO - __main__ -   test: [batch: 349/391 ] | Loss: 0.962 | Acc: 76.129% (34106/44800)/ 92.868% (41605/44800)
01/13/2023 23:17:43 - INFO - __main__ -   test: [batch: 350/391 ] | Loss: 0.962 | Acc: 76.137% (34207/44928)/ 92.866% (41723/44928)
01/13/2023 23:17:46 - INFO - __main__ -   test: [batch: 351/391 ] | Loss: 0.964 | Acc: 76.085% (34281/45056)/ 92.849% (41834/45056)
01/13/2023 23:17:49 - INFO - __main__ -   test: [batch: 352/391 ] | Loss: 0.965 | Acc: 76.082% (34377/45184)/ 92.847% (41952/45184)
01/13/2023 23:17:51 - INFO - __main__ -   test: [batch: 353/391 ] | Loss: 0.967 | Acc: 76.039% (34455/45312)/ 92.810% (42054/45312)
01/13/2023 23:17:54 - INFO - __main__ -   test: [batch: 354/391 ] | Loss: 0.970 | Acc: 75.977% (34524/45440)/ 92.788% (42163/45440)
01/13/2023 23:17:56 - INFO - __main__ -   test: [batch: 355/391 ] | Loss: 0.972 | Acc: 75.913% (34592/45568)/ 92.780% (42278/45568)
01/13/2023 23:17:59 - INFO - __main__ -   test: [batch: 356/391 ] | Loss: 0.973 | Acc: 75.910% (34688/45696)/ 92.783% (42398/45696)
01/13/2023 23:18:02 - INFO - __main__ -   test: [batch: 357/391 ] | Loss: 0.971 | Acc: 75.945% (34801/45824)/ 92.796% (42523/45824)
01/13/2023 23:18:04 - INFO - __main__ -   test: [batch: 358/391 ] | Loss: 0.970 | Acc: 75.975% (34912/45952)/ 92.799% (42643/45952)
01/13/2023 23:18:07 - INFO - __main__ -   test: [batch: 359/391 ] | Loss: 0.971 | Acc: 75.985% (35014/46080)/ 92.791% (42758/46080)
01/13/2023 23:18:09 - INFO - __main__ -   test: [batch: 360/391 ] | Loss: 0.972 | Acc: 75.959% (35099/46208)/ 92.780% (42872/46208)
01/13/2023 23:18:12 - INFO - __main__ -   test: [batch: 361/391 ] | Loss: 0.972 | Acc: 75.963% (35198/46336)/ 92.787% (42994/46336)
01/13/2023 23:18:15 - INFO - __main__ -   test: [batch: 362/391 ] | Loss: 0.972 | Acc: 75.958% (35293/46464)/ 92.801% (43119/46464)
01/13/2023 23:18:17 - INFO - __main__ -   test: [batch: 363/391 ] | Loss: 0.972 | Acc: 75.951% (35387/46592)/ 92.795% (43235/46592)
01/13/2023 23:18:20 - INFO - __main__ -   test: [batch: 364/391 ] | Loss: 0.971 | Acc: 75.980% (35498/46720)/ 92.804% (43358/46720)
01/13/2023 23:18:23 - INFO - __main__ -   test: [batch: 365/391 ] | Loss: 0.970 | Acc: 75.995% (35602/46848)/ 92.809% (43479/46848)
01/13/2023 23:18:25 - INFO - __main__ -   test: [batch: 366/391 ] | Loss: 0.969 | Acc: 76.030% (35716/46976)/ 92.826% (43606/46976)
01/13/2023 23:18:28 - INFO - __main__ -   test: [batch: 367/391 ] | Loss: 0.968 | Acc: 76.053% (35824/47104)/ 92.839% (43731/47104)
01/13/2023 23:18:30 - INFO - __main__ -   test: [batch: 368/391 ] | Loss: 0.967 | Acc: 76.056% (35923/47232)/ 92.850% (43855/47232)
01/13/2023 23:18:33 - INFO - __main__ -   test: [batch: 369/391 ] | Loss: 0.966 | Acc: 76.081% (36032/47360)/ 92.859% (43978/47360)
01/13/2023 23:18:35 - INFO - __main__ -   test: [batch: 370/391 ] | Loss: 0.967 | Acc: 76.068% (36123/47488)/ 92.861% (44098/47488)
01/13/2023 23:18:38 - INFO - __main__ -   test: [batch: 371/391 ] | Loss: 0.966 | Acc: 76.075% (36224/47616)/ 92.864% (44218/47616)
01/13/2023 23:18:41 - INFO - __main__ -   test: [batch: 372/391 ] | Loss: 0.965 | Acc: 76.121% (36343/47744)/ 92.879% (44344/47744)
01/13/2023 23:18:43 - INFO - __main__ -   test: [batch: 373/391 ] | Loss: 0.963 | Acc: 76.155% (36457/47872)/ 92.885% (44466/47872)
01/13/2023 23:18:46 - INFO - __main__ -   test: [batch: 374/391 ] | Loss: 0.962 | Acc: 76.185% (36569/48000)/ 92.890% (44587/48000)
01/13/2023 23:18:49 - INFO - __main__ -   test: [batch: 375/391 ] | Loss: 0.965 | Acc: 76.116% (36633/48128)/ 92.854% (44689/48128)
01/13/2023 23:18:51 - INFO - __main__ -   test: [batch: 376/391 ] | Loss: 0.966 | Acc: 76.111% (36728/48256)/ 92.840% (44801/48256)
01/13/2023 23:18:54 - INFO - __main__ -   test: [batch: 377/391 ] | Loss: 0.966 | Acc: 76.097% (36819/48384)/ 92.832% (44916/48384)
01/13/2023 23:18:56 - INFO - __main__ -   test: [batch: 378/391 ] | Loss: 0.970 | Acc: 76.022% (36880/48512)/ 92.791% (45015/48512)
01/13/2023 23:18:59 - INFO - __main__ -   test: [batch: 379/391 ] | Loss: 0.970 | Acc: 76.001% (36967/48640)/ 92.798% (45137/48640)
01/13/2023 23:19:01 - INFO - __main__ -   test: [batch: 380/391 ] | Loss: 0.969 | Acc: 75.999% (37063/48768)/ 92.807% (45260/48768)
01/13/2023 23:19:04 - INFO - __main__ -   test: [batch: 381/391 ] | Loss: 0.971 | Acc: 75.951% (37137/48896)/ 92.809% (45380/48896)
01/13/2023 23:19:07 - INFO - __main__ -   test: [batch: 382/391 ] | Loss: 0.973 | Acc: 75.920% (37219/49024)/ 92.795% (45492/49024)
01/13/2023 23:19:09 - INFO - __main__ -   test: [batch: 383/391 ] | Loss: 0.973 | Acc: 75.924% (37318/49152)/ 92.788% (45607/49152)
01/13/2023 23:19:12 - INFO - __main__ -   test: [batch: 384/391 ] | Loss: 0.971 | Acc: 75.964% (37435/49280)/ 92.800% (45732/49280)
01/13/2023 23:19:14 - INFO - __main__ -   test: [batch: 385/391 ] | Loss: 0.970 | Acc: 75.974% (37537/49408)/ 92.811% (45856/49408)
01/13/2023 23:19:17 - INFO - __main__ -   test: [batch: 386/391 ] | Loss: 0.968 | Acc: 76.017% (37656/49536)/ 92.827% (45983/49536)
01/13/2023 23:19:20 - INFO - __main__ -   test: [batch: 387/391 ] | Loss: 0.966 | Acc: 76.057% (37773/49664)/ 92.838% (46107/49664)
01/13/2023 23:19:22 - INFO - __main__ -   test: [batch: 388/391 ] | Loss: 0.964 | Acc: 76.103% (37893/49792)/ 92.850% (46232/49792)
01/13/2023 23:19:25 - INFO - __main__ -   test: [batch: 389/391 ] | Loss: 0.964 | Acc: 76.106% (37992/49920)/ 92.853% (46352/49920)
01/13/2023 23:19:28 - INFO - __main__ -   test: [batch: 390/391 ] | Loss: 0.966 | Acc: 76.058% (38029/50000)/ 92.842% (46421/50000)
01/13/2023 23:19:28 - INFO - __main__ -   Final accuracy: 76.058
01/13/2023 23:19:28 - INFO - __main__ -   {'milestones': Counter({2: 1, 3: 1, 4: 1}), 'gamma': 0.1, 'base_lrs': [0.00025], 'last_epoch': 1, '_step_count': 2, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.00025]}
01/13/2023 23:19:28 - INFO - __main__ -   
Epoch: 1
01/13/2023 23:19:30 - INFO - __main__ -   test: [epoch: 1 | batch: 0/10010 ] | Loss: 0.624 | Acc: 86.719% (111/128)
01/13/2023 23:23:51 - INFO - __main__ -   test: [epoch: 1 | batch: 100/10010 ] | Loss: 0.829 | Acc: 79.749% (10310/12928)
01/13/2023 23:28:14 - INFO - __main__ -   test: [epoch: 1 | batch: 200/10010 ] | Loss: 0.828 | Acc: 79.373% (20421/25728)
01/13/2023 23:32:35 - INFO - __main__ -   test: [epoch: 1 | batch: 300/10010 ] | Loss: 0.841 | Acc: 79.119% (30483/38528)
01/13/2023 23:36:56 - INFO - __main__ -   test: [epoch: 1 | batch: 400/10010 ] | Loss: 0.854 | Acc: 78.871% (40483/51328)
01/13/2023 23:41:17 - INFO - __main__ -   test: [epoch: 1 | batch: 500/10010 ] | Loss: 0.854 | Acc: 78.830% (50552/64128)
01/13/2023 23:45:39 - INFO - __main__ -   test: [epoch: 1 | batch: 600/10010 ] | Loss: 0.857 | Acc: 78.749% (60580/76928)
01/13/2023 23:50:00 - INFO - __main__ -   test: [epoch: 1 | batch: 700/10010 ] | Loss: 0.857 | Acc: 78.705% (70620/89728)
01/13/2023 23:54:23 - INFO - __main__ -   test: [epoch: 1 | batch: 800/10010 ] | Loss: 0.856 | Acc: 78.706% (80696/102528)
01/13/2023 23:58:46 - INFO - __main__ -   test: [epoch: 1 | batch: 900/10010 ] | Loss: 0.857 | Acc: 78.674% (90733/115328)
01/14/2023 00:03:07 - INFO - __main__ -   test: [epoch: 1 | batch: 1000/10010 ] | Loss: 0.857 | Acc: 78.625% (100741/128128)
01/14/2023 00:07:30 - INFO - __main__ -   test: [epoch: 1 | batch: 1100/10010 ] | Loss: 0.859 | Acc: 78.588% (110753/140928)
01/14/2023 00:11:51 - INFO - __main__ -   test: [epoch: 1 | batch: 1200/10010 ] | Loss: 0.859 | Acc: 78.608% (120842/153728)
01/14/2023 00:16:12 - INFO - __main__ -   test: [epoch: 1 | batch: 1300/10010 ] | Loss: 0.856 | Acc: 78.677% (131019/166528)
01/14/2023 00:20:33 - INFO - __main__ -   test: [epoch: 1 | batch: 1400/10010 ] | Loss: 0.856 | Acc: 78.685% (141105/179328)
01/14/2023 00:24:54 - INFO - __main__ -   test: [epoch: 1 | batch: 1500/10010 ] | Loss: 0.856 | Acc: 78.683% (151173/192128)
01/14/2023 00:29:14 - INFO - __main__ -   test: [epoch: 1 | batch: 1600/10010 ] | Loss: 0.859 | Acc: 78.653% (161183/204928)
01/14/2023 00:33:35 - INFO - __main__ -   test: [epoch: 1 | batch: 1700/10010 ] | Loss: 0.858 | Acc: 78.656% (171256/217728)
01/14/2023 00:37:58 - INFO - __main__ -   test: [epoch: 1 | batch: 1800/10010 ] | Loss: 0.858 | Acc: 78.672% (181362/230528)
01/14/2023 00:42:18 - INFO - __main__ -   test: [epoch: 1 | batch: 1900/10010 ] | Loss: 0.857 | Acc: 78.711% (191526/243328)
01/14/2023 00:46:39 - INFO - __main__ -   test: [epoch: 1 | batch: 2000/10010 ] | Loss: 0.856 | Acc: 78.720% (201624/256128)
01/14/2023 00:50:59 - INFO - __main__ -   test: [epoch: 1 | batch: 2100/10010 ] | Loss: 0.856 | Acc: 78.716% (211690/268928)
01/14/2023 00:55:19 - INFO - __main__ -   test: [epoch: 1 | batch: 2200/10010 ] | Loss: 0.858 | Acc: 78.688% (221686/281728)
01/14/2023 00:59:40 - INFO - __main__ -   test: [epoch: 1 | batch: 2300/10010 ] | Loss: 0.858 | Acc: 78.678% (231730/294528)
01/14/2023 01:04:01 - INFO - __main__ -   test: [epoch: 1 | batch: 2400/10010 ] | Loss: 0.857 | Acc: 78.707% (241889/307328)
01/14/2023 01:08:22 - INFO - __main__ -   test: [epoch: 1 | batch: 2500/10010 ] | Loss: 0.858 | Acc: 78.673% (251853/320128)
01/14/2023 01:12:43 - INFO - __main__ -   test: [epoch: 1 | batch: 2600/10010 ] | Loss: 0.858 | Acc: 78.676% (261934/332928)
01/14/2023 01:17:05 - INFO - __main__ -   test: [epoch: 1 | batch: 2700/10010 ] | Loss: 0.859 | Acc: 78.674% (271997/345728)
01/14/2023 01:21:26 - INFO - __main__ -   test: [epoch: 1 | batch: 2800/10010 ] | Loss: 0.859 | Acc: 78.664% (282032/358528)
01/14/2023 01:25:47 - INFO - __main__ -   test: [epoch: 1 | batch: 2900/10010 ] | Loss: 0.859 | Acc: 78.662% (292094/371328)
01/14/2023 01:30:08 - INFO - __main__ -   test: [epoch: 1 | batch: 3000/10010 ] | Loss: 0.858 | Acc: 78.673% (302205/384128)
01/14/2023 01:34:27 - INFO - __main__ -   test: [epoch: 1 | batch: 3100/10010 ] | Loss: 0.858 | Acc: 78.670% (312264/396928)
01/14/2023 01:38:48 - INFO - __main__ -   test: [epoch: 1 | batch: 3200/10010 ] | Loss: 0.859 | Acc: 78.650% (322253/409728)
01/14/2023 01:43:09 - INFO - __main__ -   test: [epoch: 1 | batch: 3300/10010 ] | Loss: 0.859 | Acc: 78.635% (332255/422528)
01/14/2023 01:47:30 - INFO - __main__ -   test: [epoch: 1 | batch: 3400/10010 ] | Loss: 0.859 | Acc: 78.642% (342350/435328)
01/14/2023 01:51:51 - INFO - __main__ -   test: [epoch: 1 | batch: 3500/10010 ] | Loss: 0.859 | Acc: 78.642% (352415/448128)
01/14/2023 01:56:14 - INFO - __main__ -   test: [epoch: 1 | batch: 3600/10010 ] | Loss: 0.859 | Acc: 78.641% (362479/460928)
01/14/2023 02:00:34 - INFO - __main__ -   test: [epoch: 1 | batch: 3700/10010 ] | Loss: 0.860 | Acc: 78.625% (372471/473728)
01/14/2023 02:04:56 - INFO - __main__ -   test: [epoch: 1 | batch: 3800/10010 ] | Loss: 0.860 | Acc: 78.623% (382524/486528)
01/14/2023 02:09:17 - INFO - __main__ -   test: [epoch: 1 | batch: 3900/10010 ] | Loss: 0.860 | Acc: 78.632% (392633/499328)
01/14/2023 02:13:38 - INFO - __main__ -   test: [epoch: 1 | batch: 4000/10010 ] | Loss: 0.860 | Acc: 78.631% (402689/512128)
01/14/2023 02:18:01 - INFO - __main__ -   test: [epoch: 1 | batch: 4100/10010 ] | Loss: 0.859 | Acc: 78.642% (412812/524928)
01/14/2023 02:22:23 - INFO - __main__ -   test: [epoch: 1 | batch: 4200/10010 ] | Loss: 0.859 | Acc: 78.640% (422871/537728)
01/14/2023 02:26:44 - INFO - __main__ -   test: [epoch: 1 | batch: 4300/10010 ] | Loss: 0.859 | Acc: 78.646% (432970/550528)
01/14/2023 02:31:07 - INFO - __main__ -   test: [epoch: 1 | batch: 4400/10010 ] | Loss: 0.859 | Acc: 78.650% (443058/563328)
01/14/2023 02:35:28 - INFO - __main__ -   test: [epoch: 1 | batch: 4500/10010 ] | Loss: 0.859 | Acc: 78.646% (453100/576128)
01/14/2023 02:39:49 - INFO - __main__ -   test: [epoch: 1 | batch: 4600/10010 ] | Loss: 0.859 | Acc: 78.645% (463164/588928)
01/14/2023 02:44:11 - INFO - __main__ -   test: [epoch: 1 | batch: 4700/10010 ] | Loss: 0.859 | Acc: 78.656% (473298/601728)
01/14/2023 02:48:32 - INFO - __main__ -   test: [epoch: 1 | batch: 4800/10010 ] | Loss: 0.858 | Acc: 78.665% (483417/614528)
01/14/2023 02:52:52 - INFO - __main__ -   test: [epoch: 1 | batch: 4900/10010 ] | Loss: 0.858 | Acc: 78.655% (493427/627328)
01/14/2023 02:57:14 - INFO - __main__ -   test: [epoch: 1 | batch: 5000/10010 ] | Loss: 0.858 | Acc: 78.668% (503574/640128)
01/14/2023 03:01:35 - INFO - __main__ -   test: [epoch: 1 | batch: 5100/10010 ] | Loss: 0.858 | Acc: 78.666% (513631/652928)
01/14/2023 03:05:55 - INFO - __main__ -   test: [epoch: 1 | batch: 5200/10010 ] | Loss: 0.858 | Acc: 78.657% (523644/665728)
01/14/2023 03:10:16 - INFO - __main__ -   test: [epoch: 1 | batch: 5300/10010 ] | Loss: 0.859 | Acc: 78.654% (533690/678528)
01/14/2023 03:14:39 - INFO - __main__ -   test: [epoch: 1 | batch: 5400/10010 ] | Loss: 0.859 | Acc: 78.656% (543768/691328)
01/14/2023 03:19:00 - INFO - __main__ -   test: [epoch: 1 | batch: 5500/10010 ] | Loss: 0.859 | Acc: 78.648% (553785/704128)
01/14/2023 03:23:24 - INFO - __main__ -   test: [epoch: 1 | batch: 5600/10010 ] | Loss: 0.858 | Acc: 78.659% (563926/716928)
01/14/2023 03:27:46 - INFO - __main__ -   test: [epoch: 1 | batch: 5700/10010 ] | Loss: 0.858 | Acc: 78.661% (574010/729728)
01/14/2023 03:32:07 - INFO - __main__ -   test: [epoch: 1 | batch: 5800/10010 ] | Loss: 0.858 | Acc: 78.669% (584139/742528)
01/14/2023 03:36:28 - INFO - __main__ -   test: [epoch: 1 | batch: 5900/10010 ] | Loss: 0.858 | Acc: 78.667% (594195/755328)
01/14/2023 03:40:49 - INFO - __main__ -   test: [epoch: 1 | batch: 6000/10010 ] | Loss: 0.858 | Acc: 78.673% (604313/768128)
01/14/2023 03:45:12 - INFO - __main__ -   test: [epoch: 1 | batch: 6100/10010 ] | Loss: 0.858 | Acc: 78.660% (614281/780928)
01/14/2023 03:49:33 - INFO - __main__ -   test: [epoch: 1 | batch: 6200/10010 ] | Loss: 0.858 | Acc: 78.661% (624355/793728)
01/14/2023 03:53:54 - INFO - __main__ -   test: [epoch: 1 | batch: 6300/10010 ] | Loss: 0.858 | Acc: 78.659% (634408/806528)
01/14/2023 03:58:16 - INFO - __main__ -   test: [epoch: 1 | batch: 6400/10010 ] | Loss: 0.858 | Acc: 78.648% (644381/819328)
01/14/2023 04:02:39 - INFO - __main__ -   test: [epoch: 1 | batch: 6500/10010 ] | Loss: 0.858 | Acc: 78.648% (654449/832128)
01/14/2023 04:07:00 - INFO - __main__ -   test: [epoch: 1 | batch: 6600/10010 ] | Loss: 0.858 | Acc: 78.653% (664561/844928)
01/14/2023 04:11:21 - INFO - __main__ -   test: [epoch: 1 | batch: 6700/10010 ] | Loss: 0.858 | Acc: 78.644% (674552/857728)
01/14/2023 04:15:42 - INFO - __main__ -   test: [epoch: 1 | batch: 6800/10010 ] | Loss: 0.859 | Acc: 78.635% (684543/870528)
01/14/2023 04:20:04 - INFO - __main__ -   test: [epoch: 1 | batch: 6900/10010 ] | Loss: 0.858 | Acc: 78.638% (694632/883328)
01/14/2023 04:24:25 - INFO - __main__ -   test: [epoch: 1 | batch: 7000/10010 ] | Loss: 0.859 | Acc: 78.632% (704646/896128)
01/14/2023 04:28:46 - INFO - __main__ -   test: [epoch: 1 | batch: 7100/10010 ] | Loss: 0.858 | Acc: 78.639% (714769/908928)
01/14/2023 04:33:05 - INFO - __main__ -   test: [epoch: 1 | batch: 7200/10010 ] | Loss: 0.858 | Acc: 78.640% (724844/921728)
01/14/2023 04:37:26 - INFO - __main__ -   test: [epoch: 1 | batch: 7300/10010 ] | Loss: 0.858 | Acc: 78.638% (734895/934528)
01/14/2023 04:41:47 - INFO - __main__ -   test: [epoch: 1 | batch: 7400/10010 ] | Loss: 0.859 | Acc: 78.631% (744898/947328)
01/14/2023 04:46:09 - INFO - __main__ -   test: [epoch: 1 | batch: 7500/10010 ] | Loss: 0.858 | Acc: 78.632% (754968/960128)
01/14/2023 04:50:31 - INFO - __main__ -   test: [epoch: 1 | batch: 7600/10010 ] | Loss: 0.859 | Acc: 78.627% (764985/972928)
01/14/2023 04:54:54 - INFO - __main__ -   test: [epoch: 1 | batch: 7700/10010 ] | Loss: 0.859 | Acc: 78.624% (775016/985728)
01/14/2023 04:59:14 - INFO - __main__ -   test: [epoch: 1 | batch: 7800/10010 ] | Loss: 0.859 | Acc: 78.613% (784968/998528)
01/14/2023 05:03:34 - INFO - __main__ -   test: [epoch: 1 | batch: 7900/10010 ] | Loss: 0.859 | Acc: 78.617% (795076/1011328)
01/14/2023 05:07:56 - INFO - __main__ -   test: [epoch: 1 | batch: 8000/10010 ] | Loss: 0.859 | Acc: 78.619% (805156/1024128)
01/14/2023 05:12:17 - INFO - __main__ -   test: [epoch: 1 | batch: 8100/10010 ] | Loss: 0.859 | Acc: 78.628% (815315/1036928)
01/14/2023 05:16:36 - INFO - __main__ -   test: [epoch: 1 | batch: 8200/10010 ] | Loss: 0.859 | Acc: 78.621% (825310/1049728)
01/14/2023 05:20:57 - INFO - __main__ -   test: [epoch: 1 | batch: 8300/10010 ] | Loss: 0.859 | Acc: 78.619% (835344/1062528)
01/14/2023 05:25:19 - INFO - __main__ -   test: [epoch: 1 | batch: 8400/10010 ] | Loss: 0.859 | Acc: 78.619% (845409/1075328)
01/14/2023 05:29:39 - INFO - __main__ -   test: [epoch: 1 | batch: 8500/10010 ] | Loss: 0.859 | Acc: 78.621% (855498/1088128)
01/14/2023 05:33:59 - INFO - __main__ -   test: [epoch: 1 | batch: 8600/10010 ] | Loss: 0.859 | Acc: 78.620% (865548/1100928)
01/14/2023 05:38:22 - INFO - __main__ -   test: [epoch: 1 | batch: 8700/10010 ] | Loss: 0.859 | Acc: 78.625% (875667/1113728)
01/14/2023 05:42:41 - INFO - __main__ -   test: [epoch: 1 | batch: 8800/10010 ] | Loss: 0.859 | Acc: 78.630% (885789/1126528)
01/14/2023 05:47:02 - INFO - __main__ -   test: [epoch: 1 | batch: 8900/10010 ] | Loss: 0.859 | Acc: 78.633% (895889/1139328)
01/14/2023 05:51:23 - INFO - __main__ -   test: [epoch: 1 | batch: 9000/10010 ] | Loss: 0.859 | Acc: 78.634% (905969/1152128)
01/14/2023 05:55:45 - INFO - __main__ -   test: [epoch: 1 | batch: 9100/10010 ] | Loss: 0.858 | Acc: 78.635% (916045/1164928)
01/14/2023 06:00:06 - INFO - __main__ -   test: [epoch: 1 | batch: 9200/10010 ] | Loss: 0.858 | Acc: 78.635% (926109/1177728)
01/14/2023 06:04:28 - INFO - __main__ -   test: [epoch: 1 | batch: 9300/10010 ] | Loss: 0.858 | Acc: 78.641% (936244/1190528)
01/14/2023 06:08:51 - INFO - __main__ -   test: [epoch: 1 | batch: 9400/10010 ] | Loss: 0.858 | Acc: 78.637% (946260/1203328)
01/14/2023 06:13:23 - INFO - __main__ -   test: [epoch: 1 | batch: 9500/10010 ] | Loss: 0.858 | Acc: 78.639% (956349/1216128)
01/14/2023 06:17:45 - INFO - __main__ -   test: [epoch: 1 | batch: 9600/10010 ] | Loss: 0.858 | Acc: 78.638% (966405/1228928)
01/14/2023 06:22:05 - INFO - __main__ -   test: [epoch: 1 | batch: 9700/10010 ] | Loss: 0.858 | Acc: 78.640% (976491/1241728)
01/14/2023 06:26:28 - INFO - __main__ -   test: [epoch: 1 | batch: 9800/10010 ] | Loss: 0.859 | Acc: 78.637% (986527/1254528)
01/14/2023 06:30:51 - INFO - __main__ -   test: [epoch: 1 | batch: 9900/10010 ] | Loss: 0.859 | Acc: 78.638% (996600/1267328)
01/14/2023 06:35:15 - INFO - __main__ -   test: [epoch: 1 | batch: 10000/10010 ] | Loss: 0.859 | Acc: 78.634% (1006620/1280128)
01/14/2023 06:35:38 - INFO - __main__ -   Saving Checkpoint
01/14/2023 06:35:41 - INFO - __main__ -   test: [batch: 0/391 ] | Loss: 0.440 | Acc: 86.719% (111/128)/ 97.656% (125/128)
01/14/2023 06:35:44 - INFO - __main__ -   test: [batch: 1/391 ] | Loss: 0.454 | Acc: 86.719% (222/256)/ 98.047% (251/256)
01/14/2023 06:35:47 - INFO - __main__ -   test: [batch: 2/391 ] | Loss: 0.609 | Acc: 83.333% (320/384)/ 95.833% (368/384)
01/14/2023 06:35:49 - INFO - __main__ -   test: [batch: 3/391 ] | Loss: 0.564 | Acc: 84.961% (435/512)/ 96.484% (494/512)
01/14/2023 06:35:52 - INFO - __main__ -   test: [batch: 4/391 ] | Loss: 0.484 | Acc: 87.188% (558/640)/ 97.188% (622/640)
01/14/2023 06:35:55 - INFO - __main__ -   test: [batch: 5/391 ] | Loss: 0.427 | Acc: 88.542% (680/768)/ 97.656% (750/768)
01/14/2023 06:35:57 - INFO - __main__ -   test: [batch: 6/391 ] | Loss: 0.423 | Acc: 88.728% (795/896)/ 97.545% (874/896)
01/14/2023 06:36:00 - INFO - __main__ -   test: [batch: 7/391 ] | Loss: 0.405 | Acc: 89.551% (917/1024)/ 97.559% (999/1024)
01/14/2023 06:36:03 - INFO - __main__ -   test: [batch: 8/391 ] | Loss: 0.422 | Acc: 89.497% (1031/1152)/ 97.483% (1123/1152)
01/14/2023 06:36:05 - INFO - __main__ -   test: [batch: 9/391 ] | Loss: 0.401 | Acc: 90.000% (1152/1280)/ 97.578% (1249/1280)
01/14/2023 06:36:08 - INFO - __main__ -   test: [batch: 10/391 ] | Loss: 0.450 | Acc: 88.778% (1250/1408)/ 97.585% (1374/1408)
01/14/2023 06:36:11 - INFO - __main__ -   test: [batch: 11/391 ] | Loss: 0.455 | Acc: 88.932% (1366/1536)/ 97.461% (1497/1536)
01/14/2023 06:36:13 - INFO - __main__ -   test: [batch: 12/391 ] | Loss: 0.504 | Acc: 87.800% (1461/1664)/ 97.115% (1616/1664)
01/14/2023 06:36:16 - INFO - __main__ -   test: [batch: 13/391 ] | Loss: 0.554 | Acc: 86.161% (1544/1792)/ 96.596% (1731/1792)
01/14/2023 06:36:19 - INFO - __main__ -   test: [batch: 14/391 ] | Loss: 0.575 | Acc: 85.312% (1638/1920)/ 96.719% (1857/1920)
01/14/2023 06:36:21 - INFO - __main__ -   test: [batch: 15/391 ] | Loss: 0.582 | Acc: 84.814% (1737/2048)/ 96.826% (1983/2048)
01/14/2023 06:36:24 - INFO - __main__ -   test: [batch: 16/391 ] | Loss: 0.592 | Acc: 84.605% (1841/2176)/ 96.691% (2104/2176)
01/14/2023 06:36:27 - INFO - __main__ -   test: [batch: 17/391 ] | Loss: 0.619 | Acc: 84.245% (1941/2304)/ 96.224% (2217/2304)
01/14/2023 06:36:29 - INFO - __main__ -   test: [batch: 18/391 ] | Loss: 0.642 | Acc: 83.717% (2036/2432)/ 96.094% (2337/2432)
01/14/2023 06:36:32 - INFO - __main__ -   test: [batch: 19/391 ] | Loss: 0.650 | Acc: 83.398% (2135/2560)/ 95.977% (2457/2560)
01/14/2023 06:36:35 - INFO - __main__ -   test: [batch: 20/391 ] | Loss: 0.648 | Acc: 83.482% (2244/2688)/ 95.908% (2578/2688)
01/14/2023 06:36:37 - INFO - __main__ -   test: [batch: 21/391 ] | Loss: 0.680 | Acc: 82.741% (2330/2816)/ 95.739% (2696/2816)
01/14/2023 06:36:40 - INFO - __main__ -   test: [batch: 22/391 ] | Loss: 0.679 | Acc: 82.609% (2432/2944)/ 95.754% (2819/2944)
01/14/2023 06:36:43 - INFO - __main__ -   test: [batch: 23/391 ] | Loss: 0.723 | Acc: 81.738% (2511/3072)/ 95.443% (2932/3072)
01/14/2023 06:36:45 - INFO - __main__ -   test: [batch: 24/391 ] | Loss: 0.743 | Acc: 81.219% (2599/3200)/ 95.281% (3049/3200)
01/14/2023 06:36:48 - INFO - __main__ -   test: [batch: 25/391 ] | Loss: 0.768 | Acc: 80.589% (2682/3328)/ 95.072% (3164/3328)
01/14/2023 06:36:51 - INFO - __main__ -   test: [batch: 26/391 ] | Loss: 0.783 | Acc: 79.861% (2760/3456)/ 95.023% (3284/3456)
01/14/2023 06:36:53 - INFO - __main__ -   test: [batch: 27/391 ] | Loss: 0.766 | Acc: 80.385% (2881/3584)/ 95.061% (3407/3584)
01/14/2023 06:36:56 - INFO - __main__ -   test: [batch: 28/391 ] | Loss: 0.775 | Acc: 79.822% (2963/3712)/ 95.124% (3531/3712)
01/14/2023 06:36:58 - INFO - __main__ -   test: [batch: 29/391 ] | Loss: 0.767 | Acc: 79.974% (3071/3840)/ 95.234% (3657/3840)
01/14/2023 06:37:01 - INFO - __main__ -   test: [batch: 30/391 ] | Loss: 0.781 | Acc: 79.914% (3171/3968)/ 95.111% (3774/3968)
01/14/2023 06:37:03 - INFO - __main__ -   test: [batch: 31/391 ] | Loss: 0.777 | Acc: 80.078% (3280/4096)/ 95.166% (3898/4096)
01/14/2023 06:37:06 - INFO - __main__ -   test: [batch: 32/391 ] | Loss: 0.763 | Acc: 80.374% (3395/4224)/ 95.265% (4024/4224)
01/14/2023 06:37:09 - INFO - __main__ -   test: [batch: 33/391 ] | Loss: 0.756 | Acc: 80.515% (3504/4352)/ 95.312% (4148/4352)
01/14/2023 06:37:11 - INFO - __main__ -   test: [batch: 34/391 ] | Loss: 0.743 | Acc: 80.893% (3624/4480)/ 95.379% (4273/4480)
01/14/2023 06:37:14 - INFO - __main__ -   test: [batch: 35/391 ] | Loss: 0.729 | Acc: 81.293% (3746/4608)/ 95.399% (4396/4608)
01/14/2023 06:37:16 - INFO - __main__ -   test: [batch: 36/391 ] | Loss: 0.714 | Acc: 81.736% (3871/4736)/ 95.503% (4523/4736)
01/14/2023 06:37:19 - INFO - __main__ -   test: [batch: 37/391 ] | Loss: 0.704 | Acc: 82.052% (3991/4864)/ 95.559% (4648/4864)
01/14/2023 06:37:22 - INFO - __main__ -   test: [batch: 38/391 ] | Loss: 0.697 | Acc: 82.212% (4104/4992)/ 95.633% (4774/4992)
01/14/2023 06:37:24 - INFO - __main__ -   test: [batch: 39/391 ] | Loss: 0.692 | Acc: 82.266% (4212/5120)/ 95.684% (4899/5120)
01/14/2023 06:37:27 - INFO - __main__ -   test: [batch: 40/391 ] | Loss: 0.690 | Acc: 82.355% (4322/5248)/ 95.579% (5016/5248)
01/14/2023 06:37:30 - INFO - __main__ -   test: [batch: 41/391 ] | Loss: 0.694 | Acc: 82.440% (4432/5376)/ 95.517% (5135/5376)
01/14/2023 06:37:32 - INFO - __main__ -   test: [batch: 42/391 ] | Loss: 0.693 | Acc: 82.431% (4537/5504)/ 95.549% (5259/5504)
01/14/2023 06:37:35 - INFO - __main__ -   test: [batch: 43/391 ] | Loss: 0.690 | Acc: 82.493% (4646/5632)/ 95.490% (5378/5632)
01/14/2023 06:37:37 - INFO - __main__ -   test: [batch: 44/391 ] | Loss: 0.693 | Acc: 82.500% (4752/5760)/ 95.417% (5496/5760)
01/14/2023 06:37:40 - INFO - __main__ -   test: [batch: 45/391 ] | Loss: 0.690 | Acc: 82.694% (4869/5888)/ 95.431% (5619/5888)
01/14/2023 06:37:43 - INFO - __main__ -   test: [batch: 46/391 ] | Loss: 0.693 | Acc: 82.680% (4974/6016)/ 95.462% (5743/6016)
01/14/2023 06:37:45 - INFO - __main__ -   test: [batch: 47/391 ] | Loss: 0.695 | Acc: 82.617% (5076/6144)/ 95.508% (5868/6144)
01/14/2023 06:37:48 - INFO - __main__ -   test: [batch: 48/391 ] | Loss: 0.700 | Acc: 82.462% (5172/6272)/ 95.472% (5988/6272)
01/14/2023 06:37:51 - INFO - __main__ -   test: [batch: 49/391 ] | Loss: 0.703 | Acc: 82.531% (5282/6400)/ 95.375% (6104/6400)
01/14/2023 06:37:53 - INFO - __main__ -   test: [batch: 50/391 ] | Loss: 0.694 | Acc: 82.736% (5401/6528)/ 95.435% (6230/6528)
01/14/2023 06:37:56 - INFO - __main__ -   test: [batch: 51/391 ] | Loss: 0.686 | Acc: 82.903% (5518/6656)/ 95.508% (6357/6656)
01/14/2023 06:37:59 - INFO - __main__ -   test: [batch: 52/391 ] | Loss: 0.683 | Acc: 82.989% (5630/6784)/ 95.534% (6481/6784)
01/14/2023 06:38:01 - INFO - __main__ -   test: [batch: 53/391 ] | Loss: 0.674 | Acc: 83.203% (5751/6912)/ 95.616% (6609/6912)
01/14/2023 06:38:04 - INFO - __main__ -   test: [batch: 54/391 ] | Loss: 0.667 | Acc: 83.338% (5867/7040)/ 95.653% (6734/7040)
01/14/2023 06:38:06 - INFO - __main__ -   test: [batch: 55/391 ] | Loss: 0.660 | Acc: 83.496% (5985/7168)/ 95.689% (6859/7168)
01/14/2023 06:38:09 - INFO - __main__ -   test: [batch: 56/391 ] | Loss: 0.652 | Acc: 83.717% (6108/7296)/ 95.737% (6985/7296)
01/14/2023 06:38:11 - INFO - __main__ -   test: [batch: 57/391 ] | Loss: 0.645 | Acc: 83.904% (6229/7424)/ 95.784% (7111/7424)
01/14/2023 06:38:14 - INFO - __main__ -   test: [batch: 58/391 ] | Loss: 0.643 | Acc: 83.951% (6340/7552)/ 95.776% (7233/7552)
01/14/2023 06:38:17 - INFO - __main__ -   test: [batch: 59/391 ] | Loss: 0.645 | Acc: 83.867% (6441/7680)/ 95.794% (7357/7680)
01/14/2023 06:38:19 - INFO - __main__ -   test: [batch: 60/391 ] | Loss: 0.650 | Acc: 83.760% (6540/7808)/ 95.761% (7477/7808)
01/14/2023 06:38:22 - INFO - __main__ -   test: [batch: 61/391 ] | Loss: 0.650 | Acc: 83.758% (6647/7936)/ 95.766% (7600/7936)
01/14/2023 06:38:24 - INFO - __main__ -   test: [batch: 62/391 ] | Loss: 0.650 | Acc: 83.730% (6752/8064)/ 95.784% (7724/8064)
01/14/2023 06:38:26 - INFO - __main__ -   test: [batch: 63/391 ] | Loss: 0.658 | Acc: 83.594% (6848/8192)/ 95.728% (7842/8192)
01/14/2023 06:38:29 - INFO - __main__ -   test: [batch: 64/391 ] | Loss: 0.666 | Acc: 83.401% (6939/8320)/ 95.673% (7960/8320)
01/14/2023 06:38:32 - INFO - __main__ -   test: [batch: 65/391 ] | Loss: 0.680 | Acc: 82.884% (7002/8448)/ 95.585% (8075/8448)
01/14/2023 06:38:34 - INFO - __main__ -   test: [batch: 66/391 ] | Loss: 0.685 | Acc: 82.894% (7109/8576)/ 95.546% (8194/8576)
01/14/2023 06:38:37 - INFO - __main__ -   test: [batch: 67/391 ] | Loss: 0.687 | Acc: 82.847% (7211/8704)/ 95.554% (8317/8704)
01/14/2023 06:38:39 - INFO - __main__ -   test: [batch: 68/391 ] | Loss: 0.688 | Acc: 82.790% (7312/8832)/ 95.573% (8441/8832)
01/14/2023 06:38:42 - INFO - __main__ -   test: [batch: 69/391 ] | Loss: 0.683 | Acc: 82.879% (7426/8960)/ 95.614% (8567/8960)
01/14/2023 06:38:44 - INFO - __main__ -   test: [batch: 70/391 ] | Loss: 0.685 | Acc: 82.801% (7525/9088)/ 95.610% (8689/9088)
01/14/2023 06:38:47 - INFO - __main__ -   test: [batch: 71/391 ] | Loss: 0.683 | Acc: 82.834% (7634/9216)/ 95.627% (8813/9216)
01/14/2023 06:38:49 - INFO - __main__ -   test: [batch: 72/391 ] | Loss: 0.686 | Acc: 82.716% (7729/9344)/ 95.644% (8937/9344)
01/14/2023 06:38:52 - INFO - __main__ -   test: [batch: 73/391 ] | Loss: 0.690 | Acc: 82.591% (7823/9472)/ 95.650% (9060/9472)
01/14/2023 06:38:55 - INFO - __main__ -   test: [batch: 74/391 ] | Loss: 0.690 | Acc: 82.573% (7927/9600)/ 95.635% (9181/9600)
01/14/2023 06:38:57 - INFO - __main__ -   test: [batch: 75/391 ] | Loss: 0.696 | Acc: 82.381% (8014/9728)/ 95.621% (9302/9728)
01/14/2023 06:39:00 - INFO - __main__ -   test: [batch: 76/391 ] | Loss: 0.696 | Acc: 82.366% (8118/9856)/ 95.617% (9424/9856)
01/14/2023 06:39:03 - INFO - __main__ -   test: [batch: 77/391 ] | Loss: 0.697 | Acc: 82.332% (8220/9984)/ 95.643% (9549/9984)
01/14/2023 06:39:06 - INFO - __main__ -   test: [batch: 78/391 ] | Loss: 0.696 | Acc: 82.229% (8315/10112)/ 95.688% (9676/10112)
01/14/2023 06:39:08 - INFO - __main__ -   test: [batch: 79/391 ] | Loss: 0.695 | Acc: 82.227% (8420/10240)/ 95.713% (9801/10240)
01/14/2023 06:39:11 - INFO - __main__ -   test: [batch: 80/391 ] | Loss: 0.694 | Acc: 82.215% (8524/10368)/ 95.718% (9924/10368)
01/14/2023 06:39:14 - INFO - __main__ -   test: [batch: 81/391 ] | Loss: 0.693 | Acc: 82.231% (8631/10496)/ 95.741% (10049/10496)
01/14/2023 06:39:16 - INFO - __main__ -   test: [batch: 82/391 ] | Loss: 0.693 | Acc: 82.238% (8737/10624)/ 95.736% (10171/10624)
01/14/2023 06:39:19 - INFO - __main__ -   test: [batch: 83/391 ] | Loss: 0.693 | Acc: 82.282% (8847/10752)/ 95.722% (10292/10752)
01/14/2023 06:39:22 - INFO - __main__ -   test: [batch: 84/391 ] | Loss: 0.689 | Acc: 82.408% (8966/10880)/ 95.754% (10418/10880)
01/14/2023 06:39:24 - INFO - __main__ -   test: [batch: 85/391 ] | Loss: 0.687 | Acc: 82.395% (9070/11008)/ 95.794% (10545/11008)
01/14/2023 06:39:27 - INFO - __main__ -   test: [batch: 86/391 ] | Loss: 0.691 | Acc: 82.328% (9168/11136)/ 95.761% (10664/11136)
01/14/2023 06:39:30 - INFO - __main__ -   test: [batch: 87/391 ] | Loss: 0.689 | Acc: 82.342% (9275/11264)/ 95.765% (10787/11264)
01/14/2023 06:39:32 - INFO - __main__ -   test: [batch: 88/391 ] | Loss: 0.697 | Acc: 82.259% (9371/11392)/ 95.699% (10902/11392)
01/14/2023 06:39:35 - INFO - __main__ -   test: [batch: 89/391 ] | Loss: 0.695 | Acc: 82.300% (9481/11520)/ 95.712% (11026/11520)
01/14/2023 06:39:38 - INFO - __main__ -   test: [batch: 90/391 ] | Loss: 0.696 | Acc: 82.220% (9577/11648)/ 95.716% (11149/11648)
01/14/2023 06:39:40 - INFO - __main__ -   test: [batch: 91/391 ] | Loss: 0.694 | Acc: 82.252% (9686/11776)/ 95.720% (11272/11776)
01/14/2023 06:39:43 - INFO - __main__ -   test: [batch: 92/391 ] | Loss: 0.696 | Acc: 82.241% (9790/11904)/ 95.699% (11392/11904)
01/14/2023 06:39:45 - INFO - __main__ -   test: [batch: 93/391 ] | Loss: 0.699 | Acc: 82.048% (9872/12032)/ 95.728% (11518/12032)
01/14/2023 06:39:47 - INFO - __main__ -   test: [batch: 94/391 ] | Loss: 0.701 | Acc: 81.908% (9960/12160)/ 95.740% (11642/12160)
01/14/2023 06:39:50 - INFO - __main__ -   test: [batch: 95/391 ] | Loss: 0.699 | Acc: 81.966% (10072/12288)/ 95.744% (11765/12288)
01/14/2023 06:39:53 - INFO - __main__ -   test: [batch: 96/391 ] | Loss: 0.703 | Acc: 81.854% (10163/12416)/ 95.747% (11888/12416)
01/14/2023 06:39:55 - INFO - __main__ -   test: [batch: 97/391 ] | Loss: 0.705 | Acc: 81.673% (10245/12544)/ 95.767% (12013/12544)
01/14/2023 06:39:58 - INFO - __main__ -   test: [batch: 98/391 ] | Loss: 0.701 | Acc: 81.787% (10364/12672)/ 95.802% (12140/12672)
01/14/2023 06:40:00 - INFO - __main__ -   test: [batch: 99/391 ] | Loss: 0.695 | Acc: 81.938% (10488/12800)/ 95.844% (12268/12800)
01/14/2023 06:40:03 - INFO - __main__ -   test: [batch: 100/391 ] | Loss: 0.694 | Acc: 81.993% (10600/12928)/ 95.862% (12393/12928)
01/14/2023 06:40:06 - INFO - __main__ -   test: [batch: 101/391 ] | Loss: 0.690 | Acc: 82.100% (10719/13056)/ 95.895% (12520/13056)
01/14/2023 06:40:08 - INFO - __main__ -   test: [batch: 102/391 ] | Loss: 0.686 | Acc: 82.221% (10840/13184)/ 95.927% (12647/13184)
01/14/2023 06:40:11 - INFO - __main__ -   test: [batch: 103/391 ] | Loss: 0.688 | Acc: 82.054% (10923/13312)/ 95.951% (12773/13312)
01/14/2023 06:40:13 - INFO - __main__ -   test: [batch: 104/391 ] | Loss: 0.687 | Acc: 81.994% (11020/13440)/ 95.960% (12897/13440)
01/14/2023 06:40:16 - INFO - __main__ -   test: [batch: 105/391 ] | Loss: 0.688 | Acc: 81.987% (11124/13568)/ 95.954% (13019/13568)
01/14/2023 06:40:19 - INFO - __main__ -   test: [batch: 106/391 ] | Loss: 0.697 | Acc: 81.863% (11212/13696)/ 95.882% (13132/13696)
01/14/2023 06:40:21 - INFO - __main__ -   test: [batch: 107/391 ] | Loss: 0.693 | Acc: 81.981% (11333/13824)/ 95.913% (13259/13824)
01/14/2023 06:40:24 - INFO - __main__ -   test: [batch: 108/391 ] | Loss: 0.696 | Acc: 81.852% (11420/13952)/ 95.922% (13383/13952)
01/14/2023 06:40:27 - INFO - __main__ -   test: [batch: 109/391 ] | Loss: 0.697 | Acc: 81.825% (11521/14080)/ 95.930% (13507/14080)
01/14/2023 06:40:29 - INFO - __main__ -   test: [batch: 110/391 ] | Loss: 0.700 | Acc: 81.637% (11599/14208)/ 95.939% (13631/14208)
01/14/2023 06:40:32 - INFO - __main__ -   test: [batch: 111/391 ] | Loss: 0.703 | Acc: 81.613% (11700/14336)/ 95.891% (13747/14336)
01/14/2023 06:40:34 - INFO - __main__ -   test: [batch: 112/391 ] | Loss: 0.704 | Acc: 81.637% (11808/14464)/ 95.907% (13872/14464)
01/14/2023 06:40:37 - INFO - __main__ -   test: [batch: 113/391 ] | Loss: 0.703 | Acc: 81.675% (11918/14592)/ 95.916% (13996/14592)
01/14/2023 06:40:40 - INFO - __main__ -   test: [batch: 114/391 ] | Loss: 0.700 | Acc: 81.766% (12036/14720)/ 95.938% (14122/14720)
01/14/2023 06:40:42 - INFO - __main__ -   test: [batch: 115/391 ] | Loss: 0.696 | Acc: 81.849% (12153/14848)/ 95.959% (14248/14848)
01/14/2023 06:40:45 - INFO - __main__ -   test: [batch: 116/391 ] | Loss: 0.700 | Acc: 81.804% (12251/14976)/ 95.927% (14366/14976)
01/14/2023 06:40:48 - INFO - __main__ -   test: [batch: 117/391 ] | Loss: 0.698 | Acc: 81.826% (12359/15104)/ 95.941% (14491/15104)
01/14/2023 06:40:50 - INFO - __main__ -   test: [batch: 118/391 ] | Loss: 0.702 | Acc: 81.716% (12447/15232)/ 95.956% (14616/15232)
01/14/2023 06:40:53 - INFO - __main__ -   test: [batch: 119/391 ] | Loss: 0.700 | Acc: 81.771% (12560/15360)/ 95.977% (14742/15360)
01/14/2023 06:40:55 - INFO - __main__ -   test: [batch: 120/391 ] | Loss: 0.698 | Acc: 81.812% (12671/15488)/ 95.990% (14867/15488)
01/14/2023 06:40:58 - INFO - __main__ -   test: [batch: 121/391 ] | Loss: 0.703 | Acc: 81.660% (12752/15616)/ 95.959% (14985/15616)
01/14/2023 06:41:00 - INFO - __main__ -   test: [batch: 122/391 ] | Loss: 0.705 | Acc: 81.599% (12847/15744)/ 95.935% (15104/15744)
01/14/2023 06:41:03 - INFO - __main__ -   test: [batch: 123/391 ] | Loss: 0.706 | Acc: 81.615% (12954/15872)/ 95.943% (15228/15872)
01/14/2023 06:41:06 - INFO - __main__ -   test: [batch: 124/391 ] | Loss: 0.705 | Acc: 81.625% (13060/16000)/ 95.956% (15353/16000)
01/14/2023 06:41:08 - INFO - __main__ -   test: [batch: 125/391 ] | Loss: 0.700 | Acc: 81.727% (13181/16128)/ 95.982% (15480/16128)
01/14/2023 06:41:11 - INFO - __main__ -   test: [batch: 126/391 ] | Loss: 0.697 | Acc: 81.816% (13300/16256)/ 95.995% (15605/16256)
01/14/2023 06:41:13 - INFO - __main__ -   test: [batch: 127/391 ] | Loss: 0.695 | Acc: 81.891% (13417/16384)/ 96.002% (15729/16384)
01/14/2023 06:41:16 - INFO - __main__ -   test: [batch: 128/391 ] | Loss: 0.696 | Acc: 81.880% (13520/16512)/ 95.979% (15848/16512)
01/14/2023 06:41:19 - INFO - __main__ -   test: [batch: 129/391 ] | Loss: 0.694 | Acc: 81.893% (13627/16640)/ 95.986% (15972/16640)
01/14/2023 06:41:21 - INFO - __main__ -   test: [batch: 130/391 ] | Loss: 0.691 | Acc: 81.995% (13749/16768)/ 96.004% (16098/16768)
01/14/2023 06:41:24 - INFO - __main__ -   test: [batch: 131/391 ] | Loss: 0.691 | Acc: 82.049% (13863/16896)/ 96.011% (16222/16896)
01/14/2023 06:41:27 - INFO - __main__ -   test: [batch: 132/391 ] | Loss: 0.688 | Acc: 82.119% (13980/17024)/ 96.023% (16347/17024)
01/14/2023 06:41:29 - INFO - __main__ -   test: [batch: 133/391 ] | Loss: 0.690 | Acc: 82.072% (14077/17152)/ 96.018% (16469/17152)
01/14/2023 06:41:32 - INFO - __main__ -   test: [batch: 134/391 ] | Loss: 0.687 | Acc: 82.106% (14188/17280)/ 96.036% (16595/17280)
01/14/2023 06:41:35 - INFO - __main__ -   test: [batch: 135/391 ] | Loss: 0.688 | Acc: 82.100% (14292/17408)/ 96.031% (16717/17408)
01/14/2023 06:41:37 - INFO - __main__ -   test: [batch: 136/391 ] | Loss: 0.686 | Acc: 82.054% (14389/17536)/ 96.054% (16844/17536)
01/14/2023 06:41:40 - INFO - __main__ -   test: [batch: 137/391 ] | Loss: 0.684 | Acc: 82.094% (14501/17664)/ 96.082% (16972/17664)
01/14/2023 06:41:43 - INFO - __main__ -   test: [batch: 138/391 ] | Loss: 0.683 | Acc: 82.116% (14610/17792)/ 96.083% (17095/17792)
01/14/2023 06:41:45 - INFO - __main__ -   test: [batch: 139/391 ] | Loss: 0.689 | Acc: 81.964% (14688/17920)/ 96.071% (17216/17920)
01/14/2023 06:41:48 - INFO - __main__ -   test: [batch: 140/391 ] | Loss: 0.691 | Acc: 81.909% (14783/18048)/ 96.066% (17338/18048)
01/14/2023 06:41:50 - INFO - __main__ -   test: [batch: 141/391 ] | Loss: 0.689 | Acc: 81.954% (14896/18176)/ 96.077% (17463/18176)
01/14/2023 06:41:53 - INFO - __main__ -   test: [batch: 142/391 ] | Loss: 0.687 | Acc: 82.037% (15016/18304)/ 96.094% (17589/18304)
01/14/2023 06:41:56 - INFO - __main__ -   test: [batch: 143/391 ] | Loss: 0.688 | Acc: 82.053% (15124/18432)/ 96.077% (17709/18432)
01/14/2023 06:41:58 - INFO - __main__ -   test: [batch: 144/391 ] | Loss: 0.690 | Acc: 82.026% (15224/18560)/ 96.045% (17826/18560)
01/14/2023 06:42:01 - INFO - __main__ -   test: [batch: 145/391 ] | Loss: 0.693 | Acc: 82.004% (15325/18688)/ 96.013% (17943/18688)
01/14/2023 06:42:04 - INFO - __main__ -   test: [batch: 146/391 ] | Loss: 0.694 | Acc: 81.983% (15426/18816)/ 96.003% (18064/18816)
01/14/2023 06:42:06 - INFO - __main__ -   test: [batch: 147/391 ] | Loss: 0.694 | Acc: 81.978% (15530/18944)/ 95.983% (18183/18944)
01/14/2023 06:42:09 - INFO - __main__ -   test: [batch: 148/391 ] | Loss: 0.695 | Acc: 81.926% (15625/19072)/ 95.978% (18305/19072)
01/14/2023 06:42:12 - INFO - __main__ -   test: [batch: 149/391 ] | Loss: 0.699 | Acc: 81.844% (15714/19200)/ 95.948% (18422/19200)
01/14/2023 06:42:14 - INFO - __main__ -   test: [batch: 150/391 ] | Loss: 0.700 | Acc: 81.773% (15805/19328)/ 95.954% (18546/19328)
01/14/2023 06:42:17 - INFO - __main__ -   test: [batch: 151/391 ] | Loss: 0.699 | Acc: 81.810% (15917/19456)/ 95.955% (18669/19456)
01/14/2023 06:42:19 - INFO - __main__ -   test: [batch: 152/391 ] | Loss: 0.701 | Acc: 81.801% (16020/19584)/ 95.941% (18789/19584)
01/14/2023 06:42:22 - INFO - __main__ -   test: [batch: 153/391 ] | Loss: 0.699 | Acc: 81.854% (16135/19712)/ 95.947% (18913/19712)
01/14/2023 06:42:25 - INFO - __main__ -   test: [batch: 154/391 ] | Loss: 0.699 | Acc: 81.845% (16238/19840)/ 95.927% (19032/19840)
01/14/2023 06:42:27 - INFO - __main__ -   test: [batch: 155/391 ] | Loss: 0.699 | Acc: 81.856% (16345/19968)/ 95.923% (19154/19968)
01/14/2023 06:42:30 - INFO - __main__ -   test: [batch: 156/391 ] | Loss: 0.701 | Acc: 81.772% (16433/20096)/ 95.905% (19273/20096)
01/14/2023 06:42:32 - INFO - __main__ -   test: [batch: 157/391 ] | Loss: 0.703 | Acc: 81.759% (16535/20224)/ 95.881% (19391/20224)
01/14/2023 06:42:35 - INFO - __main__ -   test: [batch: 158/391 ] | Loss: 0.704 | Acc: 81.732% (16634/20352)/ 95.853% (19508/20352)
01/14/2023 06:42:38 - INFO - __main__ -   test: [batch: 159/391 ] | Loss: 0.705 | Acc: 81.704% (16733/20480)/ 95.850% (19630/20480)
01/14/2023 06:42:40 - INFO - __main__ -   test: [batch: 160/391 ] | Loss: 0.706 | Acc: 81.687% (16834/20608)/ 95.837% (19750/20608)
01/14/2023 06:42:43 - INFO - __main__ -   test: [batch: 161/391 ] | Loss: 0.715 | Acc: 81.462% (16892/20736)/ 95.747% (19854/20736)
01/14/2023 06:42:46 - INFO - __main__ -   test: [batch: 162/391 ] | Loss: 0.720 | Acc: 81.370% (16977/20864)/ 95.672% (19961/20864)
01/14/2023 06:42:48 - INFO - __main__ -   test: [batch: 163/391 ] | Loss: 0.723 | Acc: 81.317% (17070/20992)/ 95.665% (20082/20992)
01/14/2023 06:42:51 - INFO - __main__ -   test: [batch: 164/391 ] | Loss: 0.723 | Acc: 81.307% (17172/21120)/ 95.682% (20208/21120)
01/14/2023 06:42:54 - INFO - __main__ -   test: [batch: 165/391 ] | Loss: 0.725 | Acc: 81.226% (17259/21248)/ 95.675% (20329/21248)
01/14/2023 06:42:56 - INFO - __main__ -   test: [batch: 166/391 ] | Loss: 0.725 | Acc: 81.245% (17367/21376)/ 95.659% (20448/21376)
01/14/2023 06:42:59 - INFO - __main__ -   test: [batch: 167/391 ] | Loss: 0.727 | Acc: 81.180% (17457/21504)/ 95.643% (20567/21504)
01/14/2023 06:43:02 - INFO - __main__ -   test: [batch: 168/391 ] | Loss: 0.726 | Acc: 81.185% (17562/21632)/ 95.641% (20689/21632)
01/14/2023 06:43:04 - INFO - __main__ -   test: [batch: 169/391 ] | Loss: 0.728 | Acc: 81.153% (17659/21760)/ 95.597% (20802/21760)
01/14/2023 06:43:07 - INFO - __main__ -   test: [batch: 170/391 ] | Loss: 0.733 | Acc: 81.049% (17740/21888)/ 95.550% (20914/21888)
01/14/2023 06:43:10 - INFO - __main__ -   test: [batch: 171/391 ] | Loss: 0.736 | Acc: 80.987% (17830/22016)/ 95.521% (21030/22016)
01/14/2023 06:43:12 - INFO - __main__ -   test: [batch: 172/391 ] | Loss: 0.737 | Acc: 80.943% (17924/22144)/ 95.520% (21152/22144)
01/14/2023 06:43:15 - INFO - __main__ -   test: [batch: 173/391 ] | Loss: 0.739 | Acc: 80.891% (18016/22272)/ 95.492% (21268/22272)
01/14/2023 06:43:17 - INFO - __main__ -   test: [batch: 174/391 ] | Loss: 0.743 | Acc: 80.804% (18100/22400)/ 95.446% (21380/22400)
01/14/2023 06:43:20 - INFO - __main__ -   test: [batch: 175/391 ] | Loss: 0.742 | Acc: 80.855% (18215/22528)/ 95.455% (21504/22528)
01/14/2023 06:43:23 - INFO - __main__ -   test: [batch: 176/391 ] | Loss: 0.742 | Acc: 80.862% (18320/22656)/ 95.436% (21622/22656)
01/14/2023 06:43:25 - INFO - __main__ -   test: [batch: 177/391 ] | Loss: 0.746 | Acc: 80.789% (18407/22784)/ 95.396% (21735/22784)
01/14/2023 06:43:28 - INFO - __main__ -   test: [batch: 178/391 ] | Loss: 0.748 | Acc: 80.770% (18506/22912)/ 95.382% (21854/22912)
01/14/2023 06:43:31 - INFO - __main__ -   test: [batch: 179/391 ] | Loss: 0.751 | Acc: 80.707% (18595/23040)/ 95.339% (21966/23040)
01/14/2023 06:43:34 - INFO - __main__ -   test: [batch: 180/391 ] | Loss: 0.757 | Acc: 80.581% (18669/23168)/ 95.300% (22079/23168)
01/14/2023 06:43:36 - INFO - __main__ -   test: [batch: 181/391 ] | Loss: 0.763 | Acc: 80.447% (18741/23296)/ 95.235% (22186/23296)
01/14/2023 06:43:39 - INFO - __main__ -   test: [batch: 182/391 ] | Loss: 0.762 | Acc: 80.464% (18848/23424)/ 95.236% (22308/23424)
01/14/2023 06:43:42 - INFO - __main__ -   test: [batch: 183/391 ] | Loss: 0.769 | Acc: 80.337% (18921/23552)/ 95.147% (22409/23552)
01/14/2023 06:43:44 - INFO - __main__ -   test: [batch: 184/391 ] | Loss: 0.768 | Acc: 80.363% (19030/23680)/ 95.135% (22528/23680)
01/14/2023 06:43:47 - INFO - __main__ -   test: [batch: 185/391 ] | Loss: 0.768 | Acc: 80.372% (19135/23808)/ 95.128% (22648/23808)
01/14/2023 06:43:49 - INFO - __main__ -   test: [batch: 186/391 ] | Loss: 0.771 | Acc: 80.331% (19228/23936)/ 95.083% (22759/23936)
01/14/2023 06:43:52 - INFO - __main__ -   test: [batch: 187/391 ] | Loss: 0.776 | Acc: 80.211% (19302/24064)/ 95.055% (22874/24064)
01/14/2023 06:43:55 - INFO - __main__ -   test: [batch: 188/391 ] | Loss: 0.780 | Acc: 80.080% (19373/24192)/ 95.031% (22990/24192)
01/14/2023 06:43:57 - INFO - __main__ -   test: [batch: 189/391 ] | Loss: 0.781 | Acc: 80.021% (19461/24320)/ 95.033% (23112/24320)
01/14/2023 06:43:59 - INFO - __main__ -   test: [batch: 190/391 ] | Loss: 0.786 | Acc: 79.929% (19541/24448)/ 95.006% (23227/24448)
01/14/2023 06:44:02 - INFO - __main__ -   test: [batch: 191/391 ] | Loss: 0.788 | Acc: 79.895% (19635/24576)/ 94.983% (23343/24576)
01/14/2023 06:44:05 - INFO - __main__ -   test: [batch: 192/391 ] | Loss: 0.794 | Acc: 79.785% (19710/24704)/ 94.900% (23444/24704)
01/14/2023 06:44:07 - INFO - __main__ -   test: [batch: 193/391 ] | Loss: 0.794 | Acc: 79.804% (19817/24832)/ 94.898% (23565/24832)
01/14/2023 06:44:10 - INFO - __main__ -   test: [batch: 194/391 ] | Loss: 0.797 | Acc: 79.744% (19904/24960)/ 94.876% (23681/24960)
01/14/2023 06:44:13 - INFO - __main__ -   test: [batch: 195/391 ] | Loss: 0.802 | Acc: 79.668% (19987/25088)/ 94.814% (23787/25088)
01/14/2023 06:44:15 - INFO - __main__ -   test: [batch: 196/391 ] | Loss: 0.805 | Acc: 79.565% (20063/25216)/ 94.769% (23897/25216)
01/14/2023 06:44:18 - INFO - __main__ -   test: [batch: 197/391 ] | Loss: 0.810 | Acc: 79.486% (20145/25344)/ 94.744% (24012/25344)
01/14/2023 06:44:21 - INFO - __main__ -   test: [batch: 198/391 ] | Loss: 0.812 | Acc: 79.452% (20238/25472)/ 94.712% (24125/25472)
01/14/2023 06:44:23 - INFO - __main__ -   test: [batch: 199/391 ] | Loss: 0.811 | Acc: 79.434% (20335/25600)/ 94.715% (24247/25600)
01/14/2023 06:44:26 - INFO - __main__ -   test: [batch: 200/391 ] | Loss: 0.811 | Acc: 79.396% (20427/25728)/ 94.710% (24367/25728)
01/14/2023 06:44:28 - INFO - __main__ -   test: [batch: 201/391 ] | Loss: 0.815 | Acc: 79.308% (20506/25856)/ 94.674% (24479/25856)
01/14/2023 06:44:31 - INFO - __main__ -   test: [batch: 202/391 ] | Loss: 0.816 | Acc: 79.299% (20605/25984)/ 94.666% (24598/25984)
01/14/2023 06:44:34 - INFO - __main__ -   test: [batch: 203/391 ] | Loss: 0.817 | Acc: 79.289% (20704/26112)/ 94.654% (24716/26112)
01/14/2023 06:44:36 - INFO - __main__ -   test: [batch: 204/391 ] | Loss: 0.820 | Acc: 79.192% (20780/26240)/ 94.634% (24832/26240)
01/14/2023 06:44:39 - INFO - __main__ -   test: [batch: 205/391 ] | Loss: 0.822 | Acc: 79.107% (20859/26368)/ 94.603% (24945/26368)
01/14/2023 06:44:41 - INFO - __main__ -   test: [batch: 206/391 ] | Loss: 0.824 | Acc: 79.080% (20953/26496)/ 94.599% (25065/26496)
01/14/2023 06:44:44 - INFO - __main__ -   test: [batch: 207/391 ] | Loss: 0.828 | Acc: 79.011% (21036/26624)/ 94.561% (25176/26624)
01/14/2023 06:44:46 - INFO - __main__ -   test: [batch: 208/391 ] | Loss: 0.829 | Acc: 78.970% (21126/26752)/ 94.528% (25288/26752)
01/14/2023 06:44:49 - INFO - __main__ -   test: [batch: 209/391 ] | Loss: 0.829 | Acc: 78.966% (21226/26880)/ 94.546% (25414/26880)
01/14/2023 06:44:52 - INFO - __main__ -   test: [batch: 210/391 ] | Loss: 0.830 | Acc: 78.954% (21324/27008)/ 94.531% (25531/27008)
01/14/2023 06:44:54 - INFO - __main__ -   test: [batch: 211/391 ] | Loss: 0.832 | Acc: 78.892% (21408/27136)/ 94.509% (25646/27136)
01/14/2023 06:44:57 - INFO - __main__ -   test: [batch: 212/391 ] | Loss: 0.835 | Acc: 78.829% (21492/27264)/ 94.487% (25761/27264)
01/14/2023 06:45:00 - INFO - __main__ -   test: [batch: 213/391 ] | Loss: 0.834 | Acc: 78.830% (21593/27392)/ 94.491% (25883/27392)
01/14/2023 06:45:02 - INFO - __main__ -   test: [batch: 214/391 ] | Loss: 0.835 | Acc: 78.805% (21687/27520)/ 94.491% (26004/27520)
01/14/2023 06:45:05 - INFO - __main__ -   test: [batch: 215/391 ] | Loss: 0.835 | Acc: 78.823% (21793/27648)/ 94.481% (26122/27648)
01/14/2023 06:45:07 - INFO - __main__ -   test: [batch: 216/391 ] | Loss: 0.833 | Acc: 78.863% (21905/27776)/ 94.502% (26249/27776)
01/14/2023 06:45:10 - INFO - __main__ -   test: [batch: 217/391 ] | Loss: 0.838 | Acc: 78.799% (21988/27904)/ 94.442% (26353/27904)
01/14/2023 06:45:13 - INFO - __main__ -   test: [batch: 218/391 ] | Loss: 0.841 | Acc: 78.749% (22075/28032)/ 94.406% (26464/28032)
01/14/2023 06:45:15 - INFO - __main__ -   test: [batch: 219/391 ] | Loss: 0.839 | Acc: 78.782% (22185/28160)/ 94.414% (26587/28160)
01/14/2023 06:45:18 - INFO - __main__ -   test: [batch: 220/391 ] | Loss: 0.837 | Acc: 78.821% (22297/28288)/ 94.429% (26712/28288)
01/14/2023 06:45:20 - INFO - __main__ -   test: [batch: 221/391 ] | Loss: 0.840 | Acc: 78.780% (22386/28416)/ 94.415% (26829/28416)
01/14/2023 06:45:23 - INFO - __main__ -   test: [batch: 222/391 ] | Loss: 0.838 | Acc: 78.829% (22501/28544)/ 94.430% (26954/28544)
01/14/2023 06:45:26 - INFO - __main__ -   test: [batch: 223/391 ] | Loss: 0.838 | Acc: 78.830% (22602/28672)/ 94.420% (27072/28672)
01/14/2023 06:45:28 - INFO - __main__ -   test: [batch: 224/391 ] | Loss: 0.837 | Acc: 78.851% (22709/28800)/ 94.413% (27191/28800)
01/14/2023 06:45:31 - INFO - __main__ -   test: [batch: 225/391 ] | Loss: 0.837 | Acc: 78.854% (22811/28928)/ 94.424% (27315/28928)
01/14/2023 06:45:33 - INFO - __main__ -   test: [batch: 226/391 ] | Loss: 0.837 | Acc: 78.834% (22906/29056)/ 94.425% (27436/29056)
01/14/2023 06:45:36 - INFO - __main__ -   test: [batch: 227/391 ] | Loss: 0.839 | Acc: 78.821% (23003/29184)/ 94.411% (27553/29184)
01/14/2023 06:45:39 - INFO - __main__ -   test: [batch: 228/391 ] | Loss: 0.844 | Acc: 78.749% (23083/29312)/ 94.354% (27657/29312)
01/14/2023 06:45:41 - INFO - __main__ -   test: [batch: 229/391 ] | Loss: 0.847 | Acc: 78.696% (23168/29440)/ 94.310% (27765/29440)
01/14/2023 06:45:44 - INFO - __main__ -   test: [batch: 230/391 ] | Loss: 0.850 | Acc: 78.639% (23252/29568)/ 94.281% (27877/29568)
01/14/2023 06:45:46 - INFO - __main__ -   test: [batch: 231/391 ] | Loss: 0.850 | Acc: 78.627% (23349/29696)/ 94.269% (27994/29696)
01/14/2023 06:45:49 - INFO - __main__ -   test: [batch: 232/391 ] | Loss: 0.849 | Acc: 78.658% (23459/29824)/ 94.283% (28119/29824)
01/14/2023 06:45:52 - INFO - __main__ -   test: [batch: 233/391 ] | Loss: 0.850 | Acc: 78.612% (23546/29952)/ 94.264% (28234/29952)
01/14/2023 06:45:54 - INFO - __main__ -   test: [batch: 234/391 ] | Loss: 0.857 | Acc: 78.491% (23610/30080)/ 94.192% (28333/30080)
01/14/2023 06:45:57 - INFO - __main__ -   test: [batch: 235/391 ] | Loss: 0.857 | Acc: 78.479% (23707/30208)/ 94.187% (28452/30208)
01/14/2023 06:45:59 - INFO - __main__ -   test: [batch: 236/391 ] | Loss: 0.856 | Acc: 78.517% (23819/30336)/ 94.195% (28575/30336)
01/14/2023 06:46:02 - INFO - __main__ -   test: [batch: 237/391 ] | Loss: 0.856 | Acc: 78.512% (23918/30464)/ 94.160% (28685/30464)
01/14/2023 06:46:05 - INFO - __main__ -   test: [batch: 238/391 ] | Loss: 0.855 | Acc: 78.550% (24030/30592)/ 94.168% (28808/30592)
01/14/2023 06:46:07 - INFO - __main__ -   test: [batch: 239/391 ] | Loss: 0.854 | Acc: 78.587% (24142/30720)/ 94.170% (28929/30720)
01/14/2023 06:46:10 - INFO - __main__ -   test: [batch: 240/391 ] | Loss: 0.854 | Acc: 78.605% (24248/30848)/ 94.162% (29047/30848)
01/14/2023 06:46:13 - INFO - __main__ -   test: [batch: 241/391 ] | Loss: 0.857 | Acc: 78.519% (24322/30976)/ 94.115% (29153/30976)
01/14/2023 06:46:15 - INFO - __main__ -   test: [batch: 242/391 ] | Loss: 0.859 | Acc: 78.418% (24391/31104)/ 94.100% (29269/31104)
01/14/2023 06:46:18 - INFO - __main__ -   test: [batch: 243/391 ] | Loss: 0.865 | Acc: 78.279% (24448/31232)/ 94.029% (29367/31232)
01/14/2023 06:46:20 - INFO - __main__ -   test: [batch: 244/391 ] | Loss: 0.866 | Acc: 78.288% (24551/31360)/ 94.024% (29486/31360)
01/14/2023 06:46:23 - INFO - __main__ -   test: [batch: 245/391 ] | Loss: 0.865 | Acc: 78.296% (24654/31488)/ 94.017% (29604/31488)
01/14/2023 06:46:25 - INFO - __main__ -   test: [batch: 246/391 ] | Loss: 0.866 | Acc: 78.277% (24748/31616)/ 94.009% (29722/31616)
01/14/2023 06:46:28 - INFO - __main__ -   test: [batch: 247/391 ] | Loss: 0.871 | Acc: 78.179% (24817/31744)/ 93.939% (29820/31744)
01/14/2023 06:46:30 - INFO - __main__ -   test: [batch: 248/391 ] | Loss: 0.872 | Acc: 78.141% (24905/31872)/ 93.932% (29938/31872)
01/14/2023 06:46:33 - INFO - __main__ -   test: [batch: 249/391 ] | Loss: 0.874 | Acc: 78.006% (24962/32000)/ 93.922% (30055/32000)
01/14/2023 06:46:36 - INFO - __main__ -   test: [batch: 250/391 ] | Loss: 0.873 | Acc: 78.041% (25073/32128)/ 93.940% (30181/32128)
01/14/2023 06:46:38 - INFO - __main__ -   test: [batch: 251/391 ] | Loss: 0.875 | Acc: 77.998% (25159/32256)/ 93.914% (30293/32256)
01/14/2023 06:46:41 - INFO - __main__ -   test: [batch: 252/391 ] | Loss: 0.874 | Acc: 78.026% (25268/32384)/ 93.911% (30412/32384)
01/14/2023 06:46:44 - INFO - __main__ -   test: [batch: 253/391 ] | Loss: 0.875 | Acc: 77.999% (25359/32512)/ 93.898% (30528/32512)
01/14/2023 06:46:46 - INFO - __main__ -   test: [batch: 254/391 ] | Loss: 0.879 | Acc: 77.932% (25437/32640)/ 93.854% (30634/32640)
01/14/2023 06:46:49 - INFO - __main__ -   test: [batch: 255/391 ] | Loss: 0.882 | Acc: 77.899% (25526/32768)/ 93.842% (30750/32768)
01/14/2023 06:46:51 - INFO - __main__ -   test: [batch: 256/391 ] | Loss: 0.887 | Acc: 77.781% (25587/32896)/ 93.808% (30859/32896)
01/14/2023 06:46:54 - INFO - __main__ -   test: [batch: 257/391 ] | Loss: 0.887 | Acc: 77.765% (25681/33024)/ 93.795% (30975/33024)
01/14/2023 06:46:57 - INFO - __main__ -   test: [batch: 258/391 ] | Loss: 0.888 | Acc: 77.751% (25776/33152)/ 93.783% (31091/33152)
01/14/2023 06:46:59 - INFO - __main__ -   test: [batch: 259/391 ] | Loss: 0.892 | Acc: 77.632% (25836/33280)/ 93.768% (31206/33280)
01/14/2023 06:47:02 - INFO - __main__ -   test: [batch: 260/391 ] | Loss: 0.893 | Acc: 77.604% (25926/33408)/ 93.759% (31323/33408)
01/14/2023 06:47:05 - INFO - __main__ -   test: [batch: 261/391 ] | Loss: 0.891 | Acc: 77.651% (26041/33536)/ 93.780% (31450/33536)
01/14/2023 06:47:07 - INFO - __main__ -   test: [batch: 262/391 ] | Loss: 0.890 | Acc: 77.673% (26148/33664)/ 93.786% (31572/33664)
01/14/2023 06:47:10 - INFO - __main__ -   test: [batch: 263/391 ] | Loss: 0.893 | Acc: 77.604% (26224/33792)/ 93.759% (31683/33792)
01/14/2023 06:47:13 - INFO - __main__ -   test: [batch: 264/391 ] | Loss: 0.898 | Acc: 77.547% (26304/33920)/ 93.697% (31782/33920)
01/14/2023 06:47:15 - INFO - __main__ -   test: [batch: 265/391 ] | Loss: 0.897 | Acc: 77.573% (26412/34048)/ 93.697% (31902/34048)
01/14/2023 06:47:18 - INFO - __main__ -   test: [batch: 266/391 ] | Loss: 0.900 | Acc: 77.505% (26488/34176)/ 93.680% (32016/34176)
01/14/2023 06:47:20 - INFO - __main__ -   test: [batch: 267/391 ] | Loss: 0.899 | Acc: 77.539% (26599/34304)/ 93.680% (32136/34304)
01/14/2023 06:47:23 - INFO - __main__ -   test: [batch: 268/391 ] | Loss: 0.898 | Acc: 77.559% (26705/34432)/ 93.674% (32254/34432)
01/14/2023 06:47:26 - INFO - __main__ -   test: [batch: 269/391 ] | Loss: 0.900 | Acc: 77.503% (26785/34560)/ 93.655% (32367/34560)
01/14/2023 06:47:28 - INFO - __main__ -   test: [batch: 270/391 ] | Loss: 0.903 | Acc: 77.436% (26861/34688)/ 93.623% (32476/34688)
01/14/2023 06:47:31 - INFO - __main__ -   test: [batch: 271/391 ] | Loss: 0.903 | Acc: 77.450% (26965/34816)/ 93.618% (32594/34816)
01/14/2023 06:47:34 - INFO - __main__ -   test: [batch: 272/391 ] | Loss: 0.905 | Acc: 77.412% (27051/34944)/ 93.615% (32713/34944)
01/14/2023 06:47:36 - INFO - __main__ -   test: [batch: 273/391 ] | Loss: 0.905 | Acc: 77.418% (27152/35072)/ 93.599% (32827/35072)
01/14/2023 06:47:39 - INFO - __main__ -   test: [batch: 274/391 ] | Loss: 0.905 | Acc: 77.415% (27250/35200)/ 93.602% (32948/35200)
01/14/2023 06:47:42 - INFO - __main__ -   test: [batch: 275/391 ] | Loss: 0.907 | Acc: 77.386% (27339/35328)/ 93.597% (33066/35328)
01/14/2023 06:47:44 - INFO - __main__ -   test: [batch: 276/391 ] | Loss: 0.907 | Acc: 77.352% (27426/35456)/ 93.595% (33185/35456)
01/14/2023 06:47:47 - INFO - __main__ -   test: [batch: 277/391 ] | Loss: 0.908 | Acc: 77.327% (27516/35584)/ 93.581% (33300/35584)
01/14/2023 06:47:50 - INFO - __main__ -   test: [batch: 278/391 ] | Loss: 0.908 | Acc: 77.338% (27619/35712)/ 93.579% (33419/35712)
01/14/2023 06:47:52 - INFO - __main__ -   test: [batch: 279/391 ] | Loss: 0.908 | Acc: 77.347% (27721/35840)/ 93.566% (33534/35840)
01/14/2023 06:47:55 - INFO - __main__ -   test: [batch: 280/391 ] | Loss: 0.909 | Acc: 77.327% (27813/35968)/ 93.564% (33653/35968)
01/14/2023 06:47:57 - INFO - __main__ -   test: [batch: 281/391 ] | Loss: 0.909 | Acc: 77.335% (27915/36096)/ 93.559% (33771/36096)
01/14/2023 06:48:00 - INFO - __main__ -   test: [batch: 282/391 ] | Loss: 0.908 | Acc: 77.369% (28026/36224)/ 93.565% (33893/36224)
01/14/2023 06:48:03 - INFO - __main__ -   test: [batch: 283/391 ] | Loss: 0.908 | Acc: 77.349% (28118/36352)/ 93.557% (34010/36352)
01/14/2023 06:48:05 - INFO - __main__ -   test: [batch: 284/391 ] | Loss: 0.912 | Acc: 77.300% (28199/36480)/ 93.528% (34119/36480)
01/14/2023 06:48:08 - INFO - __main__ -   test: [batch: 285/391 ] | Loss: 0.914 | Acc: 77.259% (28283/36608)/ 93.485% (34223/36608)
01/14/2023 06:48:10 - INFO - __main__ -   test: [batch: 286/391 ] | Loss: 0.915 | Acc: 77.224% (28369/36736)/ 93.475% (34339/36736)
01/14/2023 06:48:13 - INFO - __main__ -   test: [batch: 287/391 ] | Loss: 0.915 | Acc: 77.227% (28469/36864)/ 93.465% (34455/36864)
01/14/2023 06:48:16 - INFO - __main__ -   test: [batch: 288/391 ] | Loss: 0.915 | Acc: 77.246% (28575/36992)/ 93.469% (34576/36992)
01/14/2023 06:48:18 - INFO - __main__ -   test: [batch: 289/391 ] | Loss: 0.917 | Acc: 77.206% (28659/37120)/ 93.438% (34684/37120)
01/14/2023 06:48:21 - INFO - __main__ -   test: [batch: 290/391 ] | Loss: 0.918 | Acc: 77.129% (28729/37248)/ 93.441% (34805/37248)
01/14/2023 06:48:24 - INFO - __main__ -   test: [batch: 291/391 ] | Loss: 0.918 | Acc: 77.138% (28831/37376)/ 93.434% (34922/37376)
01/14/2023 06:48:26 - INFO - __main__ -   test: [batch: 292/391 ] | Loss: 0.920 | Acc: 77.085% (28910/37504)/ 93.414% (35034/37504)
01/14/2023 06:48:29 - INFO - __main__ -   test: [batch: 293/391 ] | Loss: 0.920 | Acc: 77.081% (29007/37632)/ 93.407% (35151/37632)
01/14/2023 06:48:31 - INFO - __main__ -   test: [batch: 294/391 ] | Loss: 0.921 | Acc: 77.066% (29100/37760)/ 93.398% (35267/37760)
01/14/2023 06:48:34 - INFO - __main__ -   test: [batch: 295/391 ] | Loss: 0.920 | Acc: 77.096% (29210/37888)/ 93.402% (35388/37888)
01/14/2023 06:48:37 - INFO - __main__ -   test: [batch: 296/391 ] | Loss: 0.920 | Acc: 77.091% (29307/38016)/ 93.395% (35505/38016)
01/14/2023 06:48:39 - INFO - __main__ -   test: [batch: 297/391 ] | Loss: 0.922 | Acc: 77.068% (29397/38144)/ 93.370% (35615/38144)
01/14/2023 06:48:42 - INFO - __main__ -   test: [batch: 298/391 ] | Loss: 0.924 | Acc: 77.038% (29484/38272)/ 93.342% (35724/38272)
01/14/2023 06:48:44 - INFO - __main__ -   test: [batch: 299/391 ] | Loss: 0.925 | Acc: 77.026% (29578/38400)/ 93.323% (35836/38400)
01/14/2023 06:48:47 - INFO - __main__ -   test: [batch: 300/391 ] | Loss: 0.925 | Acc: 77.032% (29679/38528)/ 93.322% (35955/38528)
01/14/2023 06:48:50 - INFO - __main__ -   test: [batch: 301/391 ] | Loss: 0.926 | Acc: 77.018% (29772/38656)/ 93.310% (36070/38656)
01/14/2023 06:48:52 - INFO - __main__ -   test: [batch: 302/391 ] | Loss: 0.928 | Acc: 76.983% (29857/38784)/ 93.294% (36183/38784)
01/14/2023 06:48:55 - INFO - __main__ -   test: [batch: 303/391 ] | Loss: 0.929 | Acc: 76.961% (29947/38912)/ 93.280% (36297/38912)
01/14/2023 06:48:57 - INFO - __main__ -   test: [batch: 304/391 ] | Loss: 0.928 | Acc: 76.977% (30052/39040)/ 93.284% (36418/39040)
01/14/2023 06:49:00 - INFO - __main__ -   test: [batch: 305/391 ] | Loss: 0.929 | Acc: 76.938% (30135/39168)/ 93.278% (36535/39168)
01/14/2023 06:49:03 - INFO - __main__ -   test: [batch: 306/391 ] | Loss: 0.930 | Acc: 76.919% (30226/39296)/ 93.254% (36645/39296)
01/14/2023 06:49:05 - INFO - __main__ -   test: [batch: 307/391 ] | Loss: 0.931 | Acc: 76.918% (30324/39424)/ 93.240% (36759/39424)
01/14/2023 06:49:08 - INFO - __main__ -   test: [batch: 308/391 ] | Loss: 0.931 | Acc: 76.901% (30416/39552)/ 93.239% (36878/39552)
01/14/2023 06:49:10 - INFO - __main__ -   test: [batch: 309/391 ] | Loss: 0.932 | Acc: 76.903% (30515/39680)/ 93.218% (36989/39680)
01/14/2023 06:49:13 - INFO - __main__ -   test: [batch: 310/391 ] | Loss: 0.932 | Acc: 76.892% (30609/39808)/ 93.207% (37104/39808)
01/14/2023 06:49:16 - INFO - __main__ -   test: [batch: 311/391 ] | Loss: 0.934 | Acc: 76.868% (30698/39936)/ 93.197% (37219/39936)
01/14/2023 06:49:18 - INFO - __main__ -   test: [batch: 312/391 ] | Loss: 0.935 | Acc: 76.852% (30790/40064)/ 93.173% (37329/40064)
01/14/2023 06:49:21 - INFO - __main__ -   test: [batch: 313/391 ] | Loss: 0.934 | Acc: 76.893% (30905/40192)/ 93.193% (37456/40192)
01/14/2023 06:49:24 - INFO - __main__ -   test: [batch: 314/391 ] | Loss: 0.934 | Acc: 76.882% (30999/40320)/ 93.189% (37574/40320)
01/14/2023 06:49:26 - INFO - __main__ -   test: [batch: 315/391 ] | Loss: 0.935 | Acc: 76.867% (31091/40448)/ 93.171% (37686/40448)
01/14/2023 06:49:29 - INFO - __main__ -   test: [batch: 316/391 ] | Loss: 0.937 | Acc: 76.779% (31154/40576)/ 93.149% (37796/40576)
01/14/2023 06:49:31 - INFO - __main__ -   test: [batch: 317/391 ] | Loss: 0.939 | Acc: 76.734% (31234/40704)/ 93.126% (37906/40704)
01/14/2023 06:49:34 - INFO - __main__ -   test: [batch: 318/391 ] | Loss: 0.938 | Acc: 76.768% (31346/40832)/ 93.143% (38032/40832)
01/14/2023 06:49:36 - INFO - __main__ -   test: [batch: 319/391 ] | Loss: 0.941 | Acc: 76.711% (31421/40960)/ 93.115% (38140/40960)
01/14/2023 06:49:39 - INFO - __main__ -   test: [batch: 320/391 ] | Loss: 0.939 | Acc: 76.745% (31533/41088)/ 93.122% (38262/41088)
01/14/2023 06:49:42 - INFO - __main__ -   test: [batch: 321/391 ] | Loss: 0.939 | Acc: 76.764% (31639/41216)/ 93.114% (38378/41216)
01/14/2023 06:49:45 - INFO - __main__ -   test: [batch: 322/391 ] | Loss: 0.941 | Acc: 76.734% (31725/41344)/ 93.107% (38494/41344)
01/14/2023 06:49:47 - INFO - __main__ -   test: [batch: 323/391 ] | Loss: 0.943 | Acc: 76.693% (31806/41472)/ 93.082% (38603/41472)
01/14/2023 06:49:50 - INFO - __main__ -   test: [batch: 324/391 ] | Loss: 0.943 | Acc: 76.690% (31903/41600)/ 93.075% (38719/41600)
01/14/2023 06:49:52 - INFO - __main__ -   test: [batch: 325/391 ] | Loss: 0.943 | Acc: 76.697% (32004/41728)/ 93.074% (38838/41728)
01/14/2023 06:49:55 - INFO - __main__ -   test: [batch: 326/391 ] | Loss: 0.946 | Acc: 76.608% (32065/41856)/ 93.045% (38945/41856)
01/14/2023 06:49:58 - INFO - __main__ -   test: [batch: 327/391 ] | Loss: 0.949 | Acc: 76.527% (32129/41984)/ 93.014% (39051/41984)
01/14/2023 06:50:00 - INFO - __main__ -   test: [batch: 328/391 ] | Loss: 0.951 | Acc: 76.475% (32205/42112)/ 92.988% (39159/42112)
01/14/2023 06:50:03 - INFO - __main__ -   test: [batch: 329/391 ] | Loss: 0.951 | Acc: 76.470% (32301/42240)/ 92.990% (39279/42240)
01/14/2023 06:50:05 - INFO - __main__ -   test: [batch: 330/391 ] | Loss: 0.953 | Acc: 76.428% (32381/42368)/ 92.966% (39388/42368)
01/14/2023 06:50:08 - INFO - __main__ -   test: [batch: 331/391 ] | Loss: 0.953 | Acc: 76.398% (32466/42496)/ 92.978% (39512/42496)
01/14/2023 06:50:11 - INFO - __main__ -   test: [batch: 332/391 ] | Loss: 0.954 | Acc: 76.389% (32560/42624)/ 92.978% (39631/42624)
01/14/2023 06:50:13 - INFO - __main__ -   test: [batch: 333/391 ] | Loss: 0.952 | Acc: 76.420% (32671/42752)/ 92.987% (39754/42752)
01/14/2023 06:50:16 - INFO - __main__ -   test: [batch: 334/391 ] | Loss: 0.954 | Acc: 76.390% (32756/42880)/ 92.971% (39866/42880)
01/14/2023 06:50:18 - INFO - __main__ -   test: [batch: 335/391 ] | Loss: 0.955 | Acc: 76.367% (32844/43008)/ 92.957% (39979/43008)
01/14/2023 06:50:21 - INFO - __main__ -   test: [batch: 336/391 ] | Loss: 0.957 | Acc: 76.321% (32922/43136)/ 92.943% (40092/43136)
01/14/2023 06:50:23 - INFO - __main__ -   test: [batch: 337/391 ] | Loss: 0.957 | Acc: 76.315% (33017/43264)/ 92.939% (40209/43264)
01/14/2023 06:50:26 - INFO - __main__ -   test: [batch: 338/391 ] | Loss: 0.957 | Acc: 76.300% (33108/43392)/ 92.948% (40332/43392)
01/14/2023 06:50:29 - INFO - __main__ -   test: [batch: 339/391 ] | Loss: 0.959 | Acc: 76.264% (33190/43520)/ 92.923% (40440/43520)
01/14/2023 06:50:32 - INFO - __main__ -   test: [batch: 340/391 ] | Loss: 0.959 | Acc: 76.258% (33285/43648)/ 92.932% (40563/43648)
01/14/2023 06:50:34 - INFO - __main__ -   test: [batch: 341/391 ] | Loss: 0.957 | Acc: 76.298% (33400/43776)/ 92.950% (40690/43776)
01/14/2023 06:50:37 - INFO - __main__ -   test: [batch: 342/391 ] | Loss: 0.958 | Acc: 76.248% (33476/43904)/ 92.944% (40806/43904)
01/14/2023 06:50:40 - INFO - __main__ -   test: [batch: 343/391 ] | Loss: 0.958 | Acc: 76.249% (33574/44032)/ 92.946% (40926/44032)
01/14/2023 06:50:42 - INFO - __main__ -   test: [batch: 344/391 ] | Loss: 0.958 | Acc: 76.248% (33671/44160)/ 92.935% (41040/44160)
01/14/2023 06:50:45 - INFO - __main__ -   test: [batch: 345/391 ] | Loss: 0.962 | Acc: 76.179% (33738/44288)/ 92.896% (41142/44288)
01/14/2023 06:50:47 - INFO - __main__ -   test: [batch: 346/391 ] | Loss: 0.963 | Acc: 76.159% (33827/44416)/ 92.883% (41255/44416)
01/14/2023 06:50:50 - INFO - __main__ -   test: [batch: 347/391 ] | Loss: 0.963 | Acc: 76.179% (33933/44544)/ 92.892% (41378/44544)
01/14/2023 06:50:53 - INFO - __main__ -   test: [batch: 348/391 ] | Loss: 0.964 | Acc: 76.155% (34020/44672)/ 92.870% (41487/44672)
01/14/2023 06:50:55 - INFO - __main__ -   test: [batch: 349/391 ] | Loss: 0.964 | Acc: 76.154% (34117/44800)/ 92.882% (41611/44800)
01/14/2023 06:50:58 - INFO - __main__ -   test: [batch: 350/391 ] | Loss: 0.964 | Acc: 76.171% (34222/44928)/ 92.882% (41730/44928)
01/14/2023 06:51:00 - INFO - __main__ -   test: [batch: 351/391 ] | Loss: 0.966 | Acc: 76.116% (34295/45056)/ 92.862% (41840/45056)
01/14/2023 06:51:03 - INFO - __main__ -   test: [batch: 352/391 ] | Loss: 0.966 | Acc: 76.113% (34391/45184)/ 92.858% (41957/45184)
01/14/2023 06:51:06 - INFO - __main__ -   test: [batch: 353/391 ] | Loss: 0.968 | Acc: 76.073% (34470/45312)/ 92.823% (42060/45312)
01/14/2023 06:51:08 - INFO - __main__ -   test: [batch: 354/391 ] | Loss: 0.971 | Acc: 76.015% (34541/45440)/ 92.799% (42168/45440)
01/14/2023 06:51:11 - INFO - __main__ -   test: [batch: 355/391 ] | Loss: 0.973 | Acc: 75.948% (34608/45568)/ 92.791% (42283/45568)
01/14/2023 06:51:13 - INFO - __main__ -   test: [batch: 356/391 ] | Loss: 0.974 | Acc: 75.943% (34703/45696)/ 92.791% (42402/45696)
01/14/2023 06:51:16 - INFO - __main__ -   test: [batch: 357/391 ] | Loss: 0.972 | Acc: 75.978% (34816/45824)/ 92.805% (42527/45824)
01/14/2023 06:51:19 - INFO - __main__ -   test: [batch: 358/391 ] | Loss: 0.972 | Acc: 76.008% (34927/45952)/ 92.808% (42647/45952)
01/14/2023 06:51:21 - INFO - __main__ -   test: [batch: 359/391 ] | Loss: 0.972 | Acc: 76.020% (35030/46080)/ 92.802% (42763/46080)
01/14/2023 06:51:24 - INFO - __main__ -   test: [batch: 360/391 ] | Loss: 0.974 | Acc: 75.993% (35115/46208)/ 92.791% (42877/46208)
01/14/2023 06:51:26 - INFO - __main__ -   test: [batch: 361/391 ] | Loss: 0.974 | Acc: 75.993% (35212/46336)/ 92.798% (42999/46336)
01/14/2023 06:51:29 - INFO - __main__ -   test: [batch: 362/391 ] | Loss: 0.973 | Acc: 75.988% (35307/46464)/ 92.812% (43124/46464)
01/14/2023 06:51:32 - INFO - __main__ -   test: [batch: 363/391 ] | Loss: 0.973 | Acc: 75.979% (35400/46592)/ 92.808% (43241/46592)
01/14/2023 06:51:34 - INFO - __main__ -   test: [batch: 364/391 ] | Loss: 0.972 | Acc: 76.006% (35510/46720)/ 92.817% (43364/46720)
01/14/2023 06:51:37 - INFO - __main__ -   test: [batch: 365/391 ] | Loss: 0.972 | Acc: 76.020% (35614/46848)/ 92.821% (43485/46848)
01/14/2023 06:51:39 - INFO - __main__ -   test: [batch: 366/391 ] | Loss: 0.970 | Acc: 76.056% (35728/46976)/ 92.839% (43612/46976)
01/14/2023 06:51:42 - INFO - __main__ -   test: [batch: 367/391 ] | Loss: 0.969 | Acc: 76.078% (35836/47104)/ 92.852% (43737/47104)
01/14/2023 06:51:45 - INFO - __main__ -   test: [batch: 368/391 ] | Loss: 0.969 | Acc: 76.084% (35936/47232)/ 92.861% (43860/47232)
01/14/2023 06:51:47 - INFO - __main__ -   test: [batch: 369/391 ] | Loss: 0.968 | Acc: 76.104% (36043/47360)/ 92.870% (43983/47360)
01/14/2023 06:51:50 - INFO - __main__ -   test: [batch: 370/391 ] | Loss: 0.968 | Acc: 76.091% (36134/47488)/ 92.870% (44102/47488)
01/14/2023 06:51:52 - INFO - __main__ -   test: [batch: 371/391 ] | Loss: 0.968 | Acc: 76.098% (36235/47616)/ 92.872% (44222/47616)
01/14/2023 06:51:55 - INFO - __main__ -   test: [batch: 372/391 ] | Loss: 0.966 | Acc: 76.146% (36355/47744)/ 92.887% (44348/47744)
01/14/2023 06:51:58 - INFO - __main__ -   test: [batch: 373/391 ] | Loss: 0.965 | Acc: 76.182% (36470/47872)/ 92.896% (44471/47872)
01/14/2023 06:52:00 - INFO - __main__ -   test: [batch: 374/391 ] | Loss: 0.963 | Acc: 76.215% (36583/48000)/ 92.902% (44593/48000)
01/14/2023 06:52:03 - INFO - __main__ -   test: [batch: 375/391 ] | Loss: 0.966 | Acc: 76.147% (36648/48128)/ 92.865% (44694/48128)
01/14/2023 06:52:05 - INFO - __main__ -   test: [batch: 376/391 ] | Loss: 0.967 | Acc: 76.140% (36742/48256)/ 92.851% (44806/48256)
01/14/2023 06:52:08 - INFO - __main__ -   test: [batch: 377/391 ] | Loss: 0.967 | Acc: 76.128% (36834/48384)/ 92.843% (44921/48384)
01/14/2023 06:52:10 - INFO - __main__ -   test: [batch: 378/391 ] | Loss: 0.971 | Acc: 76.053% (36895/48512)/ 92.796% (45017/48512)
01/14/2023 06:52:13 - INFO - __main__ -   test: [batch: 379/391 ] | Loss: 0.971 | Acc: 76.036% (36984/48640)/ 92.802% (45139/48640)
01/14/2023 06:52:16 - INFO - __main__ -   test: [batch: 380/391 ] | Loss: 0.971 | Acc: 76.033% (37080/48768)/ 92.809% (45261/48768)
01/14/2023 06:52:18 - INFO - __main__ -   test: [batch: 381/391 ] | Loss: 0.972 | Acc: 75.982% (37152/48896)/ 92.807% (45379/48896)
01/14/2023 06:52:21 - INFO - __main__ -   test: [batch: 382/391 ] | Loss: 0.974 | Acc: 75.946% (37232/49024)/ 92.791% (45490/49024)
01/14/2023 06:52:23 - INFO - __main__ -   test: [batch: 383/391 ] | Loss: 0.974 | Acc: 75.954% (37333/49152)/ 92.784% (45605/49152)
01/14/2023 06:52:26 - INFO - __main__ -   test: [batch: 384/391 ] | Loss: 0.972 | Acc: 75.994% (37450/49280)/ 92.796% (45730/49280)
01/14/2023 06:52:29 - INFO - __main__ -   test: [batch: 385/391 ] | Loss: 0.971 | Acc: 76.004% (37552/49408)/ 92.807% (45854/49408)
01/14/2023 06:52:31 - INFO - __main__ -   test: [batch: 386/391 ] | Loss: 0.969 | Acc: 76.050% (37672/49536)/ 92.821% (45980/49536)
01/14/2023 06:52:34 - INFO - __main__ -   test: [batch: 387/391 ] | Loss: 0.968 | Acc: 76.089% (37789/49664)/ 92.832% (46104/49664)
01/14/2023 06:52:36 - INFO - __main__ -   test: [batch: 388/391 ] | Loss: 0.966 | Acc: 76.135% (37909/49792)/ 92.844% (46229/49792)
01/14/2023 06:52:39 - INFO - __main__ -   test: [batch: 389/391 ] | Loss: 0.965 | Acc: 76.136% (38007/49920)/ 92.847% (46349/49920)
01/14/2023 06:52:41 - INFO - __main__ -   test: [batch: 390/391 ] | Loss: 0.967 | Acc: 76.088% (38044/50000)/ 92.838% (46419/50000)
01/14/2023 06:52:41 - INFO - __main__ -   Final accuracy: 76.088
01/14/2023 06:52:41 - INFO - __main__ -   {'milestones': Counter({2: 1, 3: 1, 4: 1}), 'gamma': 0.1, 'base_lrs': [0.00025], 'last_epoch': 2, '_step_count': 3, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [2.5e-05]}
01/14/2023 06:52:41 - INFO - __main__ -   
Epoch: 2
01/14/2023 06:52:44 - INFO - __main__ -   test: [epoch: 2 | batch: 0/10010 ] | Loss: 0.975 | Acc: 78.125% (100/128)
01/14/2023 06:57:08 - INFO - __main__ -   test: [epoch: 2 | batch: 100/10010 ] | Loss: 0.830 | Acc: 79.672% (10300/12928)
01/14/2023 07:01:28 - INFO - __main__ -   test: [epoch: 2 | batch: 200/10010 ] | Loss: 0.839 | Acc: 79.015% (20329/25728)
01/14/2023 07:05:48 - INFO - __main__ -   test: [epoch: 2 | batch: 300/10010 ] | Loss: 0.843 | Acc: 79.018% (30444/38528)
01/14/2023 07:10:08 - INFO - __main__ -   test: [epoch: 2 | batch: 400/10010 ] | Loss: 0.849 | Acc: 78.830% (40462/51328)
01/14/2023 07:14:29 - INFO - __main__ -   test: [epoch: 2 | batch: 500/10010 ] | Loss: 0.853 | Acc: 78.769% (50513/64128)
01/14/2023 07:18:51 - INFO - __main__ -   test: [epoch: 2 | batch: 600/10010 ] | Loss: 0.857 | Acc: 78.692% (60536/76928)
01/14/2023 07:23:13 - INFO - __main__ -   test: [epoch: 2 | batch: 700/10010 ] | Loss: 0.858 | Acc: 78.629% (70552/89728)
01/14/2023 07:27:34 - INFO - __main__ -   test: [epoch: 2 | batch: 800/10010 ] | Loss: 0.859 | Acc: 78.606% (80593/102528)
01/14/2023 07:31:55 - INFO - __main__ -   test: [epoch: 2 | batch: 900/10010 ] | Loss: 0.857 | Acc: 78.673% (90732/115328)
01/14/2023 07:36:16 - INFO - __main__ -   test: [epoch: 2 | batch: 1000/10010 ] | Loss: 0.859 | Acc: 78.619% (100733/128128)
01/14/2023 07:40:37 - INFO - __main__ -   test: [epoch: 2 | batch: 1100/10010 ] | Loss: 0.860 | Acc: 78.625% (110804/140928)
01/14/2023 07:44:56 - INFO - __main__ -   test: [epoch: 2 | batch: 1200/10010 ] | Loss: 0.861 | Acc: 78.595% (120823/153728)
01/14/2023 07:49:18 - INFO - __main__ -   test: [epoch: 2 | batch: 1300/10010 ] | Loss: 0.860 | Acc: 78.613% (130912/166528)
01/14/2023 07:53:38 - INFO - __main__ -   test: [epoch: 2 | batch: 1400/10010 ] | Loss: 0.860 | Acc: 78.631% (141008/179328)
01/14/2023 07:58:00 - INFO - __main__ -   test: [epoch: 2 | batch: 1500/10010 ] | Loss: 0.860 | Acc: 78.596% (151004/192128)
01/14/2023 08:02:21 - INFO - __main__ -   test: [epoch: 2 | batch: 1600/10010 ] | Loss: 0.861 | Acc: 78.569% (161009/204928)
01/14/2023 08:06:42 - INFO - __main__ -   test: [epoch: 2 | batch: 1700/10010 ] | Loss: 0.861 | Acc: 78.562% (171051/217728)
01/14/2023 08:11:04 - INFO - __main__ -   test: [epoch: 2 | batch: 1800/10010 ] | Loss: 0.860 | Acc: 78.570% (181126/230528)
01/14/2023 08:15:25 - INFO - __main__ -   test: [epoch: 2 | batch: 1900/10010 ] | Loss: 0.859 | Acc: 78.588% (191227/243328)
01/14/2023 08:19:46 - INFO - __main__ -   test: [epoch: 2 | batch: 2000/10010 ] | Loss: 0.858 | Acc: 78.605% (201329/256128)
01/14/2023 08:24:06 - INFO - __main__ -   test: [epoch: 2 | batch: 2100/10010 ] | Loss: 0.858 | Acc: 78.604% (211388/268928)
01/14/2023 08:28:26 - INFO - __main__ -   test: [epoch: 2 | batch: 2200/10010 ] | Loss: 0.859 | Acc: 78.590% (221410/281728)
01/14/2023 08:32:48 - INFO - __main__ -   test: [epoch: 2 | batch: 2300/10010 ] | Loss: 0.860 | Acc: 78.566% (231399/294528)
01/14/2023 08:37:07 - INFO - __main__ -   test: [epoch: 2 | batch: 2400/10010 ] | Loss: 0.860 | Acc: 78.575% (241482/307328)
01/14/2023 08:41:30 - INFO - __main__ -   test: [epoch: 2 | batch: 2500/10010 ] | Loss: 0.860 | Acc: 78.565% (251509/320128)
01/14/2023 08:45:51 - INFO - __main__ -   test: [epoch: 2 | batch: 2600/10010 ] | Loss: 0.860 | Acc: 78.577% (261605/332928)
01/14/2023 08:50:12 - INFO - __main__ -   test: [epoch: 2 | batch: 2700/10010 ] | Loss: 0.859 | Acc: 78.611% (271781/345728)
01/14/2023 08:54:32 - INFO - __main__ -   test: [epoch: 2 | batch: 2800/10010 ] | Loss: 0.859 | Acc: 78.611% (281841/358528)
01/14/2023 08:58:53 - INFO - __main__ -   test: [epoch: 2 | batch: 2900/10010 ] | Loss: 0.859 | Acc: 78.614% (291914/371328)
01/14/2023 09:03:14 - INFO - __main__ -   test: [epoch: 2 | batch: 3000/10010 ] | Loss: 0.859 | Acc: 78.613% (301974/384128)
01/14/2023 09:07:35 - INFO - __main__ -   test: [epoch: 2 | batch: 3100/10010 ] | Loss: 0.859 | Acc: 78.622% (312073/396928)
01/14/2023 09:11:55 - INFO - __main__ -   test: [epoch: 2 | batch: 3200/10010 ] | Loss: 0.858 | Acc: 78.629% (322163/409728)
01/14/2023 09:16:15 - INFO - __main__ -   test: [epoch: 2 | batch: 3300/10010 ] | Loss: 0.859 | Acc: 78.609% (332146/422528)
01/14/2023 09:20:36 - INFO - __main__ -   test: [epoch: 2 | batch: 3400/10010 ] | Loss: 0.859 | Acc: 78.618% (342247/435328)
01/14/2023 09:24:57 - INFO - __main__ -   test: [epoch: 2 | batch: 3500/10010 ] | Loss: 0.859 | Acc: 78.620% (352318/448128)
01/14/2023 09:29:18 - INFO - __main__ -   test: [epoch: 2 | batch: 3600/10010 ] | Loss: 0.858 | Acc: 78.637% (362459/460928)
01/14/2023 09:33:39 - INFO - __main__ -   test: [epoch: 2 | batch: 3700/10010 ] | Loss: 0.859 | Acc: 78.627% (372479/473728)
01/14/2023 09:37:59 - INFO - __main__ -   test: [epoch: 2 | batch: 3800/10010 ] | Loss: 0.858 | Acc: 78.651% (382658/486528)
01/14/2023 09:42:21 - INFO - __main__ -   test: [epoch: 2 | batch: 3900/10010 ] | Loss: 0.858 | Acc: 78.639% (392666/499328)
01/14/2023 09:46:42 - INFO - __main__ -   test: [epoch: 2 | batch: 4000/10010 ] | Loss: 0.859 | Acc: 78.623% (402650/512128)
01/14/2023 09:51:03 - INFO - __main__ -   test: [epoch: 2 | batch: 4100/10010 ] | Loss: 0.859 | Acc: 78.628% (412740/524928)
01/14/2023 09:55:26 - INFO - __main__ -   test: [epoch: 2 | batch: 4200/10010 ] | Loss: 0.858 | Acc: 78.622% (422773/537728)
01/14/2023 09:59:49 - INFO - __main__ -   test: [epoch: 2 | batch: 4300/10010 ] | Loss: 0.859 | Acc: 78.623% (432844/550528)
01/14/2023 10:04:11 - INFO - __main__ -   test: [epoch: 2 | batch: 4400/10010 ] | Loss: 0.858 | Acc: 78.637% (442984/563328)
01/14/2023 10:08:34 - INFO - __main__ -   test: [epoch: 2 | batch: 4500/10010 ] | Loss: 0.859 | Acc: 78.627% (452992/576128)
01/14/2023 10:12:54 - INFO - __main__ -   test: [epoch: 2 | batch: 4600/10010 ] | Loss: 0.859 | Acc: 78.618% (463005/588928)
01/14/2023 10:17:15 - INFO - __main__ -   test: [epoch: 2 | batch: 4700/10010 ] | Loss: 0.859 | Acc: 78.617% (473059/601728)
01/14/2023 10:21:35 - INFO - __main__ -   test: [epoch: 2 | batch: 4800/10010 ] | Loss: 0.859 | Acc: 78.626% (483180/614528)
01/14/2023 10:25:58 - INFO - __main__ -   test: [epoch: 2 | batch: 4900/10010 ] | Loss: 0.859 | Acc: 78.624% (493228/627328)
01/14/2023 10:30:18 - INFO - __main__ -   test: [epoch: 2 | batch: 5000/10010 ] | Loss: 0.858 | Acc: 78.642% (503408/640128)
01/14/2023 10:34:41 - INFO - __main__ -   test: [epoch: 2 | batch: 5100/10010 ] | Loss: 0.859 | Acc: 78.630% (513397/652928)
01/14/2023 10:39:01 - INFO - __main__ -   test: [epoch: 2 | batch: 5200/10010 ] | Loss: 0.859 | Acc: 78.625% (523427/665728)
01/14/2023 10:43:24 - INFO - __main__ -   test: [epoch: 2 | batch: 5300/10010 ] | Loss: 0.859 | Acc: 78.618% (533447/678528)
01/14/2023 10:47:45 - INFO - __main__ -   test: [epoch: 2 | batch: 5400/10010 ] | Loss: 0.860 | Acc: 78.612% (543470/691328)
01/14/2023 10:52:07 - INFO - __main__ -   test: [epoch: 2 | batch: 5500/10010 ] | Loss: 0.860 | Acc: 78.608% (553503/704128)
01/14/2023 10:56:29 - INFO - __main__ -   test: [epoch: 2 | batch: 5600/10010 ] | Loss: 0.860 | Acc: 78.613% (563602/716928)
01/14/2023 11:00:49 - INFO - __main__ -   test: [epoch: 2 | batch: 5700/10010 ] | Loss: 0.859 | Acc: 78.617% (573690/729728)
01/14/2023 11:05:10 - INFO - __main__ -   test: [epoch: 2 | batch: 5800/10010 ] | Loss: 0.859 | Acc: 78.623% (583798/742528)
01/14/2023 11:09:31 - INFO - __main__ -   test: [epoch: 2 | batch: 5900/10010 ] | Loss: 0.859 | Acc: 78.634% (593946/755328)
01/14/2023 11:13:52 - INFO - __main__ -   test: [epoch: 2 | batch: 6000/10010 ] | Loss: 0.859 | Acc: 78.632% (603992/768128)
01/14/2023 11:18:14 - INFO - __main__ -   test: [epoch: 2 | batch: 6100/10010 ] | Loss: 0.859 | Acc: 78.619% (613960/780928)
01/14/2023 11:22:34 - INFO - __main__ -   test: [epoch: 2 | batch: 6200/10010 ] | Loss: 0.859 | Acc: 78.619% (624018/793728)
01/14/2023 11:26:55 - INFO - __main__ -   test: [epoch: 2 | batch: 6300/10010 ] | Loss: 0.859 | Acc: 78.620% (634089/806528)
01/14/2023 11:31:17 - INFO - __main__ -   test: [epoch: 2 | batch: 6400/10010 ] | Loss: 0.860 | Acc: 78.611% (644085/819328)
01/14/2023 11:35:38 - INFO - __main__ -   test: [epoch: 2 | batch: 6500/10010 ] | Loss: 0.859 | Acc: 78.616% (654187/832128)
01/14/2023 11:39:58 - INFO - __main__ -   test: [epoch: 2 | batch: 6600/10010 ] | Loss: 0.859 | Acc: 78.611% (664206/844928)
01/14/2023 11:44:21 - INFO - __main__ -   test: [epoch: 2 | batch: 6700/10010 ] | Loss: 0.860 | Acc: 78.608% (674240/857728)
01/14/2023 11:48:42 - INFO - __main__ -   test: [epoch: 2 | batch: 6800/10010 ] | Loss: 0.860 | Acc: 78.604% (684269/870528)
01/14/2023 11:53:02 - INFO - __main__ -   test: [epoch: 2 | batch: 6900/10010 ] | Loss: 0.859 | Acc: 78.607% (694361/883328)
01/14/2023 11:57:24 - INFO - __main__ -   test: [epoch: 2 | batch: 7000/10010 ] | Loss: 0.860 | Acc: 78.598% (704343/896128)
01/14/2023 12:01:46 - INFO - __main__ -   test: [epoch: 2 | batch: 7100/10010 ] | Loss: 0.859 | Acc: 78.604% (714458/908928)
01/14/2023 12:06:08 - INFO - __main__ -   test: [epoch: 2 | batch: 7200/10010 ] | Loss: 0.859 | Acc: 78.608% (724553/921728)
01/14/2023 12:10:31 - INFO - __main__ -   test: [epoch: 2 | batch: 7300/10010 ] | Loss: 0.859 | Acc: 78.604% (734572/934528)
01/14/2023 12:14:51 - INFO - __main__ -   test: [epoch: 2 | batch: 7400/10010 ] | Loss: 0.859 | Acc: 78.609% (744682/947328)
01/14/2023 12:19:13 - INFO - __main__ -   test: [epoch: 2 | batch: 7500/10010 ] | Loss: 0.859 | Acc: 78.615% (754804/960128)
01/14/2023 12:23:34 - INFO - __main__ -   test: [epoch: 2 | batch: 7600/10010 ] | Loss: 0.859 | Acc: 78.623% (764942/972928)
01/14/2023 12:27:55 - INFO - __main__ -   test: [epoch: 2 | batch: 7700/10010 ] | Loss: 0.859 | Acc: 78.617% (774950/985728)
01/14/2023 12:32:16 - INFO - __main__ -   test: [epoch: 2 | batch: 7800/10010 ] | Loss: 0.859 | Acc: 78.612% (784964/998528)
01/14/2023 12:36:39 - INFO - __main__ -   test: [epoch: 2 | batch: 7900/10010 ] | Loss: 0.859 | Acc: 78.616% (795062/1011328)
01/14/2023 12:40:58 - INFO - __main__ -   test: [epoch: 2 | batch: 8000/10010 ] | Loss: 0.859 | Acc: 78.615% (805115/1024128)
01/14/2023 12:45:19 - INFO - __main__ -   test: [epoch: 2 | batch: 8100/10010 ] | Loss: 0.859 | Acc: 78.618% (815208/1036928)
01/14/2023 12:49:40 - INFO - __main__ -   test: [epoch: 2 | batch: 8200/10010 ] | Loss: 0.859 | Acc: 78.618% (825272/1049728)
01/14/2023 12:54:03 - INFO - __main__ -   test: [epoch: 2 | batch: 8300/10010 ] | Loss: 0.859 | Acc: 78.614% (835293/1062528)
01/14/2023 12:58:26 - INFO - __main__ -   test: [epoch: 2 | batch: 8400/10010 ] | Loss: 0.859 | Acc: 78.616% (845385/1075328)
01/14/2023 13:02:48 - INFO - __main__ -   test: [epoch: 2 | batch: 8500/10010 ] | Loss: 0.859 | Acc: 78.621% (855496/1088128)
01/14/2023 13:07:10 - INFO - __main__ -   test: [epoch: 2 | batch: 8600/10010 ] | Loss: 0.859 | Acc: 78.625% (865608/1100928)
01/14/2023 13:11:30 - INFO - __main__ -   test: [epoch: 2 | batch: 8700/10010 ] | Loss: 0.859 | Acc: 78.628% (875698/1113728)
01/14/2023 13:15:51 - INFO - __main__ -   test: [epoch: 2 | batch: 8800/10010 ] | Loss: 0.859 | Acc: 78.632% (885808/1126528)
01/14/2023 13:20:11 - INFO - __main__ -   test: [epoch: 2 | batch: 8900/10010 ] | Loss: 0.859 | Acc: 78.632% (895879/1139328)
01/14/2023 13:24:31 - INFO - __main__ -   test: [epoch: 2 | batch: 9000/10010 ] | Loss: 0.859 | Acc: 78.625% (905858/1152128)
01/14/2023 13:28:54 - INFO - __main__ -   test: [epoch: 2 | batch: 9100/10010 ] | Loss: 0.859 | Acc: 78.620% (915868/1164928)
01/14/2023 13:33:15 - INFO - __main__ -   test: [epoch: 2 | batch: 9200/10010 ] | Loss: 0.859 | Acc: 78.624% (925982/1177728)
01/14/2023 13:37:36 - INFO - __main__ -   test: [epoch: 2 | batch: 9300/10010 ] | Loss: 0.859 | Acc: 78.621% (936006/1190528)
01/14/2023 13:41:58 - INFO - __main__ -   test: [epoch: 2 | batch: 9400/10010 ] | Loss: 0.859 | Acc: 78.624% (946108/1203328)
01/14/2023 13:46:19 - INFO - __main__ -   test: [epoch: 2 | batch: 9500/10010 ] | Loss: 0.859 | Acc: 78.621% (956130/1216128)
01/14/2023 13:50:38 - INFO - __main__ -   test: [epoch: 2 | batch: 9600/10010 ] | Loss: 0.859 | Acc: 78.622% (966203/1228928)
01/14/2023 13:55:01 - INFO - __main__ -   test: [epoch: 2 | batch: 9700/10010 ] | Loss: 0.859 | Acc: 78.616% (976200/1241728)
01/14/2023 13:59:23 - INFO - __main__ -   test: [epoch: 2 | batch: 9800/10010 ] | Loss: 0.859 | Acc: 78.614% (986231/1254528)
01/14/2023 14:03:45 - INFO - __main__ -   test: [epoch: 2 | batch: 9900/10010 ] | Loss: 0.859 | Acc: 78.616% (996320/1267328)
01/14/2023 14:08:06 - INFO - __main__ -   test: [epoch: 2 | batch: 10000/10010 ] | Loss: 0.860 | Acc: 78.612% (1006333/1280128)
01/14/2023 14:08:30 - INFO - __main__ -   Saving Checkpoint
01/14/2023 14:08:33 - INFO - __main__ -   test: [batch: 0/391 ] | Loss: 0.481 | Acc: 86.719% (111/128)/ 97.656% (125/128)
01/14/2023 14:08:36 - INFO - __main__ -   test: [batch: 1/391 ] | Loss: 0.477 | Acc: 87.109% (223/256)/ 98.047% (251/256)
01/14/2023 14:08:38 - INFO - __main__ -   test: [batch: 2/391 ] | Loss: 0.627 | Acc: 83.073% (319/384)/ 95.833% (368/384)
01/14/2023 14:08:41 - INFO - __main__ -   test: [batch: 3/391 ] | Loss: 0.580 | Acc: 84.766% (434/512)/ 96.484% (494/512)
01/14/2023 14:08:43 - INFO - __main__ -   test: [batch: 4/391 ] | Loss: 0.497 | Acc: 87.031% (557/640)/ 97.188% (622/640)
01/14/2023 14:08:46 - INFO - __main__ -   test: [batch: 5/391 ] | Loss: 0.439 | Acc: 88.411% (679/768)/ 97.656% (750/768)
01/14/2023 14:08:49 - INFO - __main__ -   test: [batch: 6/391 ] | Loss: 0.432 | Acc: 88.504% (793/896)/ 97.545% (874/896)
01/14/2023 14:08:51 - INFO - __main__ -   test: [batch: 7/391 ] | Loss: 0.411 | Acc: 89.453% (916/1024)/ 97.656% (1000/1024)
01/14/2023 14:08:54 - INFO - __main__ -   test: [batch: 8/391 ] | Loss: 0.429 | Acc: 89.149% (1027/1152)/ 97.569% (1124/1152)
01/14/2023 14:08:56 - INFO - __main__ -   test: [batch: 9/391 ] | Loss: 0.408 | Acc: 89.688% (1148/1280)/ 97.656% (1250/1280)
01/14/2023 14:08:59 - INFO - __main__ -   test: [batch: 10/391 ] | Loss: 0.457 | Acc: 88.494% (1246/1408)/ 97.656% (1375/1408)
01/14/2023 14:09:02 - INFO - __main__ -   test: [batch: 11/391 ] | Loss: 0.461 | Acc: 88.672% (1362/1536)/ 97.526% (1498/1536)
01/14/2023 14:09:04 - INFO - __main__ -   test: [batch: 12/391 ] | Loss: 0.507 | Acc: 87.620% (1458/1664)/ 97.175% (1617/1664)
01/14/2023 14:09:06 - INFO - __main__ -   test: [batch: 13/391 ] | Loss: 0.553 | Acc: 86.217% (1545/1792)/ 96.763% (1734/1792)
01/14/2023 14:09:08 - INFO - __main__ -   test: [batch: 14/391 ] | Loss: 0.574 | Acc: 85.365% (1639/1920)/ 96.875% (1860/1920)
01/14/2023 14:09:10 - INFO - __main__ -   test: [batch: 15/391 ] | Loss: 0.583 | Acc: 84.863% (1738/2048)/ 96.973% (1986/2048)
01/14/2023 14:09:12 - INFO - __main__ -   test: [batch: 16/391 ] | Loss: 0.591 | Acc: 84.697% (1843/2176)/ 96.783% (2106/2176)
01/14/2023 14:09:14 - INFO - __main__ -   test: [batch: 17/391 ] | Loss: 0.616 | Acc: 84.245% (1941/2304)/ 96.311% (2219/2304)
01/14/2023 14:09:16 - INFO - __main__ -   test: [batch: 18/391 ] | Loss: 0.640 | Acc: 83.635% (2034/2432)/ 96.176% (2339/2432)
01/14/2023 14:09:18 - INFO - __main__ -   test: [batch: 19/391 ] | Loss: 0.647 | Acc: 83.359% (2134/2560)/ 96.133% (2461/2560)
01/14/2023 14:09:21 - INFO - __main__ -   test: [batch: 20/391 ] | Loss: 0.644 | Acc: 83.445% (2243/2688)/ 96.019% (2581/2688)
01/14/2023 14:09:23 - INFO - __main__ -   test: [batch: 21/391 ] | Loss: 0.677 | Acc: 82.706% (2329/2816)/ 95.845% (2699/2816)
01/14/2023 14:09:25 - INFO - __main__ -   test: [batch: 22/391 ] | Loss: 0.676 | Acc: 82.575% (2431/2944)/ 95.822% (2821/2944)
01/14/2023 14:09:28 - INFO - __main__ -   test: [batch: 23/391 ] | Loss: 0.720 | Acc: 81.673% (2509/3072)/ 95.475% (2933/3072)
01/14/2023 14:09:30 - INFO - __main__ -   test: [batch: 24/391 ] | Loss: 0.740 | Acc: 81.125% (2596/3200)/ 95.312% (3050/3200)
01/14/2023 14:09:32 - INFO - __main__ -   test: [batch: 25/391 ] | Loss: 0.764 | Acc: 80.499% (2679/3328)/ 95.102% (3165/3328)
01/14/2023 14:09:34 - INFO - __main__ -   test: [batch: 26/391 ] | Loss: 0.777 | Acc: 79.774% (2757/3456)/ 95.081% (3286/3456)
01/14/2023 14:09:37 - INFO - __main__ -   test: [batch: 27/391 ] | Loss: 0.759 | Acc: 80.301% (2878/3584)/ 95.117% (3409/3584)
01/14/2023 14:09:39 - INFO - __main__ -   test: [batch: 28/391 ] | Loss: 0.769 | Acc: 79.741% (2960/3712)/ 95.178% (3533/3712)
01/14/2023 14:09:41 - INFO - __main__ -   test: [batch: 29/391 ] | Loss: 0.761 | Acc: 79.896% (3068/3840)/ 95.286% (3659/3840)
01/14/2023 14:09:43 - INFO - __main__ -   test: [batch: 30/391 ] | Loss: 0.774 | Acc: 79.864% (3169/3968)/ 95.161% (3776/3968)
01/14/2023 14:09:45 - INFO - __main__ -   test: [batch: 31/391 ] | Loss: 0.769 | Acc: 80.078% (3280/4096)/ 95.239% (3901/4096)
01/14/2023 14:09:47 - INFO - __main__ -   test: [batch: 32/391 ] | Loss: 0.755 | Acc: 80.398% (3396/4224)/ 95.336% (4027/4224)
01/14/2023 14:09:49 - INFO - __main__ -   test: [batch: 33/391 ] | Loss: 0.749 | Acc: 80.584% (3507/4352)/ 95.381% (4151/4352)
01/14/2023 14:09:52 - INFO - __main__ -   test: [batch: 34/391 ] | Loss: 0.736 | Acc: 80.960% (3627/4480)/ 95.424% (4275/4480)
01/14/2023 14:09:54 - INFO - __main__ -   test: [batch: 35/391 ] | Loss: 0.723 | Acc: 81.359% (3749/4608)/ 95.464% (4399/4608)
01/14/2023 14:09:56 - INFO - __main__ -   test: [batch: 36/391 ] | Loss: 0.708 | Acc: 81.799% (3874/4736)/ 95.566% (4526/4736)
01/14/2023 14:09:58 - INFO - __main__ -   test: [batch: 37/391 ] | Loss: 0.699 | Acc: 82.093% (3993/4864)/ 95.621% (4651/4864)
01/14/2023 14:10:00 - INFO - __main__ -   test: [batch: 38/391 ] | Loss: 0.691 | Acc: 82.232% (4105/4992)/ 95.693% (4777/4992)
01/14/2023 14:10:02 - INFO - __main__ -   test: [batch: 39/391 ] | Loss: 0.687 | Acc: 82.285% (4213/5120)/ 95.723% (4901/5120)
01/14/2023 14:10:04 - INFO - __main__ -   test: [batch: 40/391 ] | Loss: 0.685 | Acc: 82.412% (4325/5248)/ 95.636% (5019/5248)
01/14/2023 14:10:07 - INFO - __main__ -   test: [batch: 41/391 ] | Loss: 0.688 | Acc: 82.496% (4435/5376)/ 95.573% (5138/5376)
01/14/2023 14:10:09 - INFO - __main__ -   test: [batch: 42/391 ] | Loss: 0.688 | Acc: 82.449% (4538/5504)/ 95.603% (5262/5504)
01/14/2023 14:10:11 - INFO - __main__ -   test: [batch: 43/391 ] | Loss: 0.686 | Acc: 82.493% (4646/5632)/ 95.561% (5382/5632)
01/14/2023 14:10:13 - INFO - __main__ -   test: [batch: 44/391 ] | Loss: 0.689 | Acc: 82.517% (4753/5760)/ 95.486% (5500/5760)
01/14/2023 14:10:15 - INFO - __main__ -   test: [batch: 45/391 ] | Loss: 0.686 | Acc: 82.677% (4868/5888)/ 95.482% (5622/5888)
01/14/2023 14:10:17 - INFO - __main__ -   test: [batch: 46/391 ] | Loss: 0.689 | Acc: 82.680% (4974/6016)/ 95.495% (5745/6016)
01/14/2023 14:10:19 - INFO - __main__ -   test: [batch: 47/391 ] | Loss: 0.690 | Acc: 82.633% (5077/6144)/ 95.540% (5870/6144)
01/14/2023 14:10:21 - INFO - __main__ -   test: [batch: 48/391 ] | Loss: 0.695 | Acc: 82.478% (5173/6272)/ 95.504% (5990/6272)
01/14/2023 14:10:23 - INFO - __main__ -   test: [batch: 49/391 ] | Loss: 0.699 | Acc: 82.547% (5283/6400)/ 95.406% (6106/6400)
01/14/2023 14:10:25 - INFO - __main__ -   test: [batch: 50/391 ] | Loss: 0.690 | Acc: 82.751% (5402/6528)/ 95.450% (6231/6528)
01/14/2023 14:10:28 - INFO - __main__ -   test: [batch: 51/391 ] | Loss: 0.683 | Acc: 82.948% (5521/6656)/ 95.523% (6358/6656)
01/14/2023 14:10:30 - INFO - __main__ -   test: [batch: 52/391 ] | Loss: 0.680 | Acc: 83.019% (5632/6784)/ 95.548% (6482/6784)
01/14/2023 14:10:32 - INFO - __main__ -   test: [batch: 53/391 ] | Loss: 0.671 | Acc: 83.232% (5753/6912)/ 95.616% (6609/6912)
01/14/2023 14:10:34 - INFO - __main__ -   test: [batch: 54/391 ] | Loss: 0.664 | Acc: 83.381% (5870/7040)/ 95.639% (6733/7040)
01/14/2023 14:10:36 - INFO - __main__ -   test: [batch: 55/391 ] | Loss: 0.658 | Acc: 83.524% (5987/7168)/ 95.675% (6858/7168)
01/14/2023 14:10:38 - INFO - __main__ -   test: [batch: 56/391 ] | Loss: 0.649 | Acc: 83.731% (6109/7296)/ 95.724% (6984/7296)
01/14/2023 14:10:40 - INFO - __main__ -   test: [batch: 57/391 ] | Loss: 0.643 | Acc: 83.930% (6231/7424)/ 95.770% (7110/7424)
01/14/2023 14:10:43 - INFO - __main__ -   test: [batch: 58/391 ] | Loss: 0.641 | Acc: 83.978% (6342/7552)/ 95.763% (7232/7552)
01/14/2023 14:10:45 - INFO - __main__ -   test: [batch: 59/391 ] | Loss: 0.643 | Acc: 83.893% (6443/7680)/ 95.781% (7356/7680)
01/14/2023 14:10:47 - INFO - __main__ -   test: [batch: 60/391 ] | Loss: 0.648 | Acc: 83.786% (6542/7808)/ 95.748% (7476/7808)
01/14/2023 14:10:49 - INFO - __main__ -   test: [batch: 61/391 ] | Loss: 0.648 | Acc: 83.795% (6650/7936)/ 95.766% (7600/7936)
01/14/2023 14:10:51 - INFO - __main__ -   test: [batch: 62/391 ] | Loss: 0.647 | Acc: 83.755% (6754/8064)/ 95.771% (7723/8064)
01/14/2023 14:10:53 - INFO - __main__ -   test: [batch: 63/391 ] | Loss: 0.655 | Acc: 83.630% (6851/8192)/ 95.715% (7841/8192)
01/14/2023 14:10:55 - INFO - __main__ -   test: [batch: 64/391 ] | Loss: 0.664 | Acc: 83.438% (6942/8320)/ 95.661% (7959/8320)
01/14/2023 14:10:57 - INFO - __main__ -   test: [batch: 65/391 ] | Loss: 0.677 | Acc: 82.955% (7008/8448)/ 95.573% (8074/8448)
01/14/2023 14:10:59 - INFO - __main__ -   test: [batch: 66/391 ] | Loss: 0.682 | Acc: 82.941% (7113/8576)/ 95.534% (8193/8576)
01/14/2023 14:11:01 - INFO - __main__ -   test: [batch: 67/391 ] | Loss: 0.685 | Acc: 82.893% (7215/8704)/ 95.542% (8316/8704)
01/14/2023 14:11:03 - INFO - __main__ -   test: [batch: 68/391 ] | Loss: 0.686 | Acc: 82.846% (7317/8832)/ 95.562% (8440/8832)
01/14/2023 14:11:06 - INFO - __main__ -   test: [batch: 69/391 ] | Loss: 0.681 | Acc: 82.946% (7432/8960)/ 95.603% (8566/8960)
01/14/2023 14:11:08 - INFO - __main__ -   test: [batch: 70/391 ] | Loss: 0.682 | Acc: 82.868% (7531/9088)/ 95.599% (8688/9088)
01/14/2023 14:11:10 - INFO - __main__ -   test: [batch: 71/391 ] | Loss: 0.681 | Acc: 82.910% (7641/9216)/ 95.616% (8812/9216)
01/14/2023 14:11:12 - INFO - __main__ -   test: [batch: 72/391 ] | Loss: 0.683 | Acc: 82.802% (7737/9344)/ 95.634% (8936/9344)
01/14/2023 14:11:14 - INFO - __main__ -   test: [batch: 73/391 ] | Loss: 0.686 | Acc: 82.686% (7832/9472)/ 95.640% (9059/9472)
01/14/2023 14:11:16 - INFO - __main__ -   test: [batch: 74/391 ] | Loss: 0.687 | Acc: 82.656% (7935/9600)/ 95.625% (9180/9600)
01/14/2023 14:11:18 - INFO - __main__ -   test: [batch: 75/391 ] | Loss: 0.693 | Acc: 82.453% (8021/9728)/ 95.611% (9301/9728)
01/14/2023 14:11:20 - INFO - __main__ -   test: [batch: 76/391 ] | Loss: 0.693 | Acc: 82.437% (8125/9856)/ 95.607% (9423/9856)
01/14/2023 14:11:23 - INFO - __main__ -   test: [batch: 77/391 ] | Loss: 0.694 | Acc: 82.392% (8226/9984)/ 95.633% (9548/9984)
01/14/2023 14:11:25 - INFO - __main__ -   test: [batch: 78/391 ] | Loss: 0.694 | Acc: 82.288% (8321/10112)/ 95.669% (9674/10112)
01/14/2023 14:11:27 - INFO - __main__ -   test: [batch: 79/391 ] | Loss: 0.693 | Acc: 82.285% (8426/10240)/ 95.693% (9799/10240)
01/14/2023 14:11:29 - INFO - __main__ -   test: [batch: 80/391 ] | Loss: 0.692 | Acc: 82.243% (8527/10368)/ 95.698% (9922/10368)
01/14/2023 14:11:31 - INFO - __main__ -   test: [batch: 81/391 ] | Loss: 0.692 | Acc: 82.250% (8633/10496)/ 95.722% (10047/10496)
01/14/2023 14:11:33 - INFO - __main__ -   test: [batch: 82/391 ] | Loss: 0.691 | Acc: 82.267% (8740/10624)/ 95.717% (10169/10624)
01/14/2023 14:11:35 - INFO - __main__ -   test: [batch: 83/391 ] | Loss: 0.691 | Acc: 82.301% (8849/10752)/ 95.703% (10290/10752)
01/14/2023 14:11:38 - INFO - __main__ -   test: [batch: 84/391 ] | Loss: 0.687 | Acc: 82.408% (8966/10880)/ 95.735% (10416/10880)
01/14/2023 14:11:39 - INFO - __main__ -   test: [batch: 85/391 ] | Loss: 0.686 | Acc: 82.413% (9072/11008)/ 95.776% (10543/11008)
01/14/2023 14:11:41 - INFO - __main__ -   test: [batch: 86/391 ] | Loss: 0.690 | Acc: 82.346% (9170/11136)/ 95.753% (10663/11136)
01/14/2023 14:11:44 - INFO - __main__ -   test: [batch: 87/391 ] | Loss: 0.688 | Acc: 82.395% (9281/11264)/ 95.756% (10786/11264)
01/14/2023 14:11:46 - INFO - __main__ -   test: [batch: 88/391 ] | Loss: 0.695 | Acc: 82.312% (9377/11392)/ 95.690% (10901/11392)
01/14/2023 14:11:48 - INFO - __main__ -   test: [batch: 89/391 ] | Loss: 0.693 | Acc: 82.361% (9488/11520)/ 95.703% (11025/11520)
01/14/2023 14:11:50 - INFO - __main__ -   test: [batch: 90/391 ] | Loss: 0.694 | Acc: 82.272% (9583/11648)/ 95.707% (11148/11648)
01/14/2023 14:11:52 - INFO - __main__ -   test: [batch: 91/391 ] | Loss: 0.693 | Acc: 82.303% (9692/11776)/ 95.712% (11271/11776)
01/14/2023 14:11:54 - INFO - __main__ -   test: [batch: 92/391 ] | Loss: 0.694 | Acc: 82.275% (9794/11904)/ 95.699% (11392/11904)
01/14/2023 14:11:57 - INFO - __main__ -   test: [batch: 93/391 ] | Loss: 0.697 | Acc: 82.089% (9877/12032)/ 95.720% (11517/12032)
01/14/2023 14:11:58 - INFO - __main__ -   test: [batch: 94/391 ] | Loss: 0.700 | Acc: 81.941% (9964/12160)/ 95.732% (11641/12160)
01/14/2023 14:12:01 - INFO - __main__ -   test: [batch: 95/391 ] | Loss: 0.698 | Acc: 82.007% (10077/12288)/ 95.736% (11764/12288)
01/14/2023 14:12:03 - INFO - __main__ -   test: [batch: 96/391 ] | Loss: 0.701 | Acc: 81.927% (10172/12416)/ 95.755% (11889/12416)
01/14/2023 14:12:05 - INFO - __main__ -   test: [batch: 97/391 ] | Loss: 0.703 | Acc: 81.760% (10256/12544)/ 95.775% (12014/12544)
01/14/2023 14:12:07 - INFO - __main__ -   test: [batch: 98/391 ] | Loss: 0.699 | Acc: 81.866% (10374/12672)/ 95.810% (12141/12672)
01/14/2023 14:12:09 - INFO - __main__ -   test: [batch: 99/391 ] | Loss: 0.693 | Acc: 82.016% (10498/12800)/ 95.852% (12269/12800)
01/14/2023 14:12:11 - INFO - __main__ -   test: [batch: 100/391 ] | Loss: 0.693 | Acc: 82.070% (10610/12928)/ 95.854% (12392/12928)
01/14/2023 14:12:13 - INFO - __main__ -   test: [batch: 101/391 ] | Loss: 0.688 | Acc: 82.177% (10729/13056)/ 95.887% (12519/13056)
01/14/2023 14:12:16 - INFO - __main__ -   test: [batch: 102/391 ] | Loss: 0.685 | Acc: 82.297% (10850/13184)/ 95.912% (12645/13184)
01/14/2023 14:12:18 - INFO - __main__ -   test: [batch: 103/391 ] | Loss: 0.686 | Acc: 82.129% (10933/13312)/ 95.936% (12771/13312)
01/14/2023 14:12:20 - INFO - __main__ -   test: [batch: 104/391 ] | Loss: 0.686 | Acc: 82.076% (11031/13440)/ 95.945% (12895/13440)
01/14/2023 14:12:22 - INFO - __main__ -   test: [batch: 105/391 ] | Loss: 0.687 | Acc: 82.068% (11135/13568)/ 95.954% (13019/13568)
01/14/2023 14:12:24 - INFO - __main__ -   test: [batch: 106/391 ] | Loss: 0.695 | Acc: 81.951% (11224/13696)/ 95.889% (13133/13696)
01/14/2023 14:12:27 - INFO - __main__ -   test: [batch: 107/391 ] | Loss: 0.691 | Acc: 82.075% (11346/13824)/ 95.920% (13260/13824)
01/14/2023 14:12:29 - INFO - __main__ -   test: [batch: 108/391 ] | Loss: 0.694 | Acc: 81.917% (11429/13952)/ 95.929% (13384/13952)
01/14/2023 14:12:31 - INFO - __main__ -   test: [batch: 109/391 ] | Loss: 0.695 | Acc: 81.889% (11530/14080)/ 95.930% (13507/14080)
01/14/2023 14:12:33 - INFO - __main__ -   test: [batch: 110/391 ] | Loss: 0.698 | Acc: 81.707% (11609/14208)/ 95.939% (13631/14208)
01/14/2023 14:12:35 - INFO - __main__ -   test: [batch: 111/391 ] | Loss: 0.701 | Acc: 81.682% (11710/14336)/ 95.912% (13750/14336)
01/14/2023 14:12:37 - INFO - __main__ -   test: [batch: 112/391 ] | Loss: 0.702 | Acc: 81.706% (11818/14464)/ 95.928% (13875/14464)
01/14/2023 14:12:39 - INFO - __main__ -   test: [batch: 113/391 ] | Loss: 0.701 | Acc: 81.743% (11928/14592)/ 95.936% (13999/14592)
01/14/2023 14:12:41 - INFO - __main__ -   test: [batch: 114/391 ] | Loss: 0.697 | Acc: 81.834% (12046/14720)/ 95.965% (14126/14720)
01/14/2023 14:12:43 - INFO - __main__ -   test: [batch: 115/391 ] | Loss: 0.694 | Acc: 81.910% (12162/14848)/ 95.979% (14251/14848)
01/14/2023 14:12:45 - INFO - __main__ -   test: [batch: 116/391 ] | Loss: 0.698 | Acc: 81.871% (12261/14976)/ 95.947% (14369/14976)
01/14/2023 14:12:47 - INFO - __main__ -   test: [batch: 117/391 ] | Loss: 0.696 | Acc: 81.892% (12369/15104)/ 95.961% (14494/15104)
01/14/2023 14:12:50 - INFO - __main__ -   test: [batch: 118/391 ] | Loss: 0.699 | Acc: 81.795% (12459/15232)/ 95.976% (14619/15232)
01/14/2023 14:12:52 - INFO - __main__ -   test: [batch: 119/391 ] | Loss: 0.697 | Acc: 81.842% (12571/15360)/ 95.996% (14745/15360)
01/14/2023 14:12:54 - INFO - __main__ -   test: [batch: 120/391 ] | Loss: 0.696 | Acc: 81.889% (12683/15488)/ 96.010% (14870/15488)
01/14/2023 14:12:56 - INFO - __main__ -   test: [batch: 121/391 ] | Loss: 0.701 | Acc: 81.743% (12765/15616)/ 95.991% (14990/15616)
01/14/2023 14:12:58 - INFO - __main__ -   test: [batch: 122/391 ] | Loss: 0.703 | Acc: 81.688% (12861/15744)/ 95.960% (15108/15744)
01/14/2023 14:13:01 - INFO - __main__ -   test: [batch: 123/391 ] | Loss: 0.704 | Acc: 81.704% (12968/15872)/ 95.961% (15231/15872)
01/14/2023 14:13:03 - INFO - __main__ -   test: [batch: 124/391 ] | Loss: 0.702 | Acc: 81.706% (13073/16000)/ 95.975% (15356/16000)
01/14/2023 14:13:05 - INFO - __main__ -   test: [batch: 125/391 ] | Loss: 0.698 | Acc: 81.814% (13195/16128)/ 96.001% (15483/16128)
01/14/2023 14:13:07 - INFO - __main__ -   test: [batch: 126/391 ] | Loss: 0.694 | Acc: 81.902% (13314/16256)/ 96.014% (15608/16256)
01/14/2023 14:13:09 - INFO - __main__ -   test: [batch: 127/391 ] | Loss: 0.692 | Acc: 81.982% (13432/16384)/ 96.021% (15732/16384)
01/14/2023 14:13:11 - INFO - __main__ -   test: [batch: 128/391 ] | Loss: 0.693 | Acc: 81.965% (13534/16512)/ 95.997% (15851/16512)
01/14/2023 14:13:13 - INFO - __main__ -   test: [batch: 129/391 ] | Loss: 0.692 | Acc: 81.977% (13641/16640)/ 96.004% (15975/16640)
01/14/2023 14:13:15 - INFO - __main__ -   test: [batch: 130/391 ] | Loss: 0.689 | Acc: 82.079% (13763/16768)/ 96.022% (16101/16768)
01/14/2023 14:13:18 - INFO - __main__ -   test: [batch: 131/391 ] | Loss: 0.688 | Acc: 82.126% (13876/16896)/ 96.029% (16225/16896)
01/14/2023 14:13:20 - INFO - __main__ -   test: [batch: 132/391 ] | Loss: 0.686 | Acc: 82.190% (13992/17024)/ 96.041% (16350/17024)
01/14/2023 14:13:22 - INFO - __main__ -   test: [batch: 133/391 ] | Loss: 0.687 | Acc: 82.142% (14089/17152)/ 96.035% (16472/17152)
01/14/2023 14:13:24 - INFO - __main__ -   test: [batch: 134/391 ] | Loss: 0.685 | Acc: 82.170% (14199/17280)/ 96.053% (16598/17280)
01/14/2023 14:13:26 - INFO - __main__ -   test: [batch: 135/391 ] | Loss: 0.685 | Acc: 82.163% (14303/17408)/ 96.048% (16720/17408)
01/14/2023 14:13:28 - INFO - __main__ -   test: [batch: 136/391 ] | Loss: 0.684 | Acc: 82.122% (14401/17536)/ 96.071% (16847/17536)
01/14/2023 14:13:30 - INFO - __main__ -   test: [batch: 137/391 ] | Loss: 0.682 | Acc: 82.161% (14513/17664)/ 96.099% (16975/17664)
01/14/2023 14:13:32 - INFO - __main__ -   test: [batch: 138/391 ] | Loss: 0.681 | Acc: 82.205% (14626/17792)/ 96.099% (17098/17792)
01/14/2023 14:13:35 - INFO - __main__ -   test: [batch: 139/391 ] | Loss: 0.686 | Acc: 82.054% (14704/17920)/ 96.083% (17218/17920)
01/14/2023 14:13:37 - INFO - __main__ -   test: [batch: 140/391 ] | Loss: 0.689 | Acc: 81.987% (14797/18048)/ 96.077% (17340/18048)
01/14/2023 14:13:39 - INFO - __main__ -   test: [batch: 141/391 ] | Loss: 0.687 | Acc: 82.015% (14907/18176)/ 96.088% (17465/18176)
01/14/2023 14:13:41 - INFO - __main__ -   test: [batch: 142/391 ] | Loss: 0.685 | Acc: 82.097% (15027/18304)/ 96.105% (17591/18304)
01/14/2023 14:13:43 - INFO - __main__ -   test: [batch: 143/391 ] | Loss: 0.685 | Acc: 82.118% (15136/18432)/ 96.094% (17712/18432)
01/14/2023 14:13:45 - INFO - __main__ -   test: [batch: 144/391 ] | Loss: 0.687 | Acc: 82.080% (15234/18560)/ 96.061% (17829/18560)
01/14/2023 14:13:47 - INFO - __main__ -   test: [batch: 145/391 ] | Loss: 0.690 | Acc: 82.053% (15334/18688)/ 96.035% (17947/18688)
01/14/2023 14:13:49 - INFO - __main__ -   test: [batch: 146/391 ] | Loss: 0.692 | Acc: 82.037% (15436/18816)/ 96.019% (18067/18816)
01/14/2023 14:13:52 - INFO - __main__ -   test: [batch: 147/391 ] | Loss: 0.692 | Acc: 82.037% (15541/18944)/ 95.999% (18186/18944)
01/14/2023 14:13:54 - INFO - __main__ -   test: [batch: 148/391 ] | Loss: 0.693 | Acc: 81.968% (15633/19072)/ 95.994% (18308/19072)
01/14/2023 14:13:56 - INFO - __main__ -   test: [batch: 149/391 ] | Loss: 0.697 | Acc: 81.880% (15721/19200)/ 95.958% (18424/19200)
01/14/2023 14:13:58 - INFO - __main__ -   test: [batch: 150/391 ] | Loss: 0.698 | Acc: 81.819% (15814/19328)/ 95.964% (18548/19328)
01/14/2023 14:14:00 - INFO - __main__ -   test: [batch: 151/391 ] | Loss: 0.698 | Acc: 81.851% (15925/19456)/ 95.965% (18671/19456)
01/14/2023 14:14:02 - INFO - __main__ -   test: [batch: 152/391 ] | Loss: 0.699 | Acc: 81.837% (16027/19584)/ 95.956% (18792/19584)
01/14/2023 14:14:04 - INFO - __main__ -   test: [batch: 153/391 ] | Loss: 0.697 | Acc: 81.904% (16145/19712)/ 95.962% (18916/19712)
01/14/2023 14:14:06 - INFO - __main__ -   test: [batch: 154/391 ] | Loss: 0.697 | Acc: 81.890% (16247/19840)/ 95.938% (19034/19840)
01/14/2023 14:14:09 - INFO - __main__ -   test: [batch: 155/391 ] | Loss: 0.698 | Acc: 81.906% (16355/19968)/ 95.933% (19156/19968)
01/14/2023 14:14:11 - INFO - __main__ -   test: [batch: 156/391 ] | Loss: 0.700 | Acc: 81.827% (16444/20096)/ 95.915% (19275/20096)
01/14/2023 14:14:13 - INFO - __main__ -   test: [batch: 157/391 ] | Loss: 0.701 | Acc: 81.819% (16547/20224)/ 95.891% (19393/20224)
01/14/2023 14:14:15 - INFO - __main__ -   test: [batch: 158/391 ] | Loss: 0.703 | Acc: 81.786% (16645/20352)/ 95.863% (19510/20352)
01/14/2023 14:14:17 - INFO - __main__ -   test: [batch: 159/391 ] | Loss: 0.704 | Acc: 81.763% (16745/20480)/ 95.859% (19632/20480)
01/14/2023 14:14:20 - INFO - __main__ -   test: [batch: 160/391 ] | Loss: 0.705 | Acc: 81.740% (16845/20608)/ 95.841% (19751/20608)
01/14/2023 14:14:22 - INFO - __main__ -   test: [batch: 161/391 ] | Loss: 0.714 | Acc: 81.525% (16905/20736)/ 95.751% (19855/20736)
01/14/2023 14:14:24 - INFO - __main__ -   test: [batch: 162/391 ] | Loss: 0.719 | Acc: 81.427% (16989/20864)/ 95.672% (19961/20864)
01/14/2023 14:14:26 - INFO - __main__ -   test: [batch: 163/391 ] | Loss: 0.722 | Acc: 81.379% (17083/20992)/ 95.655% (20080/20992)
01/14/2023 14:14:28 - INFO - __main__ -   test: [batch: 164/391 ] | Loss: 0.722 | Acc: 81.368% (17185/21120)/ 95.672% (20206/21120)
01/14/2023 14:14:30 - INFO - __main__ -   test: [batch: 165/391 ] | Loss: 0.724 | Acc: 81.288% (17272/21248)/ 95.670% (20328/21248)
01/14/2023 14:14:32 - INFO - __main__ -   test: [batch: 166/391 ] | Loss: 0.724 | Acc: 81.292% (17377/21376)/ 95.654% (20447/21376)
01/14/2023 14:14:35 - INFO - __main__ -   test: [batch: 167/391 ] | Loss: 0.726 | Acc: 81.227% (17467/21504)/ 95.638% (20566/21504)
01/14/2023 14:14:37 - INFO - __main__ -   test: [batch: 168/391 ] | Loss: 0.725 | Acc: 81.227% (17571/21632)/ 95.631% (20687/21632)
01/14/2023 14:14:39 - INFO - __main__ -   test: [batch: 169/391 ] | Loss: 0.727 | Acc: 81.195% (17668/21760)/ 95.597% (20802/21760)
01/14/2023 14:14:41 - INFO - __main__ -   test: [batch: 170/391 ] | Loss: 0.732 | Acc: 81.086% (17748/21888)/ 95.555% (20915/21888)
01/14/2023 14:14:43 - INFO - __main__ -   test: [batch: 171/391 ] | Loss: 0.736 | Acc: 81.014% (17836/22016)/ 95.521% (21030/22016)
01/14/2023 14:14:45 - INFO - __main__ -   test: [batch: 172/391 ] | Loss: 0.736 | Acc: 80.965% (17929/22144)/ 95.520% (21152/22144)
01/14/2023 14:14:47 - INFO - __main__ -   test: [batch: 173/391 ] | Loss: 0.739 | Acc: 80.913% (18021/22272)/ 95.488% (21267/22272)
01/14/2023 14:14:49 - INFO - __main__ -   test: [batch: 174/391 ] | Loss: 0.743 | Acc: 80.826% (18105/22400)/ 95.438% (21378/22400)
01/14/2023 14:14:51 - INFO - __main__ -   test: [batch: 175/391 ] | Loss: 0.741 | Acc: 80.877% (18220/22528)/ 95.446% (21502/22528)
01/14/2023 14:14:54 - INFO - __main__ -   test: [batch: 176/391 ] | Loss: 0.742 | Acc: 80.879% (18324/22656)/ 95.427% (21620/22656)
01/14/2023 14:14:56 - INFO - __main__ -   test: [batch: 177/391 ] | Loss: 0.745 | Acc: 80.807% (18411/22784)/ 95.387% (21733/22784)
01/14/2023 14:14:58 - INFO - __main__ -   test: [batch: 178/391 ] | Loss: 0.747 | Acc: 80.796% (18512/22912)/ 95.365% (21850/22912)
01/14/2023 14:15:00 - INFO - __main__ -   test: [batch: 179/391 ] | Loss: 0.750 | Acc: 80.738% (18602/23040)/ 95.326% (21963/23040)
01/14/2023 14:15:02 - INFO - __main__ -   test: [batch: 180/391 ] | Loss: 0.756 | Acc: 80.616% (18677/23168)/ 95.282% (22075/23168)
01/14/2023 14:15:04 - INFO - __main__ -   test: [batch: 181/391 ] | Loss: 0.762 | Acc: 80.482% (18749/23296)/ 95.227% (22184/23296)
01/14/2023 14:15:06 - INFO - __main__ -   test: [batch: 182/391 ] | Loss: 0.761 | Acc: 80.494% (18855/23424)/ 95.227% (22306/23424)
01/14/2023 14:15:08 - INFO - __main__ -   test: [batch: 183/391 ] | Loss: 0.768 | Acc: 80.371% (18929/23552)/ 95.138% (22407/23552)
01/14/2023 14:15:10 - INFO - __main__ -   test: [batch: 184/391 ] | Loss: 0.767 | Acc: 80.393% (19037/23680)/ 95.127% (22526/23680)
01/14/2023 14:15:12 - INFO - __main__ -   test: [batch: 185/391 ] | Loss: 0.767 | Acc: 80.397% (19141/23808)/ 95.119% (22646/23808)
01/14/2023 14:15:14 - INFO - __main__ -   test: [batch: 186/391 ] | Loss: 0.770 | Acc: 80.356% (19234/23936)/ 95.079% (22758/23936)
01/14/2023 14:15:17 - INFO - __main__ -   test: [batch: 187/391 ] | Loss: 0.775 | Acc: 80.240% (19309/24064)/ 95.051% (22873/24064)
01/14/2023 14:15:19 - INFO - __main__ -   test: [batch: 188/391 ] | Loss: 0.778 | Acc: 80.113% (19381/24192)/ 95.027% (22989/24192)
01/14/2023 14:15:21 - INFO - __main__ -   test: [batch: 189/391 ] | Loss: 0.780 | Acc: 80.037% (19465/24320)/ 95.025% (23110/24320)
01/14/2023 14:15:23 - INFO - __main__ -   test: [batch: 190/391 ] | Loss: 0.785 | Acc: 79.949% (19546/24448)/ 94.998% (23225/24448)
01/14/2023 14:15:25 - INFO - __main__ -   test: [batch: 191/391 ] | Loss: 0.787 | Acc: 79.915% (19640/24576)/ 94.979% (23342/24576)
01/14/2023 14:15:27 - INFO - __main__ -   test: [batch: 192/391 ] | Loss: 0.793 | Acc: 79.801% (19714/24704)/ 94.900% (23444/24704)
01/14/2023 14:15:29 - INFO - __main__ -   test: [batch: 193/391 ] | Loss: 0.793 | Acc: 79.824% (19822/24832)/ 94.894% (23564/24832)
01/14/2023 14:15:31 - INFO - __main__ -   test: [batch: 194/391 ] | Loss: 0.795 | Acc: 79.764% (19909/24960)/ 94.880% (23682/24960)
01/14/2023 14:15:34 - INFO - __main__ -   test: [batch: 195/391 ] | Loss: 0.801 | Acc: 79.680% (19990/25088)/ 94.814% (23787/25088)
01/14/2023 14:15:36 - INFO - __main__ -   test: [batch: 196/391 ] | Loss: 0.804 | Acc: 79.569% (20064/25216)/ 94.773% (23898/25216)
01/14/2023 14:15:38 - INFO - __main__ -   test: [batch: 197/391 ] | Loss: 0.809 | Acc: 79.490% (20146/25344)/ 94.752% (24014/25344)
01/14/2023 14:15:40 - INFO - __main__ -   test: [batch: 198/391 ] | Loss: 0.810 | Acc: 79.452% (20238/25472)/ 94.724% (24128/25472)
01/14/2023 14:15:42 - INFO - __main__ -   test: [batch: 199/391 ] | Loss: 0.810 | Acc: 79.438% (20336/25600)/ 94.730% (24251/25600)
01/14/2023 14:15:44 - INFO - __main__ -   test: [batch: 200/391 ] | Loss: 0.810 | Acc: 79.396% (20427/25728)/ 94.729% (24372/25728)
01/14/2023 14:15:47 - INFO - __main__ -   test: [batch: 201/391 ] | Loss: 0.813 | Acc: 79.312% (20507/25856)/ 94.690% (24483/25856)
01/14/2023 14:15:48 - INFO - __main__ -   test: [batch: 202/391 ] | Loss: 0.814 | Acc: 79.295% (20604/25984)/ 94.677% (24601/25984)
01/14/2023 14:15:50 - INFO - __main__ -   test: [batch: 203/391 ] | Loss: 0.815 | Acc: 79.282% (20702/26112)/ 94.665% (24719/26112)
01/14/2023 14:15:53 - INFO - __main__ -   test: [batch: 204/391 ] | Loss: 0.818 | Acc: 79.188% (20779/26240)/ 94.646% (24835/26240)
01/14/2023 14:15:55 - INFO - __main__ -   test: [batch: 205/391 ] | Loss: 0.821 | Acc: 79.103% (20858/26368)/ 94.611% (24947/26368)
01/14/2023 14:15:57 - INFO - __main__ -   test: [batch: 206/391 ] | Loss: 0.823 | Acc: 79.084% (20954/26496)/ 94.607% (25067/26496)
01/14/2023 14:15:59 - INFO - __main__ -   test: [batch: 207/391 ] | Loss: 0.826 | Acc: 79.000% (21033/26624)/ 94.569% (25178/26624)
01/14/2023 14:16:01 - INFO - __main__ -   test: [batch: 208/391 ] | Loss: 0.828 | Acc: 78.962% (21124/26752)/ 94.546% (25293/26752)
01/14/2023 14:16:03 - INFO - __main__ -   test: [batch: 209/391 ] | Loss: 0.828 | Acc: 78.962% (21225/26880)/ 94.561% (25418/26880)
01/14/2023 14:16:05 - INFO - __main__ -   test: [batch: 210/391 ] | Loss: 0.829 | Acc: 78.947% (21322/27008)/ 94.542% (25534/27008)
01/14/2023 14:16:08 - INFO - __main__ -   test: [batch: 211/391 ] | Loss: 0.831 | Acc: 78.888% (21407/27136)/ 94.517% (25648/27136)
01/14/2023 14:16:10 - INFO - __main__ -   test: [batch: 212/391 ] | Loss: 0.833 | Acc: 78.826% (21491/27264)/ 94.495% (25763/27264)
01/14/2023 14:16:12 - INFO - __main__ -   test: [batch: 213/391 ] | Loss: 0.833 | Acc: 78.822% (21591/27392)/ 94.502% (25886/27392)
01/14/2023 14:16:14 - INFO - __main__ -   test: [batch: 214/391 ] | Loss: 0.834 | Acc: 78.797% (21685/27520)/ 94.502% (26007/27520)
01/14/2023 14:16:16 - INFO - __main__ -   test: [batch: 215/391 ] | Loss: 0.834 | Acc: 78.816% (21791/27648)/ 94.495% (26126/27648)
01/14/2023 14:16:18 - INFO - __main__ -   test: [batch: 216/391 ] | Loss: 0.832 | Acc: 78.856% (21903/27776)/ 94.510% (26251/27776)
01/14/2023 14:16:20 - INFO - __main__ -   test: [batch: 217/391 ] | Loss: 0.837 | Acc: 78.788% (21985/27904)/ 94.449% (26355/27904)
01/14/2023 14:16:22 - INFO - __main__ -   test: [batch: 218/391 ] | Loss: 0.840 | Acc: 78.739% (22072/28032)/ 94.414% (26466/28032)
01/14/2023 14:16:25 - INFO - __main__ -   test: [batch: 219/391 ] | Loss: 0.838 | Acc: 78.768% (22181/28160)/ 94.418% (26588/28160)
01/14/2023 14:16:27 - INFO - __main__ -   test: [batch: 220/391 ] | Loss: 0.836 | Acc: 78.807% (22293/28288)/ 94.432% (26713/28288)
01/14/2023 14:16:29 - INFO - __main__ -   test: [batch: 221/391 ] | Loss: 0.838 | Acc: 78.762% (22381/28416)/ 94.415% (26829/28416)
01/14/2023 14:16:31 - INFO - __main__ -   test: [batch: 222/391 ] | Loss: 0.836 | Acc: 78.819% (22498/28544)/ 94.430% (26954/28544)
01/14/2023 14:16:33 - INFO - __main__ -   test: [batch: 223/391 ] | Loss: 0.837 | Acc: 78.816% (22598/28672)/ 94.420% (27072/28672)
01/14/2023 14:16:35 - INFO - __main__ -   test: [batch: 224/391 ] | Loss: 0.836 | Acc: 78.840% (22706/28800)/ 94.413% (27191/28800)
01/14/2023 14:16:37 - INFO - __main__ -   test: [batch: 225/391 ] | Loss: 0.836 | Acc: 78.844% (22808/28928)/ 94.421% (27314/28928)
01/14/2023 14:16:40 - INFO - __main__ -   test: [batch: 226/391 ] | Loss: 0.835 | Acc: 78.820% (22902/29056)/ 94.431% (27438/29056)
01/14/2023 14:16:42 - INFO - __main__ -   test: [batch: 227/391 ] | Loss: 0.838 | Acc: 78.807% (22999/29184)/ 94.422% (27556/29184)
01/14/2023 14:16:44 - INFO - __main__ -   test: [batch: 228/391 ] | Loss: 0.842 | Acc: 78.725% (23076/29312)/ 94.364% (27660/29312)
01/14/2023 14:16:46 - INFO - __main__ -   test: [batch: 229/391 ] | Loss: 0.845 | Acc: 78.665% (23159/29440)/ 94.317% (27767/29440)
01/14/2023 14:16:48 - INFO - __main__ -   test: [batch: 230/391 ] | Loss: 0.848 | Acc: 78.605% (23242/29568)/ 94.281% (27877/29568)
01/14/2023 14:16:50 - INFO - __main__ -   test: [batch: 231/391 ] | Loss: 0.848 | Acc: 78.593% (23339/29696)/ 94.269% (27994/29696)
01/14/2023 14:16:52 - INFO - __main__ -   test: [batch: 232/391 ] | Loss: 0.847 | Acc: 78.621% (23448/29824)/ 94.283% (28119/29824)
01/14/2023 14:16:55 - INFO - __main__ -   test: [batch: 233/391 ] | Loss: 0.849 | Acc: 78.572% (23534/29952)/ 94.267% (28235/29952)
01/14/2023 14:16:57 - INFO - __main__ -   test: [batch: 234/391 ] | Loss: 0.856 | Acc: 78.454% (23599/30080)/ 94.199% (28335/30080)
01/14/2023 14:16:59 - INFO - __main__ -   test: [batch: 235/391 ] | Loss: 0.856 | Acc: 78.443% (23696/30208)/ 94.194% (28454/30208)
01/14/2023 14:17:01 - INFO - __main__ -   test: [batch: 236/391 ] | Loss: 0.854 | Acc: 78.481% (23808/30336)/ 94.202% (28577/30336)
01/14/2023 14:17:03 - INFO - __main__ -   test: [batch: 237/391 ] | Loss: 0.855 | Acc: 78.486% (23910/30464)/ 94.167% (28687/30464)
01/14/2023 14:17:05 - INFO - __main__ -   test: [batch: 238/391 ] | Loss: 0.853 | Acc: 78.527% (24023/30592)/ 94.175% (28810/30592)
01/14/2023 14:17:07 - INFO - __main__ -   test: [batch: 239/391 ] | Loss: 0.852 | Acc: 78.564% (24135/30720)/ 94.176% (28931/30720)
01/14/2023 14:17:09 - INFO - __main__ -   test: [batch: 240/391 ] | Loss: 0.852 | Acc: 78.579% (24240/30848)/ 94.165% (29048/30848)
01/14/2023 14:17:11 - INFO - __main__ -   test: [batch: 241/391 ] | Loss: 0.855 | Acc: 78.487% (24312/30976)/ 94.121% (29155/30976)
01/14/2023 14:17:13 - INFO - __main__ -   test: [batch: 242/391 ] | Loss: 0.858 | Acc: 78.379% (24379/31104)/ 94.107% (29271/31104)
01/14/2023 14:17:16 - INFO - __main__ -   test: [batch: 243/391 ] | Loss: 0.864 | Acc: 78.247% (24438/31232)/ 94.038% (29370/31232)
01/14/2023 14:17:18 - INFO - __main__ -   test: [batch: 244/391 ] | Loss: 0.864 | Acc: 78.256% (24541/31360)/ 94.040% (29491/31360)
01/14/2023 14:17:20 - INFO - __main__ -   test: [batch: 245/391 ] | Loss: 0.863 | Acc: 78.268% (24645/31488)/ 94.033% (29609/31488)
01/14/2023 14:17:22 - INFO - __main__ -   test: [batch: 246/391 ] | Loss: 0.864 | Acc: 78.242% (24737/31616)/ 94.025% (29727/31616)
01/14/2023 14:17:24 - INFO - __main__ -   test: [batch: 247/391 ] | Loss: 0.869 | Acc: 78.153% (24809/31744)/ 93.952% (29824/31744)
01/14/2023 14:17:27 - INFO - __main__ -   test: [batch: 248/391 ] | Loss: 0.870 | Acc: 78.106% (24894/31872)/ 93.945% (29942/31872)
01/14/2023 14:17:29 - INFO - __main__ -   test: [batch: 249/391 ] | Loss: 0.872 | Acc: 77.969% (24950/32000)/ 93.938% (30060/32000)
01/14/2023 14:17:31 - INFO - __main__ -   test: [batch: 250/391 ] | Loss: 0.871 | Acc: 78.004% (25061/32128)/ 93.955% (30186/32128)
01/14/2023 14:17:33 - INFO - __main__ -   test: [batch: 251/391 ] | Loss: 0.873 | Acc: 77.964% (25148/32256)/ 93.933% (30299/32256)
01/14/2023 14:17:35 - INFO - __main__ -   test: [batch: 252/391 ] | Loss: 0.872 | Acc: 77.989% (25256/32384)/ 93.926% (30417/32384)
01/14/2023 14:17:37 - INFO - __main__ -   test: [batch: 253/391 ] | Loss: 0.874 | Acc: 77.959% (25346/32512)/ 93.913% (30533/32512)
01/14/2023 14:17:40 - INFO - __main__ -   test: [batch: 254/391 ] | Loss: 0.878 | Acc: 77.895% (25425/32640)/ 93.863% (30637/32640)
01/14/2023 14:17:42 - INFO - __main__ -   test: [batch: 255/391 ] | Loss: 0.880 | Acc: 77.863% (25514/32768)/ 93.851% (30753/32768)
01/14/2023 14:17:44 - INFO - __main__ -   test: [batch: 256/391 ] | Loss: 0.885 | Acc: 77.742% (25574/32896)/ 93.817% (30862/32896)
01/14/2023 14:17:46 - INFO - __main__ -   test: [batch: 257/391 ] | Loss: 0.886 | Acc: 77.728% (25669/33024)/ 93.805% (30978/33024)
01/14/2023 14:17:48 - INFO - __main__ -   test: [batch: 258/391 ] | Loss: 0.886 | Acc: 77.718% (25765/33152)/ 93.789% (31093/33152)
01/14/2023 14:17:50 - INFO - __main__ -   test: [batch: 259/391 ] | Loss: 0.890 | Acc: 77.593% (25823/33280)/ 93.780% (31210/33280)
01/14/2023 14:17:52 - INFO - __main__ -   test: [batch: 260/391 ] | Loss: 0.891 | Acc: 77.562% (25912/33408)/ 93.771% (31327/33408)
01/14/2023 14:17:55 - INFO - __main__ -   test: [batch: 261/391 ] | Loss: 0.889 | Acc: 77.609% (26027/33536)/ 93.792% (31454/33536)
01/14/2023 14:17:57 - INFO - __main__ -   test: [batch: 262/391 ] | Loss: 0.888 | Acc: 77.635% (26135/33664)/ 93.800% (31577/33664)
01/14/2023 14:17:59 - INFO - __main__ -   test: [batch: 263/391 ] | Loss: 0.891 | Acc: 77.560% (26209/33792)/ 93.774% (31688/33792)
01/14/2023 14:18:01 - INFO - __main__ -   test: [batch: 264/391 ] | Loss: 0.896 | Acc: 77.497% (26287/33920)/ 93.715% (31788/33920)
01/14/2023 14:18:03 - INFO - __main__ -   test: [batch: 265/391 ] | Loss: 0.895 | Acc: 77.520% (26394/34048)/ 93.715% (31908/34048)
01/14/2023 14:18:06 - INFO - __main__ -   test: [batch: 266/391 ] | Loss: 0.898 | Acc: 77.452% (26470/34176)/ 93.700% (32023/34176)
01/14/2023 14:18:08 - INFO - __main__ -   test: [batch: 267/391 ] | Loss: 0.897 | Acc: 77.487% (26581/34304)/ 93.700% (32143/34304)
01/14/2023 14:18:10 - INFO - __main__ -   test: [batch: 268/391 ] | Loss: 0.896 | Acc: 77.509% (26688/34432)/ 93.698% (32262/34432)
01/14/2023 14:18:12 - INFO - __main__ -   test: [batch: 269/391 ] | Loss: 0.898 | Acc: 77.459% (26770/34560)/ 93.678% (32375/34560)
01/14/2023 14:18:14 - INFO - __main__ -   test: [batch: 270/391 ] | Loss: 0.901 | Acc: 77.399% (26848/34688)/ 93.646% (32484/34688)
01/14/2023 14:18:16 - INFO - __main__ -   test: [batch: 271/391 ] | Loss: 0.902 | Acc: 77.407% (26950/34816)/ 93.641% (32602/34816)
01/14/2023 14:18:18 - INFO - __main__ -   test: [batch: 272/391 ] | Loss: 0.903 | Acc: 77.370% (27036/34944)/ 93.636% (32720/34944)
01/14/2023 14:18:21 - INFO - __main__ -   test: [batch: 273/391 ] | Loss: 0.903 | Acc: 77.369% (27135/35072)/ 93.619% (32834/35072)
01/14/2023 14:18:23 - INFO - __main__ -   test: [batch: 274/391 ] | Loss: 0.903 | Acc: 77.369% (27234/35200)/ 93.625% (32956/35200)
01/14/2023 14:18:25 - INFO - __main__ -   test: [batch: 275/391 ] | Loss: 0.905 | Acc: 77.335% (27321/35328)/ 93.620% (33074/35328)
01/14/2023 14:18:27 - INFO - __main__ -   test: [batch: 276/391 ] | Loss: 0.906 | Acc: 77.290% (27404/35456)/ 93.617% (33193/35456)
01/14/2023 14:18:29 - INFO - __main__ -   test: [batch: 277/391 ] | Loss: 0.906 | Acc: 77.265% (27494/35584)/ 93.604% (33308/35584)
01/14/2023 14:18:31 - INFO - __main__ -   test: [batch: 278/391 ] | Loss: 0.906 | Acc: 77.277% (27597/35712)/ 93.602% (33427/35712)
01/14/2023 14:18:33 - INFO - __main__ -   test: [batch: 279/391 ] | Loss: 0.906 | Acc: 77.285% (27699/35840)/ 93.588% (33542/35840)
01/14/2023 14:18:36 - INFO - __main__ -   test: [batch: 280/391 ] | Loss: 0.907 | Acc: 77.269% (27792/35968)/ 93.583% (33660/35968)
01/14/2023 14:18:38 - INFO - __main__ -   test: [batch: 281/391 ] | Loss: 0.907 | Acc: 77.277% (27894/36096)/ 93.578% (33778/36096)
01/14/2023 14:18:40 - INFO - __main__ -   test: [batch: 282/391 ] | Loss: 0.906 | Acc: 77.313% (28006/36224)/ 93.582% (33899/36224)
01/14/2023 14:18:42 - INFO - __main__ -   test: [batch: 283/391 ] | Loss: 0.907 | Acc: 77.291% (28097/36352)/ 93.577% (34017/36352)
01/14/2023 14:18:44 - INFO - __main__ -   test: [batch: 284/391 ] | Loss: 0.910 | Acc: 77.234% (28175/36480)/ 93.542% (34124/36480)
01/14/2023 14:18:46 - INFO - __main__ -   test: [batch: 285/391 ] | Loss: 0.913 | Acc: 77.194% (28259/36608)/ 93.499% (34228/36608)
01/14/2023 14:18:48 - INFO - __main__ -   test: [batch: 286/391 ] | Loss: 0.914 | Acc: 77.159% (28345/36736)/ 93.486% (34343/36736)
01/14/2023 14:18:51 - INFO - __main__ -   test: [batch: 287/391 ] | Loss: 0.914 | Acc: 77.157% (28443/36864)/ 93.481% (34461/36864)
01/14/2023 14:18:52 - INFO - __main__ -   test: [batch: 288/391 ] | Loss: 0.913 | Acc: 77.179% (28550/36992)/ 93.482% (34581/36992)
01/14/2023 14:18:55 - INFO - __main__ -   test: [batch: 289/391 ] | Loss: 0.915 | Acc: 77.139% (28634/37120)/ 93.448% (34688/37120)
01/14/2023 14:18:57 - INFO - __main__ -   test: [batch: 290/391 ] | Loss: 0.917 | Acc: 77.067% (28706/37248)/ 93.452% (34809/37248)
01/14/2023 14:18:59 - INFO - __main__ -   test: [batch: 291/391 ] | Loss: 0.917 | Acc: 77.082% (28810/37376)/ 93.448% (34927/37376)
01/14/2023 14:19:01 - INFO - __main__ -   test: [batch: 292/391 ] | Loss: 0.918 | Acc: 77.032% (28890/37504)/ 93.430% (35040/37504)
01/14/2023 14:19:03 - INFO - __main__ -   test: [batch: 293/391 ] | Loss: 0.919 | Acc: 77.025% (28986/37632)/ 93.428% (35159/37632)
01/14/2023 14:19:05 - INFO - __main__ -   test: [batch: 294/391 ] | Loss: 0.920 | Acc: 76.999% (29075/37760)/ 93.424% (35277/37760)
01/14/2023 14:19:08 - INFO - __main__ -   test: [batch: 295/391 ] | Loss: 0.919 | Acc: 77.030% (29185/37888)/ 93.428% (35398/37888)
01/14/2023 14:19:10 - INFO - __main__ -   test: [batch: 296/391 ] | Loss: 0.919 | Acc: 77.028% (29283/38016)/ 93.421% (35515/38016)
01/14/2023 14:19:12 - INFO - __main__ -   test: [batch: 297/391 ] | Loss: 0.921 | Acc: 77.006% (29373/38144)/ 93.399% (35626/38144)
01/14/2023 14:19:13 - INFO - __main__ -   test: [batch: 298/391 ] | Loss: 0.923 | Acc: 76.975% (29460/38272)/ 93.374% (35736/38272)
01/14/2023 14:19:15 - INFO - __main__ -   test: [batch: 299/391 ] | Loss: 0.924 | Acc: 76.969% (29556/38400)/ 93.352% (35847/38400)
01/14/2023 14:19:16 - INFO - __main__ -   test: [batch: 300/391 ] | Loss: 0.924 | Acc: 76.973% (29656/38528)/ 93.345% (35964/38528)
01/14/2023 14:19:18 - INFO - __main__ -   test: [batch: 301/391 ] | Loss: 0.925 | Acc: 76.961% (29750/38656)/ 93.331% (36078/38656)
01/14/2023 14:19:19 - INFO - __main__ -   test: [batch: 302/391 ] | Loss: 0.926 | Acc: 76.921% (29833/38784)/ 93.317% (36192/38784)
01/14/2023 14:19:20 - INFO - __main__ -   test: [batch: 303/391 ] | Loss: 0.927 | Acc: 76.897% (29922/38912)/ 93.308% (36308/38912)
01/14/2023 14:19:21 - INFO - __main__ -   test: [batch: 304/391 ] | Loss: 0.927 | Acc: 76.916% (30028/39040)/ 93.312% (36429/39040)
01/14/2023 14:19:21 - INFO - __main__ -   test: [batch: 305/391 ] | Loss: 0.928 | Acc: 76.871% (30109/39168)/ 93.303% (36545/39168)
01/14/2023 14:19:22 - INFO - __main__ -   test: [batch: 306/391 ] | Loss: 0.929 | Acc: 76.853% (30200/39296)/ 93.279% (36655/39296)
01/14/2023 14:19:23 - INFO - __main__ -   test: [batch: 307/391 ] | Loss: 0.930 | Acc: 76.857% (30300/39424)/ 93.263% (36768/39424)
01/14/2023 14:19:24 - INFO - __main__ -   test: [batch: 308/391 ] | Loss: 0.930 | Acc: 76.843% (30393/39552)/ 93.260% (36886/39552)
01/14/2023 14:19:25 - INFO - __main__ -   test: [batch: 309/391 ] | Loss: 0.931 | Acc: 76.840% (30490/39680)/ 93.238% (36997/39680)
01/14/2023 14:19:26 - INFO - __main__ -   test: [batch: 310/391 ] | Loss: 0.932 | Acc: 76.831% (30585/39808)/ 93.227% (37112/39808)
01/14/2023 14:19:27 - INFO - __main__ -   test: [batch: 311/391 ] | Loss: 0.933 | Acc: 76.803% (30672/39936)/ 93.217% (37227/39936)
01/14/2023 14:19:28 - INFO - __main__ -   test: [batch: 312/391 ] | Loss: 0.935 | Acc: 76.790% (30765/40064)/ 93.196% (37338/40064)
01/14/2023 14:19:29 - INFO - __main__ -   test: [batch: 313/391 ] | Loss: 0.933 | Acc: 76.829% (30879/40192)/ 93.215% (37465/40192)
01/14/2023 14:19:30 - INFO - __main__ -   test: [batch: 314/391 ] | Loss: 0.933 | Acc: 76.828% (30977/40320)/ 93.212% (37583/40320)
01/14/2023 14:19:31 - INFO - __main__ -   test: [batch: 315/391 ] | Loss: 0.934 | Acc: 76.812% (31069/40448)/ 93.199% (37697/40448)
01/14/2023 14:19:31 - INFO - __main__ -   test: [batch: 316/391 ] | Loss: 0.937 | Acc: 76.725% (31132/40576)/ 93.176% (37807/40576)
01/14/2023 14:19:32 - INFO - __main__ -   test: [batch: 317/391 ] | Loss: 0.938 | Acc: 76.683% (31213/40704)/ 93.151% (37916/40704)
01/14/2023 14:19:33 - INFO - __main__ -   test: [batch: 318/391 ] | Loss: 0.937 | Acc: 76.712% (31323/40832)/ 93.167% (38042/40832)
01/14/2023 14:19:34 - INFO - __main__ -   test: [batch: 319/391 ] | Loss: 0.940 | Acc: 76.658% (31399/40960)/ 93.140% (38150/40960)
01/14/2023 14:19:35 - INFO - __main__ -   test: [batch: 320/391 ] | Loss: 0.939 | Acc: 76.691% (31511/41088)/ 93.149% (38273/41088)
01/14/2023 14:19:36 - INFO - __main__ -   test: [batch: 321/391 ] | Loss: 0.938 | Acc: 76.708% (31616/41216)/ 93.143% (38390/41216)
01/14/2023 14:19:37 - INFO - __main__ -   test: [batch: 322/391 ] | Loss: 0.940 | Acc: 76.679% (31702/41344)/ 93.138% (38507/41344)
01/14/2023 14:19:38 - INFO - __main__ -   test: [batch: 323/391 ] | Loss: 0.942 | Acc: 76.635% (31782/41472)/ 93.116% (38617/41472)
01/14/2023 14:19:39 - INFO - __main__ -   test: [batch: 324/391 ] | Loss: 0.942 | Acc: 76.635% (31880/41600)/ 93.108% (38733/41600)
01/14/2023 14:19:40 - INFO - __main__ -   test: [batch: 325/391 ] | Loss: 0.942 | Acc: 76.639% (31980/41728)/ 93.108% (38852/41728)
01/14/2023 14:19:41 - INFO - __main__ -   test: [batch: 326/391 ] | Loss: 0.945 | Acc: 76.555% (32043/41856)/ 93.079% (38959/41856)
01/14/2023 14:19:42 - INFO - __main__ -   test: [batch: 327/391 ] | Loss: 0.948 | Acc: 76.479% (32109/41984)/ 93.047% (39065/41984)
01/14/2023 14:19:43 - INFO - __main__ -   test: [batch: 328/391 ] | Loss: 0.950 | Acc: 76.427% (32185/42112)/ 93.023% (39174/42112)
01/14/2023 14:19:44 - INFO - __main__ -   test: [batch: 329/391 ] | Loss: 0.950 | Acc: 76.425% (32282/42240)/ 93.028% (39295/42240)
01/14/2023 14:19:45 - INFO - __main__ -   test: [batch: 330/391 ] | Loss: 0.952 | Acc: 76.385% (32363/42368)/ 93.002% (39403/42368)
01/14/2023 14:19:46 - INFO - __main__ -   test: [batch: 331/391 ] | Loss: 0.952 | Acc: 76.353% (32447/42496)/ 93.013% (39527/42496)
01/14/2023 14:19:47 - INFO - __main__ -   test: [batch: 332/391 ] | Loss: 0.953 | Acc: 76.342% (32540/42624)/ 93.013% (39646/42624)
01/14/2023 14:19:48 - INFO - __main__ -   test: [batch: 333/391 ] | Loss: 0.951 | Acc: 76.371% (32650/42752)/ 93.023% (39769/42752)
01/14/2023 14:19:49 - INFO - __main__ -   test: [batch: 334/391 ] | Loss: 0.953 | Acc: 76.341% (32735/42880)/ 93.004% (39880/42880)
01/14/2023 14:19:50 - INFO - __main__ -   test: [batch: 335/391 ] | Loss: 0.954 | Acc: 76.323% (32825/43008)/ 92.992% (39994/43008)
01/14/2023 14:19:51 - INFO - __main__ -   test: [batch: 336/391 ] | Loss: 0.955 | Acc: 76.270% (32900/43136)/ 92.978% (40107/43136)
01/14/2023 14:19:52 - INFO - __main__ -   test: [batch: 337/391 ] | Loss: 0.955 | Acc: 76.264% (32995/43264)/ 92.973% (40224/43264)
01/14/2023 14:19:53 - INFO - __main__ -   test: [batch: 338/391 ] | Loss: 0.955 | Acc: 76.247% (33085/43392)/ 92.983% (40347/43392)
01/14/2023 14:19:54 - INFO - __main__ -   test: [batch: 339/391 ] | Loss: 0.958 | Acc: 76.213% (33168/43520)/ 92.957% (40455/43520)
01/14/2023 14:19:55 - INFO - __main__ -   test: [batch: 340/391 ] | Loss: 0.958 | Acc: 76.207% (33263/43648)/ 92.969% (40579/43648)
01/14/2023 14:19:56 - INFO - __main__ -   test: [batch: 341/391 ] | Loss: 0.956 | Acc: 76.245% (33377/43776)/ 92.987% (40706/43776)
01/14/2023 14:19:57 - INFO - __main__ -   test: [batch: 342/391 ] | Loss: 0.957 | Acc: 76.198% (33454/43904)/ 92.980% (40822/43904)
01/14/2023 14:19:58 - INFO - __main__ -   test: [batch: 343/391 ] | Loss: 0.956 | Acc: 76.195% (33550/44032)/ 92.982% (40942/44032)
01/14/2023 14:19:59 - INFO - __main__ -   test: [batch: 344/391 ] | Loss: 0.957 | Acc: 76.191% (33646/44160)/ 92.973% (41057/44160)
01/14/2023 14:20:00 - INFO - __main__ -   test: [batch: 345/391 ] | Loss: 0.961 | Acc: 76.124% (33714/44288)/ 92.933% (41158/44288)
01/14/2023 14:20:01 - INFO - __main__ -   test: [batch: 346/391 ] | Loss: 0.962 | Acc: 76.105% (33803/44416)/ 92.919% (41271/44416)
01/14/2023 14:20:02 - INFO - __main__ -   test: [batch: 347/391 ] | Loss: 0.962 | Acc: 76.125% (33909/44544)/ 92.928% (41394/44544)
01/14/2023 14:20:03 - INFO - __main__ -   test: [batch: 348/391 ] | Loss: 0.963 | Acc: 76.104% (33997/44672)/ 92.902% (41501/44672)
01/14/2023 14:20:04 - INFO - __main__ -   test: [batch: 349/391 ] | Loss: 0.963 | Acc: 76.105% (34095/44800)/ 92.908% (41623/44800)
01/14/2023 14:20:05 - INFO - __main__ -   test: [batch: 350/391 ] | Loss: 0.962 | Acc: 76.113% (34196/44928)/ 92.909% (41742/44928)
01/14/2023 14:20:06 - INFO - __main__ -   test: [batch: 351/391 ] | Loss: 0.965 | Acc: 76.061% (34270/45056)/ 92.896% (41855/45056)
01/14/2023 14:20:07 - INFO - __main__ -   test: [batch: 352/391 ] | Loss: 0.965 | Acc: 76.056% (34365/45184)/ 92.891% (41972/45184)
01/14/2023 14:20:08 - INFO - __main__ -   test: [batch: 353/391 ] | Loss: 0.967 | Acc: 76.017% (34445/45312)/ 92.852% (42073/45312)
01/14/2023 14:20:09 - INFO - __main__ -   test: [batch: 354/391 ] | Loss: 0.970 | Acc: 75.960% (34516/45440)/ 92.830% (42182/45440)
01/14/2023 14:20:10 - INFO - __main__ -   test: [batch: 355/391 ] | Loss: 0.972 | Acc: 75.900% (34586/45568)/ 92.820% (42296/45568)
01/14/2023 14:20:11 - INFO - __main__ -   test: [batch: 356/391 ] | Loss: 0.973 | Acc: 75.899% (34683/45696)/ 92.822% (42416/45696)
01/14/2023 14:20:12 - INFO - __main__ -   test: [batch: 357/391 ] | Loss: 0.971 | Acc: 75.934% (34796/45824)/ 92.836% (42541/45824)
01/14/2023 14:20:13 - INFO - __main__ -   test: [batch: 358/391 ] | Loss: 0.970 | Acc: 75.962% (34906/45952)/ 92.838% (42661/45952)
01/14/2023 14:20:14 - INFO - __main__ -   test: [batch: 359/391 ] | Loss: 0.971 | Acc: 75.970% (35007/46080)/ 92.832% (42777/46080)
01/14/2023 14:20:15 - INFO - __main__ -   test: [batch: 360/391 ] | Loss: 0.973 | Acc: 75.939% (35090/46208)/ 92.822% (42891/46208)
01/14/2023 14:20:16 - INFO - __main__ -   test: [batch: 361/391 ] | Loss: 0.973 | Acc: 75.941% (35188/46336)/ 92.828% (43013/46336)
01/14/2023 14:20:17 - INFO - __main__ -   test: [batch: 362/391 ] | Loss: 0.972 | Acc: 75.936% (35283/46464)/ 92.842% (43138/46464)
01/14/2023 14:20:18 - INFO - __main__ -   test: [batch: 363/391 ] | Loss: 0.973 | Acc: 75.927% (35376/46592)/ 92.834% (43253/46592)
01/14/2023 14:20:19 - INFO - __main__ -   test: [batch: 364/391 ] | Loss: 0.971 | Acc: 75.952% (35485/46720)/ 92.840% (43375/46720)
01/14/2023 14:20:20 - INFO - __main__ -   test: [batch: 365/391 ] | Loss: 0.971 | Acc: 75.965% (35588/46848)/ 92.845% (43496/46848)
01/14/2023 14:20:21 - INFO - __main__ -   test: [batch: 366/391 ] | Loss: 0.969 | Acc: 75.998% (35701/46976)/ 92.862% (43623/46976)
01/14/2023 14:20:22 - INFO - __main__ -   test: [batch: 367/391 ] | Loss: 0.968 | Acc: 76.021% (35809/47104)/ 92.873% (43747/47104)
01/14/2023 14:20:23 - INFO - __main__ -   test: [batch: 368/391 ] | Loss: 0.968 | Acc: 76.025% (35908/47232)/ 92.882% (43870/47232)
01/14/2023 14:20:24 - INFO - __main__ -   test: [batch: 369/391 ] | Loss: 0.967 | Acc: 76.047% (36016/47360)/ 92.891% (43993/47360)
01/14/2023 14:20:25 - INFO - __main__ -   test: [batch: 370/391 ] | Loss: 0.967 | Acc: 76.032% (36106/47488)/ 92.893% (44113/47488)
01/14/2023 14:20:26 - INFO - __main__ -   test: [batch: 371/391 ] | Loss: 0.967 | Acc: 76.037% (36206/47616)/ 92.895% (44233/47616)
01/14/2023 14:20:27 - INFO - __main__ -   test: [batch: 372/391 ] | Loss: 0.965 | Acc: 76.085% (36326/47744)/ 92.910% (44359/47744)
01/14/2023 14:20:27 - INFO - __main__ -   test: [batch: 373/391 ] | Loss: 0.964 | Acc: 76.122% (36441/47872)/ 92.919% (44482/47872)
01/14/2023 14:20:28 - INFO - __main__ -   test: [batch: 374/391 ] | Loss: 0.963 | Acc: 76.152% (36553/48000)/ 92.927% (44605/48000)
01/14/2023 14:20:30 - INFO - __main__ -   test: [batch: 375/391 ] | Loss: 0.966 | Acc: 76.087% (36619/48128)/ 92.894% (44708/48128)
01/14/2023 14:20:30 - INFO - __main__ -   test: [batch: 376/391 ] | Loss: 0.966 | Acc: 76.078% (36712/48256)/ 92.882% (44821/48256)
01/14/2023 14:20:31 - INFO - __main__ -   test: [batch: 377/391 ] | Loss: 0.967 | Acc: 76.064% (36803/48384)/ 92.874% (44936/48384)
01/14/2023 14:20:32 - INFO - __main__ -   test: [batch: 378/391 ] | Loss: 0.970 | Acc: 75.992% (36865/48512)/ 92.831% (45034/48512)
01/14/2023 14:20:33 - INFO - __main__ -   test: [batch: 379/391 ] | Loss: 0.970 | Acc: 75.970% (36952/48640)/ 92.837% (45156/48640)
01/14/2023 14:20:34 - INFO - __main__ -   test: [batch: 380/391 ] | Loss: 0.970 | Acc: 75.964% (37046/48768)/ 92.846% (45279/48768)
01/14/2023 14:20:35 - INFO - __main__ -   test: [batch: 381/391 ] | Loss: 0.972 | Acc: 75.916% (37120/48896)/ 92.844% (45397/48896)
01/14/2023 14:20:36 - INFO - __main__ -   test: [batch: 382/391 ] | Loss: 0.973 | Acc: 75.881% (37200/49024)/ 92.832% (45510/49024)
01/14/2023 14:20:37 - INFO - __main__ -   test: [batch: 383/391 ] | Loss: 0.973 | Acc: 75.889% (37301/49152)/ 92.826% (45626/49152)
01/14/2023 14:20:38 - INFO - __main__ -   test: [batch: 384/391 ] | Loss: 0.972 | Acc: 75.927% (37417/49280)/ 92.839% (45751/49280)
01/14/2023 14:20:39 - INFO - __main__ -   test: [batch: 385/391 ] | Loss: 0.970 | Acc: 75.939% (37520/49408)/ 92.849% (45875/49408)
01/14/2023 14:20:40 - INFO - __main__ -   test: [batch: 386/391 ] | Loss: 0.968 | Acc: 75.981% (37638/49536)/ 92.866% (46002/49536)
01/14/2023 14:20:42 - INFO - __main__ -   test: [batch: 387/391 ] | Loss: 0.967 | Acc: 76.021% (37755/49664)/ 92.876% (46126/49664)
01/14/2023 14:20:43 - INFO - __main__ -   test: [batch: 388/391 ] | Loss: 0.965 | Acc: 76.066% (37875/49792)/ 92.888% (46251/49792)
01/14/2023 14:20:43 - INFO - __main__ -   test: [batch: 389/391 ] | Loss: 0.965 | Acc: 76.068% (37973/49920)/ 92.889% (46370/49920)
01/14/2023 14:20:44 - INFO - __main__ -   test: [batch: 390/391 ] | Loss: 0.967 | Acc: 76.016% (38008/50000)/ 92.880% (46440/50000)
01/14/2023 14:20:44 - INFO - __main__ -   Final accuracy: 76.016
